# Comparing `tmp/tl2-0.1.0.tar.gz` & `tmp/tl2-0.1.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "tl2-0.1.0.tar", last modified: Thu Oct 20 08:23:00 2022, max compression
+gzip compressed data, was "tl2-0.1.1.tar", last modified: Sat Jun 24 08:55:31 2023, max compression
```

## Comparing `tl2-0.1.0.tar` & `tl2-0.1.1.tar`

### file list

```diff
@@ -1,321 +1,326 @@
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1069 2022-10-03 09:27:55.000000 tl2-0.1.0/LICENSE
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      157 2022-10-03 09:27:55.000000 tl2-0.1.0/MANIFEST.in
--rw-rw-r--   0 ma-user   (1000) ma-user   (1000)      942 2022-10-20 08:23:00.081780 tl2-0.1.0/PKG-INFO
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.061780 tl2-0.1.0/data/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/data/.gitignore
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.061780 tl2-0.1.0/data/images_r512/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   328091 2022-10-03 09:27:55.000000 tl2-0.1.0/data/images_r512/194.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   256661 2022-10-03 09:27:55.000000 tl2-0.1.0/data/images_r512/195.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   375184 2022-10-03 09:27:55.000000 tl2-0.1.0/data/images_r512/196.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   200064 2022-10-03 09:27:55.000000 tl2-0.1.0/data/images_r512/197.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   198093 2022-10-03 09:27:55.000000 tl2-0.1.0/data/images_r512/198.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   275985 2022-10-03 09:27:55.000000 tl2-0.1.0/data/images_r512/199.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   291818 2022-10-03 09:27:55.000000 tl2-0.1.0/data/images_r512/200.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   340355 2022-10-03 09:27:55.000000 tl2-0.1.0/data/images_r512/201.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   247347 2022-10-03 09:27:55.000000 tl2-0.1.0/data/images_r512/202.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   266756 2022-10-03 09:27:55.000000 tl2-0.1.0/data/images_r512/203.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)       99 2022-10-03 09:27:55.000000 tl2-0.1.0/data/images_r512/image_list.txt
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   305724 2022-10-03 09:27:55.000000 tl2-0.1.0/data/sans-serif.ttf
--rw-rw-r--   0 ma-user   (1000) ma-user   (1000)       38 2022-10-20 08:23:00.081780 tl2-0.1.0/setup.cfg
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1177 2022-10-20 08:22:40.000000 tl2-0.1.0/setup.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.061780 tl2-0.1.0/tl2/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)       41 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/__init__.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      176 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/__pycache__/__init__.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    18106 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/__pycache__/tl2_utils.cpython-38.pyc
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/imagenet/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      975 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/imagenet/imagenet_utils.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    24366 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/imagenet/map_subdir_id_name.txt
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/launch/
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/launch/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8204 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/launch/__pycache__/launch_utils.cpython-38.pyc
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/launch/configs/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)       38 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/launch/configs/Launch_v1.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    10635 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/launch/launch_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/launch/scripts/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      792 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/launch/scripts/run_ddp.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/launch/tests/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3909 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/launch/tests/test_launch.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/modelarts/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/__init__.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/modelarts/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      174 2022-10-04 09:40:38.000000 tl2-0.1.0/tl2/modelarts/__pycache__/__init__.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    11132 2022-10-04 09:40:38.000000 tl2-0.1.0/tl2/modelarts/__pycache__/modelarts_utils.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5963 2022-10-04 09:40:38.000000 tl2-0.1.0/tl2/modelarts/__pycache__/moxing_utils.cpython-38.pyc
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/modelarts/configs/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      554 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/configs/run.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    12987 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/modelarts/modelarts_utils.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7607 2022-10-20 08:17:49.000000 tl2-0.1.0/tl2/modelarts/moxing_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/modelarts/scripts/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1271 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/scripts/copy_file_list.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1691 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/scripts/copy_tool.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      707 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/scripts/make_log_dirs.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5809 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/scripts/plot_results_obs.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7661 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/modelarts/scripts/run.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2186 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/scripts/run.sh
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1931 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/modelarts/scripts/run_v2_modelarts.sh
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17152 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/modelarts/scripts/s3_downloader.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17390 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/modelarts/scripts/s3_uploader.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      374 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/modelarts/scripts/setup_env.sh
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1525 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/scripts/start_modelarts_v2.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     9121 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/modelarts/scripts/test_bash.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      256 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/scripts/test_bash.sh
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3373 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/scripts/test_resnet.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/modelarts/sources/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      143 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/sources/pip.conf.modelarts
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      952 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/sources/sources.list.modelarts
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/modelarts/tests/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8222 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/modelarts/tests/test_run.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/GAN/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1813 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/GAN/eval_FID_given_imgdir.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1945 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/GAN/frequency_spectrum.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1684 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/GAN/plot_freq_spectrum.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/__init__.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      169 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/proj/__pycache__/__init__.cpython-38.pyc
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/argparser/
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/argparser/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2505 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/proj/argparser/__pycache__/argparser_utils.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1997 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/argparser/argparser_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/cv2/
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/cv2/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    12752 2022-10-05 08:56:25.000000 tl2-0.1.0/tl2/proj/cv2/__pycache__/cv2_utils.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    12865 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/cv2/cv2_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/dlib/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/__init__.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/dlib/configs/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      903 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/dlib/configs/dlib_web.yaml
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/dlib/datasets/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    31213 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/datasets/dlib_landmarks_68.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8005 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/datasets/hand_convex_hull.jpg
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   115786 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/21242213255_abde1622df_o.jpg
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    32009 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/heben1.jfif
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   192765 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/korean1.jpeg
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   240329 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/lecun1.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    26148 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/liudehua1.jfif
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    38862 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/nanmingxing3.jfif
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   147369 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/peng.jpg
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     9984 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/xiangnong1.jpeg
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   130079 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/xinhengjieyi2.jpg
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      501 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/datasets/raw_face_list.txt
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      293 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/dlib_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/dlib/ffhq_face_align/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/ffhq_face_align/__init__.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7898 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/dlib/ffhq_face_align/align_face_stmodel.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2653 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/dlib/ffhq_face_align/align_images.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4798 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/ffhq_face_align/face_alignment.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      760 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/ffhq_face_align/landmarks_detector.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/dlib/tests/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/dlib/tests/__init__.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5772 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/dlib/tests/test_dlib.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.061780 tl2-0.1.0/tl2/proj/einops/
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/einops/resources/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/einops/resources/.gitignore
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)  1327232 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/einops/resources/test_images.npy
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/einops/tests/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    24379 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/einops/tests/test_einops.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/fvcore/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      122 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/fvcore/__init__.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/fvcore/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      354 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/proj/fvcore/__pycache__/__init__.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5455 2022-10-04 09:40:38.000000 tl2-0.1.0/tl2/proj/fvcore/__pycache__/checkpoint.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5238 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/proj/fvcore/__pycache__/config.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2978 2022-10-04 09:40:38.000000 tl2-0.1.0/tl2/proj/fvcore/__pycache__/logger.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3981 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/proj/fvcore/__pycache__/registry.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5658 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/fvcore/checkpoint.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4729 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/fvcore/config.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/fvcore/configs/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      146 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/fvcore/configs/Registry.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      130 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/fvcore/configs/TLCfgNode.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1867 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/fvcore/dummy_model.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3267 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/fvcore/logger.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4686 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/fvcore/registry.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/fvcore/tests/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    12160 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/fvcore/tests/test_checkpoint.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      701 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/fvcore/timer.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/logger/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)       58 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/logger/__init__.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/logger/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      253 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/proj/logger/__pycache__/__init__.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7829 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/proj/logger/__pycache__/logger_utils.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7167 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/proj/logger/__pycache__/plot_utils.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5648 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/proj/logger/__pycache__/textlogger.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8239 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/logger/logger_utils.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2802 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/logger/logging_utils_v2.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     6454 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/logger/plot_utils.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5900 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/logger/textlogger.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/matplot/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/matplot/__init__.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/matplot/configs/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1911 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/matplot/configs/Plot.yaml
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/matplot/data/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    87511 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/matplot/data/OmniGAN_ImageNet128_results.pkl
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    62218 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    23858 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/matplot/plot_results.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5213 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/matplot/plt_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/matplot/scripts/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/matplot/scripts/__init__.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      591 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/matplot/scripts/parse_results_dict_pkl.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/numpy/
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/numpy/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1541 2022-10-05 08:58:24.000000 tl2-0.1.0/tl2/proj/numpy/__pycache__/np_utils.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1051 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/numpy/np_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/pil/
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/pil/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    16440 2022-10-05 08:56:25.000000 tl2-0.1.0/tl2/proj/pil/__pycache__/pil_utils.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    24097 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pil/pil_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/pytorch/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/pytorch/__init__.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/pytorch/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      177 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/proj/pytorch/__pycache__/__init__.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17278 2022-10-04 09:40:38.000000 tl2-0.1.0/tl2/proj/pytorch/__pycache__/pytorch_hook.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    16198 2022-10-04 09:40:38.000000 tl2-0.1.0/tl2/proj/pytorch/__pycache__/torch_utils.cpython-38.pyc
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/pytorch/datasets/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/pytorch/datasets/__init__.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3548 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/pytorch/datasets/dataset_celeba_align.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3750 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/pytorch/datasets/dataset_danbooru2019_portraits.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3779 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/pytorch/datasets/dataset_image_list.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2/proj/pytorch/datasets/tests/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     9304 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/pytorch/datasets/tests/test_datasets.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/pytorch/ddp/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/pytorch/ddp/__init__.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/pytorch/ddp/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      181 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/proj/pytorch/ddp/__pycache__/__init__.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7262 2022-10-03 10:17:29.000000 tl2-0.1.0/tl2/proj/pytorch/ddp/__pycache__/d2_comm.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5010 2022-10-05 08:56:25.000000 tl2-0.1.0/tl2/proj/pytorch/ddp/__pycache__/ddp_utils.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7730 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/pytorch/ddp/d2_comm.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4598 2022-10-05 06:14:53.000000 tl2-0.1.0/tl2/proj/pytorch/ddp/ddp_utils.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7510 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/downsampler.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.061780 tl2-0.1.0/tl2/proj/pytorch/examples/
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/pytorch/examples/cips3d/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    46642 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/cips3d/pigan_gen_celeba.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1359 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/cips3d/pigan_gen_celeba.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5177 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/cips3d/test_pigan_gen_celeba.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    30413 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/cips3d/train_v6.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/pytorch/examples/dataset_stylegan3/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    12553 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/dataset_stylegan3/dataset.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      670 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/dataset_stylegan3/dataset.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7651 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/dataset_stylegan3/test_dataset.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2375 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/dataset_stylegan3/test_ddp.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2915 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/comm_utils.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2054 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/config.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4502 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/datasets.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4479 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/diff_aug.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3200 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/networks.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4124 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/test_gan_ddp.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    24591 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/train.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/pytorch/examples/multi_process_main/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1184 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/pytorch/examples/multi_process_main/config.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4926 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/multi_process_main/dataset.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5403 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/pytorch/examples/multi_process_main/main.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4152 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/multi_process_main/test_multi_process_main.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/pytorch/examples/nerf/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    20642 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/nerf/cam_params.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    20838 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/nerf/cam_params_pigan.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    20634 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/nerf/cam_params_v1.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3219 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/nerf/geometry.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     6497 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/nerf/geometry_tensor.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    19024 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/nerf/test_cam_params.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    11622 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/nerf/test_volume_rendering.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4427 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/nerf/vis_utils.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17022 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/nerf/volume_rendering.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/pytorch/examples/networks/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    12482 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/cips_net.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      459 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/cips_net.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1637 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/grad_norm_layer.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1144 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/grad_scale_layer.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8036 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/multi_head_mapping.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      130 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/multi_head_mapping.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    13091 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/nerf_net.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1020 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/nerf_net.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17082 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/pigan_net.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1309 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/pigan_net.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    18718 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/siren_net.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1082 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/siren_net.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    18715 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/siren_net_pigan.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1404 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    15269 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/siren_net_v1.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1068 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/siren_net_v1.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8593 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_cips_net.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2742 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_multi_head_mapping.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    12874 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_nerf_net.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    21605 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_pigan_net.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    16598 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_siren_net.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    33525 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_siren_net_pigan.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    16421 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_siren_net_v1.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      375 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/pytorch/init_func.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/pytorch/ops/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/ops/__init__.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2189 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/ops/grid_sample.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4034 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/ops/grid_sample_gradfix.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    30770 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/pytorch/optim.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    16837 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/pytorch_hook.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/pytorch/scripts/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5921 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/pytorch/scripts/resize_antialias.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17775 2022-10-08 09:08:31.000000 tl2-0.1.0/tl2/proj/pytorch/torch_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/skimage/
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/skimage/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3429 2022-10-05 08:58:24.000000 tl2-0.1.0/tl2/proj/skimage/__pycache__/skimage_utils.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3200 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/skimage/skimage_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/streamlit/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4024 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/streamlit/SessionState.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/streamlit/__pycache__/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      183 2022-10-05 08:56:25.000000 tl2-0.1.0/tl2/proj/streamlit/__pycache__/SessionState.cpython-38.pyc
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8064 2022-10-05 08:56:25.000000 tl2-0.1.0/tl2/proj/streamlit/__pycache__/st_utils.cpython-38.pyc
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/streamlit/configs/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1394 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/streamlit/configs/Streamlit.yaml
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/streamlit/scripts/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8255 2022-10-08 09:08:31.000000 tl2-0.1.0/tl2/proj/streamlit/scripts/run_web.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     9245 2022-10-08 09:08:31.000000 tl2-0.1.0/tl2/proj/streamlit/st_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/streamlit/tests/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     6141 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/streamlit/tests/test_streamlit.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/stylegan2_ada/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/stylegan2_ada/__init__.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      429 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/stylegan2_ada/ada_utils.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    10258 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/stylegan2_ada/persistence.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/stylegan2_ada/test/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5049 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/proj/stylegan2_ada/test/test_ada.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/tools3d/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    11051 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/tools3d/camera_pose_visualizer.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4161 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/tools3d/compute_normal_from_depth.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)   196736 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/tools3d/depth.npy
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    96704 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/tools3d/img_0.png
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      192 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/tools3d/intrinsic.npy
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/trimesh/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1903 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/trimesh/__init__.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      682 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/trimesh/aitvconfig.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      142 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/trimesh/aitviewer_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.061780 tl2-0.1.0/tl2/proj/trimesh/examples/
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/trimesh/examples/aitviewer/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      570 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/trimesh/examples/aitviewer/quick_start.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/trimesh/examples/open3d/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4580 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/trimesh/examples/open3d/open3d_tensorboard.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      356 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/trimesh/examples/open3d/open3d_tensorboard.yaml
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2474 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/trimesh/examples/open3d/test_open3d_tensorboard.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/proj/trimesh/examples/pyrender/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     6931 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/trimesh/examples/pyrender/example.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      297 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/trimesh/examples/pyrender/minimal_example.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    10098 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/trimesh/open3d_utils.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    11264 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/trimesh/pyrender_utils.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      862 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/proj/trimesh/trimesh_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/tests/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2393 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/tests/test_tl_utils.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    15845 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/tl2_utils.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.081780 tl2-0.1.0/tl2/tools/
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/tools/__init__.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      361 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/tools/cpu.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)    20572 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/tools/dataset_tool.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)      651 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/tools/diff_dates.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1072 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/tools/get_data_list.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2420 2022-10-03 09:35:07.000000 tl2-0.1.0/tl2/tools/label_smoothing.py
--rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2158 2022-10-03 09:27:55.000000 tl2-0.1.0/tl2/tools/test_tools.py
-drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2022-10-20 08:23:00.071780 tl2-0.1.0/tl2.egg-info/
--rw-rw-r--   0 ma-user   (1000) ma-user   (1000)      942 2022-10-20 08:23:00.000000 tl2-0.1.0/tl2.egg-info/PKG-INFO
--rw-rw-r--   0 ma-user   (1000) ma-user   (1000)     9708 2022-10-20 08:23:00.000000 tl2-0.1.0/tl2.egg-info/SOURCES.txt
--rw-rw-r--   0 ma-user   (1000) ma-user   (1000)        1 2022-10-20 08:23:00.000000 tl2-0.1.0/tl2.egg-info/dependency_links.txt
--rw-rw-r--   0 ma-user   (1000) ma-user   (1000)      118 2022-10-20 08:23:00.000000 tl2-0.1.0/tl2.egg-info/requires.txt
--rw-rw-r--   0 ma-user   (1000) ma-user   (1000)        4 2022-10-20 08:23:00.000000 tl2-0.1.0/tl2.egg-info/top_level.txt
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.498544 tl2-0.1.1/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1089 2022-05-29 09:29:21.000000 tl2-0.1.1/LICENSE
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      164 2022-05-29 09:29:21.000000 tl2-0.1.1/MANIFEST.in
+-rw-rw-r--   0 ma-user   (1000) ma-user   (1000)      905 2023-06-24 08:55:31.498544 tl2-0.1.1/PKG-INFO
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.468544 tl2-0.1.1/data/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/data/.gitignore
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/data/images_r512/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   328091 2022-05-29 09:29:21.000000 tl2-0.1.1/data/images_r512/194.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   256661 2022-05-29 09:29:21.000000 tl2-0.1.1/data/images_r512/195.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   375184 2022-05-29 09:29:21.000000 tl2-0.1.1/data/images_r512/196.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   200064 2022-05-29 09:29:21.000000 tl2-0.1.1/data/images_r512/197.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   198093 2022-05-29 09:29:21.000000 tl2-0.1.1/data/images_r512/198.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   275985 2022-05-29 09:29:21.000000 tl2-0.1.1/data/images_r512/199.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   291818 2022-05-29 09:29:21.000000 tl2-0.1.1/data/images_r512/200.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   340355 2022-05-29 09:29:21.000000 tl2-0.1.1/data/images_r512/201.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   247347 2022-05-29 09:29:21.000000 tl2-0.1.1/data/images_r512/202.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   266756 2022-05-29 09:29:21.000000 tl2-0.1.1/data/images_r512/203.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      102 2022-05-29 09:29:21.000000 tl2-0.1.1/data/images_r512/image_list.txt
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   305724 2022-05-29 09:29:21.000000 tl2-0.1.1/data/sans-serif.ttf
+-rw-rw-r--   0 ma-user   (1000) ma-user   (1000)       38 2023-06-24 08:55:31.498544 tl2-0.1.1/setup.cfg
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1177 2023-06-24 08:54:44.000000 tl2-0.1.1/setup.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)       47 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/__init__.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/__pycache__/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      146 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/__pycache__/__init__.cpython-38.pyc
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    18076 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/__pycache__/tl2_utils.cpython-38.pyc
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/imagenet/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1011 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/imagenet/imagenet_utils.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    25366 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/imagenet/map_subdir_id_name.txt
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/launch/
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/launch/__pycache__/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8174 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/launch/__pycache__/launch_utils.cpython-38.pyc
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/launch/configs/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)       45 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/launch/configs/Launch_v1.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    10948 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/launch/launch_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/launch/scripts/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      828 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/launch/scripts/run_ddp.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/launch/tests/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4149 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/launch/tests/test_launch.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/modelarts/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/__init__.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/modelarts/configs/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      577 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/configs/run.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    12987 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/modelarts/modelarts_utils.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7607 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/modelarts/moxing_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/modelarts/scripts/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1317 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/scripts/copy_file_list.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1749 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/scripts/copy_tool.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      735 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/scripts/make_log_dirs.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5981 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/scripts/plot_results_obs.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7906 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/scripts/run.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2258 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/scripts/run.sh
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2014 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/scripts/run_v2_modelarts.sh
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17590 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/scripts/s3_downloader.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17866 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/scripts/s3_uploader.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1201 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/modelarts/scripts/setup_env.sh
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      907 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/modelarts/scripts/setup_env_debug.sh
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1581 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/scripts/start_modelarts_v2.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     9121 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/modelarts/scripts/test_bash.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      271 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/scripts/test_bash.sh
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3509 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/scripts/test_resnet.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/modelarts/sources/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      152 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/sources/pip.conf.modelarts
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      972 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/sources/sources.list.modelarts
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/modelarts/tests/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8423 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/modelarts/tests/test_run.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/GAN/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1868 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/GAN/eval_FID_given_imgdir.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2011 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/GAN/frequency_spectrum.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1741 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/GAN/plot_freq_spectrum.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/__init__.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/__pycache__/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      139 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/proj/__pycache__/__init__.cpython-38.pyc
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/argparser/
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/argparser/__pycache__/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2475 2022-12-21 09:29:24.000000 tl2-0.1.1/tl2/proj/argparser/__pycache__/argparser_utils.cpython-38.pyc
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2070 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/argparser/argparser_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/cv2/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    12865 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/cv2/cv2_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/dlib/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/__init__.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/dlib/configs/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      938 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/configs/dlib_web.yaml
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/dlib/datasets/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    31213 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/datasets/dlib_landmarks_68.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8005 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/datasets/hand_convex_hull.jpg
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   115786 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/21242213255_abde1622df_o.jpg
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    32009 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/heben1.jfif
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   192765 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/korean1.jpeg
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   240329 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/lecun1.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    26148 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/liudehua1.jfif
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    38862 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/nanmingxing3.jfif
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   147369 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/peng.jpg
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     9984 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/xiangnong1.jpeg
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   130079 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/xinhengjieyi2.jpg
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      510 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/datasets/raw_face_list.txt
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      313 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/dlib_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/dlib/ffhq_face_align/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/ffhq_face_align/__init__.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8117 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/ffhq_face_align/align_face_stmodel.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2735 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/ffhq_face_align/align_images.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4903 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/ffhq_face_align/face_alignment.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      781 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/ffhq_face_align/landmarks_detector.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/dlib/tests/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/tests/__init__.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5954 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/dlib/tests/test_dlib.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.468544 tl2-0.1.1/tl2/proj/einops/
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/einops/resources/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/einops/resources/.gitignore
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)  1327232 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/einops/resources/test_images.npy
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/einops/tests/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    25004 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/einops/tests/test_einops.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/fvcore/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      124 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/fvcore/__init__.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/fvcore/__pycache__/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      324 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/proj/fvcore/__pycache__/__init__.cpython-38.pyc
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5208 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/proj/fvcore/__pycache__/config.cpython-38.pyc
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3951 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/proj/fvcore/__pycache__/registry.cpython-38.pyc
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5822 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/fvcore/checkpoint.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4910 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/fvcore/config.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/fvcore/configs/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      152 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/fvcore/configs/Registry.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      150 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/fvcore/configs/TLCfgNode.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1928 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/fvcore/dummy_model.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3371 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/fvcore/logger.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4846 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/fvcore/registry.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/fvcore/tests/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    12483 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/fvcore/tests/test_checkpoint.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      739 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/fvcore/timer.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/gradio/
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/gradio/configs/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-12-21 09:26:48.000000 tl2-0.1.1/tl2/proj/gradio/configs/Quickstart.yaml
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/gradio/examples/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      767 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/gradio/examples/blocks.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)       21 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/gradio/gradio_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/gradio/tests/
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/gradio/tests/__pycache__/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2697 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/proj/gradio/tests/__pycache__/test_gradio.cpython-38.pyc
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    25133 2022-12-21 12:07:36.000000 tl2-0.1.1/tl2/proj/gradio/tests/test_gradio.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2/proj/logger/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)       59 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/logger/__init__.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/logger/__pycache__/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      223 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/proj/logger/__pycache__/__init__.cpython-38.pyc
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7799 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/proj/logger/__pycache__/logger_utils.cpython-38.pyc
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7137 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/proj/logger/__pycache__/plot_utils.cpython-38.pyc
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5618 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/proj/logger/__pycache__/textlogger.cpython-38.pyc
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8521 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/logger/logger_utils.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2889 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/logger/logging_utils_v2.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     6650 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/logger/plot_utils.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     6083 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/logger/textlogger.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/matplot/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/matplot/__init__.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/matplot/configs/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1997 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/matplot/configs/Plot.yaml
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/matplot/data/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    87511 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/matplot/data/OmniGAN_ImageNet128_results.pkl
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    62218 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    24486 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/matplot/plot_results.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5512 2023-02-28 04:34:28.000000 tl2-0.1.1/tl2/proj/matplot/plt_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/matplot/scripts/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/matplot/scripts/__init__.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      619 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/matplot/scripts/parse_results_dict_pkl.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/numpy/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1105 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/numpy/np_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pil/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    24302 2023-02-28 04:34:28.000000 tl2-0.1.1/tl2/proj/pil/pil_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.468544 tl2-0.1.1/tl2/proj/pl/
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pl/configs/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)       39 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/pl/configs/lightning_2_steps.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)       34 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/pl/configs/step_by_step.yaml
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.468544 tl2-0.1.1/tl2/proj/pl/examples/
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pl/examples/lightning_2_steps/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1882 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/pl/examples/lightning_2_steps/module.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pl/examples/step_by_step/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3059 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/pl/examples/step_by_step/module.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pl/tests/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8108 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/pl/tests/test_pytorch_lightning.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/__init__.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/__pycache__/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      147 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/proj/pytorch/__pycache__/__init__.cpython-38.pyc
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/datasets/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/datasets/__init__.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3679 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/datasets/dataset_celeba_align.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3885 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/datasets/dataset_danbooru2019_portraits.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3918 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/datasets/dataset_image_list.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/datasets/tests/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     9537 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/datasets/tests/test_datasets.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/ddp/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/ddp/__init__.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/ddp/__pycache__/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      151 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/proj/pytorch/ddp/__pycache__/__init__.cpython-38.pyc
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7232 2022-12-21 09:29:23.000000 tl2-0.1.1/tl2/proj/pytorch/ddp/__pycache__/d2_comm.cpython-38.pyc
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7993 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/ddp/d2_comm.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4598 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/pytorch/ddp/ddp_utils.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7510 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/pytorch/downsampler.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.468544 tl2-0.1.1/tl2/proj/pytorch/examples/
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/examples/cips3d/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    48078 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/cips3d/pigan_gen_celeba.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1433 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/cips3d/pigan_gen_celeba.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5318 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/cips3d/test_pigan_gen_celeba.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    31156 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/cips3d/train_v6.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/examples/dataset_stylegan3/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    12953 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/dataset_stylegan3/dataset.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      700 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/dataset_stylegan3/dataset.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     7846 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/dataset_stylegan3/test_dataset.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2459 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/dataset_stylegan3/test_ddp.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3035 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/comm_utils.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2137 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/config.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4657 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/datasets.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4586 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/diff_aug.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3311 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/networks.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4233 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/test_gan_ddp.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    25283 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/train.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/examples/multi_process_main/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1215 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/multi_process_main/config.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4926 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/pytorch/examples/multi_process_main/dataset.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5583 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/multi_process_main/main.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4152 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/pytorch/examples/multi_process_main/test_multi_process_main.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/examples/nerf/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    21369 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/nerf/cam_params.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    21568 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/nerf/cam_params_pigan.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    21365 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/nerf/cam_params_v1.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3311 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/nerf/geometry.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     6697 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/nerf/geometry_tensor.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    19532 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/nerf/test_cam_params.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    11946 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/nerf/test_volume_rendering.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4581 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/nerf/vis_utils.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17612 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/nerf/volume_rendering.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/examples/networks/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    12935 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/cips_net.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      488 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/cips_net.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1707 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/grad_norm_layer.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1192 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/grad_scale_layer.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8323 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/multi_head_mapping.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      136 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/multi_head_mapping.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    13538 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/nerf_net.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1075 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/nerf_net.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17730 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/pigan_net.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1384 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/pigan_net.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    19439 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/siren_net.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1151 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/siren_net.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    19408 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/siren_net_pigan.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1491 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    15831 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/siren_net_v1.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1132 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/siren_net_v1.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8829 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/test_cips_net.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2821 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/test_multi_head_mapping.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    13196 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/test_nerf_net.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    22161 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/test_pigan_net.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17032 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/test_siren_net.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    34402 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/test_siren_net_pigan.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    16847 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/examples/networks/test_siren_net_v1.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      390 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/init_func.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/ops/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/pytorch/ops/__init__.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2189 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/pytorch/ops/grid_sample.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4034 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/pytorch/ops/grid_sample_gradfix.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    31596 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/optim.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17371 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/pytorch/pytorch_hook.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/pytorch/scripts/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5921 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/pytorch/scripts/resize_antialias.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    17775 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/pytorch/torch_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/skimage/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     3200 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/skimage/skimage_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/streamlit/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4024 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/streamlit/SessionState.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/streamlit/configs/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1394 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/streamlit/configs/Streamlit.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      157 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/streamlit/configs/Streamlit_v2.yaml
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/streamlit/scripts/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1072 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/streamlit/scripts/create_an_app.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8255 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/streamlit/scripts/run_web.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2421 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/streamlit/scripts/st_two_column.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     9433 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/streamlit/st_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/streamlit/tests/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     8262 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/streamlit/tests/test_streamlit.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/stylegan2_ada/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/stylegan2_ada/__init__.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      451 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/stylegan2_ada/ada_utils.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    10528 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/stylegan2_ada/persistence.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/stylegan2_ada/test/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     5201 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/proj/stylegan2_ada/test/test_ada.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/tools3d/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    11051 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/tools3d/camera_pose_visualizer.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4161 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/tools3d/compute_normal_from_depth.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)   196736 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/tools3d/depth.npy
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    96704 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/tools3d/img_0.png
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      192 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/tools3d/intrinsic.npy
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/trimesh/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1903 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/trimesh/__init__.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      682 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/trimesh/aitvconfig.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      142 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/trimesh/aitviewer_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.468544 tl2-0.1.1/tl2/proj/trimesh/examples/
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/trimesh/examples/aitviewer/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      570 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/trimesh/examples/aitviewer/quick_start.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/trimesh/examples/open3d/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     4580 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/trimesh/examples/open3d/open3d_tensorboard.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      356 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/trimesh/examples/open3d/open3d_tensorboard.yaml
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2474 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/trimesh/examples/open3d/test_open3d_tensorboard.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/proj/trimesh/examples/pyrender/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     6931 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/trimesh/examples/pyrender/example.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      297 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/trimesh/examples/pyrender/minimal_example.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    10098 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/trimesh/open3d_utils.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    11548 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/proj/trimesh/pyrender_utils.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      862 2022-12-21 09:22:10.000000 tl2-0.1.1/tl2/proj/trimesh/trimesh_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.488544 tl2-0.1.1/tl2/tests/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2460 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/tests/test_tl_utils.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    16046 2023-04-03 07:20:38.000000 tl2-0.1.1/tl2/tl2_utils.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.498544 tl2-0.1.1/tl2/tools/
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)        0 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/tools/__init__.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      371 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/tools/cpu.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)    21087 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/tools/dataset_tool.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)      678 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/tools/diff_dates.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     1118 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/tools/get_data_list.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2520 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/tools/label_smoothing.py
+-rw-r--r--   0 ma-user   (1000) ma-user   (1000)     2225 2022-05-29 09:29:21.000000 tl2-0.1.1/tl2/tools/test_tools.py
+drwxrwxr-x   0 ma-user   (1000) ma-user   (1000)        0 2023-06-24 08:55:31.478544 tl2-0.1.1/tl2.egg-info/
+-rw-rw-r--   0 ma-user   (1000) ma-user   (1000)      905 2023-06-24 08:55:31.000000 tl2-0.1.1/tl2.egg-info/PKG-INFO
+-rw-rw-r--   0 ma-user   (1000) ma-user   (1000)     9546 2023-06-24 08:55:31.000000 tl2-0.1.1/tl2.egg-info/SOURCES.txt
+-rw-rw-r--   0 ma-user   (1000) ma-user   (1000)        1 2023-06-24 08:55:31.000000 tl2-0.1.1/tl2.egg-info/dependency_links.txt
+-rw-rw-r--   0 ma-user   (1000) ma-user   (1000)      118 2023-06-24 08:55:31.000000 tl2-0.1.1/tl2.egg-info/requires.txt
+-rw-rw-r--   0 ma-user   (1000) ma-user   (1000)        4 2023-06-24 08:55:31.000000 tl2-0.1.1/tl2.egg-info/top_level.txt
```

### Comparing `tl2-0.1.0/LICENSE` & `tl2-0.1.1/LICENSE`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-MIT License
-
-Copyright (c) [2021] [Peng Zhou]
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+MIT License
+
+Copyright (c) [2021] [Peng Zhou]
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 SOFTWARE.
```

### Comparing `tl2-0.1.0/PKG-INFO` & `tl2-0.1.1/PKG-INFO`

 * *Files 5% similar despite different names*

```diff
@@ -1,16 +1,14 @@
 Metadata-Version: 2.1
 Name: tl2
-Version: 0.1.0
+Version: 0.1.1
 Summary: A personal package for research
 Home-page: https://github.com/PeterouZh/tl2
 Author: Peterou
 Author-email: pengzhoucv@gmail.com
-License: UNKNOWN
-Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
 Description-Content-Type: text/markdown
 License-File: LICENSE
 
 ## [tl2](https://github.com/PeterouZh/tl2)
@@ -48,9 +46,7 @@
 
 ### fvcore
 
 
 
 
 
-
-
```

### Comparing `tl2-0.1.0/data/images_r512/194.png` & `tl2-0.1.1/data/images_r512/194.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/data/images_r512/195.png` & `tl2-0.1.1/data/images_r512/195.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/data/images_r512/196.png` & `tl2-0.1.1/data/images_r512/196.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/data/images_r512/197.png` & `tl2-0.1.1/data/images_r512/197.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/data/images_r512/198.png` & `tl2-0.1.1/data/images_r512/198.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/data/images_r512/199.png` & `tl2-0.1.1/data/images_r512/199.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/data/images_r512/200.png` & `tl2-0.1.1/data/images_r512/200.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/data/images_r512/201.png` & `tl2-0.1.1/data/images_r512/201.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/data/images_r512/202.png` & `tl2-0.1.1/data/images_r512/202.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/data/images_r512/203.png` & `tl2-0.1.1/data/images_r512/203.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/data/sans-serif.ttf` & `tl2-0.1.1/data/sans-serif.ttf`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/setup.py` & `tl2-0.1.1/setup.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 # directories = glob.glob('tl2/*')
 # for directory in directories:
 #     files = glob.glob(directory+'*')
 #     data_files.append((directory, files))
 
 setuptools.setup(
     name="tl2",
-    version="0.1.0",
+    version="0.1.1",
     author="Peterou",
     author_email="pengzhoucv@gmail.com",
     description="A personal package for research",
     long_description=long_description,
     long_description_content_type="text/markdown",
     url="https://github.com/PeterouZh/tl2",
     packages=setuptools.find_packages(),
```

### Comparing `tl2-0.1.0/tl2/__pycache__/tl2_utils.cpython-38.pyc` & `tl2-0.1.1/tl2/__pycache__/tl2_utils.cpython-38.pyc`

 * *Format-specific differences are supported for Python .pyc files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: Byte-compiled Python module for CPython 3.8, timestamp-based, .py timestamp: Mon Oct  3 09:35:07 2022 UTC, .py size: 15845 bytes*

 * *Could not decompile bytecode: bad marshal data (unknown type code)*

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-00000000: 550d 0d0a 0000 0000 cbac 3a63 e53d 0000  U.........:c.=..
+00000000: 550d 0d0a 0000 0000 42d0 a263 e53d 0000  U.......B..c.=..
 00000010: e300 0000 0000 0000 0000 0000 0000 0000  ................
 00000020: 0004 0000 0040 0000 0073 8402 0000 6400  .....@...s....d.
 00000030: 6401 6c00 5a00 6400 6401 6c01 5a01 6400  d.l.Z.d.d.l.Z.d.
 00000040: 6401 6c02 5a03 6400 6401 6c04 5a04 6400  d.l.Z.d.d.l.Z.d.
 00000050: 6401 6c05 5a05 6400 6402 6c06 6d07 5a07  d.l.Z.d.d.l.m.Z.
 00000060: 0100 6400 6401 6c08 5a08 6400 6401 6c09  ..d.d.l.Z.d.d.l.
 00000070: 5a09 6400 6401 6c0a 5a0a 6400 6401 6c0b  Z.d.d.l.Z.d.d.l.
@@ -51,1082 +51,1080 @@
 00000320: 2041 7267 733a 0a20 2020 2020 206e 756d   Args:.      num
 00000330: 6265 7220 285b 7479 7065 5d29 3a20 5b64  ber ([type]): [d
 00000340: 6573 6372 6970 7469 6f6e 5d0a 0a20 2052  escription]..  R
 00000350: 6574 7572 6e73 3a0a 2020 2020 2020 5b74  eturns:.      [t
 00000360: 7970 655d 3a20 5b64 6573 6372 6970 7469  ype]: [descripti
 00000370: 6f6e 5d0a 2020 da01 62a9 0029 01da 066e  on].  ..b..)...n
 00000380: 756d 6265 7272 0800 0000 7208 0000 00fa  umberr....r.....
-00000390: 4a2f 686f 6d65 2f6d 612d 7573 6572 2f77  J/home/ma-user/w
-000003a0: 6f72 6b2f 636f 6465 2f73 7479 6c65 6761  ork/code/stylega
-000003b0: 6e32 2d61 6461 2d70 7974 6f72 6368 2d65  n2-ada-pytorch-e
-000003c0: 7870 2f74 6c32 5f6c 6962 2f74 6c32 2f74  xp/tl2_lib/tl2/t
-000003d0: 6c32 5f75 7469 6c73 2e70 79da 0d69 6e74  l2_utils.py..int
-000003e0: 3262 6974 7374 7269 6e67 1c00 0000 7302  2bitstring....s.
-000003f0: 0000 0000 0a72 0b00 0000 6301 0000 0000  .....r....c.....
-00000400: 0000 0000 0000 0002 0000 0003 0000 0043  ...............C
-00000410: 0000 0073 0e00 0000 7400 7c00 6401 8302  ...s....t.|.d...
-00000420: 7d01 7c01 5300 2902 7a8f 0a20 2027 3131  }.|.S.).z..  '11
-00000430: 2720 2d3e 2033 0a0a 2020 3320 5e20 3320  ' -> 3..  3 ^ 3 
-00000440: 3d20 300a 2020 3320 2620 3320 3d20 330a  = 0.  3 & 3 = 3.
-00000450: 2020 3320 3c3c 2031 203d 2036 0a0a 2020    3 << 1 = 6..  
-00000460: 4172 6773 3a0a 2020 2020 2020 6269 745f  Args:.      bit_
-00000470: 7374 7269 6e67 2028 5b74 7970 655d 293a  string ([type]):
-00000480: 205b 6465 7363 7269 7074 696f 6e5d 0a0a   [description]..
-00000490: 2020 5265 7475 726e 733a 0a20 2020 2020    Returns:.     
-000004a0: 205b 7479 7065 5d3a 205b 6465 7363 7269   [type]: [descri
-000004b0: 7074 696f 6e5d 0a20 20e9 0200 0000 2901  ption].  .....).
-000004c0: da03 696e 7429 025a 0a62 6974 5f73 7472  ..int).Z.bit_str
-000004d0: 696e 675a 0762 6974 5f69 6e74 7208 0000  ingZ.bit_intr...
-000004e0: 0072 0800 0000 720a 0000 00da 0d62 6974  .r....r......bit
-000004f0: 7374 7269 6e67 3269 6e74 2800 0000 7304  string2int(...s.
-00000500: 0000 0000 0e0a 0172 0e00 0000 6302 0000  .......r....c...
-00000510: 0000 0000 0000 0000 0003 0000 0009 0000  ................
-00000520: 0043 0000 0073 2600 0000 7400 7c00 6401  .C...s&...t.|.d.
-00000530: 8302 8f12 7d02 7401 a002 7c01 7c02 a102  ....}.t...|.|...
-00000540: 0100 5700 3500 5100 5200 5800 6400 5300  ..W.5.Q.R.X.d.S.
-00000550: 2902 4eda 0277 62a9 03da 046f 7065 6eda  ).N..wb....open.
-00000560: 0670 6963 6b6c 65da 0464 756d 7029 03da  .pickle..dump)..
-00000570: 0a73 6176 6564 5f70 6174 68da 0464 6174  .saved_path..dat
-00000580: 61da 0166 7208 0000 0072 0800 0000 720a  a..fr....r....r.
-00000590: 0000 00da 0b70 6963 6b6c 655f 6475 6d70  .....pickle_dump
-000005a0: 3a00 0000 7304 0000 0000 030c 0172 1700  :...s........r..
-000005b0: 0000 6301 0000 0000 0000 0000 0000 0003  ..c.............
-000005c0: 0000 0009 0000 0043 0000 0073 2800 0000  .......C...s(...
-000005d0: 7400 7c00 6401 8302 8f14 7d01 7401 6a02  t.|.d.....}.t.j.
-000005e0: 7c01 6402 6403 8d02 7d02 5700 3500 5100  |.d.d...}.W.5.Q.
-000005f0: 5200 5800 7c02 5300 2904 4eda 0272 62da  R.X.|.S.).N..rb.
-00000600: 066c 6174 696e 31a9 01da 0865 6e63 6f64  .latin1....encod
-00000610: 696e 6729 0372 1100 0000 7212 0000 00da  ing).r....r.....
-00000620: 046c 6f61 6429 03da 0870 6b6c 5f70 6174  .load)...pkl_pat
-00000630: 6872 1600 0000 7215 0000 0072 0800 0000  hr....r....r....
-00000640: 7208 0000 0072 0a00 0000 da10 6c6f 6164  r....r......load
-00000650: 5f70 6963 6b6c 655f 6669 6c65 4300 0000  _pickle_fileC...
-00000660: 7306 0000 0000 010c 0118 0172 1e00 0000  s..........r....
-00000670: 6301 0000 0000 0000 0000 0000 0002 0000  c...............
-00000680: 0004 0000 0043 0000 0073 1200 0000 7400  .....C...s....t.
-00000690: 6a01 7c00 6401 6402 8d02 7d01 7c01 5300  j.|.d.d...}.|.S.
-000006a0: 2903 4e72 1900 0000 721a 0000 0029 0272  ).Nr....r....).r
-000006b0: 1200 0000 721c 0000 0029 0272 1600 0000  ....r....).r....
-000006c0: 7215 0000 0072 0800 0000 7208 0000 0072  r....r....r....r
-000006d0: 0a00 0000 da17 6c6f 6164 5f70 6963 6b6c  ......load_pickl
-000006e0: 655f 6669 6c65 5f66 726f 6d5f 6648 0000  e_file_from_fH..
-000006f0: 0073 0400 0000 0002 0e01 721f 0000 0063  .s........r....c
-00000700: 0200 0000 0000 0000 0000 0000 0300 0000  ................
-00000710: 0900 0000 4300 0000 732a 0000 0074 007c  ....C...s*...t.|
-00000720: 0064 0183 028f 167d 0274 016a 027c 017c  .d.....}.t.j.|.|
-00000730: 0264 0264 038d 0301 0057 0035 0051 0052  .d.d.....W.5.Q.R
-00000740: 0058 0064 0053 0029 044e 720f 0000 0072  .X.d.S.).Nr....r
-00000750: 0c00 0000 2901 da08 7072 6f74 6f63 6f6c  ....)...protocol
-00000760: 7210 0000 0029 0372 1d00 0000 5a09 6461  r....).r....Z.da
-00000770: 7461 5f64 6963 74da 0266 7072 0800 0000  ta_dict..fpr....
-00000780: 7208 0000 0072 0a00 0000 da11 7772 6974  r....r......writ
-00000790: 655f 7069 636b 6c65 5f66 696c 654d 0000  e_pickle_fileM..
-000007a0: 0073 0400 0000 0002 0c01 7222 0000 0063  .s........r"...c
-000007b0: 0000 0000 0000 0000 0000 0000 0100 0000  ................
-000007c0: 0300 0000 4300 0000 730e 0000 0074 00a0  ....C...s....t..
-000007d0: 0164 01a1 017d 007c 0053 0029 027a 180a  .d...}.|.S.).z..
-000007e0: 2020 7465 6d70 5f66 696c 652e 636c 6f73    temp_file.clos
-000007f0: 6528 290a 0a20 207a 0377 2b74 2902 da08  e()..  z.w+t)...
-00000800: 7465 6d70 6669 6c65 5a0d 5465 6d70 6f72  tempfileZ.Tempor
-00000810: 6172 7946 696c 6529 015a 0974 656d 705f  aryFile).Z.temp_
-00000820: 6669 6c65 7208 0000 0072 0800 0000 720a  filer....r....r.
-00000830: 0000 00da 0c67 6574 5f74 656d 7066 696c  .....get_tempfil
-00000840: 6553 0000 0073 0400 0000 0005 0a01 7224  eS...s........r$
-00000850: 0000 0063 0000 0000 0000 0000 0000 0000  ...c............
-00000860: 0100 0000 0200 0000 4300 0000 730c 0000  ........C...s...
-00000870: 0074 00a0 01a1 007d 007c 0053 00a9 014e  .t.....}.|.S...N
-00000880: 2902 7223 0000 005a 1254 656d 706f 7261  ).r#...Z.Tempora
-00000890: 7279 4469 7265 6374 6f72 7929 015a 0874  ryDirectory).Z.t
-000008a0: 656d 705f 6469 7272 0800 0000 7208 0000  emp_dirr....r...
-000008b0: 0072 0a00 0000 da0b 6765 745f 7465 6d70  .r......get_temp
-000008c0: 6469 725c 0000 0073 0400 0000 0001 0801  dir\...s........
-000008d0: 7226 0000 0063 0000 0000 0000 0000 0000  r&...c..........
-000008e0: 0000 0000 0000 0300 0000 4000 0000 7348  ..........@...sH
-000008f0: 0000 0065 005a 0164 005a 0264 0164 0284  ...e.Z.d.Z.d.d..
-00000900: 005a 0364 1364 0564 0684 015a 0464 0764  .Z.d.d.d...Z.d.d
-00000910: 0884 005a 0564 0964 0a84 005a 0664 0b64  ...Z.d.d...Z.d.d
-00000920: 0c84 005a 0764 1464 0e64 0f84 015a 0864  ...Z.d.d.d...Z.d
-00000930: 1064 1184 005a 0964 1253 0029 15da 0754  .d...Z.d.S.)...T
-00000940: 4c5f 7471 646d 6301 0000 0000 0000 0000  L_tqdmc.........
-00000950: 0000 0001 0000 0002 0000 0043 0000 0073  ...........C...s
-00000960: 0a00 0000 7400 7c00 6a01 8301 5300 7225  ....t.|.j...S.r%
-00000970: 0000 00a9 02da 0373 7472 da04 7062 6172  .......str..pbar
-00000980: a901 da04 7365 6c66 7208 0000 0072 0800  ....selfr....r..
-00000990: 0000 720a 0000 00da 085f 5f72 6570 725f  ..r......__repr_
-000009a0: 5f62 0000 0073 0200 0000 0001 7a10 544c  _b...s......z.TL
-000009b0: 5f74 7164 6d2e 5f5f 7265 7072 5f5f 7201  _tqdm.__repr__r.
-000009c0: 0000 00da 0063 0400 0000 0000 0000 0000  .....c..........
-000009d0: 0000 0400 0000 0500 0000 4300 0000 734a  ..........C...sJ
-000009e0: 0000 0074 0083 007c 005f 0174 026a 027c  ...t...|._.t.j.|
-000009f0: 017c 037c 006a 0164 018d 037c 005f 037c  .|.|.j.d...|._.|
-00000a00: 0264 026b 0472 307c 006a 03a0 047c 02a1  .d.k.r0|.j...|..
-00000a10: 0101 007c 027c 005f 057c 017c 005f 067c  ...|.|._.|.|._.|
-00000a20: 0264 0318 007c 005f 0764 0053 0029 044e  .d...|._.d.S.).N
-00000a30: 2903 da05 746f 7461 6cda 0464 6573 63da  )...total..desc.
-00000a40: 0466 696c 6572 0100 0000 e901 0000 0029  .filer.........)
-00000a50: 0872 2400 0000 da07 7062 6172 5f69 6fda  .r$.....pbar_io.
-00000a60: 0474 7164 6d72 2a00 0000 da06 7570 6461  .tqdmr*.....upda
-00000a70: 7465 da05 7374 6172 7472 2f00 0000 da06  te..startr/.....
-00000a80: 5f63 6f75 6e74 2904 722c 0000 0072 2f00  _count).r,...r/.
-00000a90: 0000 7236 0000 0072 3000 0000 7208 0000  ..r6...r0...r...
-00000aa0: 0072 0800 0000 720a 0000 00da 085f 5f69  .r....r......__i
-00000ab0: 6e69 745f 5f65 0000 0073 1000 0000 0005  nit__e...s......
-00000ac0: 0802 1402 0801 0c03 0601 0601 0a01 7a10  ..............z.
-00000ad0: 544c 5f74 7164 6d2e 5f5f 696e 6974 5f5f  TL_tqdm.__init__
-00000ae0: 6301 0000 0000 0000 0000 0000 0001 0000  c...............
-00000af0: 0002 0000 0043 0000 0073 1a00 0000 7c00  .....C...s....|.
-00000b00: 6a00 6401 1800 7c00 5f01 7c00 6a02 a003  j.d...|._.|.j...
-00000b10: a100 0100 7c00 5300 a902 4e72 3200 0000  ....|.S...Nr2...
-00000b20: 2904 7236 0000 0072 3700 0000 722a 0000  ).r6...r7...r*..
-00000b30: 00da 0572 6573 6574 722b 0000 0072 0800  ...resetr+...r..
-00000b40: 0000 7208 0000 0072 0a00 0000 da08 5f5f  ..r....r......__
-00000b50: 6974 6572 5f5f 7700 0000 7306 0000 0000  iter__w...s.....
-00000b60: 010c 010a 017a 1054 4c5f 7471 646d 2e5f  .....z.TL_tqdm._
-00000b70: 5f69 7465 725f 5f63 0100 0000 0000 0000  _iter__c........
-00000b80: 0000 0000 0100 0000 0300 0000 4300 0000  ............C...
-00000b90: 7332 0000 007c 0004 006a 0064 0137 0002  s2...|...j.d.7..
-00000ba0: 005f 007c 006a 007c 006a 016b 0072 2a7c  ._.|.j.|.j.k.r*|
-00000bb0: 00a0 0264 01a1 0101 007c 006a 0053 0074  ...d.....|.j.S.t
-00000bc0: 0382 0164 0053 0072 3900 0000 2904 7237  ...d.S.r9...).r7
-00000bd0: 0000 0072 2f00 0000 7235 0000 00da 0d53  ...r/...r5.....S
-00000be0: 746f 7049 7465 7261 7469 6f6e 722b 0000  topIterationr+..
-00000bf0: 0072 0800 0000 7208 0000 0072 0a00 0000  .r....r....r....
-00000c00: da08 5f5f 6e65 7874 5f5f 7c00 0000 730a  ..__next__|...s.
-00000c10: 0000 0000 010e 010c 010a 0106 027a 1054  .............z.T
-00000c20: 4c5f 7471 646d 2e5f 5f6e 6578 745f 5f63  L_tqdm.__next__c
-00000c30: 0100 0000 0000 0000 0000 0000 0100 0000  ................
-00000c40: 0200 0000 4300 0000 730e 0000 007c 006a  ....C...s....|.j
-00000c50: 00a0 01a1 0001 0064 0053 0072 2500 0000  .......d.S.r%...
-00000c60: 2902 7233 0000 00da 0563 6c6f 7365 722b  ).r3.....closer+
-00000c70: 0000 0072 0800 0000 7208 0000 0072 0a00  ...r....r....r..
-00000c80: 0000 da07 5f5f 6465 6c5f 5f84 0000 0073  ....__del__....s
-00000c90: 0400 0000 0001 0a01 7a0f 544c 5f74 7164  ........z.TL_tqd
-00000ca0: 6d2e 5f5f 6465 6c5f 5f72 3200 0000 6302  m.__del__r2...c.
-00000cb0: 0000 0000 0000 0000 0000 0002 0000 0003  ................
-00000cc0: 0000 0043 0000 0073 1000 0000 7c00 6a00  ...C...s....|.j.
-00000cd0: a001 7c01 a101 0100 6400 5300 7225 0000  ..|.....d.S.r%..
-00000ce0: 0029 0272 2a00 0000 7235 0000 0029 0272  .).r*...r5...).r
-00000cf0: 2c00 0000 da01 6e72 0800 0000 7208 0000  ,.....nr....r...
-00000d00: 0072 0a00 0000 7235 0000 0088 0000 0073  .r....r5.......s
-00000d10: 0200 0000 0001 7a0e 544c 5f74 7164 6d2e  ......z.TL_tqdm.
-00000d20: 7570 6461 7465 6301 0000 0000 0000 0000  updatec.........
-00000d30: 0000 0001 0000 0002 0000 0043 0000 0073  ...........C...s
-00000d40: 0a00 0000 7400 7c00 6a01 8301 5300 7225  ....t.|.j...S.r%
-00000d50: 0000 0072 2800 0000 722b 0000 0072 0800  ...r(...r+...r..
-00000d60: 0000 7208 0000 0072 0a00 0000 da0a 6765  ..r....r......ge
-00000d70: 745f 7374 7269 6e67 8b00 0000 7302 0000  t_string....s...
-00000d80: 0000 017a 1254 4c5f 7471 646d 2e67 6574  ...z.TL_tqdm.get
-00000d90: 5f73 7472 696e 674e 2902 7201 0000 0072  _stringN).r....r
-00000da0: 2e00 0000 2901 7232 0000 0029 0ada 085f  ....).r2...)..._
-00000db0: 5f6e 616d 655f 5fda 0a5f 5f6d 6f64 756c  _name__..__modul
-00000dc0: 655f 5fda 0c5f 5f71 7561 6c6e 616d 655f  e__..__qualname_
-00000dd0: 5f72 2d00 0000 7238 0000 0072 3b00 0000  _r-...r8...r;...
-00000de0: 723d 0000 0072 3f00 0000 7235 0000 0072  r=...r?...r5...r
-00000df0: 4100 0000 7208 0000 0072 0800 0000 7208  A...r....r....r.
-00000e00: 0000 0072 0a00 0000 7227 0000 0061 0000  ...r....r'...a..
-00000e10: 0073 1200 0000 0801 0805 0001 00fd 0a12  .s..............
-00000e20: 0805 0808 0804 0a03 7227 0000 0063 0100  ........r'...c..
-00000e30: 0000 0000 0000 0000 0000 0100 0000 0400  ................
-00000e40: 0000 4300 0000 731e 0000 0074 0064 017c  ..C...s....t.d.|
-00000e50: 009b 0064 029d 0383 0101 0074 01a0 027c  ...d.......t...|
-00000e60: 00a1 0101 0064 0053 0029 034e 7a03 0a2b  .....d.S.).Nz..+
-00000e70: 20da 010a 2903 da05 7072 696e 74da 026f   ...)...print..o
-00000e80: 73da 0673 7973 7465 6d29 01da 0763 6f6d  s..system)...com
-00000e90: 6d61 6e64 7208 0000 0072 0800 0000 720a  mandr....r....r.
-00000ea0: 0000 00da 096f 735f 7379 7374 656d 8f00  .....os_system..
-00000eb0: 0000 7306 0000 0000 0110 010a 0172 4a00  ..s..........rJ.
-00000ec0: 0000 722e 0000 0063 0200 0000 0000 0000  ..r....c........
-00000ed0: 0000 0000 0300 0000 0600 0000 4300 0000  ............C...
-00000ee0: 7320 0000 007c 019b 0064 017c 006a 006a  s ...|...d.|.j.j
-00000ef0: 019b 0064 027c 006a 029b 0064 039d 067d  ...d.|.j...d...}
-00000f00: 027c 0253 0029 047a 650a 2020 7365 6c66  .|.S.).ze.  self
-00000f10: 2e72 6570 725f 7374 7220 3d20 746c 325f  .repr_str = tl2_
-00000f20: 7574 696c 732e 6469 6374 3273 7472 696e  utils.dict2strin
-00000f30: 6728 290a 2020 7365 6c66 2e6d 6f64 756c  g().  self.modul
-00000f40: 655f 6e61 6d65 5f6c 6973 7420 3d20 5b5d  e_name_list = []
-00000f50: 0a0a 2020 3a70 6172 616d 2073 656c 663a  ..  :param self:
-00000f60: 0a20 203a 7265 7475 726e 3a0a 2020 da01  .  :return:.  ..
-00000f70: 2efa 0128 fa01 2929 03da 095f 5f63 6c61  ...(..))...__cla
-00000f80: 7373 5f5f 7242 0000 00da 0872 6570 725f  ss__rB.....repr_
-00000f90: 7374 7229 0372 2c00 0000 da06 7072 6566  str).r,.....pref
-00000fa0: 6978 724f 0000 0072 0800 0000 7208 0000  ixrO...r....r...
-00000fb0: 0072 0a00 0000 da0e 6765 745f 636c 6173  .r......get_clas
-00000fc0: 735f 7265 7072 9500 0000 7304 0000 0000  s_repr....s.....
-00000fd0: 081c 0172 5100 0000 6301 0000 0000 0000  ...rQ...c.......
-00000fe0: 0000 0000 0005 0000 0005 0000 0043 0000  .............C..
-00000ff0: 0073 5a00 0000 6401 6402 6c00 6d01 7d01  .sZ...d.d.l.m.}.
-00001000: 0100 6900 7d02 7c00 6a02 4400 5d14 7d03  ..i.}.|.j.D.].}.
-00001010: 7403 7c00 7c03 6403 8d02 7c02 7c03 3c00  t.|.|.d...|.|.<.
-00001020: 7116 7c00 7c02 6404 3c00 7404 a005 6405  q.|.|.d.<.t...d.
-00001030: a101 7d04 7c01 6a06 7c02 7c04 6406 8d02  ..}.|.j.|.|.d...
-00001040: 0100 7c04 a007 7c00 a101 0100 6407 5300  ..|...|.....d.S.
-00001050: 2908 7a3b 0a20 2073 656c 662e 6d6f 6475  ).z;.  self.modu
-00001060: 6c65 5f6e 616d 655f 6c69 7374 203d 205b  le_name_list = [
-00001070: 5d0a 0a20 203a 7061 7261 6d20 7365 6c66  ]..  :param self
-00001080: 3a0a 2020 3a72 6574 7572 6e3a 0a20 2072  :.  :return:.  r
-00001090: 0100 0000 2901 da0b 746f 7263 685f 7574  ....)...torch_ut
-000010a0: 696c 7329 02da 066f 626a 6563 74da 0461  ils)...object..a
-000010b0: 7474 72da 0341 6c6c da02 746c 2902 da0b  ttr..All..tl)...
-000010c0: 6d6f 6465 6c73 5f64 6963 74da 066c 6f67  models_dict..log
-000010d0: 6765 724e 2908 5a10 746c 322e 7072 6f6a  gerN).Z.tl2.proj
-000010e0: 2e70 7974 6f72 6368 7252 0000 005a 106d  .pytorchrR...Z.m
-000010f0: 6f64 756c 655f 6e61 6d65 5f6c 6973 74da  odule_name_list.
-00001100: 1261 7474 7267 6574 7465 725f 6465 6661  .attrgetter_defa
-00001110: 756c 74da 076c 6f67 6769 6e67 da09 6765  ult..logging..ge
-00001120: 744c 6f67 6765 725a 1370 7269 6e74 5f6e  tLoggerZ.print_n
-00001130: 756d 6265 725f 7061 7261 6d73 da04 696e  umber_params..in
-00001140: 666f 2905 722c 0000 0072 5200 0000 7257  fo).r,...rR...rW
-00001150: 0000 00da 046e 616d 6572 5800 0000 7208  .....namerX...r.
-00001160: 0000 0072 0800 0000 720a 0000 00da 0a70  ...r....r......p
-00001170: 7269 6e74 5f72 6570 72a0 0000 0073 1200  rint_repr....s..
-00001180: 0000 0007 0c02 0401 0a01 1201 0801 0a01  ................
-00001190: 0e01 0a01 725e 0000 0063 0100 0000 0000  ....r^...c......
-000011a0: 0000 0000 0000 0400 0000 0400 0000 4300  ..............C.
-000011b0: 0000 733e 0000 0074 00a0 01a1 007d 017c  ..s>...t.....}.|
-000011c0: 00a0 02a1 0044 005d 285c 027d 027d 0374  .....D.](\.}.}.t
-000011d0: 037c 0374 0483 0272 3074 057c 0383 017c  .|.t...r0t.|...|
-000011e0: 017c 023c 0071 107c 037c 017c 023c 0071  .|.<.q.|.|.|.<.q
-000011f0: 107c 0153 0072 2500 0000 2906 da0b 636f  .|.S.r%...)...co
-00001200: 6c6c 6563 7469 6f6e 73da 0b4f 7264 6572  llections..Order
-00001210: 6564 4469 6374 da05 6974 656d 73da 0a69  edDict..items..i
-00001220: 7369 6e73 7461 6e63 65da 0464 6963 74da  sinstance..dict.
-00001230: 1174 6f5f 6469 6374 5f72 6563 7572 7369  .to_dict_recursi
-00001240: 7665 2904 5a0a 6469 6374 5f69 6e70 7574  ve).Z.dict_input
-00001250: 5a08 7265 745f 6469 6374 da01 6bda 0176  Z.ret_dict..k..v
-00001260: 7208 0000 0072 0800 0000 720a 0000 0072  r....r....r....r
-00001270: 6400 0000 b200 0000 730c 0000 0000 0208  d.......s.......
-00001280: 0110 010a 010e 020a 0172 6400 0000 6302  .........rd...c.
-00001290: 0000 0000 0000 0000 0000 0005 0000 0004  ................
-000012a0: 0000 0043 0000 0073 4a00 0000 6401 6402  ...C...sJ...d.d.
-000012b0: 6c00 6d01 7d02 0100 7402 7c00 8301 7d00  l.m.}...t.|...}.
-000012c0: 7402 7c01 8301 7d01 7c02 7c00 7c01 8302  t.|...}.|.|.|...
-000012d0: 7d03 6403 7d04 7c04 6404 7c03 a003 a100  }.d.}.|.d.|.....
-000012e0: 9b00 6405 9d03 3700 7d04 7c04 6406 3700  ..d...7.}.|.d.7.
-000012f0: 7d04 7c04 5300 2907 4e72 0100 0000 2901  }.|.S.).Nr....).
-00001300: da08 4465 6570 4469 6666 7a3b 2a2a 2a2a  ..DeepDiffz;****
+00000390: 2c2f 686f 6d65 2f6d 612d 7573 6572 2f77  ,/home/ma-user/w
+000003a0: 6f72 6b2f 636f 6465 2f74 6c32 2f74 6c32  ork/code/tl2/tl2
+000003b0: 2f74 6c32 5f75 7469 6c73 2e70 79da 0d69  /tl2_utils.py..i
+000003c0: 6e74 3262 6974 7374 7269 6e67 1c00 0000  nt2bitstring....
+000003d0: 7302 0000 0000 0a72 0b00 0000 6301 0000  s......r....c...
+000003e0: 0000 0000 0000 0000 0002 0000 0003 0000  ................
+000003f0: 0043 0000 0073 0e00 0000 7400 7c00 6401  .C...s....t.|.d.
+00000400: 8302 7d01 7c01 5300 2902 7a8f 0a20 2027  ..}.|.S.).z..  '
+00000410: 3131 2720 2d3e 2033 0a0a 2020 3320 5e20  11' -> 3..  3 ^ 
+00000420: 3320 3d20 300a 2020 3320 2620 3320 3d20  3 = 0.  3 & 3 = 
+00000430: 330a 2020 3320 3c3c 2031 203d 2036 0a0a  3.  3 << 1 = 6..
+00000440: 2020 4172 6773 3a0a 2020 2020 2020 6269    Args:.      bi
+00000450: 745f 7374 7269 6e67 2028 5b74 7970 655d  t_string ([type]
+00000460: 293a 205b 6465 7363 7269 7074 696f 6e5d  ): [description]
+00000470: 0a0a 2020 5265 7475 726e 733a 0a20 2020  ..  Returns:.   
+00000480: 2020 205b 7479 7065 5d3a 205b 6465 7363     [type]: [desc
+00000490: 7269 7074 696f 6e5d 0a20 20e9 0200 0000  ription].  .....
+000004a0: 2901 da03 696e 7429 025a 0a62 6974 5f73  )...int).Z.bit_s
+000004b0: 7472 696e 675a 0762 6974 5f69 6e74 7208  tringZ.bit_intr.
+000004c0: 0000 0072 0800 0000 720a 0000 00da 0d62  ...r....r......b
+000004d0: 6974 7374 7269 6e67 3269 6e74 2800 0000  itstring2int(...
+000004e0: 7304 0000 0000 0e0a 0172 0e00 0000 6302  s........r....c.
+000004f0: 0000 0000 0000 0000 0000 0003 0000 0009  ................
+00000500: 0000 0043 0000 0073 2600 0000 7400 7c00  ...C...s&...t.|.
+00000510: 6401 8302 8f12 7d02 7401 a002 7c01 7c02  d.....}.t...|.|.
+00000520: a102 0100 5700 3500 5100 5200 5800 6400  ....W.5.Q.R.X.d.
+00000530: 5300 2902 4eda 0277 62a9 03da 046f 7065  S.).N..wb....ope
+00000540: 6eda 0670 6963 6b6c 65da 0464 756d 7029  n..pickle..dump)
+00000550: 03da 0a73 6176 6564 5f70 6174 68da 0464  ...saved_path..d
+00000560: 6174 61da 0166 7208 0000 0072 0800 0000  ata..fr....r....
+00000570: 720a 0000 00da 0b70 6963 6b6c 655f 6475  r......pickle_du
+00000580: 6d70 3a00 0000 7304 0000 0000 030c 0172  mp:...s........r
+00000590: 1700 0000 6301 0000 0000 0000 0000 0000  ....c...........
+000005a0: 0003 0000 0009 0000 0043 0000 0073 2800  .........C...s(.
+000005b0: 0000 7400 7c00 6401 8302 8f14 7d01 7401  ..t.|.d.....}.t.
+000005c0: 6a02 7c01 6402 6403 8d02 7d02 5700 3500  j.|.d.d...}.W.5.
+000005d0: 5100 5200 5800 7c02 5300 2904 4eda 0272  Q.R.X.|.S.).N..r
+000005e0: 62da 066c 6174 696e 31a9 01da 0865 6e63  b..latin1....enc
+000005f0: 6f64 696e 6729 0372 1100 0000 7212 0000  oding).r....r...
+00000600: 00da 046c 6f61 6429 03da 0870 6b6c 5f70  ...load)...pkl_p
+00000610: 6174 6872 1600 0000 7215 0000 0072 0800  athr....r....r..
+00000620: 0000 7208 0000 0072 0a00 0000 da10 6c6f  ..r....r......lo
+00000630: 6164 5f70 6963 6b6c 655f 6669 6c65 4300  ad_pickle_fileC.
+00000640: 0000 7306 0000 0000 010c 0118 0172 1e00  ..s..........r..
+00000650: 0000 6301 0000 0000 0000 0000 0000 0002  ..c.............
+00000660: 0000 0004 0000 0043 0000 0073 1200 0000  .......C...s....
+00000670: 7400 6a01 7c00 6401 6402 8d02 7d01 7c01  t.j.|.d.d...}.|.
+00000680: 5300 2903 4e72 1900 0000 721a 0000 0029  S.).Nr....r....)
+00000690: 0272 1200 0000 721c 0000 0029 0272 1600  .r....r....).r..
+000006a0: 0000 7215 0000 0072 0800 0000 7208 0000  ..r....r....r...
+000006b0: 0072 0a00 0000 da17 6c6f 6164 5f70 6963  .r......load_pic
+000006c0: 6b6c 655f 6669 6c65 5f66 726f 6d5f 6648  kle_file_from_fH
+000006d0: 0000 0073 0400 0000 0002 0e01 721f 0000  ...s........r...
+000006e0: 0063 0200 0000 0000 0000 0000 0000 0300  .c..............
+000006f0: 0000 0900 0000 4300 0000 732a 0000 0074  ......C...s*...t
+00000700: 007c 0064 0183 028f 167d 0274 016a 027c  .|.d.....}.t.j.|
+00000710: 017c 0264 0264 038d 0301 0057 0035 0051  .|.d.d.....W.5.Q
+00000720: 0052 0058 0064 0053 0029 044e 720f 0000  .R.X.d.S.).Nr...
+00000730: 0072 0c00 0000 2901 da08 7072 6f74 6f63  .r....)...protoc
+00000740: 6f6c 7210 0000 0029 0372 1d00 0000 5a09  olr....).r....Z.
+00000750: 6461 7461 5f64 6963 74da 0266 7072 0800  data_dict..fpr..
+00000760: 0000 7208 0000 0072 0a00 0000 da11 7772  ..r....r......wr
+00000770: 6974 655f 7069 636b 6c65 5f66 696c 654d  ite_pickle_fileM
+00000780: 0000 0073 0400 0000 0002 0c01 7222 0000  ...s........r"..
+00000790: 0063 0000 0000 0000 0000 0000 0000 0100  .c..............
+000007a0: 0000 0300 0000 4300 0000 730e 0000 0074  ......C...s....t
+000007b0: 00a0 0164 01a1 017d 007c 0053 0029 027a  ...d...}.|.S.).z
+000007c0: 180a 2020 7465 6d70 5f66 696c 652e 636c  ..  temp_file.cl
+000007d0: 6f73 6528 290a 0a20 207a 0377 2b74 2902  ose()..  z.w+t).
+000007e0: da08 7465 6d70 6669 6c65 5a0d 5465 6d70  ..tempfileZ.Temp
+000007f0: 6f72 6172 7946 696c 6529 015a 0974 656d  oraryFile).Z.tem
+00000800: 705f 6669 6c65 7208 0000 0072 0800 0000  p_filer....r....
+00000810: 720a 0000 00da 0c67 6574 5f74 656d 7066  r......get_tempf
+00000820: 696c 6553 0000 0073 0400 0000 0005 0a01  ileS...s........
+00000830: 7224 0000 0063 0000 0000 0000 0000 0000  r$...c..........
+00000840: 0000 0100 0000 0200 0000 4300 0000 730c  ..........C...s.
+00000850: 0000 0074 00a0 01a1 007d 007c 0053 00a9  ...t.....}.|.S..
+00000860: 014e 2902 7223 0000 005a 1254 656d 706f  .N).r#...Z.Tempo
+00000870: 7261 7279 4469 7265 6374 6f72 7929 015a  raryDirectory).Z
+00000880: 0874 656d 705f 6469 7272 0800 0000 7208  .temp_dirr....r.
+00000890: 0000 0072 0a00 0000 da0b 6765 745f 7465  ...r......get_te
+000008a0: 6d70 6469 725c 0000 0073 0400 0000 0001  mpdir\...s......
+000008b0: 0801 7226 0000 0063 0000 0000 0000 0000  ..r&...c........
+000008c0: 0000 0000 0000 0000 0300 0000 4000 0000  ............@...
+000008d0: 7348 0000 0065 005a 0164 005a 0264 0164  sH...e.Z.d.Z.d.d
+000008e0: 0284 005a 0364 1364 0564 0684 015a 0464  ...Z.d.d.d...Z.d
+000008f0: 0764 0884 005a 0564 0964 0a84 005a 0664  .d...Z.d.d...Z.d
+00000900: 0b64 0c84 005a 0764 1464 0e64 0f84 015a  .d...Z.d.d.d...Z
+00000910: 0864 1064 1184 005a 0964 1253 0029 15da  .d.d...Z.d.S.)..
+00000920: 0754 4c5f 7471 646d 6301 0000 0000 0000  .TL_tqdmc.......
+00000930: 0000 0000 0001 0000 0002 0000 0043 0000  .............C..
+00000940: 0073 0a00 0000 7400 7c00 6a01 8301 5300  .s....t.|.j...S.
+00000950: 7225 0000 00a9 02da 0373 7472 da04 7062  r%.......str..pb
+00000960: 6172 a901 da04 7365 6c66 7208 0000 0072  ar....selfr....r
+00000970: 0800 0000 720a 0000 00da 085f 5f72 6570  ....r......__rep
+00000980: 725f 5f62 0000 0073 0200 0000 0001 7a10  r__b...s......z.
+00000990: 544c 5f74 7164 6d2e 5f5f 7265 7072 5f5f  TL_tqdm.__repr__
+000009a0: 7201 0000 00da 0063 0400 0000 0000 0000  r......c........
+000009b0: 0000 0000 0400 0000 0500 0000 4300 0000  ............C...
+000009c0: 734a 0000 0074 0083 007c 005f 0174 026a  sJ...t...|._.t.j
+000009d0: 027c 017c 037c 006a 0164 018d 037c 005f  .|.|.|.j.d...|._
+000009e0: 037c 0264 026b 0472 307c 006a 03a0 047c  .|.d.k.r0|.j...|
+000009f0: 02a1 0101 007c 027c 005f 057c 017c 005f  .....|.|._.|.|._
+00000a00: 067c 0264 0318 007c 005f 0764 0053 0029  .|.d...|._.d.S.)
+00000a10: 044e 2903 da05 746f 7461 6cda 0464 6573  .N)...total..des
+00000a20: 63da 0466 696c 6572 0100 0000 e901 0000  c..filer........
+00000a30: 0029 0872 2400 0000 da07 7062 6172 5f69  .).r$.....pbar_i
+00000a40: 6fda 0474 7164 6d72 2a00 0000 da06 7570  o..tqdmr*.....up
+00000a50: 6461 7465 da05 7374 6172 7472 2f00 0000  date..startr/...
+00000a60: da06 5f63 6f75 6e74 2904 722c 0000 0072  .._count).r,...r
+00000a70: 2f00 0000 7236 0000 0072 3000 0000 7208  /...r6...r0...r.
+00000a80: 0000 0072 0800 0000 720a 0000 00da 085f  ...r....r......_
+00000a90: 5f69 6e69 745f 5f65 0000 0073 1000 0000  _init__e...s....
+00000aa0: 0005 0802 1402 0801 0c03 0601 0601 0a01  ................
+00000ab0: 7a10 544c 5f74 7164 6d2e 5f5f 696e 6974  z.TL_tqdm.__init
+00000ac0: 5f5f 6301 0000 0000 0000 0000 0000 0001  __c.............
+00000ad0: 0000 0002 0000 0043 0000 0073 1a00 0000  .......C...s....
+00000ae0: 7c00 6a00 6401 1800 7c00 5f01 7c00 6a02  |.j.d...|._.|.j.
+00000af0: a003 a100 0100 7c00 5300 a902 4e72 3200  ......|.S...Nr2.
+00000b00: 0000 2904 7236 0000 0072 3700 0000 722a  ..).r6...r7...r*
+00000b10: 0000 00da 0572 6573 6574 722b 0000 0072  .....resetr+...r
+00000b20: 0800 0000 7208 0000 0072 0a00 0000 da08  ....r....r......
+00000b30: 5f5f 6974 6572 5f5f 7700 0000 7306 0000  __iter__w...s...
+00000b40: 0000 010c 010a 017a 1054 4c5f 7471 646d  .......z.TL_tqdm
+00000b50: 2e5f 5f69 7465 725f 5f63 0100 0000 0000  .__iter__c......
+00000b60: 0000 0000 0000 0100 0000 0300 0000 4300  ..............C.
+00000b70: 0000 7332 0000 007c 0004 006a 0064 0137  ..s2...|...j.d.7
+00000b80: 0002 005f 007c 006a 007c 006a 016b 0072  ..._.|.j.|.j.k.r
+00000b90: 2a7c 00a0 0264 01a1 0101 007c 006a 0053  *|...d.....|.j.S
+00000ba0: 0074 0382 0164 0053 0072 3900 0000 2904  .t...d.S.r9...).
+00000bb0: 7237 0000 0072 2f00 0000 7235 0000 00da  r7...r/...r5....
+00000bc0: 0d53 746f 7049 7465 7261 7469 6f6e 722b  .StopIterationr+
+00000bd0: 0000 0072 0800 0000 7208 0000 0072 0a00  ...r....r....r..
+00000be0: 0000 da08 5f5f 6e65 7874 5f5f 7c00 0000  ....__next__|...
+00000bf0: 730a 0000 0000 010e 010c 010a 0106 027a  s..............z
+00000c00: 1054 4c5f 7471 646d 2e5f 5f6e 6578 745f  .TL_tqdm.__next_
+00000c10: 5f63 0100 0000 0000 0000 0000 0000 0100  _c..............
+00000c20: 0000 0200 0000 4300 0000 730e 0000 007c  ......C...s....|
+00000c30: 006a 00a0 01a1 0001 0064 0053 0072 2500  .j.......d.S.r%.
+00000c40: 0000 2902 7233 0000 00da 0563 6c6f 7365  ..).r3.....close
+00000c50: 722b 0000 0072 0800 0000 7208 0000 0072  r+...r....r....r
+00000c60: 0a00 0000 da07 5f5f 6465 6c5f 5f84 0000  ......__del__...
+00000c70: 0073 0400 0000 0001 0a01 7a0f 544c 5f74  .s........z.TL_t
+00000c80: 7164 6d2e 5f5f 6465 6c5f 5f72 3200 0000  qdm.__del__r2...
+00000c90: 6302 0000 0000 0000 0000 0000 0002 0000  c...............
+00000ca0: 0003 0000 0043 0000 0073 1000 0000 7c00  .....C...s....|.
+00000cb0: 6a00 a001 7c01 a101 0100 6400 5300 7225  j...|.....d.S.r%
+00000cc0: 0000 0029 0272 2a00 0000 7235 0000 0029  ...).r*...r5...)
+00000cd0: 0272 2c00 0000 da01 6e72 0800 0000 7208  .r,.....nr....r.
+00000ce0: 0000 0072 0a00 0000 7235 0000 0088 0000  ...r....r5......
+00000cf0: 0073 0200 0000 0001 7a0e 544c 5f74 7164  .s......z.TL_tqd
+00000d00: 6d2e 7570 6461 7465 6301 0000 0000 0000  m.updatec.......
+00000d10: 0000 0000 0001 0000 0002 0000 0043 0000  .............C..
+00000d20: 0073 0a00 0000 7400 7c00 6a01 8301 5300  .s....t.|.j...S.
+00000d30: 7225 0000 0072 2800 0000 722b 0000 0072  r%...r(...r+...r
+00000d40: 0800 0000 7208 0000 0072 0a00 0000 da0a  ....r....r......
+00000d50: 6765 745f 7374 7269 6e67 8b00 0000 7302  get_string....s.
+00000d60: 0000 0000 017a 1254 4c5f 7471 646d 2e67  .....z.TL_tqdm.g
+00000d70: 6574 5f73 7472 696e 674e 2902 7201 0000  et_stringN).r...
+00000d80: 0072 2e00 0000 2901 7232 0000 0029 0ada  .r....).r2...)..
+00000d90: 085f 5f6e 616d 655f 5fda 0a5f 5f6d 6f64  .__name__..__mod
+00000da0: 756c 655f 5fda 0c5f 5f71 7561 6c6e 616d  ule__..__qualnam
+00000db0: 655f 5f72 2d00 0000 7238 0000 0072 3b00  e__r-...r8...r;.
+00000dc0: 0000 723d 0000 0072 3f00 0000 7235 0000  ..r=...r?...r5..
+00000dd0: 0072 4100 0000 7208 0000 0072 0800 0000  .rA...r....r....
+00000de0: 7208 0000 0072 0a00 0000 7227 0000 0061  r....r....r'...a
+00000df0: 0000 0073 1200 0000 0801 0805 0001 00fd  ...s............
+00000e00: 0a12 0805 0808 0804 0a03 7227 0000 0063  ..........r'...c
+00000e10: 0100 0000 0000 0000 0000 0000 0100 0000  ................
+00000e20: 0400 0000 4300 0000 731e 0000 0074 0064  ....C...s....t.d
+00000e30: 017c 009b 0064 029d 0383 0101 0074 01a0  .|...d.......t..
+00000e40: 027c 00a1 0101 0064 0053 0029 034e 7a03  .|.....d.S.).Nz.
+00000e50: 0a2b 20da 010a 2903 da05 7072 696e 74da  .+ ...)...print.
+00000e60: 026f 73da 0673 7973 7465 6d29 01da 0763  .os..system)...c
+00000e70: 6f6d 6d61 6e64 7208 0000 0072 0800 0000  ommandr....r....
+00000e80: 720a 0000 00da 096f 735f 7379 7374 656d  r......os_system
+00000e90: 8f00 0000 7306 0000 0000 0110 010a 0172  ....s..........r
+00000ea0: 4a00 0000 722e 0000 0063 0200 0000 0000  J...r....c......
+00000eb0: 0000 0000 0000 0300 0000 0600 0000 4300  ..............C.
+00000ec0: 0000 7320 0000 007c 019b 0064 017c 006a  ..s ...|...d.|.j
+00000ed0: 006a 019b 0064 027c 006a 029b 0064 039d  .j...d.|.j...d..
+00000ee0: 067d 027c 0253 0029 047a 650a 2020 7365  .}.|.S.).ze.  se
+00000ef0: 6c66 2e72 6570 725f 7374 7220 3d20 746c  lf.repr_str = tl
+00000f00: 325f 7574 696c 732e 6469 6374 3273 7472  2_utils.dict2str
+00000f10: 696e 6728 290a 2020 7365 6c66 2e6d 6f64  ing().  self.mod
+00000f20: 756c 655f 6e61 6d65 5f6c 6973 7420 3d20  ule_name_list = 
+00000f30: 5b5d 0a0a 2020 3a70 6172 616d 2073 656c  []..  :param sel
+00000f40: 663a 0a20 203a 7265 7475 726e 3a0a 2020  f:.  :return:.  
+00000f50: da01 2efa 0128 fa01 2929 03da 095f 5f63  .....(..))...__c
+00000f60: 6c61 7373 5f5f 7242 0000 00da 0872 6570  lass__rB.....rep
+00000f70: 725f 7374 7229 0372 2c00 0000 da06 7072  r_str).r,.....pr
+00000f80: 6566 6978 724f 0000 0072 0800 0000 7208  efixrO...r....r.
+00000f90: 0000 0072 0a00 0000 da0e 6765 745f 636c  ...r......get_cl
+00000fa0: 6173 735f 7265 7072 9500 0000 7304 0000  ass_repr....s...
+00000fb0: 0000 081c 0172 5100 0000 6301 0000 0000  .....rQ...c.....
+00000fc0: 0000 0000 0000 0005 0000 0005 0000 0043  ...............C
+00000fd0: 0000 0073 5a00 0000 6401 6402 6c00 6d01  ...sZ...d.d.l.m.
+00000fe0: 7d01 0100 6900 7d02 7c00 6a02 4400 5d14  }...i.}.|.j.D.].
+00000ff0: 7d03 7403 7c00 7c03 6403 8d02 7c02 7c03  }.t.|.|.d...|.|.
+00001000: 3c00 7116 7c00 7c02 6404 3c00 7404 a005  <.q.|.|.d.<.t...
+00001010: 6405 a101 7d04 7c01 6a06 7c02 7c04 6406  d...}.|.j.|.|.d.
+00001020: 8d02 0100 7c04 a007 7c00 a101 0100 6407  ....|...|.....d.
+00001030: 5300 2908 7a3b 0a20 2073 656c 662e 6d6f  S.).z;.  self.mo
+00001040: 6475 6c65 5f6e 616d 655f 6c69 7374 203d  dule_name_list =
+00001050: 205b 5d0a 0a20 203a 7061 7261 6d20 7365   []..  :param se
+00001060: 6c66 3a0a 2020 3a72 6574 7572 6e3a 0a20  lf:.  :return:. 
+00001070: 2072 0100 0000 2901 da0b 746f 7263 685f   r....)...torch_
+00001080: 7574 696c 7329 02da 066f 626a 6563 74da  utils)...object.
+00001090: 0461 7474 725a 0341 6c6c da02 746c 2902  .attrZ.All..tl).
+000010a0: da0b 6d6f 6465 6c73 5f64 6963 74da 066c  ..models_dict..l
+000010b0: 6f67 6765 724e 2908 5a10 746c 322e 7072  oggerN).Z.tl2.pr
+000010c0: 6f6a 2e70 7974 6f72 6368 7252 0000 005a  oj.pytorchrR...Z
+000010d0: 106d 6f64 756c 655f 6e61 6d65 5f6c 6973  .module_name_lis
+000010e0: 74da 1261 7474 7267 6574 7465 725f 6465  t..attrgetter_de
+000010f0: 6661 756c 74da 076c 6f67 6769 6e67 da09  fault..logging..
+00001100: 6765 744c 6f67 6765 725a 1370 7269 6e74  getLoggerZ.print
+00001110: 5f6e 756d 6265 725f 7061 7261 6d73 da04  _number_params..
+00001120: 696e 666f 2905 722c 0000 0072 5200 0000  info).r,...rR...
+00001130: 7256 0000 00da 046e 616d 6572 5700 0000  rV.....namerW...
+00001140: 7208 0000 0072 0800 0000 720a 0000 00da  r....r....r.....
+00001150: 0a70 7269 6e74 5f72 6570 72a0 0000 0073  .print_repr....s
+00001160: 1200 0000 0007 0c02 0401 0a01 1201 0801  ................
+00001170: 0a01 0e01 0a01 725d 0000 0063 0100 0000  ......r]...c....
+00001180: 0000 0000 0000 0000 0400 0000 0400 0000  ................
+00001190: 4300 0000 733e 0000 0074 00a0 01a1 007d  C...s>...t.....}
+000011a0: 017c 00a0 02a1 0044 005d 285c 027d 027d  .|.....D.](\.}.}
+000011b0: 0374 037c 0374 0483 0272 3074 057c 0383  .t.|.t...r0t.|..
+000011c0: 017c 017c 023c 0071 107c 037c 017c 023c  .|.|.<.q.|.|.|.<
+000011d0: 0071 107c 0153 0072 2500 0000 2906 da0b  .q.|.S.r%...)...
+000011e0: 636f 6c6c 6563 7469 6f6e 73da 0b4f 7264  collections..Ord
+000011f0: 6572 6564 4469 6374 da05 6974 656d 73da  eredDict..items.
+00001200: 0a69 7369 6e73 7461 6e63 65da 0464 6963  .isinstance..dic
+00001210: 74da 1174 6f5f 6469 6374 5f72 6563 7572  t..to_dict_recur
+00001220: 7369 7665 2904 5a0a 6469 6374 5f69 6e70  sive).Z.dict_inp
+00001230: 7574 5a08 7265 745f 6469 6374 da01 6bda  utZ.ret_dict..k.
+00001240: 0176 7208 0000 0072 0800 0000 720a 0000  .vr....r....r...
+00001250: 0072 6300 0000 b200 0000 730c 0000 0000  .rc.......s.....
+00001260: 0208 0110 010a 010e 020a 0172 6300 0000  ...........rc...
+00001270: 6302 0000 0000 0000 0000 0000 0005 0000  c...............
+00001280: 0004 0000 0043 0000 0073 4a00 0000 6401  .....C...sJ...d.
+00001290: 6402 6c00 6d01 7d02 0100 7402 7c00 8301  d.l.m.}...t.|...
+000012a0: 7d00 7402 7c01 8301 7d01 7c02 7c00 7c01  }.t.|...}.|.|.|.
+000012b0: 8302 7d03 6403 7d04 7c04 6404 7c03 a003  ..}.d.}.|.d.|...
+000012c0: a100 9b00 6405 9d03 3700 7d04 7c04 6406  ....d...7.}.|.d.
+000012d0: 3700 7d04 7c04 5300 2907 4e72 0100 0000  7.}.|.S.).Nr....
+000012e0: 2901 da08 4465 6570 4469 6666 7a3b 2a2a  )...DeepDiffz;**
+000012f0: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
+00001300: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
 00001310: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
-00001320: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
-00001330: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
-00001340: 2a2a 2a2a 2a2a 0a7a 244d 6f64 6966 6963  ******.z$Modific
-00001350: 6174 696f 6e20 6f66 2064 3220 636f 6d70  ation of d2 comp
-00001360: 6172 6564 2074 6f20 6431 3a20 0a72 4500  ared to d1: .rE.
-00001370: 0000 7a3a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ..z:************
+00001320: 2a2a 2a2a 2a2a 2a2a 0a7a 244d 6f64 6966  ********.z$Modif
+00001330: 6963 6174 696f 6e20 6f66 2064 3220 636f  ication of d2 co
+00001340: 6d70 6172 6564 2074 6f20 6431 3a20 0a72  mpared to d1: .r
+00001350: 4500 0000 7a3a 2a2a 2a2a 2a2a 2a2a 2a2a  E...z:**********
+00001360: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
+00001370: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
 00001380: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
-00001390: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
-000013a0: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2904  **************).
-000013b0: 5a08 6465 6570 6469 6666 7267 0000 0072  Z.deepdiffrg...r
-000013c0: 6400 0000 5a06 7072 6574 7479 2905 da02  d...Z.pretty)...
-000013d0: 6431 da02 6432 7267 0000 005a 0564 6469  d1..d2rg...Z.ddi
-000013e0: 6666 5a08 6469 6666 5f73 7472 7208 0000  ffZ.diff_strr...
-000013f0: 0072 0800 0000 720a 0000 00da 0c67 6574  .r....r......get
-00001400: 5f64 6966 665f 7374 72bc 0000 0073 1000  _diff_str....s..
-00001410: 0000 0001 0c01 0801 0802 0a01 0401 1401  ................
-00001420: 0801 726a 0000 0063 0200 0000 0000 0000  ..rj...c........
-00001430: 0000 0000 0300 0000 0900 0000 4300 0000  ............C...
-00001440: 7330 0000 0074 007c 009b 0064 019d 0264  s0...t.|...d...d
-00001450: 0283 028f 167d 027c 02a0 017c 019b 0064  .....}.|...|...d
-00001460: 039d 02a1 0101 0057 0035 0051 0052 0058  .......W.5.Q.R.X
-00001470: 0064 0053 0029 044e fa0a 2f30 696e 666f  .d.S.).N../0info
-00001480: 2e74 7874 da01 7772 4500 0000 2902 7211  .txt..wrE...).r.
-00001490: 0000 00da 0577 7269 7465 2903 da09 7361  .....write)...sa
-000014a0: 7665 645f 6469 72da 0869 6e66 6f5f 6d73  ved_dir..info_ms
-000014b0: 6772 1600 0000 7208 0000 0072 0800 0000  gr....r....r....
-000014c0: 720a 0000 00da 0e77 7269 7465 5f69 6e66  r......write_inf
-000014d0: 6f5f 6d73 67c8 0000 0073 0600 0000 0002  o_msg....s......
-000014e0: 1201 1a01 7270 0000 0063 0100 0000 0000  ....rp...c......
-000014f0: 0000 0000 0000 0200 0000 0500 0000 4300  ..............C.
-00001500: 0000 7318 0000 0074 00a0 017c 00a0 0264  ..s....t...|...d
-00001510: 01a1 01a1 01a0 03a1 007d 017c 0153 0029  .........}.|.S.)
-00001520: 024e 7a05 7574 662d 3829 04da 0768 6173  .Nz.utf-8)...has
-00001530: 686c 6962 da03 6d64 35da 0665 6e63 6f64  hlib..md5..encod
-00001540: 65da 0968 6578 6469 6765 7374 2902 da06  e..hexdigest)...
-00001550: 7374 7269 6e67 5a07 7374 725f 6d64 3572  stringZ.str_md5r
-00001560: 0800 0000 7208 0000 0072 0a00 0000 da0e  ....r....r......
-00001570: 6765 745f 7374 7269 6e67 5f6d 6435 cf00  get_string_md5..
-00001580: 0000 7304 0000 0000 0114 0172 7600 0000  ..s........rv...
-00001590: fa05 2b31 2e36 6663 0400 0000 0000 0000  ..+1.6fc........
-000015a0: 0000 0000 0b00 0000 0900 0000 4300 0000  ............C...
-000015b0: 73ee 0000 0064 017c 029b 0064 0274 006a  s....d.|...d.t.j
-000015c0: 01a0 0264 0364 04a1 029b 0064 059d 057d  ...d.d.....d...}
-000015d0: 0474 037c 0074 046a 0583 0273 3064 067c  .t.|.t.j...s0d.|
-000015e0: 0069 017d 007c 00a0 06a1 0044 005d 765c  .i.}.|.....D.]v\
-000015f0: 027d 057d 067c 0564 066b 0272 4e64 077d  .}.}.|.d.k.rNd.}
-00001600: 056e 0a7c 059b 0064 089d 027d 057c 06a0  .n.|...d...}.|..
-00001610: 06a1 0044 005d 4c5c 027d 077d 0874 037c  ...D.]L\.}.}.t.|
-00001620: 0874 0783 0272 927c 0464 097c 059b 007c  .t...r.|.d.|...|
-00001630: 079b 0064 0a7c 087c 019b 009b 0464 059d  ...d.|.|.....d..
-00001640: 0637 007d 0471 607c 0464 097c 059b 007c  .7.}.q`|.d.|...|
-00001650: 079b 0064 0a7c 089b 0064 059d 0637 007d  ...d.|...d...7.}
-00001660: 0471 6071 3874 08a0 08a1 007d 097c 0974  .q`q8t.....}.|.t
-00001670: 0918 007d 0a7c 0961 097c 0464 0b7c 0a64  ...}.|.a.|.d.|.d
-00001680: 0c9b 0464 0d9d 0337 007d 047c 0372 ea7c  ...d...7.}.|.r.|
-00001690: 0464 097c 039b 0064 059d 0337 007d 047c  .d.|...d...7.}.|
-000016a0: 0453 0029 0e4e fa04 203d 3e20 7a07 205b  .S.).N.. => z. [
-000016b0: 4750 553a 20da 1443 5544 415f 5649 5349  GPU: ..CUDA_VISI
-000016c0: 424c 455f 4445 5649 4345 53da 0130 fa01  BLE_DEVICES..0..
-000016d0: 5dda 0374 6d70 722e 0000 0072 4b00 0000  ]..tmpr....rK...
-000016e0: 7a02 205b fa02 3a20 7a0b 205b 656c 6170  z. [..: z. [elap
-000016f0: 7365 643a 207a 032e 3366 7a02 735d 290a  sed: z..3fz.s]).
-00001700: 7247 0000 00da 0765 6e76 6972 6f6e da03  rG.....environ..
-00001710: 6765 7472 6200 0000 725f 0000 00da 0b64  getrb...r_.....d
-00001720: 6566 6175 6c74 6469 6374 7261 0000 00da  efaultdictra....
-00001730: 0566 6c6f 6174 da04 7469 6d65 da12 746c  .float..time..tl
-00001740: 5f6c 6173 745f 7072 696e 745f 7469 6d65  _last_print_time
-00001750: 290b 5a0b 6d65 7472 6963 5f64 6963 745a  ).Z.metric_dictZ
-00001760: 0c66 6c6f 6174 5f66 6f72 6d61 74da 066f  .float_format..o
-00001770: 7574 6469 725a 0a73 7566 6669 785f 7374  utdirZ.suffix_st
-00001780: 725a 0772 6574 5f73 7472 5a0b 6e61 6d65  rZ.ret_strZ.name
-00001790: 5f70 7265 6669 785a 0676 5f64 6963 7472  _prefixZ.v_dictr
-000017a0: 6500 0000 7266 0000 00da 036e 6f77 da07  e...rf.....now..
-000017b0: 656c 6170 7365 6472 0800 0000 7208 0000  elapsedr....r...
-000017c0: 0072 0a00 0000 da12 6765 745f 7072 696e  .r......get_prin
-000017d0: 745f 6469 6374 5f73 7472 d400 0000 7324  t_dict_str....s$
-000017e0: 0000 0000 041c 020c 0108 0210 0108 0106  ................
-000017f0: 020a 0110 010a 0120 021e 0208 0208 0104  ....... ........
-00001800: 0112 0204 0110 0172 8700 0000 6300 0000  .......r....c...
-00001810: 0000 0000 0000 0000 0000 0000 0003 0000  ................
-00001820: 0040 0000 0073 2a00 0000 6500 5a01 6400  .@...s*...e.Z.d.
-00001830: 5a02 6401 5a03 6402 6403 8400 5a04 6404  Z.d.Z.d.d...Z.d.
-00001840: 6405 8400 5a05 640a 6407 6408 8401 5a06  d...Z.d.d.d...Z.
-00001850: 6409 5300 290b da0c 4176 6572 6167 654d  d.S.)...AverageM
-00001860: 6574 6572 7a33 2043 6f6d 7075 7465 7320  eterz3 Computes 
-00001870: 616e 6420 7374 6f72 6573 2074 6865 2061  and stores the a
-00001880: 7665 7261 6765 2061 6e64 2063 7572 7265  verage and curre
-00001890: 6e74 2076 616c 7565 2063 0100 0000 0000  nt value c......
-000018a0: 0000 0000 0000 0100 0000 0200 0000 4300  ..............C.
-000018b0: 0000 730c 0000 007c 00a0 00a1 0001 0064  ..s....|.......d
-000018c0: 0053 0072 2500 0000 2901 723a 0000 0072  .S.r%...).r:...r
-000018d0: 2b00 0000 7208 0000 0072 0800 0000 720a  +...r....r....r.
-000018e0: 0000 0072 3800 0000 f600 0000 7302 0000  ...r8.......s...
-000018f0: 0000 017a 1541 7665 7261 6765 4d65 7465  ...z.AverageMete
-00001900: 722e 5f5f 696e 6974 5f5f 6301 0000 0000  r.__init__c.....
-00001910: 0000 0000 0000 0001 0000 0002 0000 0043  ...............C
-00001920: 0000 0073 1c00 0000 6401 7c00 5f00 6401  ...s....d.|._.d.
-00001930: 7c00 5f01 6401 7c00 5f02 6401 7c00 5f03  |._.d.|._.d.|._.
-00001940: 6402 5300 2903 7a16 2052 6573 6574 2061  d.S.).z. Reset a
-00001950: 6c6c 2073 7461 7469 7374 6963 7320 7201  ll statistics r.
-00001960: 0000 004e 2904 da03 7661 6cda 0361 7667  ...N)...val..avg
-00001970: da03 7375 6dda 0563 6f75 6e74 722b 0000  ..sum..countr+..
-00001980: 0072 0800 0000 7208 0000 0072 0a00 0000  .r....r....r....
-00001990: 723a 0000 00f9 0000 0073 0800 0000 0002  r:.......s......
-000019a0: 0601 0601 0601 7a12 4176 6572 6167 654d  ......z.AverageM
-000019b0: 6574 6572 2e72 6573 6574 7232 0000 0063  eter.resetr2...c
-000019c0: 0300 0000 0000 0000 0000 0000 0300 0000  ................
-000019d0: 0400 0000 4300 0000 7338 0000 007c 017c  ....C...s8...|.|
-000019e0: 005f 007c 0004 006a 017c 017c 0214 0037  ._.|...j.|.|...7
-000019f0: 0002 005f 017c 0004 006a 027c 0237 0002  ..._.|...j.|.7..
-00001a00: 005f 027c 006a 017c 006a 021b 007c 005f  ._.|.j.|.j...|._
-00001a10: 0364 0153 0029 027a 1320 5570 6461 7465  .d.S.).z. Update
-00001a20: 2073 7461 7469 7374 6963 7320 4e29 0472   statistics N).r
-00001a30: 8900 0000 728b 0000 0072 8c00 0000 728a  ....r....r....r.
-00001a40: 0000 0029 0372 2c00 0000 7289 0000 0072  ...).r,...r....r
-00001a50: 4000 0000 7208 0000 0072 0800 0000 720a  @...r....r....r.
-00001a60: 0000 0072 3500 0000 0001 0000 730a 0000  ...r5.......s...
-00001a70: 0000 0406 0112 010e 010e 017a 1341 7665  ...........z.Ave
-00001a80: 7261 6765 4d65 7465 722e 7570 6461 7465  rageMeter.update
-00001a90: 4e29 0172 3200 0000 2907 7242 0000 0072  N).r2...).rB...r
-00001aa0: 4300 0000 7244 0000 00da 075f 5f64 6f63  C...rD.....__doc
-00001ab0: 5f5f 7238 0000 0072 3a00 0000 7235 0000  __r8...r:...r5..
-00001ac0: 0072 0800 0000 7208 0000 0072 0800 0000  .r....r....r....
-00001ad0: 720a 0000 0072 8800 0000 f300 0000 730a  r....r........s.
-00001ae0: 0000 0008 0104 0208 0308 0900 fe72 8800  .............r..
-00001af0: 0000 6301 0000 0000 0000 0000 0000 0002  ..c.............
-00001b00: 0000 0003 0000 0043 0000 0073 1000 0000  .......C...s....
-00001b10: 7400 6a01 a002 7c00 a101 7d01 7c01 5300  t.j...|...}.|.S.
-00001b20: 2901 7a37 0a20 2072 616e 646f 6d5f 7374  ).z7.  random_st
-00001b30: 6174 652e 7368 7566 666c 6528 290a 0a20  ate.shuffle().. 
-00001b40: 203a 7061 7261 6d20 7365 6564 3a0a 2020   :param seed:.  
-00001b50: 3a72 6574 7572 6e3a 0a20 2029 03da 026e  :return:.  )...n
-00001b60: 70da 0672 616e 646f 6dda 0b52 616e 646f  p..random..Rando
-00001b70: 6d53 7461 7465 2902 da04 7365 6564 5a0c  mState)...seedZ.
-00001b80: 7261 6e64 6f6d 5f73 7461 7465 7208 0000  random_stater...
-00001b90: 0072 0800 0000 720a 0000 00da 0f67 6574  .r....r......get
-00001ba0: 5f72 616e 646f 6d73 7461 7465 0b01 0000  _randomstate....
-00001bb0: 7304 0000 0000 070c 0172 9200 0000 6300  s........r....c.
-00001bc0: 0000 0000 0000 0000 0000 0000 0000 0002  ................
-00001bd0: 0000 0040 0000 0073 1800 0000 6500 5a01  ...@...s....e.Z.
-00001be0: 6400 5a02 6401 5a03 6402 6403 8400 5a04  d.Z.d.Z.d.d...Z.
-00001bf0: 6404 5300 2905 da06 576f 726b 6572 7a6c  d.S.)...Workerzl
-00001c00: 0a20 2063 6f6d 6d61 6e64 5b30 5d2e 7374  .  command[0].st
-00001c10: 6172 7473 7769 7468 2828 2762 6173 6827  artswith(('bash'
-00001c20: 2c20 2929 3a0a 2020 7020 3d20 576f 726b  , )):.  p = Work
-00001c30: 6572 286e 616d 653d 2743 6f6d 6d61 6e64  er(name='Command
-00001c40: 2077 6f72 6b65 7227 2c20 6172 6773 3d28   worker', args=(
-00001c50: 636f 6d6d 616e 645b 305d 2c29 290a 2020  command[0],)).  
-00001c60: 702e 7374 6172 7428 290a 2020 6301 0000  p.start().  c...
-00001c70: 0000 0000 0000 0000 0002 0000 0004 0000  ................
-00001c80: 0043 0000 0073 4200 0000 7c00 6a00 6401  .C...sB...|.j.d.
-00001c90: 1900 a001 a100 7d01 6402 7402 6a03 a004  ......}.d.t.j...
-00001ca0: 7405 6a06 a101 9b00 6403 9d03 7c01 1700  t.j.....d...|...
-00001cb0: 7d01 7407 6404 7c01 1600 8301 0100 7402  }.t.d.|.......t.
-00001cc0: a008 7c01 a101 0100 6400 5300 2905 4e72  ..|.....d.S.).Nr
-00001cd0: 0100 0000 7a0c 6578 706f 7274 2050 4154  ....z.export PAT
-00001ce0: 483d 7a0a 3a24 5041 5448 2026 2620 7a02  H=z.:$PATH && z.
-00001cf0: 2573 2909 da05 5f61 7267 73da 0573 7472  %s)..._args..str
-00001d00: 6970 7247 0000 00da 0470 6174 68da 0764  iprG.....path..d
-00001d10: 6972 6e61 6d65 da03 7379 73da 0a65 7865  irname..sys..exe
-00001d20: 6375 7461 626c 6572 4600 0000 7248 0000  cutablerF...rH..
-00001d30: 0029 0272 2c00 0000 7249 0000 0072 0800  .).r,...rI...r..
-00001d40: 0000 7208 0000 0072 0a00 0000 da03 7275  ..r....r......ru
-00001d50: 6e1c 0100 0073 0a00 0000 0001 0e01 1a01  n....s..........
-00001d60: 0c02 0a01 7a0a 576f 726b 6572 2e72 756e  ....z.Worker.run
-00001d70: 4e29 0572 4200 0000 7243 0000 0072 4400  N).rB...rC...rD.
-00001d80: 0000 728d 0000 0072 9a00 0000 7208 0000  ..r....r....r...
-00001d90: 0072 0800 0000 7208 0000 0072 0a00 0000  .r....r....r....
-00001da0: 7293 0000 0016 0100 0073 0400 0000 0801  r........s......
-00001db0: 0405 7293 0000 0063 0000 0000 0000 0000  ..r....c........
-00001dc0: 0000 0000 0100 0000 0300 0000 4300 0000  ............C...
-00001dd0: 731c 0000 0074 006a 00a0 01a1 00a0 0264  s....t.j.......d
-00001de0: 01a1 0164 0064 0285 0219 007d 007c 0053  ...d.d.....}.|.S
-00001df0: 0029 034e 7a10 2559 256d 2564 5f25 4825  .).Nz.%Y%m%d_%H%
-00001e00: 4d25 535f 2566 e9fd ffff ff29 0372 0500  M%S_%f.....).r..
-00001e10: 0000 7285 0000 00da 0873 7472 6674 696d  ..r......strftim
-00001e20: 6529 01da 0874 696d 655f 7374 7272 0800  e)...time_strr..
-00001e30: 0000 7208 0000 0072 0a00 0000 da0c 6765  ..r....r......ge
-00001e40: 745f 7469 6d65 5f73 7472 2501 0000 7304  t_time_str%...s.
-00001e50: 0000 0000 0118 0172 9e00 0000 6301 0000  .......r....c...
-00001e60: 0000 0000 0000 0000 0002 0000 0006 0000  ................
-00001e70: 0043 0000 0073 1600 0000 7400 a001 6401  .C...s....t...d.
-00001e80: 7400 a002 7c00 a101 a102 7d01 7c01 5300  t...|.....}.|.S.
-00001e90: 2902 7a28 0a20 2065 6c61 7073 6564 203d  ).z(.  elapsed =
-00001ea0: 2074 696d 652e 7469 6d65 2829 202d 2074   time.time() - t
-00001eb0: 696d 655f 7374 6172 740a 2020 7a08 2548  ime_start.  z.%H
-00001ec0: 3a25 4d3a 2553 2903 7282 0000 0072 9c00  :%M:%S).r....r..
-00001ed0: 0000 da06 676d 7469 6d65 2902 7286 0000  ....gmtime).r...
-00001ee0: 0072 9d00 0000 7208 0000 0072 0800 0000  .r....r....r....
-00001ef0: 720a 0000 00da 0b74 696d 6532 7374 7269  r......time2stri
-00001f00: 6e67 2a01 0000 7304 0000 0000 0712 0172  ng*...s........r
-00001f10: a000 0000 6301 0000 0000 0000 0000 0000  ....c...........
-00001f20: 0004 0000 0002 0000 0043 0000 0073 1e00  .........C...s..
-00001f30: 0000 7400 a001 a100 7d01 7c01 7c00 1800  ..t.....}.|.|...
-00001f40: 7d02 7c02 9b00 6401 9d02 7d03 7c03 5300  }.|...d...}.|.S.
-00001f50: 2902 7a91 0a20 2070 7974 686f 6e20 3e3d  ).z..  python >=
-00001f60: 2033 2e37 0a0a 2020 3173 203d 2031 305e   3.7..  1s = 10^
-00001f70: 3920 6e73 0a0a 2020 7469 6d65 5f73 7461  9 ns..  time_sta
-00001f80: 7274 203d 2074 696d 652e 7065 7266 5f63  rt = time.perf_c
-00001f90: 6f75 6e74 6572 5f6e 7328 290a 2020 7469  ounter_ns().  ti
-00001fa0: 6d65 5f65 6e64 203d 2074 696d 652e 7065  me_end = time.pe
-00001fb0: 7266 5f63 6f75 6e74 6572 5f6e 7328 290a  rf_counter_ns().
-00001fc0: 2020 656c 6170 7365 6420 3d20 7469 6d65    elapsed = time
-00001fd0: 5f65 6e64 202d 2074 696d 655f 7374 6172  _end - time_star
-00001fe0: 740a 0a20 207a 0320 6e73 2902 7282 0000  t..  z. ns).r...
-00001ff0: 00da 0f70 6572 665f 636f 756e 7465 725f  ...perf_counter_
-00002000: 6e73 2904 da0a 7469 6d65 5f73 7461 7274  ns)...time_start
-00002010: da08 7469 6d65 5f65 6e64 7286 0000 0072  ..time_endr....r
-00002020: 9d00 0000 7208 0000 0072 0800 0000 720a  ....r....r....r.
-00002030: 0000 00da 0e74 696d 655f 6e73 3273 7472  .....time_ns2str
-00002040: 696e 6734 0100 0073 0800 0000 000b 0801  ing4...s........
-00002050: 0802 0a01 72a4 0000 0063 0200 0000 0000  ....r....c......
-00002060: 0000 0000 0000 0500 0000 0500 0000 4300  ..............C.
-00002070: 0000 735c 0000 0074 00a0 01a1 007d 027c  ..s\...t.....}.|
-00002080: 027c 0018 007d 0364 017c 019b 0064 027c  .|...}.d.|...d.|
-00002090: 039b 0064 039d 057d 047c 0164 046b 0872  ...d...}.|.d.k.r
-000020a0: 2e64 057d 017c 0464 067c 037c 011b 009b  .d.}.|.d.|.|....
-000020b0: 0064 039d 0337 007d 047c 0464 0764 057c  .d...7.}.|.d.d.|
-000020c0: 037c 011b 001b 009b 009d 0237 007d 047c  .|.........7.}.|
-000020d0: 0453 0029 087a 930a 2020 3173 203d 2031  .S.).z..  1s = 1
-000020e0: 305e 3620 7573 0a0a 2020 7469 6d65 5f73  0^6 us..  time_s
-000020f0: 7461 7274 203d 2074 696d 652e 7065 7266  tart = time.perf
-00002100: 5f63 6f75 6e74 6572 2829 0a20 2074 696d  _counter().  tim
-00002110: 655f 656e 6420 3d20 7469 6d65 2e70 6572  e_end = time.per
-00002120: 665f 636f 756e 7465 7228 290a 2020 656c  f_counter().  el
-00002130: 6170 7365 6420 3d20 7469 6d65 5f65 6e64  apsed = time_end
-00002140: 202d 2074 696d 655f 7374 6172 740a 0a20   - time_start.. 
-00002150: 2072 6574 7572 6e3a 2073 2028 7072 6563   return: s (prec
-00002160: 6973 6520 7573 290a 2020 7a0c 616c 6c20  ise us).  z.all 
-00002170: 2872 6570 6561 743d 7a03 293a 207a 0220  (repeat=z.): z. 
-00002180: 734e 6700 0000 0000 00f0 3f7a 0e2c 2061  sNg.......?z., a
-00002190: 6c6c 2f72 6570 6561 743a 207a 072c 2066  ll/repeat: z., f
-000021a0: 7073 3a20 2902 7282 0000 00da 0c70 6572  ps: ).r......per
-000021b0: 665f 636f 756e 7465 7229 0572 a200 0000  f_counter).r....
-000021c0: 5a0c 7265 7065 6174 5f74 696d 6573 72a3  Z.repeat_timesr.
-000021d0: 0000 0072 8600 0000 729d 0000 0072 0800  ...r....r....r..
-000021e0: 0000 7208 0000 0072 0a00 0000 da0e 7469  ..r....r......ti
-000021f0: 6d65 5f75 7332 7374 7269 6e67 4601 0000  me_us2stringF...
-00002200: 7310 0000 0000 0a08 0108 0212 0208 0104  s...............
-00002210: 0214 0216 0272 a600 0000 6301 0000 0000  .....r....c.....
-00002220: 0000 0000 0000 0004 0000 0005 0000 0043  ...............C
-00002230: 0000 0073 3200 0000 7400 a001 7402 6a03  ...s2...t...t.j.
-00002240: a004 7c00 a101 a101 7d01 7400 a005 a100  ..|.....}.t.....
-00002250: 7c01 1800 7d02 7c02 a006 a100 6401 1a00  |...}.|.....d...
-00002260: 7d03 7407 7c03 8301 5300 2902 4ee9 3c00  }.t.|...S.).N.<.
-00002270: 0000 2908 7205 0000 00da 0d66 726f 6d74  ..).r......fromt
-00002280: 696d 6573 7461 6d70 7247 0000 0072 9600  imestamprG...r..
-00002290: 0000 da08 6765 746d 7469 6d65 7285 0000  ....getmtimer...
-000022a0: 00da 0d74 6f74 616c 5f73 6563 6f6e 6473  ...total_seconds
-000022b0: 720d 0000 0029 04da 0866 696c 6570 6174  r....)...filepat
-000022c0: 685a 096d 6f64 695f 7469 6d65 5a0a 6d6f  hZ.modi_timeZ.mo
-000022d0: 6469 5f69 6e74 6572 5a0c 6d6f 6469 5f6d  di_interZ.modi_m
-000022e0: 696e 7574 6573 7208 0000 0072 0800 0000  inutesr....r....
-000022f0: 720a 0000 00da 1667 6574 5f74 696d 655f  r......get_time_
-00002300: 7369 6e63 655f 6c61 7374 5f6d 645e 0100  since_last_md^..
-00002310: 0073 0800 0000 0002 1201 0c01 0c01 72ac  .s............r.
-00002320: 0000 0063 0000 0000 0000 0000 0000 0000  ...c............
-00002330: 0000 0000 0400 0000 4000 0000 732c 0000  ........@...s,..
-00002340: 0065 005a 0164 005a 0264 0a64 0264 0384  .e.Z.d.Z.d.d.d..
-00002350: 015a 0365 0464 0b64 0564 0684 0183 015a  .Z.e.d.d.d.....Z
-00002360: 0564 0764 0884 005a 0664 0953 0029 0cda  .d.d...Z.d.S.)..
-00002370: 0c43 6972 636c 654e 756d 6265 72e9 0400  .CircleNumber...
-00002380: 0000 6302 0000 0000 0000 0000 0000 0002  ..c.............
-00002390: 0000 0002 0000 0043 0000 0073 1000 0000  .......C...s....
-000023a0: 7c01 7c00 5f00 6401 7c00 5f01 6400 5300  |.|._.d.|._.d.S.
-000023b0: 2902 4ee9 ffff ffff 2902 da0b 6d61 785f  ).N.....)...max_
-000023c0: 746f 5f6b 6565 70da 0763 7572 5f6e 756d  to_keep..cur_num
-000023d0: 2902 722c 0000 0072 b000 0000 7208 0000  ).r,...r....r...
-000023e0: 0072 0800 0000 720a 0000 0072 3800 0000  .r....r....r8...
-000023f0: 6801 0000 7306 0000 0000 0106 0106 017a  h...s..........z
-00002400: 1543 6972 636c 654e 756d 6265 722e 5f5f  .CircleNumber.__
-00002410: 696e 6974 5f5f 720c 0000 0063 0200 0000  init__r....c....
-00002420: 0000 0000 0000 0000 0300 0000 0300 0000  ................
-00002430: 4300 0000 7328 0000 007c 0074 006b 0772  C...s(...|.t.k.r
-00002440: 1c74 017c 0164 018d 017d 027c 0274 007c  .t.|.d...}.|.t.|
-00002450: 003c 006e 0874 007c 0019 007d 027c 0253  .<.n.t.|...}.|.S
-00002460: 0029 024e a901 72b0 0000 0029 02da 0c5f  .).N..r....)..._
-00002470: 6369 7263 6c65 5f64 6963 7472 ad00 0000  circle_dictr....
-00002480: 2903 725d 0000 0072 b000 0000 5a0c 6e61  ).r]...r....Z.na
-00002490: 6d65 645f 6369 7263 6c65 7208 0000 0072  med_circler....r
-000024a0: 0800 0000 720a 0000 00da 1067 6574 5f6e  ....r......get_n
-000024b0: 616d 6564 5f63 6972 636c 656d 0100 0073  amed_circlem...s
-000024c0: 0a00 0000 0004 0801 0a01 0a02 0802 7a1d  ..............z.
-000024d0: 4369 7263 6c65 4e75 6d62 6572 2e67 6574  CircleNumber.get
-000024e0: 5f6e 616d 6564 5f63 6972 636c 6563 0100  _named_circlec..
-000024f0: 0000 0000 0000 0000 0000 0200 0000 0300  ................
-00002500: 0000 4300 0000 732a 0000 007c 0004 006a  ..C...s*...|...j
-00002510: 0064 0137 0002 005f 007c 006a 007c 006a  .d.7..._.|.j.|.j
-00002520: 0116 007c 005f 007c 006a 0064 029b 047d  ...|._.|.j.d...}
-00002530: 017c 0153 0029 034e 7232 0000 005a 0330  .|.S.).Nr2...Z.0
-00002540: 3264 2902 72b1 0000 0072 b000 0000 2902  2d).r....r....).
-00002550: 722c 0000 0072 0900 0000 7208 0000 0072  r,...r....r....r
-00002560: 0800 0000 720a 0000 00da 0a67 6574 5f6e  ....r......get_n
-00002570: 756d 6265 7279 0100 0073 0800 0000 0001  umbery...s......
-00002580: 0e01 0e01 0a01 7a17 4369 7263 6c65 4e75  ......z.CircleNu
-00002590: 6d62 6572 2e67 6574 5f6e 756d 6265 724e  mber.get_numberN
-000025a0: 2901 72ae 0000 0029 0172 0c00 0000 2907  ).r....).r....).
-000025b0: 7242 0000 0072 4300 0000 7244 0000 0072  rB...rC...rD...r
-000025c0: 3800 0000 da0c 7374 6174 6963 6d65 7468  8.....staticmeth
-000025d0: 6f64 72b4 0000 0072 b500 0000 7208 0000  odr....r....r...
-000025e0: 0072 0800 0000 7208 0000 0072 0a00 0000  .r....r....r....
-000025f0: 72ad 0000 0067 0100 0073 0a00 0000 0801  r....g...s......
-00002600: 0a05 0202 00ff 0c0b 72ad 0000 0063 0000  ........r....c..
-00002610: 0000 0000 0000 0000 0000 0000 0000 0400  ................
-00002620: 0000 4000 0000 7336 0000 0065 005a 0164  ..@...s6...e.Z.d
-00002630: 005a 0264 0c64 0364 0484 015a 0365 0464  .Z.d.d.d...Z.e.d
-00002640: 0d64 0664 0784 0183 015a 0564 0864 0984  .d.d.....Z.d.d..
-00002650: 005a 0664 0e64 0a64 0b84 015a 0764 0153  .Z.d.d.d...Z.d.S
-00002660: 0029 0fda 094d 6178 546f 4b65 6570 4e54  .)...MaxToKeepNT
-00002670: 6303 0000 0000 0000 0000 0000 0003 0000  c...............
-00002680: 0003 0000 0043 0000 0073 2800 0000 7c01  .....C...s(...|.
-00002690: 7c00 5f00 6700 7c00 5f01 7c02 7224 7c01  |._.g.|._.|.r$|.
-000026a0: 6401 6b04 7224 7402 7c01 6402 8d01 7c00  d.k.r$t.|.d...|.
-000026b0: 5f03 6400 5300 2903 4e72 0100 0000 72b2  _.d.S.).Nr....r.
-000026c0: 0000 0029 0472 b000 0000 da12 7265 6365  ...).r......rece
-000026d0: 6e74 5f63 6865 636b 706f 696e 7473 72ad  nt_checkpointsr.
-000026e0: 0000 00da 1163 6972 636c 655f 6e75 6d62  .....circle_numb
-000026f0: 6572 5f67 656e 2903 722c 0000 0072 b000  er_gen).r,...r..
-00002700: 0000 da11 7573 655f 6369 7263 6c65 5f6e  ....use_circle_n
-00002710: 756d 6265 7272 0800 0000 7208 0000 0072  umberr....r....r
-00002720: 0a00 0000 7238 0000 0083 0100 0073 0a00  ....r8.......s..
-00002730: 0000 0003 0601 0602 0c01 0c01 7a12 4d61  ............z.Ma
-00002740: 7854 6f4b 6565 702e 5f5f 696e 6974 5f5f  xToKeep.__init__
-00002750: 720c 0000 0063 0300 0000 0000 0000 0000  r....c..........
-00002760: 0000 0400 0000 0400 0000 4300 0000 732a  ..........C...s*
-00002770: 0000 007c 0074 006b 0772 1e74 017c 017c  ...|.t.k.r.t.|.|
-00002780: 0264 018d 027d 037c 0374 007c 003c 006e  .d...}.|.t.|.<.n
-00002790: 0874 007c 0019 007d 037c 0353 0029 024e  .t.|...}.|.S.).N
-000027a0: 2902 72b0 0000 0072 ba00 0000 2902 da0f  ).r....r....)...
-000027b0: 5f4d 6178 546f 4b65 6570 5f64 6963 7472  _MaxToKeep_dictr
-000027c0: b700 0000 2904 725d 0000 0072 b000 0000  ....).r]...r....
-000027d0: 72ba 0000 005a 0f6e 616d 6564 5f6d 6178  r....Z.named_max
-000027e0: 746f 6b65 6570 7208 0000 0072 0800 0000  tokeepr....r....
-000027f0: 720a 0000 00da 1567 6574 5f6e 616d 6564  r......get_named
-00002800: 5f6d 6178 5f74 6f5f 6b65 6570 8d01 0000  _max_to_keep....
-00002810: 730a 0000 0000 0508 010c 010a 0208 027a  s..............z
-00002820: 1f4d 6178 546f 4b65 6570 2e67 6574 5f6e  .MaxToKeep.get_n
-00002830: 616d 6564 5f6d 6178 5f74 6f5f 6b65 6570  amed_max_to_keep
-00002840: 6302 0000 0000 0000 0000 0000 0003 0000  c...............
-00002850: 0003 0000 0043 0000 0073 6400 0000 7c00  .....C...sd...|.
-00002860: 6a00 6400 6b09 7260 7c00 6a01 a002 7c01  j.d.k.r`|.j...|.
-00002870: a101 0100 7403 7c00 6a01 8301 7c00 6a00  ....t.|.j...|.j.
-00002880: 6b04 7260 7c00 6a01 a004 6401 a101 7d02  k.r`|.j...d...}.
-00002890: 7405 6a06 a007 7c02 a101 7260 7405 6a06  t.j...|...r`t.j.
-000028a0: a008 7c02 a101 7256 7409 a00a 7c02 a101  ..|...rVt...|...
-000028b0: 0100 6e0a 7405 a00b 7c02 a101 0100 6400  ..n.t...|.....d.
-000028c0: 5300 2902 4e72 0100 0000 290c 72b0 0000  S.).Nr....).r...
-000028d0: 0072 b800 0000 da06 6170 7065 6e64 da03  .r......append..
-000028e0: 6c65 6eda 0370 6f70 7247 0000 0072 9600  len..poprG...r..
-000028f0: 0000 da06 6578 6973 7473 da05 6973 6469  ....exists..isdi
-00002900: 72da 0673 6875 7469 6cda 0672 6d74 7265  r..shutil..rmtre
-00002910: 65da 0672 656d 6f76 6529 0372 2c00 0000  e..remove).r,...
-00002920: da09 6669 6c65 5f70 6174 685a 0e66 696c  ..file_pathZ.fil
-00002930: 655f 746f 5f64 656c 6574 6572 0800 0000  e_to_deleter....
-00002940: 7208 0000 0072 0a00 0000 da04 7374 6570  r....r......step
-00002950: 9a01 0000 7312 0000 0000 010a 010c 0110  ....s...........
-00002960: 010c 010c 010c 010c 020a 017a 0e4d 6178  ...........z.Max
-00002970: 546f 4b65 6570 2e73 7465 7063 0300 0000  ToKeep.stepc....
-00002980: 0000 0000 0000 0000 0800 0000 0900 0000  ................
-00002990: 4300 0000 73dc 0000 0074 006a 017c 0164  C...s....t.j.|.d
-000029a0: 0164 028d 0201 007c 006a 02a0 03a1 007d  .d.....|.j.....}
-000029b0: 0374 04a0 047c 019b 0064 039d 02a1 017d  .t...|...d.....}
-000029c0: 047c 0444 005d 1a7d 0574 006a 05a0 067c  .|.D.].}.t.j...|
-000029d0: 05a1 0172 2c74 00a0 077c 05a1 0101 0071  ...r,t...|.....q
-000029e0: 2c74 087c 019b 0064 047c 039b 0064 059d  ,t.|...d.|...d..
-000029f0: 0464 0683 028f 167d 067c 06a0 097c 029b  .d.....}.|...|..
-00002a00: 0064 079d 02a1 0101 0057 0035 0051 0052  .d.......W.5.Q.R
-00002a10: 0058 0074 006a 05a0 0a7c 0174 0b7c 0383  .X.t.j...|.t.|..
-00002a20: 01a1 027d 077c 00a0 0c7c 07a1 0101 0074  ...}.|...|.....t
-00002a30: 006a 017c 0764 0164 028d 0201 007c 0264  .j.|.d.d.....|.d
-00002a40: 006b 0972 d874 087c 079b 0064 089d 0264  .k.r.t.|...d...d
-00002a50: 0683 028f 167d 067c 06a0 097c 029b 0064  .....}.|...|...d
-00002a60: 079d 02a1 0101 0057 0035 0051 0052 0058  .......W.5.Q.R.X
-00002a70: 007c 0753 0029 094e 54a9 01da 0865 7869  .|.S.).NT....exi
-00002a80: 7374 5f6f 6b7a 0e2f 3072 6563 656e 745f  st_okz./0recent_
-00002a90: 2a2e 7478 747a 092f 3072 6563 656e 745f  *.txtz./0recent_
-00002aa0: 7a04 2e74 7874 726c 0000 0072 4500 0000  z..txtrl...rE...
-00002ab0: 726b 0000 0029 0d72 4700 0000 da08 6d61  rk...).rG.....ma
-00002ac0: 6b65 6469 7273 72b9 0000 0072 b500 0000  kedirsr....r....
-00002ad0: da04 676c 6f62 7296 0000 00da 0669 7366  ..globr......isf
-00002ae0: 696c 6572 c400 0000 7211 0000 0072 6d00  iler....r....rm.
-00002af0: 0000 da04 6a6f 696e 7229 0000 0072 c600  ....joinr)...r..
-00002b00: 0000 2908 722c 0000 00da 0872 6f6f 745f  ..).r,.....root_
-00002b10: 6469 7272 6f00 0000 5a0a 6375 725f 6e75  dirro...Z.cur_nu
-00002b20: 6d62 6572 5a0a 6c61 7374 5f66 696c 6573  mberZ.last_files
-00002b30: da09 6c61 7374 5f66 696c 6572 1600 0000  ..last_filer....
-00002b40: da07 6473 745f 6469 7272 0800 0000 7208  ..dst_dirr....r.
-00002b50: 0000 0072 0a00 0000 da17 7374 6570 5f61  ...r......step_a
-00002b60: 6e64 5f72 6574 5f63 6972 636c 655f 6469  nd_ret_circle_di
-00002b70: 72a6 0100 0073 1e00 0000 0003 0e01 0a01  r....s..........
-00002b80: 1001 0801 0c01 0c01 1801 1a02 1202 0a02  ................
-00002b90: 0e01 0801 1201 1a02 7a21 4d61 7854 6f4b  ........z!MaxToK
-00002ba0: 6565 702e 7374 6570 5f61 6e64 5f72 6574  eep.step_and_ret
-00002bb0: 5f63 6972 636c 655f 6469 7229 024e 5429  _circle_dir).NT)
-00002bc0: 0272 0c00 0000 5429 014e 2908 7242 0000  .r....T).N).rB..
-00002bd0: 0072 4300 0000 7244 0000 0072 3800 0000  .rC...rD...r8...
-00002be0: 72b6 0000 0072 bc00 0000 72c6 0000 0072  r....r....r....r
-00002bf0: d000 0000 7208 0000 0072 0800 0000 7208  ....r....r....r.
-00002c00: 0000 0072 0a00 0000 72b7 0000 0082 0100  ...r....r.......
-00002c10: 0073 1400 0000 0802 0001 00fe 0a0a 0202  .s..............
-00002c20: 0001 00fe 0c0c 080e 00fe 72b7 0000 0063  ..........r....c
-00002c30: 0200 0000 0000 0000 0000 0000 0b00 0000  ................
-00002c40: 0600 0000 4300 0000 7380 0000 0064 0164  ....C...s....d.d
-00002c50: 006c 007d 027c 02a0 017c 0164 02a1 027d  .l.}.|...|.d...}
-00002c60: 0374 0274 036a 04a0 057c 00a1 0183 017d  .t.t.j...|.....}
-00002c70: 0474 03a0 067c 00a1 0144 005d 445c 037d  .t...|...D.]D\.}
-00002c80: 057d 067d 077c 0744 005d 347d 0874 036a  .}.}.|.D.]4}.t.j
-00002c90: 04a0 077c 057c 08a1 027d 097c 097c 0464  ...|.|...}.|.|.d
-00002ca0: 0085 0219 00a0 0874 036a 046a 09a1 017d  .......t.j.j...}
-00002cb0: 0a7c 03a0 0a7c 097c 0aa1 0201 0071 3c71  .|...|.|.....q<q
-00002cc0: 2e7c 03a0 0ba1 0001 0064 0053 0029 034e  .|.......d.S.).N
-00002cd0: 7201 0000 0072 6c00 0000 290c da07 7a69  r....rl...)...zi
-00002ce0: 7066 696c 65da 075a 6970 4669 6c65 72be  pfile..ZipFiler.
-00002cf0: 0000 0072 4700 0000 7296 0000 0072 9700  ...rG...r....r..
-00002d00: 0000 da04 7761 6c6b 72cc 0000 0072 9500  ....walkr....r..
-00002d10: 0000 da03 7365 7072 6d00 0000 723e 0000  ....seprm...r>..
-00002d20: 0029 0bda 0a73 6f75 7263 655f 6469 725a  .)...source_dirZ
-00002d30: 0f6f 7574 7075 745f 6669 6c65 6e61 6d65  .output_filename
-00002d40: 72d1 0000 00da 047a 6970 665a 0770 7265  r......zipfZ.pre
-00002d50: 5f6c 656e da06 7061 7265 6e74 da08 6469  _len..parent..di
-00002d60: 726e 616d 6573 da09 6669 6c65 6e61 6d65  rnames..filename
-00002d70: 73da 0866 696c 656e 616d 655a 0870 6174  s..filenameZ.pat
-00002d80: 6866 696c 655a 0761 7263 6e61 6d65 7208  hfileZ.arcnamer.
-00002d90: 0000 0072 0800 0000 720a 0000 00da 086d  ...r....r......m
-00002da0: 616b 655f 7a69 70be 0100 0073 1200 0000  ake_zip....s....
-00002db0: 0002 0801 0c01 1001 1401 0801 0e01 1601  ................
-00002dc0: 1001 72db 0000 0063 0200 0000 0000 0000  ..r....c........
-00002dd0: 0000 0000 0400 0000 0500 0000 4300 0000  ............C...
-00002de0: 7362 0000 0074 006a 017c 0164 0164 028d  sb...t.j.|.d.d..
-00002df0: 0201 0074 02a0 037c 00a1 0173 1c74 0482  ...t...|...s.t..
-00002e00: 0174 02a0 057c 0064 03a1 027d 027c 02a0  .t...|.d...}.|..
-00002e10: 06a1 0044 005d 107d 037c 02a0 077c 037c  ...D.].}.|...|.|
-00002e20: 01a1 0201 0071 307c 02a0 08a1 0001 0074  .....q0|.......t
-00002e30: 0964 047c 009b 0064 057c 019b 009d 0483  .d.|...d.|......
-00002e40: 0101 0064 0053 0029 064e 5472 c700 0000  ...d.S.).NTr....
-00002e50: da01 727a 0655 6e7a 6970 207a 0420 746f  ..rz.Unzip z. to
-00002e60: 2029 0a72 4700 0000 72c9 0000 0072 d100   ).rG...r....r..
-00002e70: 0000 da0a 6973 5f7a 6970 6669 6c65 da0e  ....is_zipfile..
-00002e80: 4173 7365 7274 696f 6e45 7272 6f72 72d2  AssertionErrorr.
-00002e90: 0000 00da 086e 616d 656c 6973 74da 0765  .....namelist..e
-00002ea0: 7874 7261 6374 723e 0000 0072 4600 0000  xtractr>...rF...
-00002eb0: 2904 5a08 7a69 705f 6669 6c65 72cf 0000  ).Z.zip_filer...
-00002ec0: 005a 0266 7a72 3100 0000 7208 0000 0072  .Z.fzr1...r....r
-00002ed0: 0800 0000 720a 0000 00da 0a75 6e7a 6970  ....r......unzip
-00002ee0: 5f66 696c 65cb 0100 0073 0e00 0000 0001  _file....s......
-00002ef0: 0e01 0e02 0c01 0c01 0e01 0801 72e1 0000  ............r...
-00002f00: 00a9 02fa 052a 2e6a 7067 fa05 2a2e 706e  .....*.jpg..*.pn
-00002f10: 6754 4663 0500 0000 0000 0000 0000 0000  gTFc............
-00002f20: 0800 0000 0700 0000 4300 0000 738a 0000  ........C...s...
-00002f30: 0074 007c 0174 0174 0266 0283 0273 147c  .t.|.t.t.f...s.|
-00002f40: 0167 017d 0167 007d 057c 0144 005d 3a7d  .g.}.g.}.|.D.]:}
-00002f50: 067c 0472 3e7c 05a0 0374 0174 047c 0083  .|.r>|...t.t.|..
-00002f60: 01a0 057c 06a1 0183 01a1 0101 0071 1c7c  ...|.........q.|
-00002f70: 05a0 0374 0174 047c 0083 01a0 067c 06a1  ...t.t.|.....|..
-00002f80: 0183 01a1 0101 0071 1c7c 0272 6c74 077c  .......q.|.rlt.|
-00002f90: 0564 0164 0284 0064 038d 027d 057c 0372  .d.d...d...}.|.r
-00002fa0: 8664 0464 0284 007d 0774 0174 087c 077c  .d.d...}.t.t.|.|
-00002fb0: 0583 0283 017d 057c 0553 0029 054e 6301  .....}.|.S.).Nc.
-00002fc0: 0000 0000 0000 0000 0000 0001 0000 0001  ................
-00002fd0: 0000 0053 0000 0073 0600 0000 7c00 6a00  ...S...s....|.j.
-00002fe0: 5300 7225 0000 0029 0172 5d00 0000 2901  S.r%...).r]...).
-00002ff0: 7296 0000 0072 0800 0000 7208 0000 0072  r....r....r....r
-00003000: 0a00 0000 da08 3c6c 616d 6264 613e e501  ......<lambda>..
-00003010: 0000 f300 0000 007a 2867 6574 5f66 696c  .......z(get_fil
-00003020: 656c 6973 745f 7265 6375 7273 6976 652e  elist_recursive.
-00003030: 3c6c 6f63 616c 733e 2e3c 6c61 6d62 6461  <locals>.<lambda
-00003040: 3e29 01da 036b 6579 6301 0000 0000 0000  >)...keyc.......
-00003050: 0000 0000 0001 0000 0002 0000 0053 0000  .............S..
-00003060: 0073 0800 0000 7400 7c00 8301 5300 7225  .s....t.|...S.r%
-00003070: 0000 00a9 0172 2900 0000 a901 da01 7872  .....r).......xr
-00003080: 0800 0000 7208 0000 0072 0a00 0000 72e5  ....r....r....r.
-00003090: 0000 00e8 0100 0072 e600 0000 2909 7262  .......r....).rb
-000030a0: 0000 00da 046c 6973 74da 0574 7570 6c65  .....list..tuple
-000030b0: da06 6578 7465 6e64 7202 0000 00da 0572  ..extendr......r
-000030c0: 676c 6f62 72ca 0000 00da 0673 6f72 7465  globr......sorte
-000030d0: 64da 036d 6170 2908 5a09 6469 7265 6374  d..map).Z.direct
-000030e0: 6f72 79da 0365 7874 da04 736f 7274 5a06  ory..ext..sortZ.
-000030f0: 746f 5f73 7472 da09 7265 6375 7273 6976  to_str..recursiv
-00003100: 655a 0966 696c 655f 6c69 7374 5a04 5f65  eZ.file_listZ._e
-00003110: 7874 da04 6675 6e63 7208 0000 0072 0800  xt..funcr....r..
-00003120: 0000 720a 0000 00da 1667 6574 5f66 696c  ..r......get_fil
-00003130: 656c 6973 745f 7265 6375 7273 6976 65d6  elist_recursive.
-00003140: 0100 0073 1a00 0000 0006 0e01 0601 0401  ...s............
-00003150: 0801 0401 1a02 1a01 0401 1002 0401 0801  ................
-00003160: 0e02 72f5 0000 0063 0100 0000 0000 0000  ..r....c........
-00003170: 0000 0000 0500 0000 0600 0000 4300 0000  ............C...
-00003180: 735e 0000 0074 007c 0083 017d 0174 01a0  s^...t.|...}.t..
-00003190: 017c 01a1 017d 0274 027c 0264 0164 028d  .|...}.t.|.d.d..
-000031a0: 0244 005d 3a5c 027d 037d 0474 037c 0474  .D.]:\.}.}.t.|.t
-000031b0: 0483 0272 387c 0464 0319 007d 0474 056a  ...r8|.d...}.t.j
-000031c0: 06a0 077c 04a1 0173 1e74 0864 047c 039b  ...|...s.t.d.|..
-000031d0: 0064 057c 049b 009d 0483 0101 0071 1e64  .d.|.........q.d
-000031e0: 0053 0029 064e 7232 0000 0029 0172 3600  .S.).Nr2...).r6.
-000031f0: 0000 7201 0000 007a 0745 7272 6f72 3a20  ..r....z.Error: 
-00003200: 727d 0000 0029 09da 1a72 6561 645f 696d  r}...)...read_im
-00003210: 6167 655f 6c69 7374 5f66 726f 6d5f 6669  age_list_from_fi
-00003220: 6c65 7372 3400 0000 da09 656e 756d 6572  lesr4.....enumer
-00003230: 6174 6572 6200 0000 72eb 0000 0072 4700  aterb...r....rG.
-00003240: 0000 7296 0000 0072 c000 0000 7246 0000  ..r....r....rF..
-00003250: 0029 05da 0a69 6d61 6765 5f66 696c 65da  .)...image_file.
-00003260: 0a69 6d61 6765 5f6c 6973 7472 2a00 0000  .image_listr*...
-00003270: da03 6964 78da 0a69 6d61 6765 5f70 6174  ..idx..image_pat
-00003280: 6872 0800 0000 7208 0000 0072 0a00 0000  hr....r....r....
-00003290: da19 6368 6563 6b5f 696d 6167 655f 6c69  ..check_image_li
-000032a0: 7374 5f76 616c 6964 6974 79ee 0100 0073  st_validity....s
-000032b0: 1000 0000 0001 0801 0a01 1401 0a01 0801  ................
-000032c0: 0c01 1601 72fc 0000 00a9 0372 e300 0000  ....r......r....
-000032d0: 72e4 0000 007a 062a 2e6a 7065 6763 0300  r....z.*.jpegc..
-000032e0: 0000 0000 0000 0000 0000 0700 0000 0a00  ................
-000032f0: 0000 4300 0000 739c 0000 0074 007c 0074  ..C...s....t.|.t
-00003300: 0174 0266 0283 0273 147c 0067 017d 0067  .t.f...s.|.g.}.g
-00003310: 007d 037c 0044 005d 647d 0474 036a 04a0  .}.|.D.]d}.t.j..
-00003320: 057c 04a1 0172 4c74 067c 047c 0264 018d  .|...rLt.|.|.d..
-00003330: 027d 0574 0174 0764 0264 0384 007c 0583  .}.t.t.d.d...|..
-00003340: 0283 017d 056e 2a74 087c 0483 018f 0e7d  ...}.n*t.|.....}
-00003350: 067c 06a0 09a1 007d 0557 0035 0051 0052  .|.....}.W.5.Q.R
-00003360: 0058 0064 0464 0584 007c 0544 0083 017d  .X.d.d...|.D...}
-00003370: 057c 03a0 0a7c 05a1 0101 0071 1c7c 0172  .|...|.....q.|.r
-00003380: 9874 0174 0764 0664 0384 007c 0383 0283  .t.t.d.d...|....
-00003390: 017d 037c 0353 0029 077a 6e0a 0a20 203a  .}.|.S.).zn..  :
-000033a0: 7061 7261 6d20 696d 6167 655f 6c69 7374  param image_list
-000033b0: 5f66 696c 653a 205b 696d 6167 655f 6c69  _file: [image_li
-000033c0: 7374 2e74 7874 2c20 5d20 6f72 205b 696d  st.txt, ] or [im
-000033d0: 6167 655f 6469 722c 205d 0a20 203a 7061  age_dir, ].  :pa
-000033e0: 7261 6d20 636f 6d70 7265 7373 3a0a 2020  ram compress:.  
-000033f0: 3a70 6172 616d 2065 7874 3a0a 2020 3a72  :param ext:.  :r
-00003400: 6574 7572 6e3a 0a20 2029 0172 f100 0000  eturn:.  ).r....
-00003410: 6301 0000 0000 0000 0000 0000 0001 0000  c...............
-00003420: 0002 0000 0053 0000 0073 0a00 0000 7400  .....S...s....t.
-00003430: 7c00 8301 6701 5300 7225 0000 0072 e800  |...g.S.r%...r..
-00003440: 0000 72e9 0000 0072 0800 0000 7208 0000  ..r....r....r...
-00003450: 0072 0a00 0000 72e5 0000 000a 0200 0072  .r....r........r
-00003460: e600 0000 7a2c 7265 6164 5f69 6d61 6765  ....z,read_image
-00003470: 5f6c 6973 745f 6672 6f6d 5f66 696c 6573  _list_from_files
-00003480: 2e3c 6c6f 6361 6c73 3e2e 3c6c 616d 6264  .<locals>.<lambd
-00003490: 613e 6301 0000 0000 0000 0000 0000 0002  a>c.............
-000034a0: 0000 0005 0000 0053 0000 0073 1a00 0000  .......S...s....
-000034b0: 6700 7c00 5d12 7d01 7c01 a000 a100 a001  g.|.].}.|.......
-000034c0: 6400 a101 9102 7104 5300 2901 fa01 2029  d.....q.S.)... )
-000034d0: 0272 9500 0000 da05 7370 6c69 7429 02da  .r......split)..
-000034e0: 022e 3072 6600 0000 7208 0000 0072 0800  ..0rf...r....r..
-000034f0: 0000 720a 0000 00da 0a3c 6c69 7374 636f  ..r......<listco
-00003500: 6d70 3e0e 0200 0073 0400 0000 0600 0200  mp>....s........
-00003510: 7a2e 7265 6164 5f69 6d61 6765 5f6c 6973  z.read_image_lis
-00003520: 745f 6672 6f6d 5f66 696c 6573 2e3c 6c6f  t_from_files.<lo
-00003530: 6361 6c73 3e2e 3c6c 6973 7463 6f6d 703e  cals>.<listcomp>
-00003540: 6301 0000 0000 0000 0000 0000 0001 0000  c...............
-00003550: 0002 0000 0053 0000 0073 1800 0000 7400  .....S...s....t.
-00003560: 7c00 8301 6401 6b02 7214 7c00 6402 1900  |...d.k.r.|.d...
-00003570: 5300 7c00 5300 2903 4e72 3200 0000 7201  S.|.S.).Nr2...r.
-00003580: 0000 0029 0172 be00 0000 72e9 0000 0072  ...).r....r....r
-00003590: 0800 0000 7208 0000 0072 0a00 0000 72e5  ....r....r....r.
-000035a0: 0000 0012 0200 0072 e600 0000 290b 7262  .......r....).rb
-000035b0: 0000 0072 eb00 0000 72ec 0000 0072 4700  ...r....r....rG.
-000035c0: 0000 7296 0000 0072 c100 0000 72f5 0000  ..r....r....r...
-000035d0: 0072 f000 0000 7211 0000 00da 0972 6561  .r....r......rea
-000035e0: 646c 696e 6573 72ed 0000 0029 07da 0f69  dlinesr....)...i
-000035f0: 6d61 6765 5f6c 6973 745f 6669 6c65 da08  mage_list_file..
-00003600: 636f 6d70 7265 7373 72f1 0000 005a 0e61  compressr....Z.a
-00003610: 6c6c 5f69 6d61 6765 5f6c 6973 7472 f800  ll_image_listr..
-00003620: 0000 72f9 0000 0072 1600 0000 7208 0000  ..r....r....r...
-00003630: 0072 0800 0000 720a 0000 0072 f600 0000  .r....r....r....
-00003640: f901 0000 731c 0000 0000 0a0e 0106 0204  ....s...........
-00003650: 0108 010c 010c 0114 020a 0112 010e 010c  ................
-00003660: 0204 0112 0172 f600 0000 6303 0000 0000  .....r....c.....
-00003670: 0000 0000 0000 0004 0000 0006 0000 0043  ...............C
-00003680: 0000 0073 2600 0000 7c02 7d03 7a10 7400  ...s&...|.}.z.t.
-00003690: 7c01 8301 7c00 8301 7d03 5700 6e0c 0100  |...|...}.W.n...
-000036a0: 0100 0100 5900 6e02 5800 7c03 5300 7225  ....Y.n.X.|.S.r%
-000036b0: 0000 0072 0300 0000 2904 7253 0000 0072  ...r....).rS...r
-000036c0: 5400 0000 da07 6465 6661 756c 74da 0372  T.....default..r
-000036d0: 6574 7208 0000 0072 0800 0000 720a 0000  etr....r....r...
-000036e0: 0072 5900 0000 1602 0000 730c 0000 0000  .rY.......s.....
-000036f0: 0104 0102 0110 0106 0106 0172 5900 0000  ...........rY...
-00003700: 6300 0000 0000 0000 0000 0000 0000 0000  c...............
-00003710: 0002 0000 0040 0000 0073 2000 0000 6500  .....@...s ...e.
-00003720: 5a01 6400 5a02 6401 5a03 6402 6403 8400  Z.d.Z.d.Z.d.d...
-00003730: 5a04 6404 6405 8400 5a05 6406 5300 2907  Z.d.d...Z.d.S.).
-00003740: da09 5465 726d 436f 6c6f 727a 230a 2020  ..TermColorz#.  
-00003750: 6578 706f 7274 2041 4e53 495f 434f 4c4f  export ANSI_COLO
-00003760: 5253 5f44 4953 4142 4c45 443d 310a 2020  RS_DISABLED=1.  
-00003770: 6301 0000 0000 0000 0000 0000 0003 0000  c...............
-00003780: 0005 0000 0043 0000 0073 3600 0000 6401  .....C...s6...d.
-00003790: 6402 6c00 6d01 7d01 6d02 7d02 0100 6403  d.l.m.}.m.}...d.
-000037a0: 7c00 5f03 6404 7c00 5f04 6405 6406 6407  |._.d.|._.d.d.d.
-000037b0: 6408 6409 6705 7c00 5f05 6401 7c00 5f06  d.d.g.|._.d.|._.
-000037c0: 6400 5300 290a 4e72 0100 0000 2902 da07  d.S.).Nr....)...
-000037d0: 636f 6c6f 7265 64da 0643 4f4c 4f52 535a  colored..COLORSZ
-000037e0: 0467 7265 79da 0567 7265 656e 5a03 7265  .grey..greenZ.re
-000037f0: 645a 0679 656c 6c6f 775a 0462 6c75 655a  dZ.yellowZ.blueZ
-00003800: 076d 6167 656e 7461 5a04 6379 616e 2907  .magentaZ.cyan).
-00003810: 5a09 7465 726d 636f 6c6f 7272 0801 0000  Z.termcolorr....
-00003820: 7209 0100 00da 0562 6c61 636b 720a 0100  r......blackr...
-00003830: 00da 0663 6f6c 6f72 73da 0963 7572 5f63  ...colors..cur_c
-00003840: 6f6c 6f72 2903 722c 0000 0072 0801 0000  olor).r,...r....
-00003850: 7209 0100 0072 0800 0000 7208 0000 0072  r....r....r....r
-00003860: 0a00 0000 7238 0000 0022 0200 0073 0c00  ....r8..."...s..
-00003870: 0000 0001 1001 0601 0601 1001 0601 7a12  ..............z.
-00003880: 5465 726d 436f 6c6f 722e 5f5f 696e 6974  TermColor.__init
-00003890: 5f5f 6301 0000 0000 0000 0000 0000 0002  __c.............
-000038a0: 0000 0003 0000 0043 0000 0073 1e00 0000  .......C...s....
-000038b0: 7c00 6a00 7c00 6a01 1900 7d01 7c00 0400  |.j.|.j...}.|...
-000038c0: 6a01 6401 3700 0200 5f01 7c01 5300 7239  j.d.7..._.|.S.r9
-000038d0: 0000 0029 0272 0c01 0000 720d 0100 0029  ...).r....r....)
-000038e0: 0272 2c00 0000 da05 636f 6c6f 7272 0800  .r,.....colorr..
-000038f0: 0000 7208 0000 0072 0a00 0000 da0b 6765  ..r....r......ge
-00003900: 745f 615f 636f 6c6f 722a 0200 0073 0600  t_a_color*...s..
-00003910: 0000 0001 0c01 0e01 7a15 5465 726d 436f  ........z.TermCo
-00003920: 6c6f 722e 6765 745f 615f 636f 6c6f 724e  lor.get_a_colorN
-00003930: 2906 7242 0000 0072 4300 0000 7244 0000  ).rB...rC...rD..
-00003940: 0072 8d00 0000 7238 0000 0072 0f01 0000  .r....r8...r....
-00003950: 7208 0000 0072 0800 0000 7208 0000 0072  r....r....r....r
-00003960: 0a00 0000 7207 0100 001e 0200 0073 0600  ....r........s..
-00003970: 0000 0801 0403 0808 7207 0100 0063 0300  ........r....c..
-00003980: 0000 0000 0000 0000 0000 0400 0000 0600  ................
-00003990: 0000 4300 0000 735c 0000 0074 00a0 017c  ..C...s\...t...|
-000039a0: 00a1 017d 0074 027c 0083 017d 0064 017d  ...}.t.|...}.d.}
-000039b0: 037c 037c 029b 0064 029d 0237 007d 037c  .|.|...d...7.}.|
-000039c0: 0172 3e7c 0374 03a0 0474 05a0 067c 00a1  .r>|.t...t...|..
-000039d0: 01a1 0137 007d 036e 127c 0374 076a 087c  ...7.}.n.|.t.j.|
-000039e0: 0064 0364 048d 0237 007d 037c 0364 0537  .d.d...7.}.|.d.7
-000039f0: 007d 037c 0353 0029 064e 722e 0000 007a  .}.|.S.).Nr....z
-00003a00: 2920 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ) --------------
-00003a10: 2d2d 2d20 7374 6172 7420 2d2d 2d2d 2d2d  --- start ------
-00003a20: 2d2d 2d2d 2d2d 2d2d 2d0a 720c 0000 00a9  ---------.r.....
-00003a30: 01da 0669 6e64 656e 747a 2a0a 2d2d 2d2d  ...indentz*.----
-00003a40: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d20 456e  ------------- En
-00003a50: 6420 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  d --------------
-00003a60: 2d2d 2d2d 2d29 09da 0463 6f70 79da 0864  -----)...copy..d
-00003a70: 6565 7063 6f70 7972 6400 0000 da06 7070  eepcopyrd.....pp
-00003a80: 7269 6e74 da07 7066 6f72 6d61 7472 5f00  rint..pformatr_.
-00003a90: 0000 7260 0000 00da 046a 736f 6eda 0564  ..r`.....json..d
-00003aa0: 756d 7073 2904 da08 6469 6374 5f6f 626a  umps)...dict_obj
-00003ab0: 5a0a 7573 655f 7070 7269 6e74 5a0a 7072  Z.use_pprintZ.pr
-00003ac0: 6566 6978 5f73 7472 da07 6d65 7373 6167  efix_str..messag
-00003ad0: 6572 0800 0000 7208 0000 0072 0a00 0000  er....r....r....
-00003ae0: da0b 6469 6374 3273 7472 696e 6730 0200  ..dict2string0..
-00003af0: 0073 1200 0000 0001 0a01 0801 0401 0e01  .s..............
-00003b00: 0401 1602 1201 0801 721a 0100 0063 0200  ........r....c..
-00003b10: 0000 0000 0000 0000 0000 0300 0000 0900  ................
-00003b20: 0000 4300 0000 732a 0000 0074 007c 0164  ..C...s*...t.|.d
-00003b30: 0183 028f 167d 0274 016a 027c 007c 0264  .....}.t.j.|.|.d
-00003b40: 0264 038d 0301 0057 0035 0051 0052 0058  .d.....W.5.Q.R.X
-00003b50: 0064 0053 0029 044e 726c 0000 0072 0c00  .d.S.).Nrl...r..
-00003b60: 0000 7210 0100 0029 0372 1100 0000 7216  ..r....).r....r.
-00003b70: 0100 0072 1300 0000 2903 5a08 6f62 6a5f  ...r....).Z.obj_
-00003b80: 6469 6374 72c5 0000 0072 1600 0000 7208  dictr....r....r.
-00003b90: 0000 0072 0800 0000 720a 0000 00da 096a  ...r....r......j
-00003ba0: 736f 6e5f 6475 6d70 3d02 0000 7306 0000  son_dump=...s...
-00003bb0: 0000 010c 011a 0172 1b01 0000 72eb 0000  .......r....r...
-00003bc0: 0063 0300 0000 0000 0000 0000 0000 0700  .c..............
-00003bd0: 0000 0600 0000 4300 0000 7380 0000 0074  ......C...s....t
-00003be0: 0064 017c 009b 0064 027c 019b 009d 0483  .d.|...d.|......
-00003bf0: 0101 0074 01a0 02a1 007d 037c 0264 036b  ...t.....}.|.d.k
-00003c00: 0272 387c 036a 037c 0074 0464 0467 0064  .r8|.j.|.t.d.g.d
-00003c10: 058d 0401 006e 0a7c 03a0 037c 00a1 0101  .....n.|...|....
-00003c20: 007c 036a 057c 0164 068d 015c 027d 047d  .|.j.|.d...\.}.}
-00003c30: 0574 067c 047c 00a0 0764 07a1 01a0 0864  .t.|.|...d.....d
-00003c40: 0764 08a1 0283 027d 0674 007c 009b 0064  .d.....}.t.|...d
-00003c50: 097c 069b 009d 0383 0101 007c 0653 0029  .|.........|.S.)
-00003c60: 0a7a 3e0a 0a20 203a 7061 7261 6d20 6e61  .z>..  :param na
-00003c70: 6d65 3a20 272d 2d74 6c5f 6f70 7473 270a  me: '--tl_opts'.
-00003c80: 2020 3a70 6172 616d 2061 7267 765f 6c69    :param argv_li
-00003c90: 7374 3a0a 2020 3a72 6574 7572 6e3a 0a20  st:.  :return:. 
-00003ca0: 207a 0a50 6172 7365 7269 6e67 207a 0720   z.Parsering z. 
-00003cb0: 6672 6f6d 200a 72eb 0000 00da 012a 2903  from .r......*).
-00003cc0: da04 7479 7065 da05 6e61 7267 7372 0501  ..type..nargsr..
-00003cd0: 0000 2901 da04 6172 6773 fa01 2dda 015f  ..)...args..-.._
-00003ce0: fa01 3d29 0972 4600 0000 da08 6172 6770  ..=).rF.....argp
-00003cf0: 6172 7365 da0e 4172 6775 6d65 6e74 5061  arse..ArgumentPa
-00003d00: 7273 6572 da0c 6164 645f 6172 6775 6d65  rser..add_argume
-00003d10: 6e74 7229 0000 00da 1070 6172 7365 5f6b  ntr).....parse_k
-00003d20: 6e6f 776e 5f61 7267 73da 0767 6574 6174  nown_args..getat
-00003d30: 7472 7295 0000 00da 0772 6570 6c61 6365  trr......replace
-00003d40: 2907 725d 0000 00da 0961 7267 765f 6c69  ).r].....argv_li
-00003d50: 7374 721d 0100 00da 0670 6172 7365 7272  str......parserr
-00003d60: 1f01 0000 7221 0100 00da 0576 616c 7565  ....r!.....value
-00003d70: 7208 0000 0072 0800 0000 720a 0000 00da  r....r....r.....
-00003d80: 1570 6172 7365 725f 6172 6773 5f66 726f  .parser_args_fro
-00003d90: 6d5f 6c69 7374 4302 0000 7312 0000 0000  m_listC...s.....
-00003da0: 0714 0108 0108 0114 020a 0110 0218 0112  ................
-00003db0: 0172 2c01 0000 6300 0000 0000 0000 0000  .r,...c.........
-00003dc0: 0000 0002 0000 0004 0000 0043 0000 0073  ...........C...s
-00003dd0: 3c00 0000 6401 6400 6c00 7d00 7401 7c00  <...d.d.l.}.t.|.
-00003de0: 6402 6400 8303 7d01 7c01 6400 6b08 722a  d.d...}.|.d.k.r*
-00003df0: 6401 7338 7402 6403 8301 8201 6e0e 7c01  d.s8t.d.....n.|.
-00003e00: 8300 7234 6404 5300 6405 5300 6400 5300  ..r4d.S.d.S.d.S.
-00003e10: 2906 4e72 0100 0000 da08 6765 7474 7261  ).Nr......gettra
-00003e20: 6365 7a0f 4e6f 2073 7973 2e67 6574 7472  cez.No sys.gettr
-00003e30: 6163 6554 4629 0372 9800 0000 7227 0100  aceTF).r....r'..
-00003e40: 0072 de00 0000 2902 7298 0000 0072 2d01  .r....).r....r-.
-00003e50: 0000 7208 0000 0072 0800 0000 720a 0000  ..r....r....r...
-00003e60: 00da 0c69 735f 6465 6275 6767 696e 6757  ...is_debuggingW
-00003e70: 0200 0073 0e00 0000 0001 0801 0c02 0801  ...s............
-00003e80: 0e01 0601 0402 722e 0100 00fa 1363 6f6e  ......r......con
-00003e90: 6669 675f 636f 6d6d 616e 642e 7961 6d6c  fig_command.yaml
-00003ea0: 6302 0000 0000 0000 0000 0000 0004 0000  c...............
-00003eb0: 0006 0000 0043 0000 0073 3000 0000 6401  .....C...s0...d.
-00003ec0: 6402 6c00 6d01 7d02 0100 7402 7c02 a003  d.l.m.}...t.|...
-00003ed0: 7c00 9b00 6403 7c01 9b00 9d03 a101 a004  |...d.|.........
-00003ee0: a100 8301 6401 1900 7d03 7c03 5300 2904  ....d...}.|.S.).
-00003ef0: 4e72 0100 0000 2901 da09 544c 4366 674e  Nr....)...TLCfgN
-00003f00: 6f64 65fa 012f 2905 5a0f 746c 322e 7072  ode../).Z.tl2.pr
-00003f10: 6f6a 2e66 7663 6f72 6572 3001 0000 72eb  oj.fvcorer0...r.
-00003f20: 0000 005a 0e6c 6f61 645f 7961 6d6c 5f66  ...Z.load_yaml_f
-00003f30: 696c 65da 0676 616c 7565 7329 0472 cd00  ile..values).r..
-00003f40: 0000 da08 6366 675f 6669 6c65 7230 0100  ....cfg_filer0..
-00003f50: 005a 0a6c 6f61 6465 645f 6366 6772 0800  .Z.loaded_cfgr..
-00003f60: 0000 7208 0000 0072 0a00 0000 da19 6c6f  ..r....r......lo
-00003f70: 6164 5f63 6f6e 6669 675f 636f 6d6d 616e  ad_config_comman
-00003f80: 645f 7661 6c75 6563 0200 0073 0600 0000  d_valuec...s....
-00003f90: 0002 0c02 2001 7234 0100 0063 0200 0000  .... .r4...c....
-00003fa0: 0000 0000 0000 0000 0400 0000 0400 0000  ................
-00003fb0: 4300 0000 7324 0000 007c 01a0 00a1 0044  C...s$...|.....D
-00003fc0: 005d 165c 027d 027d 037c 007c 0219 00a0  .].\.}.}.|.|....
-00003fd0: 017c 03a1 0101 0071 0864 0053 0072 2500  .|.....q.d.S.r%.
-00003fe0: 0000 2902 7261 0000 0072 3500 0000 2904  ..).ra...r5...).
-00003ff0: 5a09 6473 745f 6464 6963 745a 0973 7263  Z.dst_ddictZ.src
-00004000: 5f64 6469 6374 7265 0000 0072 6600 0000  _ddictre...rf...
-00004010: 7208 0000 0072 0800 0000 720a 0000 00da  r....r....r.....
-00004020: 166d 6572 6765 5f64 6566 6175 6c74 6469  .merge_defaultdi
-00004030: 6374 5f64 6963 746b 0200 0073 0600 0000  ct_dictk...s....
-00004040: 0002 1001 1001 7235 0100 0072 3200 0000  ......r5...r2...
-00004050: fa08 2530 3664 2e70 6e67 6304 0000 0000  ..%06d.pngc.....
-00004060: 0000 0000 0000 0006 0000 000c 0000 0043  ...............C
-00004070: 0000 0073 7c00 0000 7400 6a01 7c01 6401  ...s|...t.j.|.d.
-00004080: 6402 8d02 0100 6403 6404 7c00 6405 7c02  d.....d.d.|.d.|.
-00004090: 9b00 6406 6407 6408 6409 7c01 9b00 640a  ..d.d.d.d.|...d.
-000040a0: 7c03 9b00 9d03 670a 7d04 7402 640b 640c  |.....g.}.t.d.d.
-000040b0: a003 7c04 a101 9b00 9d02 8301 0100 7404  ..|...........t.
-000040c0: a005 7c04 a101 0100 7406 7c01 8301 7d05  ..|.....t.|...}.
-000040d0: 7402 640d 7407 7c05 8301 9b00 640e 7c01  t.d.t.|.....d.|.
-000040e0: 9b00 640a 7c03 9b00 640f 9d07 8301 0100  ..d.|...d.......
-000040f0: 6400 5300 2910 4e54 72c7 0000 005a 0666  d.S.).NTr....Z.f
-00004100: 666d 7065 677a 022d 697a 022d 727a 022d  fmpegz.-iz.-rz.-
-00004110: 665a 0669 6d61 6765 327a 022d 76da 0565  fZ.image2z.-v..e
-00004120: 7272 6f72 7231 0100 007a 1020 3d3e 2052  rrorr1...z. => R
-00004130: 756e 6e69 6e67 3a20 0a09 2072 fe00 0000  unning: .. r....
-00004140: 7278 0000 007a 1120 696d 6167 6573 2073  rx...z. images s
-00004150: 6176 6564 2074 6f20 fa01 2229 0872 4700  aved to ..").rG.
-00004160: 0000 72c9 0000 0072 4600 0000 72cc 0000  ..r....rF...r...
-00004170: 00da 0a73 7562 7072 6f63 6573 73da 0463  ...subprocess..c
-00004180: 616c 6c72 f500 0000 72be 0000 0029 065a  allr....r....).Z
-00004190: 0876 6964 5f66 696c 655a 0a69 6d67 5f66  .vid_fileZ.img_f
-000041a0: 6f6c 6465 725a 0a66 7261 6d65 5f66 7265  olderZ.frame_fre
-000041b0: 71da 0666 6f72 6d61 7472 4900 0000 5a08  q..formatrI...Z.
-000041c0: 696d 675f 6c69 7374 7208 0000 0072 0800  img_listr....r..
-000041d0: 0000 720a 0000 00da 1666 666d 7065 675f  ..r......ffmpeg_
-000041e0: 7669 6465 6f5f 746f 5f66 7261 6d65 7373  video_to_framess
-000041f0: 0200 0073 2200 0000 0004 0e02 0201 0200  ...s"...........
-00004200: 0201 0200 0401 0200 0201 0200 0201 0cfb  ................
-00004210: 0407 1401 0a02 0802 2002 723c 0100 0063  ........ .r<...c
-00004220: 0200 0000 0000 0000 0000 0000 0300 0000  ................
-00004230: 0900 0000 4300 0000 734e 0000 0074 006a  ....C...sN...t.j
-00004240: 0174 006a 02a0 037c 00a1 0164 0164 028d  .t.j...|...d.d..
-00004250: 0201 0074 047c 0064 0383 028f 247d 0274  ...t.|.d....$}.t
-00004260: 057c 0174 0683 0272 367c 01a0 0764 04a1  .|.t...r6|...d..
-00004270: 017d 017c 02a0 087c 01a1 0101 0057 0035  .}.|...|.....W.5
-00004280: 0051 0052 0058 0064 0553 0029 067a 470a  .Q.R.X.d.S.).zG.
-00004290: 2020 0a20 203a 7061 7261 6d20 7361 7665    .  :param save
-000042a0: 645f 7061 7468 3a0a 2020 3a70 6172 616d  d_path:.  :param
-000042b0: 2064 6174 613a 2055 6e69 6f6e 5b62 7974   data: Union[byt
-000042c0: 6573 2c20 7374 725d 0a20 203a 7265 7475  es, str].  :retu
-000042d0: 726e 3a0a 2020 5472 c700 0000 720f 0000  rn:.  Tr....r...
-000042e0: 00da 0475 7466 384e 2909 7247 0000 0072  ...utf8N).rG...r
-000042f0: c900 0000 7296 0000 0072 9700 0000 7211  ....r....r....r.
-00004300: 0000 0072 6200 0000 7229 0000 0072 7300  ...rb...r)...rs.
-00004310: 0000 726d 0000 0029 0372 1400 0000 7215  ..rm...).r....r.
-00004320: 0000 005a 0466 6f75 7472 0800 0000 7208  ...Z.foutr....r.
-00004330: 0000 0072 0a00 0000 da0b 7772 6974 655f  ...r......write_
-00004340: 6279 7465 738c 0200 0073 0c00 0000 0008  bytes....s......
-00004350: 1601 0c01 0a01 0a01 1401 723e 0100 0063  ..........r>...c
-00004360: 0100 0000 0000 0000 0000 0000 0300 0000  ................
-00004370: 0900 0000 4300 0000 7322 0000 0074 007c  ....C...s"...t.|
-00004380: 0064 0183 028f 0e7d 017c 01a0 01a1 007d  .d.....}.|.....}
-00004390: 0257 0035 0051 0052 0058 007c 0253 0029  .W.5.Q.R.X.|.S.)
-000043a0: 024e 7218 0000 0029 0272 1100 0000 da04  .Nr....).r......
-000043b0: 7265 6164 2903 72c5 0000 005a 0366 696e  read).r....Z.fin
-000043c0: 5a0a 6461 7461 5f62 7974 6573 7208 0000  Z.data_bytesr...
-000043d0: 0072 0800 0000 720a 0000 00da 0a72 6561  .r....r......rea
-000043e0: 645f 6279 7465 739b 0200 0073 0600 0000  d_bytes....s....
-000043f0: 0001 0c01 1201 7240 0100 0029 0172 2e00  ......r@...).r..
-00004400: 0000 2903 7277 0000 0072 2e00 0000 722e  ..).rw...r....r.
-00004410: 0000 0029 014e 2904 72e2 0000 0054 4654  ...).N).r....TFT
-00004420: 2902 4672 fd00 0000 2901 4e29 0254 722e  ).Fr....).N).Tr.
-00004430: 0000 0029 0172 eb00 0000 2901 722f 0100  ...).r....).r/..
-00004440: 0029 0272 3200 0000 7236 0100 0029 4dda  .).r2...r6...)M.
-00004450: 0269 6f72 1201 0000 da05 6e75 6d70 7972  .ior......numpyr
-00004460: 8e00 0000 7282 0000 0072 ca00 0000 da07  ....r....r......
-00004470: 7061 7468 6c69 6272 0200 0000 7234 0000  pathlibr....r4..
-00004480: 0072 2301 0000 7247 0000 00da 0272 6572  .r#...rG.....rer
-00004490: 5a00 0000 7298 0000 00da 0969 6d70 6f72  Z...r......impor
-000044a0: 746c 6962 7216 0100 0072 1401 0000 725f  tlibr....r....r_
-000044b0: 0000 00da 086f 7065 7261 746f 7272 0400  .....operatorr..
-000044c0: 0000 72d1 0000 0072 c200 0000 7205 0000  ..r....r....r...
-000044d0: 0072 0600 0000 da0f 6d75 6c74 6970 726f  .r......multipro
-000044e0: 6365 7373 696e 6772 7100 0000 7223 0000  cessingrq...r#..
-000044f0: 0072 1200 0000 7239 0100 0072 0b00 0000  .r....r9...r....
-00004500: 720e 0000 0072 1700 0000 721e 0000 0072  r....r....r....r
-00004510: 1f00 0000 7222 0000 0072 2400 0000 7226  ....r"...r$...r&
-00004520: 0000 0072 5300 0000 7227 0000 0072 4a00  ...rS...r'...rJ.
-00004530: 0000 7251 0000 0072 5e00 0000 7264 0000  ..rQ...r^...rd..
-00004540: 0072 6a00 0000 7270 0000 0072 7600 0000  .rj...rp...rv...
-00004550: 7283 0000 0072 8700 0000 7288 0000 0072  r....r....r....r
-00004560: 9200 0000 da07 5072 6f63 6573 7372 9300  ......Processr..
-00004570: 0000 729e 0000 0072 a000 0000 72a4 0000  ..r....r....r...
-00004580: 0072 a600 0000 72ac 0000 0072 b300 0000  .r....r....r....
-00004590: 72ad 0000 0072 bb00 0000 72b7 0000 0072  r....r....r....r
-000045a0: db00 0000 72e1 0000 0072 f500 0000 72fc  ....r....r....r.
-000045b0: 0000 0072 f600 0000 7259 0000 0072 0701  ...r....rY...r..
-000045c0: 0000 721a 0100 0072 1b01 0000 722c 0100  ..r....r....r,..
-000045d0: 0072 2e01 0000 7234 0100 0072 3501 0000  .r....r4...r5...
-000045e0: 723c 0100 0072 3e01 0000 7240 0100 0072  r<...r>...r@...r
-000045f0: 0800 0000 7208 0000 0072 0800 0000 720a  ....r....r....r.
-00004600: 0000 00da 083c 6d6f 6475 6c65 3e01 0000  .....<module>...
-00004610: 0073 a400 0000 0801 0801 0801 0801 0801  .s..............
-00004620: 0c01 0801 0801 0801 0801 0801 0801 0801  ................
-00004630: 0801 0801 0801 0c01 0801 0801 1001 0801  ................
-00004640: 0801 0801 0801 0803 080c 0812 0809 0805  ................
-00004650: 0805 0806 0809 0805 102e 0806 0a0b 0812  ................
-00004660: 080a 080c 0807 0804 0402 0001 0001 00fd  ................
-00004670: 0a1f 0e18 080b 120f 0805 080a 0812 0a18  ................
-00004680: 0808 0401 1019 0402 103c 080d 080c 0001  .........<......
-00004690: 0001 0001 00fc 0a18 080c 0001 00fe 0a1d  ................
-000046a0: 0a08 1012 0a0d 0806 0a14 080d 00ff 0a08  ................
-000046b0: 080a 0001 00fd 0a19 080f                 ..........
+00001390: 2904 5a08 6465 6570 6469 6666 7266 0000  ).Z.deepdiffrf..
+000013a0: 0072 6300 0000 5a06 7072 6574 7479 2905  .rc...Z.pretty).
+000013b0: da02 6431 da02 6432 7266 0000 005a 0564  ..d1..d2rf...Z.d
+000013c0: 6469 6666 5a08 6469 6666 5f73 7472 7208  diffZ.diff_strr.
+000013d0: 0000 0072 0800 0000 720a 0000 00da 0c67  ...r....r......g
+000013e0: 6574 5f64 6966 665f 7374 72bc 0000 0073  et_diff_str....s
+000013f0: 1000 0000 0001 0c01 0801 0802 0a01 0401  ................
+00001400: 1401 0801 7269 0000 0063 0200 0000 0000  ....ri...c......
+00001410: 0000 0000 0000 0300 0000 0900 0000 4300  ..............C.
+00001420: 0000 7330 0000 0074 007c 009b 0064 019d  ..s0...t.|...d..
+00001430: 0264 0283 028f 167d 027c 02a0 017c 019b  .d.....}.|...|..
+00001440: 0064 039d 02a1 0101 0057 0035 0051 0052  .d.......W.5.Q.R
+00001450: 0058 0064 0053 0029 044e fa0a 2f30 696e  .X.d.S.).N../0in
+00001460: 666f 2e74 7874 da01 7772 4500 0000 2902  fo.txt..wrE...).
+00001470: 7211 0000 00da 0577 7269 7465 2903 5a09  r......write).Z.
+00001480: 7361 7665 645f 6469 72da 0869 6e66 6f5f  saved_dir..info_
+00001490: 6d73 6772 1600 0000 7208 0000 0072 0800  msgr....r....r..
+000014a0: 0000 720a 0000 00da 0e77 7269 7465 5f69  ..r......write_i
+000014b0: 6e66 6f5f 6d73 67c8 0000 0073 0600 0000  nfo_msg....s....
+000014c0: 0002 1201 1a01 726e 0000 0063 0100 0000  ......rn...c....
+000014d0: 0000 0000 0000 0000 0200 0000 0500 0000  ................
+000014e0: 4300 0000 7318 0000 0074 00a0 017c 00a0  C...s....t...|..
+000014f0: 0264 01a1 01a1 01a0 03a1 007d 017c 0153  .d.........}.|.S
+00001500: 0029 024e 7a05 7574 662d 3829 04da 0768  .).Nz.utf-8)...h
+00001510: 6173 686c 6962 5a03 6d64 35da 0665 6e63  ashlibZ.md5..enc
+00001520: 6f64 655a 0968 6578 6469 6765 7374 2902  odeZ.hexdigest).
+00001530: da06 7374 7269 6e67 5a07 7374 725f 6d64  ..stringZ.str_md
+00001540: 3572 0800 0000 7208 0000 0072 0a00 0000  5r....r....r....
+00001550: da0e 6765 745f 7374 7269 6e67 5f6d 6435  ..get_string_md5
+00001560: cf00 0000 7304 0000 0000 0114 0172 7200  ....s........rr.
+00001570: 0000 fa05 2b31 2e36 6663 0400 0000 0000  ....+1.6fc......
+00001580: 0000 0000 0000 0b00 0000 0900 0000 4300  ..............C.
+00001590: 0000 73ee 0000 0064 017c 029b 0064 0274  ..s....d.|...d.t
+000015a0: 006a 01a0 0264 0364 04a1 029b 0064 059d  .j...d.d.....d..
+000015b0: 057d 0474 037c 0074 046a 0583 0273 3064  .}.t.|.t.j...s0d
+000015c0: 067c 0069 017d 007c 00a0 06a1 0044 005d  .|.i.}.|.....D.]
+000015d0: 765c 027d 057d 067c 0564 066b 0272 4e64  v\.}.}.|.d.k.rNd
+000015e0: 077d 056e 0a7c 059b 0064 089d 027d 057c  .}.n.|...d...}.|
+000015f0: 06a0 06a1 0044 005d 4c5c 027d 077d 0874  .....D.]L\.}.}.t
+00001600: 037c 0874 0783 0272 927c 0464 097c 059b  .|.t...r.|.d.|..
+00001610: 007c 079b 0064 0a7c 087c 019b 009b 0464  .|...d.|.|.....d
+00001620: 059d 0637 007d 0471 607c 0464 097c 059b  ...7.}.q`|.d.|..
+00001630: 007c 079b 0064 0a7c 089b 0064 059d 0637  .|...d.|...d...7
+00001640: 007d 0471 6071 3874 08a0 08a1 007d 097c  .}.q`q8t.....}.|
+00001650: 0974 0918 007d 0a7c 0961 097c 0464 0b7c  .t...}.|.a.|.d.|
+00001660: 0a64 0c9b 0464 0d9d 0337 007d 047c 0372  .d...d...7.}.|.r
+00001670: ea7c 0464 097c 039b 0064 059d 0337 007d  .|.d.|...d...7.}
+00001680: 047c 0453 0029 0e4e fa04 203d 3e20 7a07  .|.S.).N.. => z.
+00001690: 205b 4750 553a 20da 1443 5544 415f 5649   [GPU: ..CUDA_VI
+000016a0: 5349 424c 455f 4445 5649 4345 53da 0130  SIBLE_DEVICES..0
+000016b0: fa01 5d5a 0374 6d70 722e 0000 0072 4b00  ..]Z.tmpr....rK.
+000016c0: 0000 7a02 205b fa02 3a20 7a0b 205b 656c  ..z. [..: z. [el
+000016d0: 6170 7365 643a 207a 032e 3366 7a02 735d  apsed: z..3fz.s]
+000016e0: 290a 7247 0000 00da 0765 6e76 6972 6f6e  ).rG.....environ
+000016f0: da03 6765 7472 6100 0000 725e 0000 00da  ..getra...r^....
+00001700: 0b64 6566 6175 6c74 6469 6374 7260 0000  .defaultdictr`..
+00001710: 00da 0566 6c6f 6174 da04 7469 6d65 da12  ...float..time..
+00001720: 746c 5f6c 6173 745f 7072 696e 745f 7469  tl_last_print_ti
+00001730: 6d65 290b 5a0b 6d65 7472 6963 5f64 6963  me).Z.metric_dic
+00001740: 745a 0c66 6c6f 6174 5f66 6f72 6d61 74da  tZ.float_format.
+00001750: 066f 7574 6469 725a 0a73 7566 6669 785f  .outdirZ.suffix_
+00001760: 7374 725a 0772 6574 5f73 7472 5a0b 6e61  strZ.ret_strZ.na
+00001770: 6d65 5f70 7265 6669 785a 0676 5f64 6963  me_prefixZ.v_dic
+00001780: 7472 6400 0000 7265 0000 00da 036e 6f77  trd...re.....now
+00001790: da07 656c 6170 7365 6472 0800 0000 7208  ..elapsedr....r.
+000017a0: 0000 0072 0a00 0000 da12 6765 745f 7072  ...r......get_pr
+000017b0: 696e 745f 6469 6374 5f73 7472 d400 0000  int_dict_str....
+000017c0: 7324 0000 0000 041c 020c 0108 0210 0108  s$..............
+000017d0: 0106 020a 0110 010a 0120 021e 0208 0208  ......... ......
+000017e0: 0104 0112 0204 0110 0172 8200 0000 6300  .........r....c.
+000017f0: 0000 0000 0000 0000 0000 0000 0000 0003  ................
+00001800: 0000 0040 0000 0073 2a00 0000 6500 5a01  ...@...s*...e.Z.
+00001810: 6400 5a02 6401 5a03 6402 6403 8400 5a04  d.Z.d.Z.d.d...Z.
+00001820: 6404 6405 8400 5a05 640a 6407 6408 8401  d.d...Z.d.d.d...
+00001830: 5a06 6409 5300 290b da0c 4176 6572 6167  Z.d.S.)...Averag
+00001840: 654d 6574 6572 7a33 2043 6f6d 7075 7465  eMeterz3 Compute
+00001850: 7320 616e 6420 7374 6f72 6573 2074 6865  s and stores the
+00001860: 2061 7665 7261 6765 2061 6e64 2063 7572   average and cur
+00001870: 7265 6e74 2076 616c 7565 2063 0100 0000  rent value c....
+00001880: 0000 0000 0000 0000 0100 0000 0200 0000  ................
+00001890: 4300 0000 730c 0000 007c 00a0 00a1 0001  C...s....|......
+000018a0: 0064 0053 0072 2500 0000 2901 723a 0000  .d.S.r%...).r:..
+000018b0: 0072 2b00 0000 7208 0000 0072 0800 0000  .r+...r....r....
+000018c0: 720a 0000 0072 3800 0000 f600 0000 7302  r....r8.......s.
+000018d0: 0000 0000 017a 1541 7665 7261 6765 4d65  .....z.AverageMe
+000018e0: 7465 722e 5f5f 696e 6974 5f5f 6301 0000  ter.__init__c...
+000018f0: 0000 0000 0000 0000 0001 0000 0002 0000  ................
+00001900: 0043 0000 0073 1c00 0000 6401 7c00 5f00  .C...s....d.|._.
+00001910: 6401 7c00 5f01 6401 7c00 5f02 6401 7c00  d.|._.d.|._.d.|.
+00001920: 5f03 6402 5300 2903 7a16 2052 6573 6574  _.d.S.).z. Reset
+00001930: 2061 6c6c 2073 7461 7469 7374 6963 7320   all statistics 
+00001940: 7201 0000 004e 2904 da03 7661 6cda 0361  r....N)...val..a
+00001950: 7667 da03 7375 6dda 0563 6f75 6e74 722b  vg..sum..countr+
+00001960: 0000 0072 0800 0000 7208 0000 0072 0a00  ...r....r....r..
+00001970: 0000 723a 0000 00f9 0000 0073 0800 0000  ..r:.......s....
+00001980: 0002 0601 0601 0601 7a12 4176 6572 6167  ........z.Averag
+00001990: 654d 6574 6572 2e72 6573 6574 7232 0000  eMeter.resetr2..
+000019a0: 0063 0300 0000 0000 0000 0000 0000 0300  .c..............
+000019b0: 0000 0400 0000 4300 0000 7338 0000 007c  ......C...s8...|
+000019c0: 017c 005f 007c 0004 006a 017c 017c 0214  .|._.|...j.|.|..
+000019d0: 0037 0002 005f 017c 0004 006a 027c 0237  .7..._.|...j.|.7
+000019e0: 0002 005f 027c 006a 017c 006a 021b 007c  ..._.|.j.|.j...|
+000019f0: 005f 0364 0153 0029 027a 1320 5570 6461  ._.d.S.).z. Upda
+00001a00: 7465 2073 7461 7469 7374 6963 7320 4e29  te statistics N)
+00001a10: 0472 8400 0000 7286 0000 0072 8700 0000  .r....r....r....
+00001a20: 7285 0000 0029 0372 2c00 0000 7284 0000  r....).r,...r...
+00001a30: 0072 4000 0000 7208 0000 0072 0800 0000  .r@...r....r....
+00001a40: 720a 0000 0072 3500 0000 0001 0000 730a  r....r5.......s.
+00001a50: 0000 0000 0406 0112 010e 010e 017a 1341  .............z.A
+00001a60: 7665 7261 6765 4d65 7465 722e 7570 6461  verageMeter.upda
+00001a70: 7465 4e29 0172 3200 0000 2907 7242 0000  teN).r2...).rB..
+00001a80: 0072 4300 0000 7244 0000 00da 075f 5f64  .rC...rD.....__d
+00001a90: 6f63 5f5f 7238 0000 0072 3a00 0000 7235  oc__r8...r:...r5
+00001aa0: 0000 0072 0800 0000 7208 0000 0072 0800  ...r....r....r..
+00001ab0: 0000 720a 0000 0072 8300 0000 f300 0000  ..r....r........
+00001ac0: 730a 0000 0008 0104 0208 0308 0900 fe72  s..............r
+00001ad0: 8300 0000 6301 0000 0000 0000 0000 0000  ....c...........
+00001ae0: 0002 0000 0003 0000 0043 0000 0073 1000  .........C...s..
+00001af0: 0000 7400 6a01 a002 7c00 a101 7d01 7c01  ..t.j...|...}.|.
+00001b00: 5300 2901 7a37 0a20 2072 616e 646f 6d5f  S.).z7.  random_
+00001b10: 7374 6174 652e 7368 7566 666c 6528 290a  state.shuffle().
+00001b20: 0a20 203a 7061 7261 6d20 7365 6564 3a0a  .  :param seed:.
+00001b30: 2020 3a72 6574 7572 6e3a 0a20 2029 03da    :return:.  )..
+00001b40: 026e 705a 0672 616e 646f 6d5a 0b52 616e  .npZ.randomZ.Ran
+00001b50: 646f 6d53 7461 7465 2902 5a04 7365 6564  domState).Z.seed
+00001b60: 5a0c 7261 6e64 6f6d 5f73 7461 7465 7208  Z.random_stater.
+00001b70: 0000 0072 0800 0000 720a 0000 00da 0f67  ...r....r......g
+00001b80: 6574 5f72 616e 646f 6d73 7461 7465 0b01  et_randomstate..
+00001b90: 0000 7304 0000 0000 070c 0172 8a00 0000  ..s........r....
+00001ba0: 6300 0000 0000 0000 0000 0000 0000 0000  c...............
+00001bb0: 0002 0000 0040 0000 0073 1800 0000 6500  .....@...s....e.
+00001bc0: 5a01 6400 5a02 6401 5a03 6402 6403 8400  Z.d.Z.d.Z.d.d...
+00001bd0: 5a04 6404 5300 2905 da06 576f 726b 6572  Z.d.S.)...Worker
+00001be0: 7a6c 0a20 2063 6f6d 6d61 6e64 5b30 5d2e  zl.  command[0].
+00001bf0: 7374 6172 7473 7769 7468 2828 2762 6173  startswith(('bas
+00001c00: 6827 2c20 2929 3a0a 2020 7020 3d20 576f  h', )):.  p = Wo
+00001c10: 726b 6572 286e 616d 653d 2743 6f6d 6d61  rker(name='Comma
+00001c20: 6e64 2077 6f72 6b65 7227 2c20 6172 6773  nd worker', args
+00001c30: 3d28 636f 6d6d 616e 645b 305d 2c29 290a  =(command[0],)).
+00001c40: 2020 702e 7374 6172 7428 290a 2020 6301    p.start().  c.
+00001c50: 0000 0000 0000 0000 0000 0002 0000 0004  ................
+00001c60: 0000 0043 0000 0073 4200 0000 7c00 6a00  ...C...sB...|.j.
+00001c70: 6401 1900 a001 a100 7d01 6402 7402 6a03  d.......}.d.t.j.
+00001c80: a004 7405 6a06 a101 9b00 6403 9d03 7c01  ..t.j.....d...|.
+00001c90: 1700 7d01 7407 6404 7c01 1600 8301 0100  ..}.t.d.|.......
+00001ca0: 7402 a008 7c01 a101 0100 6400 5300 2905  t...|.....d.S.).
+00001cb0: 4e72 0100 0000 7a0c 6578 706f 7274 2050  Nr....z.export P
+00001cc0: 4154 483d 7a0a 3a24 5041 5448 2026 2620  ATH=z.:$PATH && 
+00001cd0: 7a02 2573 2909 da05 5f61 7267 73da 0573  z.%s)..._args..s
+00001ce0: 7472 6970 7247 0000 00da 0470 6174 68da  triprG.....path.
+00001cf0: 0764 6972 6e61 6d65 da03 7379 73da 0a65  .dirname..sys..e
+00001d00: 7865 6375 7461 626c 6572 4600 0000 7248  xecutablerF...rH
+00001d10: 0000 0029 0272 2c00 0000 7249 0000 0072  ...).r,...rI...r
+00001d20: 0800 0000 7208 0000 0072 0a00 0000 da03  ....r....r......
+00001d30: 7275 6e1c 0100 0073 0a00 0000 0001 0e01  run....s........
+00001d40: 1a01 0c02 0a01 7a0a 576f 726b 6572 2e72  ......z.Worker.r
+00001d50: 756e 4e29 0572 4200 0000 7243 0000 0072  unN).rB...rC...r
+00001d60: 4400 0000 7288 0000 0072 9200 0000 7208  D...r....r....r.
+00001d70: 0000 0072 0800 0000 7208 0000 0072 0a00  ...r....r....r..
+00001d80: 0000 728b 0000 0016 0100 0073 0400 0000  ..r........s....
+00001d90: 0801 0405 728b 0000 0063 0000 0000 0000  ....r....c......
+00001da0: 0000 0000 0000 0100 0000 0300 0000 4300  ..............C.
+00001db0: 0000 731c 0000 0074 006a 00a0 01a1 00a0  ..s....t.j......
+00001dc0: 0264 01a1 0164 0064 0285 0219 007d 007c  .d...d.d.....}.|
+00001dd0: 0053 0029 034e 7a10 2559 256d 2564 5f25  .S.).Nz.%Y%m%d_%
+00001de0: 4825 4d25 535f 2566 e9fd ffff ff29 0372  H%M%S_%f.....).r
+00001df0: 0500 0000 7280 0000 00da 0873 7472 6674  ....r......strft
+00001e00: 696d 6529 01da 0874 696d 655f 7374 7272  ime)...time_strr
+00001e10: 0800 0000 7208 0000 0072 0a00 0000 da0c  ....r....r......
+00001e20: 6765 745f 7469 6d65 5f73 7472 2501 0000  get_time_str%...
+00001e30: 7304 0000 0000 0118 0172 9600 0000 6301  s........r....c.
+00001e40: 0000 0000 0000 0000 0000 0002 0000 0006  ................
+00001e50: 0000 0043 0000 0073 1600 0000 7400 a001  ...C...s....t...
+00001e60: 6401 7400 a002 7c00 a101 a102 7d01 7c01  d.t...|.....}.|.
+00001e70: 5300 2902 7a28 0a20 2065 6c61 7073 6564  S.).z(.  elapsed
+00001e80: 203d 2074 696d 652e 7469 6d65 2829 202d   = time.time() -
+00001e90: 2074 696d 655f 7374 6172 740a 2020 7a08   time_start.  z.
+00001ea0: 2548 3a25 4d3a 2553 2903 727d 0000 0072  %H:%M:%S).r}...r
+00001eb0: 9400 0000 da06 676d 7469 6d65 2902 7281  ......gmtime).r.
+00001ec0: 0000 0072 9500 0000 7208 0000 0072 0800  ...r....r....r..
+00001ed0: 0000 720a 0000 00da 0b74 696d 6532 7374  ..r......time2st
+00001ee0: 7269 6e67 2a01 0000 7304 0000 0000 0712  ring*...s.......
+00001ef0: 0172 9800 0000 6301 0000 0000 0000 0000  .r....c.........
+00001f00: 0000 0004 0000 0002 0000 0043 0000 0073  ...........C...s
+00001f10: 1e00 0000 7400 a001 a100 7d01 7c01 7c00  ....t.....}.|.|.
+00001f20: 1800 7d02 7c02 9b00 6401 9d02 7d03 7c03  ..}.|...d...}.|.
+00001f30: 5300 2902 7a91 0a20 2070 7974 686f 6e20  S.).z..  python 
+00001f40: 3e3d 2033 2e37 0a0a 2020 3173 203d 2031  >= 3.7..  1s = 1
+00001f50: 305e 3920 6e73 0a0a 2020 7469 6d65 5f73  0^9 ns..  time_s
+00001f60: 7461 7274 203d 2074 696d 652e 7065 7266  tart = time.perf
+00001f70: 5f63 6f75 6e74 6572 5f6e 7328 290a 2020  _counter_ns().  
+00001f80: 7469 6d65 5f65 6e64 203d 2074 696d 652e  time_end = time.
+00001f90: 7065 7266 5f63 6f75 6e74 6572 5f6e 7328  perf_counter_ns(
+00001fa0: 290a 2020 656c 6170 7365 6420 3d20 7469  ).  elapsed = ti
+00001fb0: 6d65 5f65 6e64 202d 2074 696d 655f 7374  me_end - time_st
+00001fc0: 6172 740a 0a20 207a 0320 6e73 2902 727d  art..  z. ns).r}
+00001fd0: 0000 00da 0f70 6572 665f 636f 756e 7465  .....perf_counte
+00001fe0: 725f 6e73 2904 da0a 7469 6d65 5f73 7461  r_ns)...time_sta
+00001ff0: 7274 da08 7469 6d65 5f65 6e64 7281 0000  rt..time_endr...
+00002000: 0072 9500 0000 7208 0000 0072 0800 0000  .r....r....r....
+00002010: 720a 0000 00da 0e74 696d 655f 6e73 3273  r......time_ns2s
+00002020: 7472 696e 6734 0100 0073 0800 0000 000b  tring4...s......
+00002030: 0801 0802 0a01 729c 0000 0063 0200 0000  ......r....c....
+00002040: 0000 0000 0000 0000 0500 0000 0500 0000  ................
+00002050: 4300 0000 735c 0000 0074 00a0 01a1 007d  C...s\...t.....}
+00002060: 027c 027c 0018 007d 0364 017c 019b 0064  .|.|...}.d.|...d
+00002070: 027c 039b 0064 039d 057d 047c 0164 046b  .|...d...}.|.d.k
+00002080: 0872 2e64 057d 017c 0464 067c 037c 011b  .r.d.}.|.d.|.|..
+00002090: 009b 0064 039d 0337 007d 047c 0464 0764  ...d...7.}.|.d.d
+000020a0: 057c 037c 011b 001b 009b 009d 0237 007d  .|.|.........7.}
+000020b0: 047c 0453 0029 087a 930a 2020 3173 203d  .|.S.).z..  1s =
+000020c0: 2031 305e 3620 7573 0a0a 2020 7469 6d65   10^6 us..  time
+000020d0: 5f73 7461 7274 203d 2074 696d 652e 7065  _start = time.pe
+000020e0: 7266 5f63 6f75 6e74 6572 2829 0a20 2074  rf_counter().  t
+000020f0: 696d 655f 656e 6420 3d20 7469 6d65 2e70  ime_end = time.p
+00002100: 6572 665f 636f 756e 7465 7228 290a 2020  erf_counter().  
+00002110: 656c 6170 7365 6420 3d20 7469 6d65 5f65  elapsed = time_e
+00002120: 6e64 202d 2074 696d 655f 7374 6172 740a  nd - time_start.
+00002130: 0a20 2072 6574 7572 6e3a 2073 2028 7072  .  return: s (pr
+00002140: 6563 6973 6520 7573 290a 2020 7a0c 616c  ecise us).  z.al
+00002150: 6c20 2872 6570 6561 743d 7a03 293a 207a  l (repeat=z.): z
+00002160: 0220 734e 6700 0000 0000 00f0 3f7a 0e2c  . sNg.......?z.,
+00002170: 2061 6c6c 2f72 6570 6561 743a 207a 072c   all/repeat: z.,
+00002180: 2066 7073 3a20 2902 727d 0000 00da 0c70   fps: ).r}.....p
+00002190: 6572 665f 636f 756e 7465 7229 0572 9a00  erf_counter).r..
+000021a0: 0000 5a0c 7265 7065 6174 5f74 696d 6573  ..Z.repeat_times
+000021b0: 729b 0000 0072 8100 0000 7295 0000 0072  r....r....r....r
+000021c0: 0800 0000 7208 0000 0072 0a00 0000 da0e  ....r....r......
+000021d0: 7469 6d65 5f75 7332 7374 7269 6e67 4601  time_us2stringF.
+000021e0: 0000 7310 0000 0000 0a08 0108 0212 0208  ..s.............
+000021f0: 0104 0214 0216 0272 9e00 0000 6301 0000  .......r....c...
+00002200: 0000 0000 0000 0000 0004 0000 0005 0000  ................
+00002210: 0043 0000 0073 3200 0000 7400 a001 7402  .C...s2...t...t.
+00002220: 6a03 a004 7c00 a101 a101 7d01 7400 a005  j...|.....}.t...
+00002230: a100 7c01 1800 7d02 7c02 a006 a100 6401  ..|...}.|.....d.
+00002240: 1a00 7d03 7407 7c03 8301 5300 2902 4ee9  ..}.t.|...S.).N.
+00002250: 3c00 0000 2908 7205 0000 005a 0d66 726f  <...).r....Z.fro
+00002260: 6d74 696d 6573 7461 6d70 7247 0000 0072  mtimestamprG...r
+00002270: 8e00 0000 da08 6765 746d 7469 6d65 7280  ......getmtimer.
+00002280: 0000 005a 0d74 6f74 616c 5f73 6563 6f6e  ...Z.total_secon
+00002290: 6473 720d 0000 0029 04da 0866 696c 6570  dsr....)...filep
+000022a0: 6174 685a 096d 6f64 695f 7469 6d65 5a0a  athZ.modi_timeZ.
+000022b0: 6d6f 6469 5f69 6e74 6572 5a0c 6d6f 6469  modi_interZ.modi
+000022c0: 5f6d 696e 7574 6573 7208 0000 0072 0800  _minutesr....r..
+000022d0: 0000 720a 0000 00da 1667 6574 5f74 696d  ..r......get_tim
+000022e0: 655f 7369 6e63 655f 6c61 7374 5f6d 645e  e_since_last_md^
+000022f0: 0100 0073 0800 0000 0002 1201 0c01 0c01  ...s............
+00002300: 72a2 0000 0063 0000 0000 0000 0000 0000  r....c..........
+00002310: 0000 0000 0000 0400 0000 4000 0000 732c  ..........@...s,
+00002320: 0000 0065 005a 0164 005a 0264 0a64 0264  ...e.Z.d.Z.d.d.d
+00002330: 0384 015a 0365 0464 0b64 0564 0684 0183  ...Z.e.d.d.d....
+00002340: 015a 0564 0764 0884 005a 0664 0953 0029  .Z.d.d...Z.d.S.)
+00002350: 0cda 0c43 6972 636c 654e 756d 6265 72e9  ...CircleNumber.
+00002360: 0400 0000 6302 0000 0000 0000 0000 0000  ....c...........
+00002370: 0002 0000 0002 0000 0043 0000 0073 1000  .........C...s..
+00002380: 0000 7c01 7c00 5f00 6401 7c00 5f01 6400  ..|.|._.d.|._.d.
+00002390: 5300 2902 4ee9 ffff ffff 2902 da0b 6d61  S.).N.....)...ma
+000023a0: 785f 746f 5f6b 6565 70da 0763 7572 5f6e  x_to_keep..cur_n
+000023b0: 756d 2902 722c 0000 0072 a600 0000 7208  um).r,...r....r.
+000023c0: 0000 0072 0800 0000 720a 0000 0072 3800  ...r....r....r8.
+000023d0: 0000 6801 0000 7306 0000 0000 0106 0106  ..h...s.........
+000023e0: 017a 1543 6972 636c 654e 756d 6265 722e  .z.CircleNumber.
+000023f0: 5f5f 696e 6974 5f5f 720c 0000 0063 0200  __init__r....c..
+00002400: 0000 0000 0000 0000 0000 0300 0000 0300  ................
+00002410: 0000 4300 0000 7328 0000 007c 0074 006b  ..C...s(...|.t.k
+00002420: 0772 1c74 017c 0164 018d 017d 027c 0274  .r.t.|.d...}.|.t
+00002430: 007c 003c 006e 0874 007c 0019 007d 027c  .|.<.n.t.|...}.|
+00002440: 0253 0029 024e a901 72a6 0000 0029 02da  .S.).N..r....)..
+00002450: 0c5f 6369 7263 6c65 5f64 6963 7472 a300  ._circle_dictr..
+00002460: 0000 2903 725c 0000 0072 a600 0000 5a0c  ..).r\...r....Z.
+00002470: 6e61 6d65 645f 6369 7263 6c65 7208 0000  named_circler...
+00002480: 0072 0800 0000 720a 0000 00da 1067 6574  .r....r......get
+00002490: 5f6e 616d 6564 5f63 6972 636c 656d 0100  _named_circlem..
+000024a0: 0073 0a00 0000 0004 0801 0a01 0a02 0802  .s..............
+000024b0: 7a1d 4369 7263 6c65 4e75 6d62 6572 2e67  z.CircleNumber.g
+000024c0: 6574 5f6e 616d 6564 5f63 6972 636c 6563  et_named_circlec
+000024d0: 0100 0000 0000 0000 0000 0000 0200 0000  ................
+000024e0: 0300 0000 4300 0000 732a 0000 007c 0004  ....C...s*...|..
+000024f0: 006a 0064 0137 0002 005f 007c 006a 007c  .j.d.7..._.|.j.|
+00002500: 006a 0116 007c 005f 007c 006a 0064 029b  .j...|._.|.j.d..
+00002510: 047d 017c 0153 0029 034e 7232 0000 005a  .}.|.S.).Nr2...Z
+00002520: 0330 3264 2902 72a7 0000 0072 a600 0000  .02d).r....r....
+00002530: 2902 722c 0000 0072 0900 0000 7208 0000  ).r,...r....r...
+00002540: 0072 0800 0000 720a 0000 00da 0a67 6574  .r....r......get
+00002550: 5f6e 756d 6265 7279 0100 0073 0800 0000  _numbery...s....
+00002560: 0001 0e01 0e01 0a01 7a17 4369 7263 6c65  ........z.Circle
+00002570: 4e75 6d62 6572 2e67 6574 5f6e 756d 6265  Number.get_numbe
+00002580: 724e 2901 72a4 0000 0029 0172 0c00 0000  rN).r....).r....
+00002590: 2907 7242 0000 0072 4300 0000 7244 0000  ).rB...rC...rD..
+000025a0: 0072 3800 0000 da0c 7374 6174 6963 6d65  .r8.....staticme
+000025b0: 7468 6f64 72aa 0000 0072 ab00 0000 7208  thodr....r....r.
+000025c0: 0000 0072 0800 0000 7208 0000 0072 0a00  ...r....r....r..
+000025d0: 0000 72a3 0000 0067 0100 0073 0a00 0000  ..r....g...s....
+000025e0: 0801 0a05 0202 00ff 0c0b 72a3 0000 0063  ..........r....c
+000025f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00002600: 0400 0000 4000 0000 7336 0000 0065 005a  ....@...s6...e.Z
+00002610: 0164 005a 0264 0c64 0364 0484 015a 0365  .d.Z.d.d.d...Z.e
+00002620: 0464 0d64 0664 0784 0183 015a 0564 0864  .d.d.d.....Z.d.d
+00002630: 0984 005a 0664 0e64 0a64 0b84 015a 0764  ...Z.d.d.d...Z.d
+00002640: 0153 0029 0fda 094d 6178 546f 4b65 6570  .S.)...MaxToKeep
+00002650: 4e54 6303 0000 0000 0000 0000 0000 0003  NTc.............
+00002660: 0000 0003 0000 0043 0000 0073 2800 0000  .......C...s(...
+00002670: 7c01 7c00 5f00 6700 7c00 5f01 7c02 7224  |.|._.g.|._.|.r$
+00002680: 7c01 6401 6b04 7224 7402 7c01 6402 8d01  |.d.k.r$t.|.d...
+00002690: 7c00 5f03 6400 5300 2903 4e72 0100 0000  |._.d.S.).Nr....
+000026a0: 72a8 0000 0029 0472 a600 0000 da12 7265  r....).r......re
+000026b0: 6365 6e74 5f63 6865 636b 706f 696e 7473  cent_checkpoints
+000026c0: 72a3 0000 00da 1163 6972 636c 655f 6e75  r......circle_nu
+000026d0: 6d62 6572 5f67 656e 2903 722c 0000 0072  mber_gen).r,...r
+000026e0: a600 0000 da11 7573 655f 6369 7263 6c65  ......use_circle
+000026f0: 5f6e 756d 6265 7272 0800 0000 7208 0000  _numberr....r...
+00002700: 0072 0a00 0000 7238 0000 0083 0100 0073  .r....r8.......s
+00002710: 0a00 0000 0003 0601 0602 0c01 0c01 7a12  ..............z.
+00002720: 4d61 7854 6f4b 6565 702e 5f5f 696e 6974  MaxToKeep.__init
+00002730: 5f5f 720c 0000 0063 0300 0000 0000 0000  __r....c........
+00002740: 0000 0000 0400 0000 0400 0000 4300 0000  ............C...
+00002750: 732a 0000 007c 0074 006b 0772 1e74 017c  s*...|.t.k.r.t.|
+00002760: 017c 0264 018d 027d 037c 0374 007c 003c  .|.d...}.|.t.|.<
+00002770: 006e 0874 007c 0019 007d 037c 0353 0029  .n.t.|...}.|.S.)
+00002780: 024e 2902 72a6 0000 0072 b000 0000 2902  .N).r....r....).
+00002790: da0f 5f4d 6178 546f 4b65 6570 5f64 6963  .._MaxToKeep_dic
+000027a0: 7472 ad00 0000 2904 725c 0000 0072 a600  tr....).r\...r..
+000027b0: 0000 72b0 0000 005a 0f6e 616d 6564 5f6d  ..r....Z.named_m
+000027c0: 6178 746f 6b65 6570 7208 0000 0072 0800  axtokeepr....r..
+000027d0: 0000 720a 0000 00da 1567 6574 5f6e 616d  ..r......get_nam
+000027e0: 6564 5f6d 6178 5f74 6f5f 6b65 6570 8d01  ed_max_to_keep..
+000027f0: 0000 730a 0000 0000 0508 010c 010a 0208  ..s.............
+00002800: 027a 1f4d 6178 546f 4b65 6570 2e67 6574  .z.MaxToKeep.get
+00002810: 5f6e 616d 6564 5f6d 6178 5f74 6f5f 6b65  _named_max_to_ke
+00002820: 6570 6302 0000 0000 0000 0000 0000 0003  epc.............
+00002830: 0000 0003 0000 0043 0000 0073 6400 0000  .......C...sd...
+00002840: 7c00 6a00 6400 6b09 7260 7c00 6a01 a002  |.j.d.k.r`|.j...
+00002850: 7c01 a101 0100 7403 7c00 6a01 8301 7c00  |.....t.|.j...|.
+00002860: 6a00 6b04 7260 7c00 6a01 a004 6401 a101  j.k.r`|.j...d...
+00002870: 7d02 7405 6a06 a007 7c02 a101 7260 7405  }.t.j...|...r`t.
+00002880: 6a06 a008 7c02 a101 7256 7409 a00a 7c02  j...|...rVt...|.
+00002890: a101 0100 6e0a 7405 a00b 7c02 a101 0100  ....n.t...|.....
+000028a0: 6400 5300 2902 4e72 0100 0000 290c 72a6  d.S.).Nr....).r.
+000028b0: 0000 0072 ae00 0000 da06 6170 7065 6e64  ...r......append
+000028c0: da03 6c65 6eda 0370 6f70 7247 0000 0072  ..len..poprG...r
+000028d0: 8e00 0000 da06 6578 6973 7473 da05 6973  ......exists..is
+000028e0: 6469 72da 0673 6875 7469 6cda 0672 6d74  dir..shutil..rmt
+000028f0: 7265 65da 0672 656d 6f76 6529 0372 2c00  ree..remove).r,.
+00002900: 0000 da09 6669 6c65 5f70 6174 685a 0e66  ....file_pathZ.f
+00002910: 696c 655f 746f 5f64 656c 6574 6572 0800  ile_to_deleter..
+00002920: 0000 7208 0000 0072 0a00 0000 da04 7374  ..r....r......st
+00002930: 6570 9a01 0000 7312 0000 0000 010a 010c  ep....s.........
+00002940: 0110 010c 010c 010c 010c 020a 017a 0e4d  .............z.M
+00002950: 6178 546f 4b65 6570 2e73 7465 7063 0300  axToKeep.stepc..
+00002960: 0000 0000 0000 0000 0000 0800 0000 0900  ................
+00002970: 0000 4300 0000 73dc 0000 0074 006a 017c  ..C...s....t.j.|
+00002980: 0164 0164 028d 0201 007c 006a 02a0 03a1  .d.d.....|.j....
+00002990: 007d 0374 04a0 047c 019b 0064 039d 02a1  .}.t...|...d....
+000029a0: 017d 047c 0444 005d 1a7d 0574 006a 05a0  .}.|.D.].}.t.j..
+000029b0: 067c 05a1 0172 2c74 00a0 077c 05a1 0101  .|...r,t...|....
+000029c0: 0071 2c74 087c 019b 0064 047c 039b 0064  .q,t.|...d.|...d
+000029d0: 059d 0464 0683 028f 167d 067c 06a0 097c  ...d.....}.|...|
+000029e0: 029b 0064 079d 02a1 0101 0057 0035 0051  ...d.......W.5.Q
+000029f0: 0052 0058 0074 006a 05a0 0a7c 0174 0b7c  .R.X.t.j...|.t.|
+00002a00: 0383 01a1 027d 077c 00a0 0c7c 07a1 0101  .....}.|...|....
+00002a10: 0074 006a 017c 0764 0164 028d 0201 007c  .t.j.|.d.d.....|
+00002a20: 0264 006b 0972 d874 087c 079b 0064 089d  .d.k.r.t.|...d..
+00002a30: 0264 0683 028f 167d 067c 06a0 097c 029b  .d.....}.|...|..
+00002a40: 0064 079d 02a1 0101 0057 0035 0051 0052  .d.......W.5.Q.R
+00002a50: 0058 007c 0753 0029 094e 54a9 01da 0865  .X.|.S.).NT....e
+00002a60: 7869 7374 5f6f 6b7a 0e2f 3072 6563 656e  xist_okz./0recen
+00002a70: 745f 2a2e 7478 747a 092f 3072 6563 656e  t_*.txtz./0recen
+00002a80: 745f 7a04 2e74 7874 726b 0000 0072 4500  t_z..txtrk...rE.
+00002a90: 0000 726a 0000 0029 0d72 4700 0000 da08  ..rj...).rG.....
+00002aa0: 6d61 6b65 6469 7273 72af 0000 0072 ab00  makedirsr....r..
+00002ab0: 0000 da04 676c 6f62 728e 0000 00da 0669  ....globr......i
+00002ac0: 7366 696c 6572 ba00 0000 7211 0000 0072  sfiler....r....r
+00002ad0: 6c00 0000 da04 6a6f 696e 7229 0000 0072  l.....joinr)...r
+00002ae0: bc00 0000 2908 722c 0000 00da 0872 6f6f  ....).r,.....roo
+00002af0: 745f 6469 7272 6d00 0000 5a0a 6375 725f  t_dirrm...Z.cur_
+00002b00: 6e75 6d62 6572 5a0a 6c61 7374 5f66 696c  numberZ.last_fil
+00002b10: 6573 da09 6c61 7374 5f66 696c 6572 1600  es..last_filer..
+00002b20: 0000 da07 6473 745f 6469 7272 0800 0000  ....dst_dirr....
+00002b30: 7208 0000 0072 0a00 0000 da17 7374 6570  r....r......step
+00002b40: 5f61 6e64 5f72 6574 5f63 6972 636c 655f  _and_ret_circle_
+00002b50: 6469 72a6 0100 0073 1e00 0000 0003 0e01  dir....s........
+00002b60: 0a01 1001 0801 0c01 0c01 1801 1a02 1202  ................
+00002b70: 0a02 0e01 0801 1201 1a02 7a21 4d61 7854  ..........z!MaxT
+00002b80: 6f4b 6565 702e 7374 6570 5f61 6e64 5f72  oKeep.step_and_r
+00002b90: 6574 5f63 6972 636c 655f 6469 7229 024e  et_circle_dir).N
+00002ba0: 5429 0272 0c00 0000 5429 014e 2908 7242  T).r....T).N).rB
+00002bb0: 0000 0072 4300 0000 7244 0000 0072 3800  ...rC...rD...r8.
+00002bc0: 0000 72ac 0000 0072 b200 0000 72bc 0000  ..r....r....r...
+00002bd0: 0072 c600 0000 7208 0000 0072 0800 0000  .r....r....r....
+00002be0: 7208 0000 0072 0a00 0000 72ad 0000 0082  r....r....r.....
+00002bf0: 0100 0073 1400 0000 0802 0001 00fe 0a0a  ...s............
+00002c00: 0202 0001 00fe 0c0c 080e 00fe 72ad 0000  ............r...
+00002c10: 0063 0200 0000 0000 0000 0000 0000 0b00  .c..............
+00002c20: 0000 0600 0000 4300 0000 7380 0000 0064  ......C...s....d
+00002c30: 0164 006c 007d 027c 02a0 017c 0164 02a1  .d.l.}.|...|.d..
+00002c40: 027d 0374 0274 036a 04a0 057c 00a1 0183  .}.t.t.j...|....
+00002c50: 017d 0474 03a0 067c 00a1 0144 005d 445c  .}.t...|...D.]D\
+00002c60: 037d 057d 067d 077c 0744 005d 347d 0874  .}.}.}.|.D.]4}.t
+00002c70: 036a 04a0 077c 057c 08a1 027d 097c 097c  .j...|.|...}.|.|
+00002c80: 0464 0085 0219 00a0 0874 036a 046a 09a1  .d.......t.j.j..
+00002c90: 017d 0a7c 03a0 0a7c 097c 0aa1 0201 0071  .}.|...|.|.....q
+00002ca0: 3c71 2e7c 03a0 0ba1 0001 0064 0053 0029  <q.|.......d.S.)
+00002cb0: 034e 7201 0000 0072 6b00 0000 290c da07  .Nr....rk...)...
+00002cc0: 7a69 7066 696c 65da 075a 6970 4669 6c65  zipfile..ZipFile
+00002cd0: 72b4 0000 0072 4700 0000 728e 0000 0072  r....rG...r....r
+00002ce0: 8f00 0000 da04 7761 6c6b 72c2 0000 0072  ......walkr....r
+00002cf0: 8d00 0000 da03 7365 7072 6c00 0000 723e  ......seprl...r>
+00002d00: 0000 0029 0b5a 0a73 6f75 7263 655f 6469  ...).Z.source_di
+00002d10: 725a 0f6f 7574 7075 745f 6669 6c65 6e61  rZ.output_filena
+00002d20: 6d65 72c7 0000 005a 047a 6970 665a 0770  mer....Z.zipfZ.p
+00002d30: 7265 5f6c 656e da06 7061 7265 6e74 da08  re_len..parent..
+00002d40: 6469 726e 616d 6573 da09 6669 6c65 6e61  dirnames..filena
+00002d50: 6d65 73da 0866 696c 656e 616d 655a 0870  mes..filenameZ.p
+00002d60: 6174 6866 696c 655a 0761 7263 6e61 6d65  athfileZ.arcname
+00002d70: 7208 0000 0072 0800 0000 720a 0000 00da  r....r....r.....
+00002d80: 086d 616b 655f 7a69 70be 0100 0073 1200  .make_zip....s..
+00002d90: 0000 0002 0801 0c01 1001 1401 0801 0e01  ................
+00002da0: 1601 1001 72cf 0000 0063 0200 0000 0000  ....r....c......
+00002db0: 0000 0000 0000 0400 0000 0500 0000 4300  ..............C.
+00002dc0: 0000 7362 0000 0074 006a 017c 0164 0164  ..sb...t.j.|.d.d
+00002dd0: 028d 0201 0074 02a0 037c 00a1 0173 1c74  .....t...|...s.t
+00002de0: 0482 0174 02a0 057c 0064 03a1 027d 027c  ...t...|.d...}.|
+00002df0: 02a0 06a1 0044 005d 107d 037c 02a0 077c  .....D.].}.|...|
+00002e00: 037c 01a1 0201 0071 307c 02a0 08a1 0001  .|.....q0|......
+00002e10: 0074 0964 047c 009b 0064 057c 019b 009d  .t.d.|...d.|....
+00002e20: 0483 0101 0064 0053 0029 064e 5472 bd00  .....d.S.).NTr..
+00002e30: 0000 da01 727a 0655 6e7a 6970 207a 0420  ....rz.Unzip z. 
+00002e40: 746f 2029 0a72 4700 0000 72bf 0000 0072  to ).rG...r....r
+00002e50: c700 0000 da0a 6973 5f7a 6970 6669 6c65  ......is_zipfile
+00002e60: da0e 4173 7365 7274 696f 6e45 7272 6f72  ..AssertionError
+00002e70: 72c8 0000 005a 086e 616d 656c 6973 74da  r....Z.namelist.
+00002e80: 0765 7874 7261 6374 723e 0000 0072 4600  .extractr>...rF.
+00002e90: 0000 2904 5a08 7a69 705f 6669 6c65 72c5  ..).Z.zip_filer.
+00002ea0: 0000 005a 0266 7a72 3100 0000 7208 0000  ...Z.fzr1...r...
+00002eb0: 0072 0800 0000 720a 0000 00da 0a75 6e7a  .r....r......unz
+00002ec0: 6970 5f66 696c 65cb 0100 0073 0e00 0000  ip_file....s....
+00002ed0: 0001 0e01 0e02 0c01 0c01 0e01 0801 72d4  ..............r.
+00002ee0: 0000 00a9 02fa 052a 2e6a 7067 fa05 2a2e  .......*.jpg..*.
+00002ef0: 706e 6754 4663 0500 0000 0000 0000 0000  pngTFc..........
+00002f00: 0000 0800 0000 0700 0000 4300 0000 738a  ..........C...s.
+00002f10: 0000 0074 007c 0174 0174 0266 0283 0273  ...t.|.t.t.f...s
+00002f20: 147c 0167 017d 0167 007d 057c 0144 005d  .|.g.}.g.}.|.D.]
+00002f30: 3a7d 067c 0472 3e7c 05a0 0374 0174 047c  :}.|.r>|...t.t.|
+00002f40: 0083 01a0 057c 06a1 0183 01a1 0101 0071  .....|.........q
+00002f50: 1c7c 05a0 0374 0174 047c 0083 01a0 067c  .|...t.t.|.....|
+00002f60: 06a1 0183 01a1 0101 0071 1c7c 0272 6c74  .........q.|.rlt
+00002f70: 077c 0564 0164 0284 0064 038d 027d 057c  .|.d.d...d...}.|
+00002f80: 0372 8664 0464 0284 007d 0774 0174 087c  .r.d.d...}.t.t.|
+00002f90: 077c 0583 0283 017d 057c 0553 0029 054e  .|.....}.|.S.).N
+00002fa0: 6301 0000 0000 0000 0000 0000 0001 0000  c...............
+00002fb0: 0001 0000 0053 0000 0073 0600 0000 7c00  .....S...s....|.
+00002fc0: 6a00 5300 7225 0000 0029 0172 5c00 0000  j.S.r%...).r\...
+00002fd0: 2901 728e 0000 0072 0800 0000 7208 0000  ).r....r....r...
+00002fe0: 0072 0a00 0000 da08 3c6c 616d 6264 613e  .r......<lambda>
+00002ff0: e501 0000 f300 0000 007a 2867 6574 5f66  .........z(get_f
+00003000: 696c 656c 6973 745f 7265 6375 7273 6976  ilelist_recursiv
+00003010: 652e 3c6c 6f63 616c 733e 2e3c 6c61 6d62  e.<locals>.<lamb
+00003020: 6461 3e29 01da 036b 6579 6301 0000 0000  da>)...keyc.....
+00003030: 0000 0000 0000 0001 0000 0002 0000 0053  ...............S
+00003040: 0000 0073 0800 0000 7400 7c00 8301 5300  ...s....t.|...S.
+00003050: 7225 0000 00a9 0172 2900 0000 a901 da01  r%.....r).......
+00003060: 7872 0800 0000 7208 0000 0072 0a00 0000  xr....r....r....
+00003070: 72d8 0000 00e8 0100 0072 d900 0000 2909  r........r....).
+00003080: 7261 0000 00da 046c 6973 74da 0574 7570  ra.....list..tup
+00003090: 6c65 da06 6578 7465 6e64 7202 0000 005a  le..extendr....Z
+000030a0: 0572 676c 6f62 72c0 0000 00da 0673 6f72  .rglobr......sor
+000030b0: 7465 64da 036d 6170 2908 5a09 6469 7265  ted..map).Z.dire
+000030c0: 6374 6f72 79da 0365 7874 da04 736f 7274  ctory..ext..sort
+000030d0: 5a06 746f 5f73 7472 da09 7265 6375 7273  Z.to_str..recurs
+000030e0: 6976 655a 0966 696c 655f 6c69 7374 5a04  iveZ.file_listZ.
+000030f0: 5f65 7874 da04 6675 6e63 7208 0000 0072  _ext..funcr....r
+00003100: 0800 0000 720a 0000 00da 1667 6574 5f66  ....r......get_f
+00003110: 696c 656c 6973 745f 7265 6375 7273 6976  ilelist_recursiv
+00003120: 65d6 0100 0073 1a00 0000 0006 0e01 0601  e....s..........
+00003130: 0401 0801 0401 1a02 1a01 0401 1002 0401  ................
+00003140: 0801 0e02 72e7 0000 0063 0100 0000 0000  ....r....c......
+00003150: 0000 0000 0000 0500 0000 0600 0000 4300  ..............C.
+00003160: 0000 735e 0000 0074 007c 0083 017d 0174  ..s^...t.|...}.t
+00003170: 01a0 017c 01a1 017d 0274 027c 0264 0164  ...|...}.t.|.d.d
+00003180: 028d 0244 005d 3a5c 027d 037d 0474 037c  ...D.]:\.}.}.t.|
+00003190: 0474 0483 0272 387c 0464 0319 007d 0474  .t...r8|.d...}.t
+000031a0: 056a 06a0 077c 04a1 0173 1e74 0864 047c  .j...|...s.t.d.|
+000031b0: 039b 0064 057c 049b 009d 0483 0101 0071  ...d.|.........q
+000031c0: 1e64 0053 0029 064e 7232 0000 0029 0172  .d.S.).Nr2...).r
+000031d0: 3600 0000 7201 0000 007a 0745 7272 6f72  6...r....z.Error
+000031e0: 3a20 7278 0000 0029 09da 1a72 6561 645f  : rx...)...read_
+000031f0: 696d 6167 655f 6c69 7374 5f66 726f 6d5f  image_list_from_
+00003200: 6669 6c65 7372 3400 0000 da09 656e 756d  filesr4.....enum
+00003210: 6572 6174 6572 6100 0000 72de 0000 0072  eratera...r....r
+00003220: 4700 0000 728e 0000 0072 b600 0000 7246  G...r....r....rF
+00003230: 0000 0029 05da 0a69 6d61 6765 5f66 696c  ...)...image_fil
+00003240: 65da 0a69 6d61 6765 5f6c 6973 7472 2a00  e..image_listr*.
+00003250: 0000 da03 6964 785a 0a69 6d61 6765 5f70  ....idxZ.image_p
+00003260: 6174 6872 0800 0000 7208 0000 0072 0a00  athr....r....r..
+00003270: 0000 da19 6368 6563 6b5f 696d 6167 655f  ....check_image_
+00003280: 6c69 7374 5f76 616c 6964 6974 79ee 0100  list_validity...
+00003290: 0073 1000 0000 0001 0801 0a01 1401 0a01  .s..............
+000032a0: 0801 0c01 1601 72ed 0000 00a9 0372 d600  ......r......r..
+000032b0: 0000 72d7 0000 007a 062a 2e6a 7065 6763  ..r....z.*.jpegc
+000032c0: 0300 0000 0000 0000 0000 0000 0700 0000  ................
+000032d0: 0a00 0000 4300 0000 739c 0000 0074 007c  ....C...s....t.|
+000032e0: 0074 0174 0266 0283 0273 147c 0067 017d  .t.t.f...s.|.g.}
+000032f0: 0067 007d 037c 0044 005d 647d 0474 036a  .g.}.|.D.]d}.t.j
+00003300: 04a0 057c 04a1 0172 4c74 067c 047c 0264  ...|...rLt.|.|.d
+00003310: 018d 027d 0574 0174 0764 0264 0384 007c  ...}.t.t.d.d...|
+00003320: 0583 0283 017d 056e 2a74 087c 0483 018f  .....}.n*t.|....
+00003330: 0e7d 067c 06a0 09a1 007d 0557 0035 0051  .}.|.....}.W.5.Q
+00003340: 0052 0058 0064 0464 0584 007c 0544 0083  .R.X.d.d...|.D..
+00003350: 017d 057c 03a0 0a7c 05a1 0101 0071 1c7c  .}.|...|.....q.|
+00003360: 0172 9874 0174 0764 0664 0384 007c 0383  .r.t.t.d.d...|..
+00003370: 0283 017d 037c 0353 0029 077a 6e0a 0a20  ...}.|.S.).zn.. 
+00003380: 203a 7061 7261 6d20 696d 6167 655f 6c69   :param image_li
+00003390: 7374 5f66 696c 653a 205b 696d 6167 655f  st_file: [image_
+000033a0: 6c69 7374 2e74 7874 2c20 5d20 6f72 205b  list.txt, ] or [
+000033b0: 696d 6167 655f 6469 722c 205d 0a20 203a  image_dir, ].  :
+000033c0: 7061 7261 6d20 636f 6d70 7265 7373 3a0a  param compress:.
+000033d0: 2020 3a70 6172 616d 2065 7874 3a0a 2020    :param ext:.  
+000033e0: 3a72 6574 7572 6e3a 0a20 2029 0172 e300  :return:.  ).r..
+000033f0: 0000 6301 0000 0000 0000 0000 0000 0001  ..c.............
+00003400: 0000 0002 0000 0053 0000 0073 0a00 0000  .......S...s....
+00003410: 7400 7c00 8301 6701 5300 7225 0000 0072  t.|...g.S.r%...r
+00003420: db00 0000 72dc 0000 0072 0800 0000 7208  ....r....r....r.
+00003430: 0000 0072 0a00 0000 72d8 0000 000a 0200  ...r....r.......
+00003440: 0072 d900 0000 7a2c 7265 6164 5f69 6d61  .r....z,read_ima
+00003450: 6765 5f6c 6973 745f 6672 6f6d 5f66 696c  ge_list_from_fil
+00003460: 6573 2e3c 6c6f 6361 6c73 3e2e 3c6c 616d  es.<locals>.<lam
+00003470: 6264 613e 6301 0000 0000 0000 0000 0000  bda>c...........
+00003480: 0002 0000 0005 0000 0053 0000 0073 1a00  .........S...s..
+00003490: 0000 6700 7c00 5d12 7d01 7c01 a000 a100  ..g.|.].}.|.....
+000034a0: a001 6400 a101 9102 7104 5300 2901 fa01  ..d.....q.S.)...
+000034b0: 2029 0272 8d00 0000 da05 7370 6c69 7429   ).r......split)
+000034c0: 02da 022e 3072 6500 0000 7208 0000 0072  ....0re...r....r
+000034d0: 0800 0000 720a 0000 00da 0a3c 6c69 7374  ....r......<list
+000034e0: 636f 6d70 3e0e 0200 0073 0400 0000 0600  comp>....s......
+000034f0: 0200 7a2e 7265 6164 5f69 6d61 6765 5f6c  ..z.read_image_l
+00003500: 6973 745f 6672 6f6d 5f66 696c 6573 2e3c  ist_from_files.<
+00003510: 6c6f 6361 6c73 3e2e 3c6c 6973 7463 6f6d  locals>.<listcom
+00003520: 703e 6301 0000 0000 0000 0000 0000 0001  p>c.............
+00003530: 0000 0002 0000 0053 0000 0073 1800 0000  .......S...s....
+00003540: 7400 7c00 8301 6401 6b02 7214 7c00 6402  t.|...d.k.r.|.d.
+00003550: 1900 5300 7c00 5300 2903 4e72 3200 0000  ..S.|.S.).Nr2...
+00003560: 7201 0000 0029 0172 b400 0000 72dc 0000  r....).r....r...
+00003570: 0072 0800 0000 7208 0000 0072 0a00 0000  .r....r....r....
+00003580: 72d8 0000 0012 0200 0072 d900 0000 290b  r........r....).
+00003590: 7261 0000 0072 de00 0000 72df 0000 0072  ra...r....r....r
+000035a0: 4700 0000 728e 0000 0072 b700 0000 72e7  G...r....r....r.
+000035b0: 0000 0072 e200 0000 7211 0000 00da 0972  ...r....r......r
+000035c0: 6561 646c 696e 6573 72e0 0000 0029 075a  eadlinesr....).Z
+000035d0: 0f69 6d61 6765 5f6c 6973 745f 6669 6c65  .image_list_file
+000035e0: da08 636f 6d70 7265 7373 72e3 0000 005a  ..compressr....Z
+000035f0: 0e61 6c6c 5f69 6d61 6765 5f6c 6973 7472  .all_image_listr
+00003600: ea00 0000 72eb 0000 0072 1600 0000 7208  ....r....r....r.
+00003610: 0000 0072 0800 0000 720a 0000 0072 e800  ...r....r....r..
+00003620: 0000 f901 0000 731c 0000 0000 0a0e 0106  ......s.........
+00003630: 0204 0108 010c 010c 0114 020a 0112 010e  ................
+00003640: 010c 0204 0112 0172 e800 0000 6303 0000  .......r....c...
+00003650: 0000 0000 0000 0000 0004 0000 0006 0000  ................
+00003660: 0043 0000 0073 2600 0000 7c02 7d03 7a10  .C...s&...|.}.z.
+00003670: 7400 7c01 8301 7c00 8301 7d03 5700 6e0c  t.|...|...}.W.n.
+00003680: 0100 0100 0100 5900 6e02 5800 7c03 5300  ......Y.n.X.|.S.
+00003690: 7225 0000 0072 0300 0000 2904 7253 0000  r%...r....).rS..
+000036a0: 0072 5400 0000 da07 6465 6661 756c 74da  .rT.....default.
+000036b0: 0372 6574 7208 0000 0072 0800 0000 720a  .retr....r....r.
+000036c0: 0000 0072 5800 0000 1602 0000 730c 0000  ...rX.......s...
+000036d0: 0000 0104 0102 0110 0106 0106 0172 5800  .............rX.
+000036e0: 0000 6300 0000 0000 0000 0000 0000 0000  ..c.............
+000036f0: 0000 0002 0000 0040 0000 0073 2000 0000  .......@...s ...
+00003700: 6500 5a01 6400 5a02 6401 5a03 6402 6403  e.Z.d.Z.d.Z.d.d.
+00003710: 8400 5a04 6404 6405 8400 5a05 6406 5300  ..Z.d.d...Z.d.S.
+00003720: 2907 da09 5465 726d 436f 6c6f 727a 230a  )...TermColorz#.
+00003730: 2020 6578 706f 7274 2041 4e53 495f 434f    export ANSI_CO
+00003740: 4c4f 5253 5f44 4953 4142 4c45 443d 310a  LORS_DISABLED=1.
+00003750: 2020 6301 0000 0000 0000 0000 0000 0003    c.............
+00003760: 0000 0005 0000 0043 0000 0073 3600 0000  .......C...s6...
+00003770: 6401 6402 6c00 6d01 7d01 6d02 7d02 0100  d.d.l.m.}.m.}...
+00003780: 6403 7c00 5f03 6404 7c00 5f04 6405 6406  d.|._.d.|._.d.d.
+00003790: 6407 6408 6409 6705 7c00 5f05 6401 7c00  d.d.d.g.|._.d.|.
+000037a0: 5f06 6400 5300 290a 4e72 0100 0000 2902  _.d.S.).Nr....).
+000037b0: da07 636f 6c6f 7265 64da 0643 4f4c 4f52  ..colored..COLOR
+000037c0: 535a 0467 7265 79da 0567 7265 656e 5a03  SZ.grey..greenZ.
+000037d0: 7265 645a 0679 656c 6c6f 775a 0462 6c75  redZ.yellowZ.blu
+000037e0: 655a 076d 6167 656e 7461 5a04 6379 616e  eZ.magentaZ.cyan
+000037f0: 2907 5a09 7465 726d 636f 6c6f 7272 f800  ).Z.termcolorr..
+00003800: 0000 72f9 0000 005a 0562 6c61 636b 72fa  ..r....Z.blackr.
+00003810: 0000 00da 0663 6f6c 6f72 73da 0963 7572  .....colors..cur
+00003820: 5f63 6f6c 6f72 2903 722c 0000 0072 f800  _color).r,...r..
+00003830: 0000 72f9 0000 0072 0800 0000 7208 0000  ..r....r....r...
+00003840: 0072 0a00 0000 7238 0000 0022 0200 0073  .r....r8..."...s
+00003850: 0c00 0000 0001 1001 0601 0601 1001 0601  ................
+00003860: 7a12 5465 726d 436f 6c6f 722e 5f5f 696e  z.TermColor.__in
+00003870: 6974 5f5f 6301 0000 0000 0000 0000 0000  it__c...........
+00003880: 0002 0000 0003 0000 0043 0000 0073 1e00  .........C...s..
+00003890: 0000 7c00 6a00 7c00 6a01 1900 7d01 7c00  ..|.j.|.j...}.|.
+000038a0: 0400 6a01 6401 3700 0200 5f01 7c01 5300  ..j.d.7..._.|.S.
+000038b0: 7239 0000 0029 0272 fb00 0000 72fc 0000  r9...).r....r...
+000038c0: 0029 0272 2c00 0000 5a05 636f 6c6f 7272  .).r,...Z.colorr
+000038d0: 0800 0000 7208 0000 0072 0a00 0000 da0b  ....r....r......
+000038e0: 6765 745f 615f 636f 6c6f 722a 0200 0073  get_a_color*...s
+000038f0: 0600 0000 0001 0c01 0e01 7a15 5465 726d  ..........z.Term
+00003900: 436f 6c6f 722e 6765 745f 615f 636f 6c6f  Color.get_a_colo
+00003910: 724e 2906 7242 0000 0072 4300 0000 7244  rN).rB...rC...rD
+00003920: 0000 0072 8800 0000 7238 0000 0072 fd00  ...r....r8...r..
+00003930: 0000 7208 0000 0072 0800 0000 7208 0000  ..r....r....r...
+00003940: 0072 0a00 0000 72f7 0000 001e 0200 0073  .r....r........s
+00003950: 0600 0000 0801 0403 0808 72f7 0000 0063  ..........r....c
+00003960: 0300 0000 0000 0000 0000 0000 0400 0000  ................
+00003970: 0600 0000 4300 0000 735c 0000 0074 00a0  ....C...s\...t..
+00003980: 017c 00a1 017d 0074 027c 0083 017d 0064  .|...}.t.|...}.d
+00003990: 017d 037c 037c 029b 0064 029d 0237 007d  .}.|.|...d...7.}
+000039a0: 037c 0172 3e7c 0374 03a0 0474 05a0 067c  .|.r>|.t...t...|
+000039b0: 00a1 01a1 0137 007d 036e 127c 0374 076a  .....7.}.n.|.t.j
+000039c0: 087c 0064 0364 048d 0237 007d 037c 0364  .|.d.d...7.}.|.d
+000039d0: 0537 007d 037c 0353 0029 064e 722e 0000  .7.}.|.S.).Nr...
+000039e0: 007a 2920 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  .z) ------------
+000039f0: 2d2d 2d2d 2d20 7374 6172 7420 2d2d 2d2d  ----- start ----
+00003a00: 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a 720c 0000  -----------.r...
+00003a10: 00a9 01da 0669 6e64 656e 747a 2a0a 2d2d  .....indentz*.--
+00003a20: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d20  --------------- 
+00003a30: 456e 6420 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  End ------------
+00003a40: 2d2d 2d2d 2d2d 2d29 09da 0463 6f70 79da  -------)...copy.
+00003a50: 0864 6565 7063 6f70 7972 6300 0000 da06  .deepcopyrc.....
+00003a60: 7070 7269 6e74 da07 7066 6f72 6d61 7472  pprint..pformatr
+00003a70: 5e00 0000 725f 0000 00da 046a 736f 6eda  ^...r_.....json.
+00003a80: 0564 756d 7073 2904 da08 6469 6374 5f6f  .dumps)...dict_o
+00003a90: 626a 5a0a 7573 655f 7070 7269 6e74 5a0a  bjZ.use_pprintZ.
+00003aa0: 7072 6566 6978 5f73 7472 da07 6d65 7373  prefix_str..mess
+00003ab0: 6167 6572 0800 0000 7208 0000 0072 0a00  ager....r....r..
+00003ac0: 0000 da0b 6469 6374 3273 7472 696e 6730  ....dict2string0
+00003ad0: 0200 0073 1200 0000 0001 0a01 0801 0401  ...s............
+00003ae0: 0e01 0401 1602 1201 0801 7208 0100 0063  ..........r....c
+00003af0: 0200 0000 0000 0000 0000 0000 0300 0000  ................
+00003b00: 0900 0000 4300 0000 732a 0000 0074 007c  ....C...s*...t.|
+00003b10: 0164 0183 028f 167d 0274 016a 027c 007c  .d.....}.t.j.|.|
+00003b20: 0264 0264 038d 0301 0057 0035 0051 0052  .d.d.....W.5.Q.R
+00003b30: 0058 0064 0053 0029 044e 726b 0000 0072  .X.d.S.).Nrk...r
+00003b40: 0c00 0000 72fe 0000 0029 0372 1100 0000  ....r....).r....
+00003b50: 7204 0100 0072 1300 0000 2903 5a08 6f62  r....r....).Z.ob
+00003b60: 6a5f 6469 6374 72bb 0000 0072 1600 0000  j_dictr....r....
+00003b70: 7208 0000 0072 0800 0000 720a 0000 00da  r....r....r.....
+00003b80: 096a 736f 6e5f 6475 6d70 3d02 0000 7306  .json_dump=...s.
+00003b90: 0000 0000 010c 011a 0172 0901 0000 72de  .........r....r.
+00003ba0: 0000 0063 0300 0000 0000 0000 0000 0000  ...c............
+00003bb0: 0700 0000 0600 0000 4300 0000 7380 0000  ........C...s...
+00003bc0: 0074 0064 017c 009b 0064 027c 019b 009d  .t.d.|...d.|....
+00003bd0: 0483 0101 0074 01a0 02a1 007d 037c 0264  .....t.....}.|.d
+00003be0: 036b 0272 387c 036a 037c 0074 0464 0467  .k.r8|.j.|.t.d.g
+00003bf0: 0064 058d 0401 006e 0a7c 03a0 037c 00a1  .d.....n.|...|..
+00003c00: 0101 007c 036a 057c 0164 068d 015c 027d  ...|.j.|.d...\.}
+00003c10: 047d 0574 067c 047c 00a0 0764 07a1 01a0  .}.t.|.|...d....
+00003c20: 0864 0764 08a1 0283 027d 0674 007c 009b  .d.d.....}.t.|..
+00003c30: 0064 097c 069b 009d 0383 0101 007c 0653  .d.|.........|.S
+00003c40: 0029 0a7a 3e0a 0a20 203a 7061 7261 6d20  .).z>..  :param 
+00003c50: 6e61 6d65 3a20 272d 2d74 6c5f 6f70 7473  name: '--tl_opts
+00003c60: 270a 2020 3a70 6172 616d 2061 7267 765f  '.  :param argv_
+00003c70: 6c69 7374 3a0a 2020 3a72 6574 7572 6e3a  list:.  :return:
+00003c80: 0a20 207a 0a50 6172 7365 7269 6e67 207a  .  z.Parsering z
+00003c90: 0720 6672 6f6d 200a 72de 0000 00da 012a  . from .r......*
+00003ca0: 2903 da04 7479 7065 da05 6e61 7267 7372  )...type..nargsr
+00003cb0: f500 0000 2901 da04 6172 6773 fa01 2dda  ....)...args..-.
+00003cc0: 015f fa01 3d29 0972 4600 0000 da08 6172  ._..=).rF.....ar
+00003cd0: 6770 6172 7365 da0e 4172 6775 6d65 6e74  gparse..Argument
+00003ce0: 5061 7273 6572 da0c 6164 645f 6172 6775  Parser..add_argu
+00003cf0: 6d65 6e74 7229 0000 00da 1070 6172 7365  mentr).....parse
+00003d00: 5f6b 6e6f 776e 5f61 7267 73da 0767 6574  _known_args..get
+00003d10: 6174 7472 728d 0000 00da 0772 6570 6c61  attrr......repla
+00003d20: 6365 2907 725c 0000 00da 0961 7267 765f  ce).r\.....argv_
+00003d30: 6c69 7374 720b 0100 00da 0670 6172 7365  listr......parse
+00003d40: 7272 0d01 0000 720f 0100 00da 0576 616c  rr....r......val
+00003d50: 7565 7208 0000 0072 0800 0000 720a 0000  uer....r....r...
+00003d60: 00da 1570 6172 7365 725f 6172 6773 5f66  ...parser_args_f
+00003d70: 726f 6d5f 6c69 7374 4302 0000 7312 0000  rom_listC...s...
+00003d80: 0000 0714 0108 0108 0114 020a 0110 0218  ................
+00003d90: 0112 0172 1a01 0000 6300 0000 0000 0000  ...r....c.......
+00003da0: 0000 0000 0002 0000 0004 0000 0043 0000  .............C..
+00003db0: 0073 3c00 0000 6401 6400 6c00 7d00 7401  .s<...d.d.l.}.t.
+00003dc0: 7c00 6402 6400 8303 7d01 7c01 6400 6b08  |.d.d...}.|.d.k.
+00003dd0: 722a 6401 7338 7402 6403 8301 8201 6e0e  r*d.s8t.d.....n.
+00003de0: 7c01 8300 7234 6404 5300 6405 5300 6400  |...r4d.S.d.S.d.
+00003df0: 5300 2906 4e72 0100 0000 da08 6765 7474  S.).Nr......gett
+00003e00: 7261 6365 7a0f 4e6f 2073 7973 2e67 6574  racez.No sys.get
+00003e10: 7472 6163 6554 4629 0372 9000 0000 7215  traceTF).r....r.
+00003e20: 0100 0072 d200 0000 2902 7290 0000 0072  ...r....).r....r
+00003e30: 1b01 0000 7208 0000 0072 0800 0000 720a  ....r....r....r.
+00003e40: 0000 00da 0c69 735f 6465 6275 6767 696e  .....is_debuggin
+00003e50: 6757 0200 0073 0e00 0000 0001 0801 0c02  gW...s..........
+00003e60: 0801 0e01 0601 0402 721c 0100 00fa 1363  ........r......c
+00003e70: 6f6e 6669 675f 636f 6d6d 616e 642e 7961  onfig_command.ya
+00003e80: 6d6c 6302 0000 0000 0000 0000 0000 0004  mlc.............
+00003e90: 0000 0006 0000 0043 0000 0073 3000 0000  .......C...s0...
+00003ea0: 6401 6402 6c00 6d01 7d02 0100 7402 7c02  d.d.l.m.}...t.|.
+00003eb0: a003 7c00 9b00 6403 7c01 9b00 9d03 a101  ..|...d.|.......
+00003ec0: a004 a100 8301 6401 1900 7d03 7c03 5300  ......d...}.|.S.
+00003ed0: 2904 4e72 0100 0000 2901 da09 544c 4366  ).Nr....)...TLCf
+00003ee0: 674e 6f64 65fa 012f 2905 5a0f 746c 322e  gNode../).Z.tl2.
+00003ef0: 7072 6f6a 2e66 7663 6f72 6572 1e01 0000  proj.fvcorer....
+00003f00: 72de 0000 005a 0e6c 6f61 645f 7961 6d6c  r....Z.load_yaml
+00003f10: 5f66 696c 65da 0676 616c 7565 7329 0472  _file..values).r
+00003f20: c300 0000 5a08 6366 675f 6669 6c65 721e  ....Z.cfg_filer.
+00003f30: 0100 005a 0a6c 6f61 6465 645f 6366 6772  ...Z.loaded_cfgr
+00003f40: 0800 0000 7208 0000 0072 0a00 0000 da19  ....r....r......
+00003f50: 6c6f 6164 5f63 6f6e 6669 675f 636f 6d6d  load_config_comm
+00003f60: 616e 645f 7661 6c75 6563 0200 0073 0600  and_valuec...s..
+00003f70: 0000 0002 0c02 2001 7221 0100 0063 0200  ...... .r!...c..
+00003f80: 0000 0000 0000 0000 0000 0400 0000 0400  ................
+00003f90: 0000 4300 0000 7324 0000 007c 01a0 00a1  ..C...s$...|....
+00003fa0: 0044 005d 165c 027d 027d 037c 007c 0219  .D.].\.}.}.|.|..
+00003fb0: 00a0 017c 03a1 0101 0071 0864 0053 0072  ...|.....q.d.S.r
+00003fc0: 2500 0000 2902 7260 0000 0072 3500 0000  %...).r`...r5...
+00003fd0: 2904 5a09 6473 745f 6464 6963 745a 0973  ).Z.dst_ddictZ.s
+00003fe0: 7263 5f64 6469 6374 7264 0000 0072 6500  rc_ddictrd...re.
+00003ff0: 0000 7208 0000 0072 0800 0000 720a 0000  ..r....r....r...
+00004000: 00da 166d 6572 6765 5f64 6566 6175 6c74  ...merge_default
+00004010: 6469 6374 5f64 6963 746b 0200 0073 0600  dict_dictk...s..
+00004020: 0000 0002 1001 1001 7222 0100 0072 3200  ........r"...r2.
+00004030: 0000 fa08 2530 3664 2e70 6e67 6304 0000  ....%06d.pngc...
+00004040: 0000 0000 0000 0000 0006 0000 000c 0000  ................
+00004050: 0043 0000 0073 7c00 0000 7400 6a01 7c01  .C...s|...t.j.|.
+00004060: 6401 6402 8d02 0100 6403 6404 7c00 6405  d.d.....d.d.|.d.
+00004070: 7c02 9b00 6406 6407 6408 6409 7c01 9b00  |...d.d.d.d.|...
+00004080: 640a 7c03 9b00 9d03 670a 7d04 7402 640b  d.|.....g.}.t.d.
+00004090: 640c a003 7c04 a101 9b00 9d02 8301 0100  d...|...........
+000040a0: 7404 a005 7c04 a101 0100 7406 7c01 8301  t...|.....t.|...
+000040b0: 7d05 7402 640d 7407 7c05 8301 9b00 640e  }.t.d.t.|.....d.
+000040c0: 7c01 9b00 640a 7c03 9b00 640f 9d07 8301  |...d.|...d.....
+000040d0: 0100 6400 5300 2910 4e54 72bd 0000 005a  ..d.S.).NTr....Z
+000040e0: 0666 666d 7065 677a 022d 697a 022d 727a  .ffmpegz.-iz.-rz
+000040f0: 022d 665a 0669 6d61 6765 327a 022d 76da  .-fZ.image2z.-v.
+00004100: 0565 7272 6f72 721f 0100 007a 1020 3d3e  .errorr....z. =>
+00004110: 2052 756e 6e69 6e67 3a20 0a09 2072 ef00   Running: .. r..
+00004120: 0000 7274 0000 007a 1120 696d 6167 6573  ..rt...z. images
+00004130: 2073 6176 6564 2074 6f20 fa01 2229 0872   saved to ..").r
+00004140: 4700 0000 72bf 0000 0072 4600 0000 72c2  G...r....rF...r.
+00004150: 0000 00da 0a73 7562 7072 6f63 6573 73da  .....subprocess.
+00004160: 0463 616c 6c72 e700 0000 72b4 0000 0029  .callr....r....)
+00004170: 065a 0876 6964 5f66 696c 655a 0a69 6d67  .Z.vid_fileZ.img
+00004180: 5f66 6f6c 6465 725a 0a66 7261 6d65 5f66  _folderZ.frame_f
+00004190: 7265 71da 0666 6f72 6d61 7472 4900 0000  req..formatrI...
+000041a0: 5a08 696d 675f 6c69 7374 7208 0000 0072  Z.img_listr....r
+000041b0: 0800 0000 720a 0000 00da 1666 666d 7065  ....r......ffmpe
+000041c0: 675f 7669 6465 6f5f 746f 5f66 7261 6d65  g_video_to_frame
+000041d0: 7373 0200 0073 2200 0000 0004 0e02 0201  ss...s".........
+000041e0: 0200 0201 0200 0401 0200 0201 0200 0201  ................
+000041f0: 0cfb 0407 1401 0a02 0802 2002 7229 0100  .......... .r)..
+00004200: 0063 0200 0000 0000 0000 0000 0000 0300  .c..............
+00004210: 0000 0900 0000 4300 0000 734e 0000 0074  ......C...sN...t
+00004220: 006a 0174 006a 02a0 037c 00a1 0164 0164  .j.t.j...|...d.d
+00004230: 028d 0201 0074 047c 0064 0383 028f 247d  .....t.|.d....$}
+00004240: 0274 057c 0174 0683 0272 367c 01a0 0764  .t.|.t...r6|...d
+00004250: 04a1 017d 017c 02a0 087c 01a1 0101 0057  ...}.|...|.....W
+00004260: 0035 0051 0052 0058 0064 0553 0029 067a  .5.Q.R.X.d.S.).z
+00004270: 470a 2020 0a20 203a 7061 7261 6d20 7361  G.  .  :param sa
+00004280: 7665 645f 7061 7468 3a0a 2020 3a70 6172  ved_path:.  :par
+00004290: 616d 2064 6174 613a 2055 6e69 6f6e 5b62  am data: Union[b
+000042a0: 7974 6573 2c20 7374 725d 0a20 203a 7265  ytes, str].  :re
+000042b0: 7475 726e 3a0a 2020 5472 bd00 0000 720f  turn:.  Tr....r.
+000042c0: 0000 00da 0475 7466 384e 2909 7247 0000  .....utf8N).rG..
+000042d0: 0072 bf00 0000 728e 0000 0072 8f00 0000  .r....r....r....
+000042e0: 7211 0000 0072 6100 0000 7229 0000 0072  r....ra...r)...r
+000042f0: 7000 0000 726c 0000 0029 0372 1400 0000  p...rl...).r....
+00004300: 7215 0000 005a 0466 6f75 7472 0800 0000  r....Z.foutr....
+00004310: 7208 0000 0072 0a00 0000 da0b 7772 6974  r....r......writ
+00004320: 655f 6279 7465 738c 0200 0073 0c00 0000  e_bytes....s....
+00004330: 0008 1601 0c01 0a01 0a01 1401 722b 0100  ............r+..
+00004340: 0063 0100 0000 0000 0000 0000 0000 0300  .c..............
+00004350: 0000 0900 0000 4300 0000 7322 0000 0074  ......C...s"...t
+00004360: 007c 0064 0183 028f 0e7d 017c 01a0 01a1  .|.d.....}.|....
+00004370: 007d 0257 0035 0051 0052 0058 007c 0253  .}.W.5.Q.R.X.|.S
+00004380: 0029 024e 7218 0000 0029 0272 1100 0000  .).Nr....).r....
+00004390: da04 7265 6164 2903 72bb 0000 005a 0366  ..read).r....Z.f
+000043a0: 696e 5a0a 6461 7461 5f62 7974 6573 7208  inZ.data_bytesr.
+000043b0: 0000 0072 0800 0000 720a 0000 00da 0a72  ...r....r......r
+000043c0: 6561 645f 6279 7465 739b 0200 0073 0600  ead_bytes....s..
+000043d0: 0000 0001 0c01 1201 722d 0100 0029 0172  ........r-...).r
+000043e0: 2e00 0000 2903 7273 0000 0072 2e00 0000  ....).rs...r....
+000043f0: 722e 0000 0029 014e 2904 72d5 0000 0054  r....).N).r....T
+00004400: 4654 2902 4672 ee00 0000 2901 4e29 0254  FT).Fr....).N).T
+00004410: 722e 0000 0029 0172 de00 0000 2901 721d  r....).r....).r.
+00004420: 0100 0029 0272 3200 0000 7223 0100 0029  ...).r2...r#...)
+00004430: 4dda 0269 6f72 0001 0000 5a05 6e75 6d70  M..ior....Z.nump
+00004440: 7972 8900 0000 727d 0000 0072 c000 0000  yr....r}...r....
+00004450: da07 7061 7468 6c69 6272 0200 0000 7234  ..pathlibr....r4
+00004460: 0000 0072 1101 0000 7247 0000 00da 0272  ...r....rG.....r
+00004470: 6572 5900 0000 7290 0000 00da 0969 6d70  erY...r......imp
+00004480: 6f72 746c 6962 7204 0100 0072 0201 0000  ortlibr....r....
+00004490: 725e 0000 00da 086f 7065 7261 746f 7272  r^.....operatorr
+000044a0: 0400 0000 72c7 0000 0072 b800 0000 7205  ....r....r....r.
+000044b0: 0000 0072 0600 0000 da0f 6d75 6c74 6970  ...r......multip
+000044c0: 726f 6365 7373 696e 6772 6f00 0000 7223  rocessingro...r#
+000044d0: 0000 0072 1200 0000 7226 0100 0072 0b00  ...r....r&...r..
+000044e0: 0000 720e 0000 0072 1700 0000 721e 0000  ..r....r....r...
+000044f0: 0072 1f00 0000 7222 0000 0072 2400 0000  .r....r"...r$...
+00004500: 7226 0000 0072 5300 0000 7227 0000 0072  r&...rS...r'...r
+00004510: 4a00 0000 7251 0000 0072 5d00 0000 7263  J...rQ...r]...rc
+00004520: 0000 0072 6900 0000 726e 0000 0072 7200  ...ri...rn...rr.
+00004530: 0000 727e 0000 0072 8200 0000 7283 0000  ..r~...r....r...
+00004540: 0072 8a00 0000 da07 5072 6f63 6573 7372  .r......Processr
+00004550: 8b00 0000 7296 0000 0072 9800 0000 729c  ....r....r....r.
+00004560: 0000 0072 9e00 0000 72a2 0000 0072 a900  ...r....r....r..
+00004570: 0000 72a3 0000 0072 b100 0000 72ad 0000  ..r....r....r...
+00004580: 0072 cf00 0000 72d4 0000 0072 e700 0000  .r....r....r....
+00004590: 72ed 0000 0072 e800 0000 7258 0000 0072  r....r....rX...r
+000045a0: f700 0000 7208 0100 0072 0901 0000 721a  ....r....r....r.
+000045b0: 0100 0072 1c01 0000 7221 0100 0072 2201  ...r....r!...r".
+000045c0: 0000 7229 0100 0072 2b01 0000 722d 0100  ..r)...r+...r-..
+000045d0: 0072 0800 0000 7208 0000 0072 0800 0000  .r....r....r....
+000045e0: 720a 0000 00da 083c 6d6f 6475 6c65 3e01  r......<module>.
+000045f0: 0000 0073 a400 0000 0801 0801 0801 0801  ...s............
+00004600: 0801 0c01 0801 0801 0801 0801 0801 0801  ................
+00004610: 0801 0801 0801 0801 0c01 0801 0801 1001  ................
+00004620: 0801 0801 0801 0801 0803 080c 0812 0809  ................
+00004630: 0805 0805 0806 0809 0805 102e 0806 0a0b  ................
+00004640: 0812 080a 080c 0807 0804 0402 0001 0001  ................
+00004650: 00fd 0a1f 0e18 080b 120f 0805 080a 0812  ................
+00004660: 0a18 0808 0401 1019 0402 103c 080d 080c  ...........<....
+00004670: 0001 0001 0001 00fc 0a18 080c 0001 00fe  ................
+00004680: 0a1d 0a08 1012 0a0d 0806 0a14 080d 00ff  ................
+00004690: 0a08 080a 0001 00fd 0a19 080f            ............
```

### Comparing `tl2-0.1.0/tl2/imagenet/imagenet_utils.py` & `tl2-0.1.1/tl2/imagenet/imagenet_utils.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-import os
-
-
-def _get_subdir2name_dict(map_file=None):
-  if map_file is None:
-    map_file = "tl2_lib/tl2/imagenet/map_subdir_id_name.txt"
-  with open(map_file, 'r') as f:
-    map_list = f.readlines()
-  subdir2name_dict = {}
-  for line in map_list:
-    line = line.strip()
-    subdir, _, name = line.split(' ')
-    subdir2name_dict[subdir] = name
-  return subdir2name_dict
-
-subdir2name_dict = _get_subdir2name_dict()
-
-def get_subdir2name_dict(map_file=None):
-  if map_file is None:
-    return subdir2name_dict
-  else:
-    return _get_subdir2name_dict(map_file=map_file)
-
-
-# def get_imagenet_id2class_for_classification():
-#   cur_dir = os.path.dirname(__file__)
-#   label_file = os.path.join(cur_dir, 'imagenet_label.txt')
-#   id_to_label = {}
-#   with open(label_file) as f:
-#     for label_str in f.readlines():
-#       class_idx, name = label_str.strip('{ ,\n').split(':')
-#       name = name.strip("' ")
-#       id_to_label[int(class_idx)] = name
-#   return id_to_label
-
-
+import os
+
+
+def _get_subdir2name_dict(map_file=None):
+  if map_file is None:
+    map_file = "tl2_lib/tl2/imagenet/map_subdir_id_name.txt"
+  with open(map_file, 'r') as f:
+    map_list = f.readlines()
+  subdir2name_dict = {}
+  for line in map_list:
+    line = line.strip()
+    subdir, _, name = line.split(' ')
+    subdir2name_dict[subdir] = name
+  return subdir2name_dict
+
+subdir2name_dict = _get_subdir2name_dict()
+
+def get_subdir2name_dict(map_file=None):
+  if map_file is None:
+    return subdir2name_dict
+  else:
+    return _get_subdir2name_dict(map_file=map_file)
+
+
+# def get_imagenet_id2class_for_classification():
+#   cur_dir = os.path.dirname(__file__)
+#   label_file = os.path.join(cur_dir, 'imagenet_label.txt')
+#   id_to_label = {}
+#   with open(label_file) as f:
+#     for label_str in f.readlines():
+#       class_idx, name = label_str.strip('{ ,\n').split(':')
+#       name = name.strip("' ")
+#       id_to_label[int(class_idx)] = name
+#   return id_to_label
+
+
```

### Comparing `tl2-0.1.0/tl2/launch/__pycache__/launch_utils.cpython-38.pyc` & `tl2-0.1.1/tl2/launch/__pycache__/launch_utils.cpython-38.pyc`

 * *Format-specific differences are supported for Python .pyc files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: Byte-compiled Python module for CPython 3.8, timestamp-based, .py timestamp: Mon Oct  3 09:35:07 2022 UTC, .py size: 10635 bytes*

 * *Could not decompile bytecode: bad marshal data (unknown type code)*

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-00000000: 550d 0d0a 0000 0000 cbac 3a63 8b29 0000  U.........:c.)..
+00000000: 550d 0d0a 0000 0000 f13c 9362 c42a 0000  U........<.b.*..
 00000010: e300 0000 0000 0000 0000 0000 0000 0000  ................
 00000020: 0004 0000 0040 0000 0073 3601 0000 6400  .....@...s6...d.
 00000030: 6401 6c00 5a00 6400 6401 6c01 5a01 6400  d.l.Z.d.d.l.Z.d.
 00000040: 6401 6c02 5a02 6400 6401 6c03 5a03 6400  d.l.Z.d.d.l.Z.d.
 00000050: 6401 6c04 5a04 6400 6401 6c05 5a05 6400  d.l.Z.d.d.l.Z.d.
 00000060: 6401 6c06 5a06 6400 6402 6c07 6d07 5a07  d.l.Z.d.d.l.m.Z.
 00000070: 0100 6400 6401 6c08 5a08 6400 6401 6c09  ..d.d.l.Z.d.d.l.
@@ -76,438 +76,436 @@
 000004b0: 0373 7973 da04 6172 6776 da07 656e 7669  .sys..argv..envi
 000004c0: 726f 6e29 09da 0869 6e73 7461 6e63 65da  ron)...instance.
 000004d0: 0966 756e 635f 6e61 6d65 da04 6669 6c65  .func_name..file
 000004e0: da07 636f 6d6d 616e 64da 0a63 6c61 7373  ..command..class
 000004f0: 5f6e 616d 655a 0673 7562 6469 72da 066f  _nameZ.subdir..o
 00000500: 7574 6469 725a 0772 756e 5f73 7472 da09  utdirZ.run_str..
 00000510: 746c 5f6f 7574 6469 72a9 0072 3100 0000  tl_outdir..r1...
-00000520: fa54 2f68 6f6d 652f 6d61 2d75 7365 722f  .T/home/ma-user/
-00000530: 776f 726b 2f63 6f64 652f 7374 796c 6567  work/code/styleg
-00000540: 616e 322d 6164 612d 7079 746f 7263 682d  an2-ada-pytorch-
-00000550: 6578 702f 746c 325f 6c69 622f 746c 322f  exp/tl2_lib/tl2/
-00000560: 6c61 756e 6368 2f6c 6175 6e63 685f 7574  launch/launch_ut
-00000570: 696c 732e 7079 da16 6765 745f 636f 6d6d  ils.py..get_comm
-00000580: 616e 645f 616e 645f 6f75 7464 6972 1800  and_and_outdir..
-00000590: 0000 732e 0000 0000 050e 010c 0108 011e  ..s.............
-000005a0: 010a 0110 0212 010c 0102 020a fe04 0202  ................
-000005b0: fe04 0202 fe04 0202 fe08 050e 0212 0108  ................
-000005c0: 0104 010a 0172 3300 0000 6300 0000 0000  .....r3...c.....
-000005d0: 0000 0000 0000 0002 0000 0006 0000 0043  ...............C
-000005e0: 0000 0073 2600 0000 6401 7d00 7a0c 6402  ...s&...d.}.z.d.
-000005f0: 6400 6c00 7d01 5700 6e10 0100 0100 0100  d.l.}.W.n.......
-00000600: 6403 7d00 5900 6e02 5800 7c00 5300 2904  d.}.Y.n.X.|.S.).
-00000610: 4e54 7201 0000 0046 2901 5a06 6d6f 7869  NTr....F).Z.moxi
-00000620: 6e67 2902 da06 746c 5f6d 6f78 5a03 6d6f  ng)...tl_moxZ.mo
-00000630: 7872 3100 0000 7231 0000 0072 3200 0000  xr1...r1...r2...
-00000640: da0a 7573 655f 6d6f 7869 6e67 3300 0000  ..use_moxing3...
-00000650: 730c 0000 0000 0104 0102 010c 0106 010a  s...............
-00000660: 0172 3500 0000 4663 0200 0000 0000 0000  .r5...Fc........
-00000670: 0000 0000 0200 0000 0600 0000 4300 0000  ............C...
-00000680: 73b8 0000 007c 0073 0c74 00a0 01a1 007d  s....|.s.t.....}
-00000690: 007c 006a 0264 0174 0364 0264 038d 0301  .|.j.d.t.d.d....
-000006a0: 007c 006a 0264 0474 0364 0264 038d 0301  .|.j.d.t.d.d....
-000006b0: 007c 006a 0264 0574 0364 0664 038d 0301  .|.j.d.t.d.d....
-000006c0: 007c 006a 0264 0774 0364 0867 0064 098d  .|.j.d.t.d.g.d..
-000006d0: 0401 007c 006a 0264 0a64 0b64 0c64 0d8d  ...|.j.d.d.d.d..
-000006e0: 0301 007c 006a 0264 0e74 0364 0664 038d  ...|.j.d.t.d.d..
-000006f0: 0301 007c 006a 0264 0f64 0b64 0c64 0d8d  ...|.j.d.d.d.d..
-00000700: 0301 0074 046a 057c 0064 1074 0683 0064  ...t.j.|.d.t...d
-00000710: 118d 0301 007c 006a 0264 1274 0364 0264  .....|.j.d.t.d.d
-00000720: 038d 0301 007c 0172 b47c 006a 0264 1374  .....|.r.|.j.d.t
-00000730: 0764 1464 038d 0301 007c 0053 0029 154e  .d.d.....|.S.).N
-00000740: 7a10 2d2d 746c 5f63 6f6e 6669 675f 6669  z.--tl_config_fi
-00000750: 6c65 da00 2902 7218 0000 00da 0764 6566  le..).r......def
-00000760: 6175 6c74 7a0c 2d2d 746c 5f63 6f6d 6d61  aultz.--tl_comma
-00000770: 6e64 7214 0000 007a 0c72 6573 756c 7473  ndr....z.results
-00000780: 2f74 656d 707a 092d 2d74 6c5f 6f70 7473  /tempz.--tl_opts
-00000790: da01 2a29 0372 1800 0000 da05 6e61 7267  ..*).r......narg
-000007a0: 7372 3700 0000 7a0b 2d2d 746c 5f72 6573  sr7...z.--tl_res
-000007b0: 756d 65da 0a73 746f 7265 5f74 7275 6546  ume..store_trueF
-000007c0: 2902 da06 6163 7469 6f6e 7237 0000 007a  )...actionr7...z
-000007d0: 0e2d 2d74 6c5f 7265 7375 6d65 6469 727a  .--tl_resumedirz
-000007e0: 0a2d 2d74 6c5f 6465 6275 6772 3400 0000  .--tl_debugr4...
-000007f0: 2901 7237 0000 007a 0d2d 2d74 6c5f 7469  ).r7...z.--tl_ti
-00000800: 6d65 5f73 7472 7a0c 2d2d 6c6f 6361 6c5f  me_strz.--local_
-00000810: 7261 6e6b 7201 0000 0029 08da 0861 7267  rankr....)...arg
-00000820: 7061 7273 65da 0e41 7267 756d 656e 7450  parse..ArgumentP
-00000830: 6172 7365 72da 0c61 6464 5f61 7267 756d  arser..add_argum
-00000840: 656e 7472 1500 0000 720c 0000 005a 1161  entr....r....Z.a
-00000850: 6464 5f61 7267 756d 656e 745f 626f 6f6c  dd_argument_bool
-00000860: 7235 0000 00da 0369 6e74 2902 da06 7061  r5.....int)...pa
-00000870: 7273 6572 da11 6170 7065 6e64 5f6c 6f63  rser..append_loc
-00000880: 616c 5f72 616e 6b72 3100 0000 7231 0000  al_rankr1...r1..
-00000890: 0072 3200 0000 da0d 5f62 7569 6c64 5f70  .r2....._build_p
-000008a0: 6172 7365 723b 0000 0073 1c00 0000 0001  arser;...s......
-000008b0: 0401 0801 1001 1001 1001 1201 1001 1001  ................
-000008c0: 1001 1202 1001 0401 1001 7242 0000 0063  ..........rB...c
-000008d0: 0200 0000 0000 0000 0000 0000 0300 0000  ................
-000008e0: 0600 0000 4300 0000 73e6 0000 007c 0172  ....C...s....|.r
-000008f0: 387c 006a 007c 005f 0174 026a 03a0 047c  8|.j.|._.t.j...|
-00000900: 006a 0164 01a1 027c 005f 0574 06a0 07a1  .j.d...|._.t....
-00000910: 00a0 0864 02a1 0164 0064 0385 0219 007c  ...d...d.d.....|
-00000920: 005f 096e 6474 0a74 0b74 02a0 0c64 0464  ._.ndt.t.t...d.d
-00000930: 05a1 0283 0183 017d 0274 06a0 07a1 00a0  .......}.t......
-00000940: 0864 02a1 0164 0064 0385 0219 007c 005f  .d...d.d.....|._
-00000950: 097c 0273 6e7c 006a 016e 0e7c 006a 0164  .|.sn|.j.n.|.j.d
-00000960: 0617 007c 006a 0917 007c 005f 0174 0d6a  ...|.j...|._.t.j
-00000970: 0e7c 006a 0164 0764 088d 0201 0074 0f7c  .|.j.d.d.....t.|
-00000980: 006a 0164 098d 0101 0074 026a 03a0 107c  .j.d.....t.j...|
-00000990: 006a 01a1 017c 005f 1174 026a 03a0 047c  .j...|._.t.j...|
-000009a0: 006a 0164 0aa1 027c 005f 1274 026a 03a0  .j.d...|._.t.j..
-000009b0: 047c 006a 0164 0ba1 027c 005f 1374 026a  .|.j.d...|._.t.j
-000009c0: 03a0 047c 006a 0164 01a1 027c 005f 1464  ...|.j.d...|._.d
-000009d0: 0053 0029 0c4e 7a13 636f 6e66 6967 5f63  .S.).Nz.config_c
-000009e0: 6f6d 6d61 6e64 2e79 616d 6c7a 1025 5925  ommand.yamlz.%Y%
-000009f0: 6d25 645f 2548 254d 2553 5f25 6672 1200  m%d_%H%M%S_%fr..
-00000a00: 0000 721a 0000 0072 0100 0000 fa01 2d54  ..r....r......-T
-00000a10: 2901 da0d 6967 6e6f 7265 5f65 7272 6f72  )...ignore_error
-00000a20: 73a9 0172 3000 0000 fa07 6c6f 672e 7478  s..r0.....log.tx
-00000a30: 747a 0b63 6f6e 6669 672e 7961 6d6c 2915  tz.config.yaml).
-00000a40: da0c 746c 5f72 6573 756d 6564 6972 7230  ..tl_resumedirr0
-00000a50: 0000 0072 2000 0000 7221 0000 00da 046a  ...r ...r!.....j
-00000a60: 6f69 6eda 1574 6c5f 636f 6e66 6967 5f66  oin..tl_config_f
-00000a70: 696c 655f 7265 7375 6d65 7202 0000 00da  ile_resumer.....
-00000a80: 036e 6f77 da08 7374 7266 7469 6d65 da0b  .now..strftime..
-00000a90: 746c 5f74 696d 655f 7374 72da 0462 6f6f  tl_time_str..boo
-00000aa0: 6c72 3f00 0000 da06 6765 7465 6e76 da06  lr?.....getenv..
-00000ab0: 7368 7574 696c da06 726d 7472 6565 da0a  shutil..rmtree..
-00000ac0: 5f6d 616b 655f 6469 7273 da08 7265 616c  _make_dirs..real
-00000ad0: 7061 7468 5a0d 746c 5f61 6273 5f6f 7574  pathZ.tl_abs_out
-00000ae0: 6469 72da 0a74 6c5f 6c6f 6766 696c 65da  dir..tl_logfile.
-00000af0: 1474 6c5f 7361 7665 645f 636f 6e66 6967  .tl_saved_config
-00000b00: 5f66 696c 65da 1c74 6c5f 7361 7665 645f  _file..tl_saved_
-00000b10: 636f 6e66 6967 5f63 6f6d 6d61 6e64 5f66  config_command_f
-00000b20: 696c 6529 03da 0461 7267 73da 0672 6573  ile)...args..res
-00000b30: 756d 6572 1a00 0000 7231 0000 0072 3100  umer....r1...r1.
-00000b40: 0000 7232 0000 00da 0d5f 7365 7475 705f  ..r2....._setup_
-00000b50: 6f75 7464 6972 4d00 0000 731c 0000 0000  outdirM...s.....
-00000b60: 0104 0108 0112 011a 0214 0118 011c 0210  ................
-00000b70: 010c 0310 0312 0112 0112 0172 5800 0000  ...........rX...
-00000b80: 6302 0000 0000 0000 0000 0000 000a 0000  c...............
-00000b90: 0005 0000 0043 0000 0073 3e01 0000 7400  .....C...s>...t.
-00000ba0: 6401 6402 8d01 7d02 7c02 a001 7c00 a101  d.d...}.|...|...
-00000bb0: 0100 7c02 a002 7c01 6a03 a101 0100 7400  ..|...|.j.....t.
-00000bc0: 6a04 7c00 7c01 6a05 6403 8d02 7d03 7c03  j.|.|.j.d...}.|.
-00000bd0: a006 7c01 6a07 a101 0100 7c01 6a08 9001  ..|.j.....|.j...
-00000be0: 7224 6404 6405 6c09 6d0a 7d04 0100 740b  r$d.d.l.m.}...t.
-00000bf0: a00c 6406 a101 a00d 6407 a101 0100 740e  ..d.....d.....t.
-00000c00: 6a0f a010 7c01 6a11 a101 9b00 6408 9d02  j...|.j.....d...
-00000c10: 7d05 740e 6a0f a012 7c05 a101 7398 7400  }.t.j...|...s.t.
-00000c20: a013 7c01 6a11 a101 7d06 7c06 a002 7c05  ..|.j...}.|...|.
-00000c30: a101 0100 6e0a 7400 a013 7c05 a101 7d06  ....n.t...|...}.
-00000c40: 7414 7c06 8301 6409 6b02 73b2 7415 8201  t.|...d.k.s.t...
-00000c50: 7416 7c06 a017 a100 8301 6404 1900 7d07  t.|.......d...}.
-00000c60: 740b a00c 6406 a101 a00d 640a 7c01 6a11  t...d.....d.|.j.
-00000c70: 9b00 9d02 a101 0100 7c07 a018 a100 7d08  ........|.....}.
-00000c80: 7c08 a019 7c03 a101 0100 7c08 7d03 7c04  |...|.....|.}.|.
-00000c90: 7c07 7c03 8302 7d09 740b a00c 6406 a101  |.|...}.t...d...
-00000ca0: a00d 640b 7c09 a01a a100 9b00 9d02 a101  ..d.|...........
-00000cb0: 0100 740b a00c 6406 a101 a00d 6407 a101  ..t...d.....d...
-00000cc0: 0100 7c03 6a1b 7c01 6a1c 7c01 6a05 640c  ..|.j.|.j.|.j.d.
-00000cd0: 8d02 0100 7c02 7c03 6602 5300 290d 7a24  ....|.|.f.S.).z$
-00000ce0: 0a20 204c 6f61 6420 7961 6d6c 2061 6e64  .  Load yaml and
-00000cf0: 2073 6176 6520 636f 6d6d 616e 645f 6366   save command_cf
-00000d00: 670a 2020 54a9 015a 0b6e 6577 5f61 6c6c  g.  T..Z.new_all
-00000d10: 6f77 6564 2901 722d 0000 0072 0100 0000  owed).r-...r....
-00000d20: 2901 da08 4465 6570 4469 6666 da02 746c  )...DeepDiff..tl
-00000d30: 7a3a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  z:**************
-00000d40: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
-00000d50: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
-00000d60: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 7a13 2f63  ************z./c
-00000d70: 6f6e 6669 675f 7265 7375 6d65 2e79 616d  onfig_resume.yam
-00000d80: 6ce9 0100 0000 7a15 5570 6461 7469 6e67  l.....z.Updating
-00000d90: 2072 6573 756d 655f 6366 673a 207a 2264   resume_cfg: z"d
-00000da0: 6966 6620 6265 7477 6565 6e20 7265 7375  iff between resu
-00000db0: 6d65 5f63 6667 2061 6e64 2063 6667 3a20  me_cfg and cfg: 
-00000dc0: 0aa9 025a 0a73 6176 6564 5f66 696c 6572  ...Z.saved_filer
-00000dd0: 2d00 0000 291d 7206 0000 005a 0f6d 6572  -...).r....Z.mer
-00000de0: 6765 5f66 726f 6d5f 6669 6c65 5a0c 6475  ge_from_fileZ.du
-00000df0: 6d70 5f74 6f5f 6669 6c65 7254 0000 00da  mp_to_filerT....
-00000e00: 166c 6f61 645f 7961 6d6c 5f77 6974 685f  .load_yaml_with_
-00000e10: 636f 6d6d 616e 64da 0a74 6c5f 636f 6d6d  command..tl_comm
-00000e20: 616e 64da 0f6d 6572 6765 5f66 726f 6d5f  and..merge_from_
-00000e30: 6c69 7374 da07 746c 5f6f 7074 73da 0974  list..tl_opts..t
-00000e40: 6c5f 7265 7375 6d65 da08 6465 6570 6469  l_resume..deepdi
-00000e50: 6666 725a 0000 00da 076c 6f67 6769 6e67  ffrZ.....logging
-00000e60: da09 6765 744c 6f67 6765 72da 0469 6e66  ..getLogger..inf
-00000e70: 6f72 2000 0000 7221 0000 00da 0764 6972  or ...r!.....dir
-00000e80: 6e61 6d65 7249 0000 00da 0665 7869 7374  namerI.....exist
-00000e90: 73da 0e6c 6f61 645f 7961 6d6c 5f66 696c  s..load_yaml_fil
-00000ea0: 65da 036c 656e 721c 0000 00da 046c 6973  e..lenr......lis
-00000eb0: 74da 0676 616c 7565 735a 0563 6c6f 6e65  t..valuesZ.clone
-00000ec0: da06 7570 6461 7465 da06 7072 6574 7479  ..update..pretty
-00000ed0: da19 6475 6d70 5f74 6f5f 6669 6c65 5f77  ..dump_to_file_w
-00000ee0: 6974 685f 636f 6d6d 616e 6472 5500 0000  ith_commandrU...
-00000ef0: 290a da0b 636f 6e66 6967 5f66 696c 6572  )...config_filer
-00000f00: 5600 0000 da03 6366 67da 0b63 6f6d 6d61  V.....cfg..comma
-00000f10: 6e64 5f63 6667 725a 0000 005a 0f72 6573  nd_cfgrZ...Z.res
-00000f20: 756d 655f 6366 675f 6669 6c65 5a08 7465  ume_cfg_fileZ.te
-00000f30: 6d70 5f63 6667 5a0a 7265 7375 6d65 5f63  mp_cfgZ.resume_c
-00000f40: 6667 5a10 7265 7375 6d65 5f63 6667 5f63  fgZ.resume_cfg_c
-00000f50: 6c6f 6e65 da05 6464 6966 6672 3100 0000  lone..ddiffr1...
-00000f60: 7231 0000 0072 3200 0000 da0d 5f73 6574  r1...r2....._set
-00000f70: 7570 5f63 6f6e 6669 6764 0000 0073 3400  up_configd...s4.
-00000f80: 0000 0004 0a01 0a01 0c02 1001 0c02 0801  ................
-00000f90: 0c01 1001 1401 0c01 0c01 0c02 0a01 1001  ................
-00000fa0: 1002 1801 0801 0a01 0402 0a01 1a01 1002  ................
-00000fb0: 0801 04ff 0605 7274 0000 0063 0200 0000  ......rt...c....
-00000fc0: 0000 0000 0000 0000 0b00 0000 0700 0000  ................
-00000fd0: 4300 0000 7318 0100 007c 00a0 00a1 007d  C...s....|.....}
-00000fe0: 0274 0183 007d 037c 036a 027c 0264 018d  .t...}.|.j.|.d..
-00000ff0: 015c 027d 047d 0574 0374 047c 0483 0183  .\.}.}.t.t.|....
-00001000: 017d 0474 057c 047c 046a 0664 028d 0201  .}.t.|.|.j.d....
-00001010: 0074 077c 046a 0864 0364 0467 0264 0564  .t.|.j.d.d.g.d.d
-00001020: 068d 037d 067c 06a0 0964 0774 0a6a 0b7c  ...}.|...d.t.j.|
-00001030: 0464 0864 098d 0217 00a1 0101 007c 046a  .d.d.........|.j
-00001040: 0672 7c7c 06a0 0964 0a7c 046a 0c9b 009d  .r||...d.|.j....
-00001050: 02a1 0101 007c 046a 0da0 0ea1 0064 0b6b  .....|.j.....d.k
-00001060: 0272 9a7c 0172 967c 0464 0c66 0253 007c  .r.|.r.|.d.f.S.|
-00001070: 0453 0074 0f7c 046a 107c 0464 0d8d 025c  .S.t.|.j.|.d...\
-00001080: 027d 077d 087c 06a0 0964 0e7c 08a0 11a1  .}.}.|...d.|....
-00001090: 009b 009d 02a1 0101 007c 0190 0172 1074  .........|...r.t
-000010a0: 12a0 137c 08a1 0101 0074 047c 0483 01a0  ...|.....t.|....
-000010b0: 14a1 0044 005d 205c 027d 097d 0a7c 09a0  ...D.] \.}.}.|..
-000010c0: 1564 0fa1 0172 dc74 12a0 137c 097c 0a69  .d...r.t...|.|.i
-000010d0: 01a1 0101 0071 dc7c 08a0 1374 12a1 0101  .....q.|...t....
-000010e0: 007c 047c 0866 0253 007c 0453 0064 0c53  .|.|.f.S.|.S.d.S
-000010f0: 0029 107a 180a 2020 5573 6167 653a 0a0a  .).z..  Usage:..
-00001100: 2020 3a72 6574 7572 6e3a 0a20 2029 0172    :return:.  ).r
-00001110: 5600 0000 2902 7256 0000 0072 5700 0000  V...).rV...rW...
-00001120: 5a0c 7465 6d70 6c61 7465 5f6c 6962 725b  Z.template_libr[
-00001130: 0000 0054 2903 da08 6669 6c65 6e61 6d65  ...T)...filename
-00001140: 5a0c 6c6f 6767 6572 5f6e 616d 6573 da06  Z.logger_names..
-00001150: 7374 7265 616d 7a07 0a61 7267 733a 0a46  streamz..args:.F
-00001160: 2901 da0a 7573 655f 7070 7269 6e74 7a0d  )...use_pprintz.
-00001170: 5265 7375 6d65 2066 726f 6d20 0ada 046e  Resume from ...n
-00001180: 6f6e 654e 2902 7270 0000 0072 5600 0000  oneN).rp...rV...
-00001190: 7a0b 0a54 6865 2063 6667 3a20 0ada 0374  z..The cfg: ...t
-000011a0: 6c5f 2916 da05 7370 6c69 7472 4200 0000  l_)...splitrB...
-000011b0: da10 7061 7273 655f 6b6e 6f77 6e5f 6172  ..parse_known_ar
-000011c0: 6773 7203 0000 00da 0476 6172 7372 5800  gsr......varsrX.
-000011d0: 0000 7262 0000 0072 0800 0000 7253 0000  ..rb...r....rS..
-000011e0: 0072 6600 0000 7204 0000 00da 0b64 6963  .rf...r......dic
-000011f0: 7432 7374 7269 6e67 7247 0000 0072 5f00  t2stringrG...r_.
-00001200: 0000 da05 6c6f 7765 7272 7400 0000 da0e  ....lowerrt.....
-00001210: 746c 5f63 6f6e 6669 675f 6669 6c65 da04  tl_config_file..
-00001220: 6475 6d70 7205 0000 00da 0f6d 6572 6765  dumpr......merge
-00001230: 5f66 726f 6d5f 6469 6374 da05 6974 656d  _from_dict..item
-00001240: 7372 1b00 0000 290b da08 6172 6776 5f73  sr....)...argv_s
-00001250: 7472 da0a 7265 7475 726e 5f63 6667 5a0d  tr..return_cfgZ.
-00001260: 6172 6776 5f73 7472 5f6c 6973 7472 4000  argv_str_listr@.
-00001270: 0000 7256 0000 005a 0d75 6e70 6172 7365  ..rV...Z.unparse
-00001280: 645f 6172 6776 da06 6c6f 6767 6572 720f  d_argv..loggerr.
-00001290: 0000 0072 7200 0000 da01 6bda 0176 7231  ...rr.....k..vr1
-000012a0: 0000 0072 3100 0000 7232 0000 00da 1573  ...r1...r2.....s
-000012b0: 6574 7570 5f6f 7574 6469 725f 616e 645f  etup_outdir_and_
-000012c0: 7961 6d6c 8c00 0000 732e 0000 0000 0808  yaml....s.......
-000012d0: 0106 0110 020c 010e 0314 0118 0206 0112  ................
-000012e0: 050e 0104 0008 0104 0312 0114 0106 010a  ................
-000012f0: 0214 010a 0110 010a 0108 0272 8800 0000  ...........r....
-00001300: 6301 0000 0000 0000 0000 0000 0002 0000  c...............
-00001310: 000b 0000 0043 0000 0073 4000 0000 6401  .....C...s@...d.
-00001320: 7c00 6a00 9b00 6402 7c00 6a01 9b00 6403  |.j...d.|.j...d.
-00001330: 7c00 6a02 9b00 6404 7c00 6a03 722a 6405  |.j...d.|.j.r*d.
-00001340: 7c00 6a04 1700 6e02 6406 9b00 6407 7c00  |.j...n.d...d.|.
-00001350: 6a05 9b00 6404 9d0b 7d01 7c01 5300 2908  j...d...}.|.S.).
-00001360: 4e7a 1e0a 2020 2020 2020 2020 2020 2020  Nz..            
-00001370: 2d2d 746c 5f63 6f6e 6669 675f 6669 6c65  --tl_config_file
-00001380: 207a 1a0a 2020 2020 2020 2020 2020 2020   z..            
-00001390: 2d2d 746c 5f63 6f6d 6d61 6e64 207a 190a  --tl_command z..
-000013a0: 2020 2020 2020 2020 2020 2020 2d2d 746c              --tl
-000013b0: 5f6f 7574 6469 7220 7a0d 0a20 2020 2020  _outdir z..     
-000013c0: 2020 2020 2020 207a 1b2d 2d74 6c5f 7265         z.--tl_re
-000013d0: 7375 6d65 202d 2d74 6c5f 7265 7375 6d65  sume --tl_resume
-000013e0: 6469 7220 7236 0000 007a 1b0a 2020 2020  dir r6...z..    
-000013f0: 2020 2020 2020 2020 2d2d 746c 5f74 696d          --tl_tim
-00001400: 655f 7374 7220 2906 7255 0000 0072 5f00  e_str ).rU...r_.
-00001410: 0000 7230 0000 0072 6200 0000 7247 0000  ..r0...rb...rG..
-00001420: 0072 4c00 0000 2902 7256 0000 005a 0e63  .rL...).rV...Z.c
-00001430: 6d64 5f73 7472 5f61 7070 656e 6472 3100  md_str_appendr1.
-00001440: 0000 7231 0000 0072 3200 0000 da12 6765  ..r1...r2.....ge
-00001450: 745f 6170 7065 6e64 5f63 6d64 5f73 7472  t_append_cmd_str
-00001460: b800 0000 7318 0000 0000 0102 0104 ff04  ....s...........
-00001470: 0204 fe04 0304 fd04 0412 fc04 0504 fb08  ................
-00001480: 0772 8900 0000 6301 0000 0000 0000 0000  .r....c.........
-00001490: 0000 0005 0000 000a 0000 0043 0000 0073  ...........C...s
-000014a0: be00 0000 7c00 a000 a100 7d01 7401 a002  ....|.....}.t...
-000014b0: 6401 a101 7d02 7c02 a003 6402 6403 a004  d...}.|...d.d...
-000014c0: 7c01 a101 1700 a101 0100 7405 6a06 a007  |.........t.j...
-000014d0: a100 7d03 7c01 6404 1900 6405 6b02 7256  ..}.|.d...d.k.rV
-000014e0: 7408 6a09 7c01 6404 3c00 740a 6a0b 7c01  t.j.|.d.<.t.j.|.
-000014f0: 7c03 6406 8d02 7d04 6e42 7c01 6404 1900  |.d...}.nB|.d...
-00001500: 6407 6b02 728a 740a 6a0b 6408 6409 640a  d.k.r.t.j.d.d.d.
-00001510: 640b 640c a004 7c01 640d 6400 8502 1900  d.d...|.d.d.....
-00001520: a101 6705 7c03 6406 8d02 7d04 6e0e 740a  ..g.|.d...}.n.t.
-00001530: 6a0b 7c01 7c03 6406 8d02 7d04 7c04 a00c  j.|.|.d...}.|...
-00001540: a100 0100 7c04 6a0d 6404 6b03 72ba 740a  ....|.j.d.k.r.t.
-00001550: 6a0e 7c04 6a0d 7c01 640e 8d02 8201 6400  j.|.j.|.d.....d.
-00001560: 5300 290f 4e72 5b00 0000 7a0a 0a72 756e  S.).Nr[...z..run
-00001570: 5f73 7472 3a0a 7a05 205c 0a20 2072 0100  _str:.z. \.  r..
-00001580: 0000 5a06 7079 7468 6f6e 2901 da03 656e  ..Z.python)...en
-00001590: 765a 0462 6173 687a 092f 6269 6e2f 6261  vZ.bashz./bin/ba
-000015a0: 7368 7a02 2d6f 5a06 7874 7261 6365 7a02  shz.-oZ.xtracez.
-000015b0: 2d63 7213 0000 0072 5c00 0000 2902 da0a  -cr....r\...)...
-000015c0: 7265 7475 726e 636f 6465 da03 636d 6429  returncode..cmd)
-000015d0: 0f72 7a00 0000 7264 0000 0072 6500 0000  .rz...rd...re...
-000015e0: 7266 0000 0072 4800 0000 7220 0000 0072  rf...rH...r ...r
-000015f0: 2900 0000 da04 636f 7079 7227 0000 00da  ).....copyr'....
-00001600: 0a65 7865 6375 7461 626c 65da 0a73 7562  .executable..sub
-00001610: 7072 6f63 6573 73da 0550 6f70 656e da04  process..Popen..
-00001620: 7761 6974 728b 0000 00da 1243 616c 6c65  waitr......Calle
-00001630: 6450 726f 6365 7373 4572 726f 7229 05da  dProcessError)..
-00001640: 0763 6d64 5f73 7472 728c 0000 0072 8500  .cmd_strr....r..
-00001650: 0000 5a0b 6375 7272 656e 745f 656e 76da  ..Z.current_env.
-00001660: 0770 726f 6365 7373 7231 0000 0072 3100  .processr1...r1.
-00001670: 0000 7232 0000 00da 0d73 7461 7274 5f63  ..r2.....start_c
-00001680: 6d64 5f72 756e c300 0000 731c 0000 0000  md_run....s.....
-00001690: 0108 010a 0114 010a 010c 010a 0110 010c  ................
-000016a0: 0128 020e 0208 010a 0110 0172 9500 0000  .(.........r....
-000016b0: 5463 0300 0000 0000 0000 0000 0000 0700  Tc..............
-000016c0: 0000 0600 0000 4300 0000 734c 0100 0074  ......C...sL...t
-000016d0: 006a 01a0 027c 006a 0364 01a1 027d 037c  .j...|.j.d...}.|
-000016e0: 0272 3474 0474 05a0 0664 02a1 016a 0783  .r4t.t...d...j..
-000016f0: 0164 036b 0072 4474 087c 0364 048d 017d  .d.k.rDt.|.d...}
-00001700: 046e 1074 05a0 0664 02a1 017d 0464 057c  .n.t...d...}.d.|
-00001710: 045f 097c 0272 5c74 0a7c 0164 068d 017d  ._.|.r\t.|.d...}
-00001720: 0574 0b7c 0564 078d 0101 007c 006a 0ca0  .t.|.d.....|.j..
-00001730: 0da1 0064 086b 0273 e674 006a 01a0 0e7c  ...d.k.s.t.j...|
-00001740: 006a 0fa1 0173 7c74 1082 0174 11a0 127c  .j...s|t...t...|
-00001750: 006a 0f7c 006a 0ca1 027d 067c 06a0 137c  .j.|.j...}.|...|
-00001760: 006a 14a1 0101 007c 006a 039b 0064 099d  .j.....|.j...d..
-00001770: 027c 065f 1574 167c 0683 0101 0074 05a0  .|._.t.|.....t..
-00001780: 0664 02a1 01a0 1764 0a74 18a0 19a1 0017  .d.....d.t......
-00001790: 00a1 0101 0074 1aa0 1ba1 0001 007c 0272  .....t.......|.r
-000017a0: e47c 066a 1c74 186a 157c 006a 0c64 0b8d  .|.j.t.j.|.j.d..
-000017b0: 0201 006e 5e74 1183 007d 067c 066a 137c  ...n^t...}.|.j.|
-000017c0: 006a 1464 0c64 0d8d 0201 007c 006a 039b  .j.d.d.....|.j..
-000017d0: 0064 099d 027c 065f 1574 167c 0683 0101  .d...|._.t.|....
-000017e0: 0074 05a0 0664 02a1 01a0 1764 0a74 1da0  .t...d.....d.t..
-000017f0: 1e74 18a1 0117 00a1 0101 007c 0290 0172  .t.........|...r
-00001800: 447c 066a 1c74 186a 157c 006a 0c64 0b8d  D|.j.t.j.|.j.d..
-00001810: 0201 007c 067c 0366 0253 0029 0e4e 7246  ...|.|.f.S.).NrF
-00001820: 0000 0072 5b00 0000 e902 0000 0029 0172  ...r[........).r
-00001830: 7500 0000 4629 015a 086c 6f67 5f72 6f6f  u...F).Z.log_roo
-00001840: 7429 01da 0a74 6578 746c 6f67 6765 7272  t)...textloggerr
-00001850: 7800 0000 7a14 2f63 6f6e 6669 675f 636f  x...z./config_co
-00001860: 6d6d 616e 642e 7961 6d6c 7a0e 0a67 6c6f  mmand.yamlz..glo
-00001870: 6261 6c5f 6366 673a 200a 725d 0000 0054  bal_cfg: .r]...T
-00001880: 7259 0000 0029 1f72 2000 0000 7221 0000  rY...).r ...r!..
-00001890: 0072 4800 0000 7230 0000 0072 6a00 0000  .rH...r0...rj...
-000018a0: 7264 0000 0072 6500 0000 da08 6861 6e64  rd...re.....hand
-000018b0: 6c65 7273 7208 0000 00da 0970 726f 7061  lersr......propa
-000018c0: 6761 7465 720a 0000 0072 0900 0000 725f  gater....r....r_
-000018d0: 0000 0072 7e00 0000 7268 0000 0072 7f00  ...r~...rh...r..
-000018e0: 0000 721c 0000 0072 0600 0000 725e 0000  ..r....r....r^..
-000018f0: 0072 6000 0000 7261 0000 0072 5400 0000  .r`...ra...rT...
-00001900: 7207 0000 0072 6600 0000 7205 0000 0072  r....rf...r....r
-00001910: 8000 0000 720b 0000 00da 0b73 796e 6368  ....r......synch
-00001920: 726f 6e69 7a65 726f 0000 0072 0400 0000  ronizero...r....
-00001930: 727d 0000 0029 0772 5600 0000 da0a 746c  r}...).rV.....tl
-00001940: 5f74 6578 7464 6972 da0f 6973 5f6d 6169  _textdir..is_mai
-00001950: 6e5f 7072 6f63 6573 7372 5300 0000 7285  n_processrS...r.
-00001960: 0000 0072 9700 0000 7271 0000 0072 3100  ...r....rq...r1.
-00001970: 0000 7231 0000 0072 3200 0000 da2a 5f73  ..r1...r2....*_s
-00001980: 6574 7570 5f6c 6f67 6765 725f 676c 6f62  etup_logger_glob
-00001990: 616c 5f63 6667 5f67 6c6f 6261 6c5f 7465  al_cfg_global_te
-000019a0: 7874 6c6f 6767 6572 d600 0000 7336 0000  xtlogger....s6..
-000019b0: 0000 0510 0104 0114 010c 020a 0106 0304  ................
-000019c0: 010a 010a 030e 0112 0110 010c 020e 0108  ................
-000019d0: 0218 0208 0104 0114 0506 0110 020e 0108  ................
-000019e0: 011a 0106 0112 0172 9d00 0000 6302 0000  .......r....c...
-000019f0: 0000 0000 0000 0000 0005 0000 0005 0000  ................
-00001a00: 004b 0000 0073 4c00 0000 7c01 7226 7c01  .K...sL...|.r&|.
-00001a10: a000 a100 4400 5d18 5c02 7d03 7d04 7c00  ....D.].\.}.}.|.
-00001a20: 6a01 6600 7c03 7c04 6901 8e01 0100 710c  j.f.|.|.i.....q.
-00001a30: 7c02 a000 a100 4400 5d18 5c02 7d03 7d04  |.....D.].\.}.}.
-00001a40: 7c00 6a01 6600 7c03 7c04 6901 8e01 0100  |.j.f.|.|.i.....
-00001a50: 712e 7c00 5300 2901 4e29 0272 8200 0000  q.|.S.).N).r....
-00001a60: da0c 7365 745f 6465 6661 756c 7473 2905  ..set_defaults).
-00001a70: 7240 0000 0072 7100 0000 da06 6b77 6172  r@...rq.....kwar
-00001a80: 6773 7286 0000 0072 8700 0000 7231 0000  gsr....r....r1..
-00001a90: 0072 3100 0000 7232 0000 00da 145f 7061  .r1...r2....._pa
-00001aa0: 7273 6572 5f73 6574 5f64 6566 6175 6c74  rser_set_default
-00001ab0: 7305 0100 0073 0c00 0000 0001 0401 1001  s....s..........
-00001ac0: 1201 1001 1201 72a0 0000 0063 0100 0000  ......r....c....
-00001ad0: 0000 0000 0000 0000 0400 0000 0400 0000  ................
-00001ae0: 4300 0000 7360 0000 007c 009b 0064 019d  C...s`...|...d..
-00001af0: 027d 017c 009b 0064 029d 027d 027c 009b  .}.|...d...}.|..
-00001b00: 0064 039d 027d 0374 006a 017c 0064 0464  .d...}.t.j.|.d.d
-00001b10: 058d 0201 0074 006a 017c 0164 0464 058d  .....t.j.|.d.d..
-00001b20: 0201 0074 006a 017c 0264 0464 058d 0201  ...t.j.|.d.d....
-00001b30: 0074 006a 017c 0364 0464 058d 0201 007c  .t.j.|.d.d.....|
-00001b40: 017c 027c 0366 0353 0029 064e 7a08 2f63  .|.|.f.S.).Nz./c
-00001b50: 6b70 7464 6972 7a07 2f69 6d67 6469 727a  kptdirz./imgdirz
-00001b60: 082f 7465 7874 6469 7254 2901 da08 6578  ./textdirT)...ex
-00001b70: 6973 745f 6f6b 2902 7220 0000 00da 086d  ist_ok).r .....m
-00001b80: 616b 6564 6972 7329 0472 3000 0000 da0a  akedirs).r0.....
-00001b90: 746c 5f63 6b70 7464 6972 da09 746c 5f69  tl_ckptdir..tl_i
-00001ba0: 6d67 6469 7272 9b00 0000 7231 0000 0072  mgdirr....r1...r
-00001bb0: 3100 0000 7232 0000 0072 5100 0000 0e01  1...r2...rQ.....
-00001bc0: 0000 7310 0000 0000 010a 010a 010a 020e  ..s.............
-00001bd0: 010e 010e 010e 0172 5100 0000 7256 0000  .......rQ...rV..
-00001be0: 0063 0500 0000 0000 0000 0000 0000 0f00  .c..............
-00001bf0: 0000 0800 0000 4300 0000 73c8 0000 0074  ......C...s....t
-00001c00: 007c 007c 0464 018d 027d 007c 00a0 01a1  .|.|.d...}.|....
-00001c10: 005c 027d 057d 0674 027c 056a 0364 028d  .\.}.}.t.|.j.d..
-00001c20: 015c 037d 077d 087d 0974 047c 057c 097c  .\.}.}.}.t.|.|.|
-00001c30: 0364 038d 035c 027d 0a7d 0b7c 0272 467c  .d...\.}.}.|.rF|
-00001c40: 0a7d 0c6e 147c 017c 0a6b 0672 567c 0a7c  .}.n.|.|.k.rV|.|
-00001c50: 0119 006e 0264 007d 0c74 057c 007c 0c7c  ...n.d.}.t.|.|.|
-00001c60: 087c 077c 097c 0b64 048d 0601 0074 06a0  .|.|.|.d.....t..
-00001c70: 0764 05a1 01a0 0864 0664 07a0 0974 0a6a  .d.....d.d...t.j
-00001c80: 0ba1 0117 00a1 0101 007c 00a0 01a1 005c  .........|.....\
-00001c90: 027d 057d 0674 0c7c 0583 01a0 0da1 0044  .}.}.t.|.......D
-00001ca0: 005d 205c 027d 0d7d 0e7c 0da0 0e64 08a1  .] \.}.}.|...d..
-00001cb0: 0172 a274 0fa0 107c 0d7c 0e69 01a1 0101  .r.t...|.|.i....
-00001cc0: 0071 a27c 0053 0029 094e 2901 7241 0000  .q.|.S.).N).rA..
-00001cd0: 0072 4500 0000 2901 729c 0000 0029 0572  .rE...).r....).r
-00001ce0: 7100 0000 72a4 0000 0072 a300 0000 729b  q...r....r....r.
-00001cf0: 0000 0072 5300 0000 725b 0000 007a 1473  ...rS...r[...z.s
-00001d00: 7973 2e61 7267 763a 200a 2070 7974 686f  ys.argv: . pytho
-00001d10: 6e20 0a7a 0220 0a72 7900 0000 2911 7242  n .z. .ry...).rB
-00001d20: 0000 0072 7b00 0000 7251 0000 0072 3000  ...r{...rQ...r0.
-00001d30: 0000 729d 0000 0072 a000 0000 7264 0000  ..r....r....rd..
-00001d40: 0072 6500 0000 7266 0000 0072 4800 0000  .re...rf...rH...
-00001d50: 7227 0000 0072 2800 0000 727c 0000 0072  r'...r(...r|...r
-00001d60: 8200 0000 721b 0000 0072 0500 0000 7281  ....r....r....r.
-00001d70: 0000 0029 0f72 4000 0000 7216 0000 005a  ...).r@...r....Z
-00001d80: 0f75 7365 5f63 6667 5f61 735f 6172 6773  .use_cfg_as_args
-00001d90: 729c 0000 0072 4100 0000 7256 0000 0072  r....rA...rV...r
-00001da0: 0f00 0000 72a3 0000 0072 a400 0000 729b  ....r....r....r.
-00001db0: 0000 0072 7100 0000 7253 0000 005a 0c64  ...rq...rS...Z.d
-00001dc0: 6566 6175 6c74 5f61 7267 7372 8600 0000  efault_argsr....
-00001dd0: 7287 0000 0072 3100 0000 7231 0000 0072  r....r1...r1...r
-00001de0: 3200 0000 da20 7570 6461 7465 5f70 6172  2.... update_par
-00001df0: 7365 725f 6465 6661 756c 7473 5f66 726f  ser_defaults_fro
-00001e00: 6d5f 7961 6d6c 1a01 0000 7326 0000 0000  m_yaml....s&....
-00001e10: 020c 020c 0212 0212 0204 0106 0214 0206  ................
-00001e20: 0102 0002 0002 0102 fe06 031c 010c 0114  ................
-00001e30: 010a 0110 0772 a500 0000 2902 4e46 2902  .....r....).NF).
-00001e40: 4e46 2901 5429 0472 5600 0000 4654 4629  NF).T).rV...FTF)
-00001e50: 2cda 0474 696d 6572 2000 0000 7227 0000  ,..timer ...r'..
-00001e60: 00da 046a 736f 6e72 6400 0000 da06 7070  ...jsonrd.....pp
-00001e70: 7269 6e74 728f 0000 0072 0200 0000 723c  rintr....r....r<
-00001e80: 0000 0072 4f00 0000 5a04 7961 6d6c 5a08  ...rO...Z.yamlZ.
-00001e90: 6561 7379 6469 6374 7203 0000 00da 0374  easydictr......t
-00001ea0: 6c32 7204 0000 00da 0f74 6c32 2e70 726f  l2r......tl2.pro
-00001eb0: 6a2e 6676 636f 7265 7205 0000 0072 0600  j.fvcorer....r..
-00001ec0: 0000 7207 0000 005a 1c74 6c32 2e70 726f  ..r....Z.tl2.pro
-00001ed0: 6a2e 6c6f 6767 6572 2e6c 6f67 6765 725f  j.logger.logger_
-00001ee0: 7574 696c 7372 0800 0000 5a0f 746c 322e  utilsr....Z.tl2.
-00001ef0: 7072 6f6a 2e6c 6f67 6765 7272 0900 0000  proj.loggerr....
-00001f00: 720a 0000 005a 1474 6c32 2e70 726f 6a2e  r....Z.tl2.proj.
-00001f10: 7079 746f 7263 682e 6464 7072 0b00 0000  pytorch.ddpr....
-00001f20: 5a12 746c 322e 7072 6f6a 2e61 7267 7061  Z.tl2.proj.argpa
-00001f30: 7273 6572 720c 0000 00da 095f 6765 7466  rserr......_getf
-00001f40: 7261 6d65 da06 665f 636f 6465 da07 636f  rame..f_code..co
-00001f50: 5f6e 616d 65da 085f 5f66 696c 655f 5f72  _name..__file__r
-00001f60: 3300 0000 7235 0000 0072 4200 0000 7258  3...r5...rB...rX
-00001f70: 0000 0072 7400 0000 7288 0000 0072 8900  ...rt...r....r..
-00001f80: 0000 7295 0000 0072 9d00 0000 72a0 0000  ..r....r....r...
-00001f90: 0072 5100 0000 72a5 0000 0072 3100 0000  .rQ...r....r1...
-00001fa0: 7231 0000 0072 3100 0000 7232 0000 00da  r1...r1...r2....
-00001fb0: 083c 6d6f 6475 6c65 3e01 0000 0073 4a00  .<module>....sJ.
-00001fc0: 0000 0801 0801 0801 0801 0801 0801 0801  ................
-00001fd0: 0c01 0802 0802 0801 0c02 0c01 1401 0c01  ................
-00001fe0: 1001 0c01 0c05 0a01 02fd 0a1b 0808 0a12  ................
-00001ff0: 0817 0829 0001 00fe 0a2c 080b 0816 00fd  ...).....,......
-00002000: 0a2f 0809 080c 0001 0000 00ff            ./..........
+00000520: fa36 2f68 6f6d 652f 6d61 2d75 7365 722f  .6/home/ma-user/
+00000530: 776f 726b 2f63 6f64 652f 746c 322f 746c  work/code/tl2/tl
+00000540: 322f 6c61 756e 6368 2f6c 6175 6e63 685f  2/launch/launch_
+00000550: 7574 696c 732e 7079 da16 6765 745f 636f  utils.py..get_co
+00000560: 6d6d 616e 645f 616e 645f 6f75 7464 6972  mmand_and_outdir
+00000570: 1800 0000 732e 0000 0000 050e 010c 0108  ....s...........
+00000580: 011e 010a 0110 0212 010c 0102 020a fe04  ................
+00000590: 0202 fe04 0202 fe04 0202 fe08 050e 0212  ................
+000005a0: 0108 0104 010a 0172 3300 0000 6300 0000  .......r3...c...
+000005b0: 0000 0000 0000 0000 0002 0000 0006 0000  ................
+000005c0: 0043 0000 0073 2600 0000 6401 7d00 7a0c  .C...s&...d.}.z.
+000005d0: 6402 6400 6c00 7d01 5700 6e10 0100 0100  d.d.l.}.W.n.....
+000005e0: 0100 6403 7d00 5900 6e02 5800 7c00 5300  ..d.}.Y.n.X.|.S.
+000005f0: 2904 4e54 7201 0000 0046 2901 5a06 6d6f  ).NTr....F).Z.mo
+00000600: 7869 6e67 2902 da06 746c 5f6d 6f78 5a03  xing)...tl_moxZ.
+00000610: 6d6f 7872 3100 0000 7231 0000 0072 3200  moxr1...r1...r2.
+00000620: 0000 da0a 7573 655f 6d6f 7869 6e67 3300  ....use_moxing3.
+00000630: 0000 730c 0000 0000 0104 0102 010c 0106  ..s.............
+00000640: 010a 0172 3500 0000 4663 0200 0000 0000  ...r5...Fc......
+00000650: 0000 0000 0000 0200 0000 0600 0000 4300  ..............C.
+00000660: 0000 73b8 0000 007c 0073 0c74 00a0 01a1  ..s....|.s.t....
+00000670: 007d 007c 006a 0264 0174 0364 0264 038d  .}.|.j.d.t.d.d..
+00000680: 0301 007c 006a 0264 0474 0364 0264 038d  ...|.j.d.t.d.d..
+00000690: 0301 007c 006a 0264 0574 0364 0664 038d  ...|.j.d.t.d.d..
+000006a0: 0301 007c 006a 0264 0774 0364 0867 0064  ...|.j.d.t.d.g.d
+000006b0: 098d 0401 007c 006a 0264 0a64 0b64 0c64  .....|.j.d.d.d.d
+000006c0: 0d8d 0301 007c 006a 0264 0e74 0364 0664  .....|.j.d.t.d.d
+000006d0: 038d 0301 007c 006a 0264 0f64 0b64 0c64  .....|.j.d.d.d.d
+000006e0: 0d8d 0301 0074 046a 057c 0064 1074 0683  .....t.j.|.d.t..
+000006f0: 0064 118d 0301 007c 006a 0264 1274 0364  .d.....|.j.d.t.d
+00000700: 0264 038d 0301 007c 0172 b47c 006a 0264  .d.....|.r.|.j.d
+00000710: 1374 0764 1464 038d 0301 007c 0053 0029  .t.d.d.....|.S.)
+00000720: 154e 7a10 2d2d 746c 5f63 6f6e 6669 675f  .Nz.--tl_config_
+00000730: 6669 6c65 da00 2902 7218 0000 00da 0764  file..).r......d
+00000740: 6566 6175 6c74 7a0c 2d2d 746c 5f63 6f6d  efaultz.--tl_com
+00000750: 6d61 6e64 7214 0000 007a 0c72 6573 756c  mandr....z.resul
+00000760: 7473 2f74 656d 707a 092d 2d74 6c5f 6f70  ts/tempz.--tl_op
+00000770: 7473 da01 2a29 0372 1800 0000 da05 6e61  ts..*).r......na
+00000780: 7267 7372 3700 0000 7a0b 2d2d 746c 5f72  rgsr7...z.--tl_r
+00000790: 6573 756d 65da 0a73 746f 7265 5f74 7275  esume..store_tru
+000007a0: 6546 2902 da06 6163 7469 6f6e 7237 0000  eF)...actionr7..
+000007b0: 007a 0e2d 2d74 6c5f 7265 7375 6d65 6469  .z.--tl_resumedi
+000007c0: 727a 0a2d 2d74 6c5f 6465 6275 6772 3400  rz.--tl_debugr4.
+000007d0: 0000 2901 7237 0000 007a 0d2d 2d74 6c5f  ..).r7...z.--tl_
+000007e0: 7469 6d65 5f73 7472 7a0c 2d2d 6c6f 6361  time_strz.--loca
+000007f0: 6c5f 7261 6e6b 7201 0000 0029 08da 0861  l_rankr....)...a
+00000800: 7267 7061 7273 65da 0e41 7267 756d 656e  rgparse..Argumen
+00000810: 7450 6172 7365 72da 0c61 6464 5f61 7267  tParser..add_arg
+00000820: 756d 656e 7472 1500 0000 720c 0000 005a  umentr....r....Z
+00000830: 1161 6464 5f61 7267 756d 656e 745f 626f  .add_argument_bo
+00000840: 6f6c 7235 0000 00da 0369 6e74 2902 da06  olr5.....int)...
+00000850: 7061 7273 6572 da11 6170 7065 6e64 5f6c  parser..append_l
+00000860: 6f63 616c 5f72 616e 6b72 3100 0000 7231  ocal_rankr1...r1
+00000870: 0000 0072 3200 0000 da0d 5f62 7569 6c64  ...r2....._build
+00000880: 5f70 6172 7365 723b 0000 0073 1c00 0000  _parser;...s....
+00000890: 0001 0401 0801 1001 1001 1001 1201 1001  ................
+000008a0: 1001 1001 1202 1001 0401 1001 7242 0000  ............rB..
+000008b0: 0063 0200 0000 0000 0000 0000 0000 0300  .c..............
+000008c0: 0000 0600 0000 4300 0000 73e6 0000 007c  ......C...s....|
+000008d0: 0172 387c 006a 007c 005f 0174 026a 03a0  .r8|.j.|._.t.j..
+000008e0: 047c 006a 0164 01a1 027c 005f 0574 06a0  .|.j.d...|._.t..
+000008f0: 07a1 00a0 0864 02a1 0164 0064 0385 0219  .....d...d.d....
+00000900: 007c 005f 096e 6474 0a74 0b74 02a0 0c64  .|._.ndt.t.t...d
+00000910: 0464 05a1 0283 0183 017d 0274 06a0 07a1  .d.......}.t....
+00000920: 00a0 0864 02a1 0164 0064 0385 0219 007c  ...d...d.d.....|
+00000930: 005f 097c 0273 6e7c 006a 016e 0e7c 006a  ._.|.sn|.j.n.|.j
+00000940: 0164 0617 007c 006a 0917 007c 005f 0174  .d...|.j...|._.t
+00000950: 0d6a 0e7c 006a 0164 0764 088d 0201 0074  .j.|.j.d.d.....t
+00000960: 0f7c 006a 0164 098d 0101 0074 026a 03a0  .|.j.d.....t.j..
+00000970: 107c 006a 01a1 017c 005f 1174 026a 03a0  .|.j...|._.t.j..
+00000980: 047c 006a 0164 0aa1 027c 005f 1274 026a  .|.j.d...|._.t.j
+00000990: 03a0 047c 006a 0164 0ba1 027c 005f 1374  ...|.j.d...|._.t
+000009a0: 026a 03a0 047c 006a 0164 01a1 027c 005f  .j...|.j.d...|._
+000009b0: 1464 0053 0029 0c4e 7a13 636f 6e66 6967  .d.S.).Nz.config
+000009c0: 5f63 6f6d 6d61 6e64 2e79 616d 6c7a 1025  _command.yamlz.%
+000009d0: 5925 6d25 645f 2548 254d 2553 5f25 6672  Y%m%d_%H%M%S_%fr
+000009e0: 1200 0000 721a 0000 0072 0100 0000 fa01  ....r....r......
+000009f0: 2d54 2901 da0d 6967 6e6f 7265 5f65 7272  -T)...ignore_err
+00000a00: 6f72 73a9 0172 3000 0000 fa07 6c6f 672e  ors..r0.....log.
+00000a10: 7478 747a 0b63 6f6e 6669 672e 7961 6d6c  txtz.config.yaml
+00000a20: 2915 da0c 746c 5f72 6573 756d 6564 6972  )...tl_resumedir
+00000a30: 7230 0000 0072 2000 0000 7221 0000 00da  r0...r ...r!....
+00000a40: 046a 6f69 6eda 1574 6c5f 636f 6e66 6967  .join..tl_config
+00000a50: 5f66 696c 655f 7265 7375 6d65 7202 0000  _file_resumer...
+00000a60: 00da 036e 6f77 da08 7374 7266 7469 6d65  ...now..strftime
+00000a70: da0b 746c 5f74 696d 655f 7374 72da 0462  ..tl_time_str..b
+00000a80: 6f6f 6c72 3f00 0000 da06 6765 7465 6e76  oolr?.....getenv
+00000a90: da06 7368 7574 696c da06 726d 7472 6565  ..shutil..rmtree
+00000aa0: da0a 5f6d 616b 655f 6469 7273 da08 7265  .._make_dirs..re
+00000ab0: 616c 7061 7468 5a0d 746c 5f61 6273 5f6f  alpathZ.tl_abs_o
+00000ac0: 7574 6469 72da 0a74 6c5f 6c6f 6766 696c  utdir..tl_logfil
+00000ad0: 65da 1474 6c5f 7361 7665 645f 636f 6e66  e..tl_saved_conf
+00000ae0: 6967 5f66 696c 65da 1c74 6c5f 7361 7665  ig_file..tl_save
+00000af0: 645f 636f 6e66 6967 5f63 6f6d 6d61 6e64  d_config_command
+00000b00: 5f66 696c 6529 03da 0461 7267 73da 0672  _file)...args..r
+00000b10: 6573 756d 6572 1a00 0000 7231 0000 0072  esumer....r1...r
+00000b20: 3100 0000 7232 0000 00da 0d5f 7365 7475  1...r2....._setu
+00000b30: 705f 6f75 7464 6972 4d00 0000 731c 0000  p_outdirM...s...
+00000b40: 0000 0104 0108 0112 011a 0214 0118 011c  ................
+00000b50: 0210 010c 0310 0312 0112 0112 0172 5800  .............rX.
+00000b60: 0000 6302 0000 0000 0000 0000 0000 000a  ..c.............
+00000b70: 0000 0005 0000 0043 0000 0073 3e01 0000  .......C...s>...
+00000b80: 7400 6401 6402 8d01 7d02 7c02 a001 7c00  t.d.d...}.|...|.
+00000b90: a101 0100 7c02 a002 7c01 6a03 a101 0100  ....|...|.j.....
+00000ba0: 7400 6a04 7c00 7c01 6a05 6403 8d02 7d03  t.j.|.|.j.d...}.
+00000bb0: 7c03 a006 7c01 6a07 a101 0100 7c01 6a08  |...|.j.....|.j.
+00000bc0: 9001 7224 6404 6405 6c09 6d0a 7d04 0100  ..r$d.d.l.m.}...
+00000bd0: 740b a00c 6406 a101 a00d 6407 a101 0100  t...d.....d.....
+00000be0: 740e 6a0f a010 7c01 6a11 a101 9b00 6408  t.j...|.j.....d.
+00000bf0: 9d02 7d05 740e 6a0f a012 7c05 a101 7398  ..}.t.j...|...s.
+00000c00: 7400 a013 7c01 6a11 a101 7d06 7c06 a002  t...|.j...}.|...
+00000c10: 7c05 a101 0100 6e0a 7400 a013 7c05 a101  |.....n.t...|...
+00000c20: 7d06 7414 7c06 8301 6409 6b02 73b2 7415  }.t.|...d.k.s.t.
+00000c30: 8201 7416 7c06 a017 a100 8301 6404 1900  ..t.|.......d...
+00000c40: 7d07 740b a00c 6406 a101 a00d 640a 7c01  }.t...d.....d.|.
+00000c50: 6a11 9b00 9d02 a101 0100 7c07 a018 a100  j.........|.....
+00000c60: 7d08 7c08 a019 7c03 a101 0100 7c08 7d03  }.|...|.....|.}.
+00000c70: 7c04 7c07 7c03 8302 7d09 740b a00c 6406  |.|.|...}.t...d.
+00000c80: a101 a00d 640b 7c09 a01a a100 9b00 9d02  ....d.|.........
+00000c90: a101 0100 740b a00c 6406 a101 a00d 6407  ....t...d.....d.
+00000ca0: a101 0100 7c03 6a1b 7c01 6a1c 7c01 6a05  ....|.j.|.j.|.j.
+00000cb0: 640c 8d02 0100 7c02 7c03 6602 5300 290d  d.....|.|.f.S.).
+00000cc0: 7a24 0a20 204c 6f61 6420 7961 6d6c 2061  z$.  Load yaml a
+00000cd0: 6e64 2073 6176 6520 636f 6d6d 616e 645f  nd save command_
+00000ce0: 6366 670a 2020 54a9 015a 0b6e 6577 5f61  cfg.  T..Z.new_a
+00000cf0: 6c6c 6f77 6564 2901 722d 0000 0072 0100  llowed).r-...r..
+00000d00: 0000 2901 da08 4465 6570 4469 6666 da02  ..)...DeepDiff..
+00000d10: 746c 7a3a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  tlz:************
+00000d20: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
+00000d30: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a  ****************
+00000d40: 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 2a2a 7a13  **************z.
+00000d50: 2f63 6f6e 6669 675f 7265 7375 6d65 2e79  /config_resume.y
+00000d60: 616d 6ce9 0100 0000 7a15 5570 6461 7469  aml.....z.Updati
+00000d70: 6e67 2072 6573 756d 655f 6366 673a 207a  ng resume_cfg: z
+00000d80: 2264 6966 6620 6265 7477 6565 6e20 7265  "diff between re
+00000d90: 7375 6d65 5f63 6667 2061 6e64 2063 6667  sume_cfg and cfg
+00000da0: 3a20 0aa9 025a 0a73 6176 6564 5f66 696c  : ...Z.saved_fil
+00000db0: 6572 2d00 0000 291d 7206 0000 005a 0f6d  er-...).r....Z.m
+00000dc0: 6572 6765 5f66 726f 6d5f 6669 6c65 5a0c  erge_from_fileZ.
+00000dd0: 6475 6d70 5f74 6f5f 6669 6c65 7254 0000  dump_to_filerT..
+00000de0: 00da 166c 6f61 645f 7961 6d6c 5f77 6974  ...load_yaml_wit
+00000df0: 685f 636f 6d6d 616e 64da 0a74 6c5f 636f  h_command..tl_co
+00000e00: 6d6d 616e 64da 0f6d 6572 6765 5f66 726f  mmand..merge_fro
+00000e10: 6d5f 6c69 7374 da07 746c 5f6f 7074 73da  m_list..tl_opts.
+00000e20: 0974 6c5f 7265 7375 6d65 da08 6465 6570  .tl_resume..deep
+00000e30: 6469 6666 725a 0000 00da 076c 6f67 6769  diffrZ.....loggi
+00000e40: 6e67 da09 6765 744c 6f67 6765 72da 0469  ng..getLogger..i
+00000e50: 6e66 6f72 2000 0000 7221 0000 00da 0764  nfor ...r!.....d
+00000e60: 6972 6e61 6d65 7249 0000 00da 0665 7869  irnamerI.....exi
+00000e70: 7374 73da 0e6c 6f61 645f 7961 6d6c 5f66  sts..load_yaml_f
+00000e80: 696c 65da 036c 656e 721c 0000 00da 046c  ile..lenr......l
+00000e90: 6973 74da 0676 616c 7565 735a 0563 6c6f  ist..valuesZ.clo
+00000ea0: 6e65 da06 7570 6461 7465 da06 7072 6574  ne..update..pret
+00000eb0: 7479 da19 6475 6d70 5f74 6f5f 6669 6c65  ty..dump_to_file
+00000ec0: 5f77 6974 685f 636f 6d6d 616e 6472 5500  _with_commandrU.
+00000ed0: 0000 290a da0b 636f 6e66 6967 5f66 696c  ..)...config_fil
+00000ee0: 6572 5600 0000 da03 6366 67da 0b63 6f6d  erV.....cfg..com
+00000ef0: 6d61 6e64 5f63 6667 725a 0000 005a 0f72  mand_cfgrZ...Z.r
+00000f00: 6573 756d 655f 6366 675f 6669 6c65 5a08  esume_cfg_fileZ.
+00000f10: 7465 6d70 5f63 6667 5a0a 7265 7375 6d65  temp_cfgZ.resume
+00000f20: 5f63 6667 5a10 7265 7375 6d65 5f63 6667  _cfgZ.resume_cfg
+00000f30: 5f63 6c6f 6e65 da05 6464 6966 6672 3100  _clone..ddiffr1.
+00000f40: 0000 7231 0000 0072 3200 0000 da0d 5f73  ..r1...r2....._s
+00000f50: 6574 7570 5f63 6f6e 6669 6764 0000 0073  etup_configd...s
+00000f60: 3400 0000 0004 0a01 0a01 0c02 1001 0c02  4...............
+00000f70: 0801 0c01 1001 1401 0c01 0c01 0c02 0a01  ................
+00000f80: 1001 1002 1801 0801 0a01 0402 0a01 1a01  ................
+00000f90: 1002 0801 04ff 0605 7274 0000 0063 0200  ........rt...c..
+00000fa0: 0000 0000 0000 0000 0000 0b00 0000 0700  ................
+00000fb0: 0000 4300 0000 7318 0100 007c 00a0 00a1  ..C...s....|....
+00000fc0: 007d 0274 0183 007d 037c 036a 027c 0264  .}.t...}.|.j.|.d
+00000fd0: 018d 015c 027d 047d 0574 0374 047c 0483  ...\.}.}.t.t.|..
+00000fe0: 0183 017d 0474 057c 047c 046a 0664 028d  ...}.t.|.|.j.d..
+00000ff0: 0201 0074 077c 046a 0864 0364 0467 0264  ...t.|.j.d.d.g.d
+00001000: 0564 068d 037d 067c 06a0 0964 0774 0a6a  .d...}.|...d.t.j
+00001010: 0b7c 0464 0864 098d 0217 00a1 0101 007c  .|.d.d.........|
+00001020: 046a 0672 7c7c 06a0 0964 0a7c 046a 0c9b  .j.r||...d.|.j..
+00001030: 009d 02a1 0101 007c 046a 0da0 0ea1 0064  .......|.j.....d
+00001040: 0b6b 0272 9a7c 0172 967c 0464 0c66 0253  .k.r.|.r.|.d.f.S
+00001050: 007c 0453 0074 0f7c 046a 107c 0464 0d8d  .|.S.t.|.j.|.d..
+00001060: 025c 027d 077d 087c 06a0 0964 0e7c 08a0  .\.}.}.|...d.|..
+00001070: 11a1 009b 009d 02a1 0101 007c 0190 0172  ...........|...r
+00001080: 1074 12a0 137c 08a1 0101 0074 047c 0483  .t...|.....t.|..
+00001090: 01a0 14a1 0044 005d 205c 027d 097d 0a7c  .....D.] \.}.}.|
+000010a0: 09a0 1564 0fa1 0172 dc74 12a0 137c 097c  ...d...r.t...|.|
+000010b0: 0a69 01a1 0101 0071 dc7c 08a0 1374 12a1  .i.....q.|...t..
+000010c0: 0101 007c 047c 0866 0253 007c 0453 0064  ...|.|.f.S.|.S.d
+000010d0: 0c53 0029 107a 180a 2020 5573 6167 653a  .S.).z..  Usage:
+000010e0: 0a0a 2020 3a72 6574 7572 6e3a 0a20 2029  ..  :return:.  )
+000010f0: 0172 5600 0000 2902 7256 0000 0072 5700  .rV...).rV...rW.
+00001100: 0000 5a0c 7465 6d70 6c61 7465 5f6c 6962  ..Z.template_lib
+00001110: 725b 0000 0054 2903 da08 6669 6c65 6e61  r[...T)...filena
+00001120: 6d65 5a0c 6c6f 6767 6572 5f6e 616d 6573  meZ.logger_names
+00001130: da06 7374 7265 616d 7a07 0a61 7267 733a  ..streamz..args:
+00001140: 0a46 2901 da0a 7573 655f 7070 7269 6e74  .F)...use_pprint
+00001150: 7a0d 5265 7375 6d65 2066 726f 6d20 0ada  z.Resume from ..
+00001160: 046e 6f6e 654e 2902 7270 0000 0072 5600  .noneN).rp...rV.
+00001170: 0000 7a0b 0a54 6865 2063 6667 3a20 0ada  ..z..The cfg: ..
+00001180: 0374 6c5f 2916 da05 7370 6c69 7472 4200  .tl_)...splitrB.
+00001190: 0000 da10 7061 7273 655f 6b6e 6f77 6e5f  ....parse_known_
+000011a0: 6172 6773 7203 0000 00da 0476 6172 7372  argsr......varsr
+000011b0: 5800 0000 7262 0000 0072 0800 0000 7253  X...rb...r....rS
+000011c0: 0000 0072 6600 0000 7204 0000 00da 0b64  ...rf...r......d
+000011d0: 6963 7432 7374 7269 6e67 7247 0000 0072  ict2stringrG...r
+000011e0: 5f00 0000 da05 6c6f 7765 7272 7400 0000  _.....lowerrt...
+000011f0: da0e 746c 5f63 6f6e 6669 675f 6669 6c65  ..tl_config_file
+00001200: da04 6475 6d70 7205 0000 00da 0f6d 6572  ..dumpr......mer
+00001210: 6765 5f66 726f 6d5f 6469 6374 da05 6974  ge_from_dict..it
+00001220: 656d 7372 1b00 0000 290b da08 6172 6776  emsr....)...argv
+00001230: 5f73 7472 da0a 7265 7475 726e 5f63 6667  _str..return_cfg
+00001240: 5a0d 6172 6776 5f73 7472 5f6c 6973 7472  Z.argv_str_listr
+00001250: 4000 0000 7256 0000 005a 0d75 6e70 6172  @...rV...Z.unpar
+00001260: 7365 645f 6172 6776 da06 6c6f 6767 6572  sed_argv..logger
+00001270: 720f 0000 0072 7200 0000 da01 6bda 0176  r....rr.....k..v
+00001280: 7231 0000 0072 3100 0000 7232 0000 00da  r1...r1...r2....
+00001290: 1573 6574 7570 5f6f 7574 6469 725f 616e  .setup_outdir_an
+000012a0: 645f 7961 6d6c 8c00 0000 732e 0000 0000  d_yaml....s.....
+000012b0: 0808 0106 0110 020c 010e 0314 0118 0206  ................
+000012c0: 0112 050e 0104 0008 0104 0312 0114 0106  ................
+000012d0: 010a 0214 010a 0110 010a 0108 0272 8800  .............r..
+000012e0: 0000 6301 0000 0000 0000 0000 0000 0002  ..c.............
+000012f0: 0000 000b 0000 0043 0000 0073 4000 0000  .......C...s@...
+00001300: 6401 7c00 6a00 9b00 6402 7c00 6a01 9b00  d.|.j...d.|.j...
+00001310: 6403 7c00 6a02 9b00 6404 7c00 6a03 722a  d.|.j...d.|.j.r*
+00001320: 6405 7c00 6a04 1700 6e02 6406 9b00 6407  d.|.j...n.d...d.
+00001330: 7c00 6a05 9b00 6404 9d0b 7d01 7c01 5300  |.j...d...}.|.S.
+00001340: 2908 4e7a 1e0a 2020 2020 2020 2020 2020  ).Nz..          
+00001350: 2020 2d2d 746c 5f63 6f6e 6669 675f 6669    --tl_config_fi
+00001360: 6c65 207a 1a0a 2020 2020 2020 2020 2020  le z..          
+00001370: 2020 2d2d 746c 5f63 6f6d 6d61 6e64 207a    --tl_command z
+00001380: 190a 2020 2020 2020 2020 2020 2020 2d2d  ..            --
+00001390: 746c 5f6f 7574 6469 7220 7a0d 0a20 2020  tl_outdir z..   
+000013a0: 2020 2020 2020 2020 207a 1b2d 2d74 6c5f           z.--tl_
+000013b0: 7265 7375 6d65 202d 2d74 6c5f 7265 7375  resume --tl_resu
+000013c0: 6d65 6469 7220 7236 0000 007a 1b0a 2020  medir r6...z..  
+000013d0: 2020 2020 2020 2020 2020 2d2d 746c 5f74            --tl_t
+000013e0: 696d 655f 7374 7220 2906 7255 0000 0072  ime_str ).rU...r
+000013f0: 5f00 0000 7230 0000 0072 6200 0000 7247  _...r0...rb...rG
+00001400: 0000 0072 4c00 0000 2902 7256 0000 005a  ...rL...).rV...Z
+00001410: 0e63 6d64 5f73 7472 5f61 7070 656e 6472  .cmd_str_appendr
+00001420: 3100 0000 7231 0000 0072 3200 0000 da12  1...r1...r2.....
+00001430: 6765 745f 6170 7065 6e64 5f63 6d64 5f73  get_append_cmd_s
+00001440: 7472 b800 0000 7318 0000 0000 0102 0104  tr....s.........
+00001450: ff04 0204 fe04 0304 fd04 0412 fc04 0504  ................
+00001460: fb08 0772 8900 0000 6301 0000 0000 0000  ...r....c.......
+00001470: 0000 0000 0005 0000 000a 0000 0043 0000  .............C..
+00001480: 0073 be00 0000 7c00 a000 a100 7d01 7401  .s....|.....}.t.
+00001490: a002 6401 a101 7d02 7c02 a003 6402 6403  ..d...}.|...d.d.
+000014a0: a004 7c01 a101 1700 a101 0100 7405 6a06  ..|.........t.j.
+000014b0: a007 a100 7d03 7c01 6404 1900 6405 6b02  ....}.|.d...d.k.
+000014c0: 7256 7408 6a09 7c01 6404 3c00 740a 6a0b  rVt.j.|.d.<.t.j.
+000014d0: 7c01 7c03 6406 8d02 7d04 6e42 7c01 6404  |.|.d...}.nB|.d.
+000014e0: 1900 6407 6b02 728a 740a 6a0b 6408 6409  ..d.k.r.t.j.d.d.
+000014f0: 640a 640b 640c a004 7c01 640d 6400 8502  d.d.d...|.d.d...
+00001500: 1900 a101 6705 7c03 6406 8d02 7d04 6e0e  ....g.|.d...}.n.
+00001510: 740a 6a0b 7c01 7c03 6406 8d02 7d04 7c04  t.j.|.|.d...}.|.
+00001520: a00c a100 0100 7c04 6a0d 6404 6b03 72ba  ......|.j.d.k.r.
+00001530: 740a 6a0e 7c04 6a0d 7c01 640e 8d02 8201  t.j.|.j.|.d.....
+00001540: 6400 5300 290f 4e72 5b00 0000 7a0a 0a72  d.S.).Nr[...z..r
+00001550: 756e 5f73 7472 3a0a 7a05 205c 0a20 2072  un_str:.z. \.  r
+00001560: 0100 0000 5a06 7079 7468 6f6e 2901 da03  ....Z.python)...
+00001570: 656e 765a 0462 6173 687a 092f 6269 6e2f  envZ.bashz./bin/
+00001580: 6261 7368 7a02 2d6f 5a06 7874 7261 6365  bashz.-oZ.xtrace
+00001590: 7a02 2d63 7213 0000 0072 5c00 0000 2902  z.-cr....r\...).
+000015a0: da0a 7265 7475 726e 636f 6465 da03 636d  ..returncode..cm
+000015b0: 6429 0f72 7a00 0000 7264 0000 0072 6500  d).rz...rd...re.
+000015c0: 0000 7266 0000 0072 4800 0000 7220 0000  ..rf...rH...r ..
+000015d0: 0072 2900 0000 da04 636f 7079 7227 0000  .r).....copyr'..
+000015e0: 00da 0a65 7865 6375 7461 626c 65da 0a73  ...executable..s
+000015f0: 7562 7072 6f63 6573 73da 0550 6f70 656e  ubprocess..Popen
+00001600: da04 7761 6974 728b 0000 00da 1243 616c  ..waitr......Cal
+00001610: 6c65 6450 726f 6365 7373 4572 726f 7229  ledProcessError)
+00001620: 055a 0763 6d64 5f73 7472 728c 0000 0072  .Z.cmd_strr....r
+00001630: 8500 0000 5a0b 6375 7272 656e 745f 656e  ....Z.current_en
+00001640: 76da 0770 726f 6365 7373 7231 0000 0072  v..processr1...r
+00001650: 3100 0000 7232 0000 00da 0d73 7461 7274  1...r2.....start
+00001660: 5f63 6d64 5f72 756e c300 0000 731c 0000  _cmd_run....s...
+00001670: 0000 0108 010a 0114 010a 010c 010a 0110  ................
+00001680: 010c 0128 020e 0208 010a 0110 0172 9400  ...(.........r..
+00001690: 0000 5463 0300 0000 0000 0000 0000 0000  ..Tc............
+000016a0: 0700 0000 0600 0000 4300 0000 734c 0100  ........C...sL..
+000016b0: 0074 006a 01a0 027c 006a 0364 01a1 027d  .t.j...|.j.d...}
+000016c0: 037c 0272 3474 0474 05a0 0664 02a1 016a  .|.r4t.t...d...j
+000016d0: 0783 0164 036b 0072 4474 087c 0364 048d  ...d.k.rDt.|.d..
+000016e0: 017d 046e 1074 05a0 0664 02a1 017d 0464  .}.n.t...d...}.d
+000016f0: 057c 045f 097c 0272 5c74 0a7c 0164 068d  .|._.|.r\t.|.d..
+00001700: 017d 0574 0b7c 0564 078d 0101 007c 006a  .}.t.|.d.....|.j
+00001710: 0ca0 0da1 0064 086b 0273 e674 006a 01a0  .....d.k.s.t.j..
+00001720: 0e7c 006a 0fa1 0173 7c74 1082 0174 11a0  .|.j...s|t...t..
+00001730: 127c 006a 0f7c 006a 0ca1 027d 067c 06a0  .|.j.|.j...}.|..
+00001740: 137c 006a 14a1 0101 007c 006a 039b 0064  .|.j.....|.j...d
+00001750: 099d 027c 065f 1574 167c 0683 0101 0074  ...|._.t.|.....t
+00001760: 05a0 0664 02a1 01a0 1764 0a74 18a0 19a1  ...d.....d.t....
+00001770: 0017 00a1 0101 0074 1aa0 1ba1 0001 007c  .......t.......|
+00001780: 0272 e47c 066a 1c74 186a 157c 006a 0c64  .r.|.j.t.j.|.j.d
+00001790: 0b8d 0201 006e 5e74 1183 007d 067c 066a  .....n^t...}.|.j
+000017a0: 137c 006a 1464 0c64 0d8d 0201 007c 006a  .|.j.d.d.....|.j
+000017b0: 039b 0064 099d 027c 065f 1574 167c 0683  ...d...|._.t.|..
+000017c0: 0101 0074 05a0 0664 02a1 01a0 1764 0a74  ...t...d.....d.t
+000017d0: 1da0 1e74 18a1 0117 00a1 0101 007c 0290  ...t.........|..
+000017e0: 0172 447c 066a 1c74 186a 157c 006a 0c64  .rD|.j.t.j.|.j.d
+000017f0: 0b8d 0201 007c 067c 0366 0253 0029 0e4e  .....|.|.f.S.).N
+00001800: 7246 0000 0072 5b00 0000 e902 0000 0029  rF...r[........)
+00001810: 0172 7500 0000 4629 015a 086c 6f67 5f72  .ru...F).Z.log_r
+00001820: 6f6f 7429 01da 0a74 6578 746c 6f67 6765  oot)...textlogge
+00001830: 7272 7800 0000 7a14 2f63 6f6e 6669 675f  rrx...z./config_
+00001840: 636f 6d6d 616e 642e 7961 6d6c 7a0e 0a67  command.yamlz..g
+00001850: 6c6f 6261 6c5f 6366 673a 200a 725d 0000  lobal_cfg: .r]..
+00001860: 0054 7259 0000 0029 1f72 2000 0000 7221  .TrY...).r ...r!
+00001870: 0000 0072 4800 0000 7230 0000 0072 6a00  ...rH...r0...rj.
+00001880: 0000 7264 0000 0072 6500 0000 da08 6861  ..rd...re.....ha
+00001890: 6e64 6c65 7273 7208 0000 00da 0970 726f  ndlersr......pro
+000018a0: 7061 6761 7465 720a 0000 0072 0900 0000  pagater....r....
+000018b0: 725f 0000 0072 7e00 0000 7268 0000 0072  r_...r~...rh...r
+000018c0: 7f00 0000 721c 0000 0072 0600 0000 725e  ....r....r....r^
+000018d0: 0000 0072 6000 0000 7261 0000 0072 5400  ...r`...ra...rT.
+000018e0: 0000 7207 0000 0072 6600 0000 7205 0000  ..r....rf...r...
+000018f0: 0072 8000 0000 720b 0000 00da 0b73 796e  .r....r......syn
+00001900: 6368 726f 6e69 7a65 726f 0000 0072 0400  chronizero...r..
+00001910: 0000 727d 0000 0029 0772 5600 0000 da0a  ..r}...).rV.....
+00001920: 746c 5f74 6578 7464 6972 da0f 6973 5f6d  tl_textdir..is_m
+00001930: 6169 6e5f 7072 6f63 6573 7372 5300 0000  ain_processrS...
+00001940: 7285 0000 0072 9600 0000 7271 0000 0072  r....r....rq...r
+00001950: 3100 0000 7231 0000 0072 3200 0000 da2a  1...r1...r2....*
+00001960: 5f73 6574 7570 5f6c 6f67 6765 725f 676c  _setup_logger_gl
+00001970: 6f62 616c 5f63 6667 5f67 6c6f 6261 6c5f  obal_cfg_global_
+00001980: 7465 7874 6c6f 6767 6572 d600 0000 7336  textlogger....s6
+00001990: 0000 0000 0510 0104 0114 010c 020a 0106  ................
+000019a0: 0304 010a 010a 030e 0112 0110 010c 020e  ................
+000019b0: 0108 0218 0208 0104 0114 0506 0110 020e  ................
+000019c0: 0108 011a 0106 0112 0172 9c00 0000 6302  .........r....c.
+000019d0: 0000 0000 0000 0000 0000 0005 0000 0005  ................
+000019e0: 0000 004b 0000 0073 4c00 0000 7c01 7226  ...K...sL...|.r&
+000019f0: 7c01 a000 a100 4400 5d18 5c02 7d03 7d04  |.....D.].\.}.}.
+00001a00: 7c00 6a01 6600 7c03 7c04 6901 8e01 0100  |.j.f.|.|.i.....
+00001a10: 710c 7c02 a000 a100 4400 5d18 5c02 7d03  q.|.....D.].\.}.
+00001a20: 7d04 7c00 6a01 6600 7c03 7c04 6901 8e01  }.|.j.f.|.|.i...
+00001a30: 0100 712e 7c00 5300 2901 4e29 0272 8200  ..q.|.S.).N).r..
+00001a40: 0000 da0c 7365 745f 6465 6661 756c 7473  ....set_defaults
+00001a50: 2905 7240 0000 0072 7100 0000 da06 6b77  ).r@...rq.....kw
+00001a60: 6172 6773 7286 0000 0072 8700 0000 7231  argsr....r....r1
+00001a70: 0000 0072 3100 0000 7232 0000 00da 145f  ...r1...r2....._
+00001a80: 7061 7273 6572 5f73 6574 5f64 6566 6175  parser_set_defau
+00001a90: 6c74 7305 0100 0073 0c00 0000 0001 0401  lts....s........
+00001aa0: 1001 1201 1001 1201 729f 0000 0063 0100  ........r....c..
+00001ab0: 0000 0000 0000 0000 0000 0400 0000 0400  ................
+00001ac0: 0000 4300 0000 7360 0000 007c 009b 0064  ..C...s`...|...d
+00001ad0: 019d 027d 017c 009b 0064 029d 027d 027c  ...}.|...d...}.|
+00001ae0: 009b 0064 039d 027d 0374 006a 017c 0064  ...d...}.t.j.|.d
+00001af0: 0464 058d 0201 0074 006a 017c 0164 0464  .d.....t.j.|.d.d
+00001b00: 058d 0201 0074 006a 017c 0264 0464 058d  .....t.j.|.d.d..
+00001b10: 0201 0074 006a 017c 0364 0464 058d 0201  ...t.j.|.d.d....
+00001b20: 007c 017c 027c 0366 0353 0029 064e 7a08  .|.|.|.f.S.).Nz.
+00001b30: 2f63 6b70 7464 6972 7a07 2f69 6d67 6469  /ckptdirz./imgdi
+00001b40: 727a 082f 7465 7874 6469 7254 2901 da08  rz./textdirT)...
+00001b50: 6578 6973 745f 6f6b 2902 7220 0000 00da  exist_ok).r ....
+00001b60: 086d 616b 6564 6972 7329 0472 3000 0000  .makedirs).r0...
+00001b70: da0a 746c 5f63 6b70 7464 6972 da09 746c  ..tl_ckptdir..tl
+00001b80: 5f69 6d67 6469 7272 9a00 0000 7231 0000  _imgdirr....r1..
+00001b90: 0072 3100 0000 7232 0000 0072 5100 0000  .r1...r2...rQ...
+00001ba0: 0e01 0000 7310 0000 0000 010a 010a 010a  ....s...........
+00001bb0: 020e 010e 010e 010e 0172 5100 0000 7256  .........rQ...rV
+00001bc0: 0000 0063 0500 0000 0000 0000 0000 0000  ...c............
+00001bd0: 0f00 0000 0800 0000 4300 0000 73c8 0000  ........C...s...
+00001be0: 0074 007c 007c 0464 018d 027d 007c 00a0  .t.|.|.d...}.|..
+00001bf0: 01a1 005c 027d 057d 0674 027c 056a 0364  ...\.}.}.t.|.j.d
+00001c00: 028d 015c 037d 077d 087d 0974 047c 057c  ...\.}.}.}.t.|.|
+00001c10: 097c 0364 038d 035c 027d 0a7d 0b7c 0272  .|.d...\.}.}.|.r
+00001c20: 467c 0a7d 0c6e 147c 017c 0a6b 0672 567c  F|.}.n.|.|.k.rV|
+00001c30: 0a7c 0119 006e 0264 007d 0c74 057c 007c  .|...n.d.}.t.|.|
+00001c40: 0c7c 087c 077c 097c 0b64 048d 0601 0074  .|.|.|.|.d.....t
+00001c50: 06a0 0764 05a1 01a0 0864 0664 07a0 0974  ...d.....d.d...t
+00001c60: 0a6a 0ba1 0117 00a1 0101 007c 00a0 01a1  .j.........|....
+00001c70: 005c 027d 057d 0674 0c7c 0583 01a0 0da1  .\.}.}.t.|......
+00001c80: 0044 005d 205c 027d 0d7d 0e7c 0da0 0e64  .D.] \.}.}.|...d
+00001c90: 08a1 0172 a274 0fa0 107c 0d7c 0e69 01a1  ...r.t...|.|.i..
+00001ca0: 0101 0071 a27c 0053 0029 094e 2901 7241  ...q.|.S.).N).rA
+00001cb0: 0000 0072 4500 0000 2901 729b 0000 0029  ...rE...).r....)
+00001cc0: 0572 7100 0000 72a3 0000 0072 a200 0000  .rq...r....r....
+00001cd0: 729a 0000 0072 5300 0000 725b 0000 007a  r....rS...r[...z
+00001ce0: 1473 7973 2e61 7267 763a 200a 2070 7974  .sys.argv: . pyt
+00001cf0: 686f 6e20 0a7a 0220 0a72 7900 0000 2911  hon .z. .ry...).
+00001d00: 7242 0000 0072 7b00 0000 7251 0000 0072  rB...r{...rQ...r
+00001d10: 3000 0000 729c 0000 0072 9f00 0000 7264  0...r....r....rd
+00001d20: 0000 0072 6500 0000 7266 0000 0072 4800  ...re...rf...rH.
+00001d30: 0000 7227 0000 0072 2800 0000 727c 0000  ..r'...r(...r|..
+00001d40: 0072 8200 0000 721b 0000 0072 0500 0000  .r....r....r....
+00001d50: 7281 0000 0029 0f72 4000 0000 7216 0000  r....).r@...r...
+00001d60: 005a 0f75 7365 5f63 6667 5f61 735f 6172  .Z.use_cfg_as_ar
+00001d70: 6773 729b 0000 0072 4100 0000 7256 0000  gsr....rA...rV..
+00001d80: 0072 0f00 0000 72a2 0000 0072 a300 0000  .r....r....r....
+00001d90: 729a 0000 0072 7100 0000 7253 0000 005a  r....rq...rS...Z
+00001da0: 0c64 6566 6175 6c74 5f61 7267 7372 8600  .default_argsr..
+00001db0: 0000 7287 0000 0072 3100 0000 7231 0000  ..r....r1...r1..
+00001dc0: 0072 3200 0000 da20 7570 6461 7465 5f70  .r2.... update_p
+00001dd0: 6172 7365 725f 6465 6661 756c 7473 5f66  arser_defaults_f
+00001de0: 726f 6d5f 7961 6d6c 1a01 0000 7326 0000  rom_yaml....s&..
+00001df0: 0000 020c 020c 0212 0212 0204 0106 0214  ................
+00001e00: 0206 0102 0002 0002 0102 fe06 031c 010c  ................
+00001e10: 0114 010a 0110 0772 a400 0000 2902 4e46  .......r....).NF
+00001e20: 2902 4e46 2901 5429 0472 5600 0000 4654  ).NF).T).rV...FT
+00001e30: 4629 2cda 0474 696d 6572 2000 0000 7227  F),..timer ...r'
+00001e40: 0000 00da 046a 736f 6e72 6400 0000 da06  .....jsonrd.....
+00001e50: 7070 7269 6e74 728f 0000 0072 0200 0000  pprintr....r....
+00001e60: 723c 0000 0072 4f00 0000 5a04 7961 6d6c  r<...rO...Z.yaml
+00001e70: 5a08 6561 7379 6469 6374 7203 0000 00da  Z.easydictr.....
+00001e80: 0374 6c32 7204 0000 00da 0f74 6c32 2e70  .tl2r......tl2.p
+00001e90: 726f 6a2e 6676 636f 7265 7205 0000 0072  roj.fvcorer....r
+00001ea0: 0600 0000 7207 0000 005a 1c74 6c32 2e70  ....r....Z.tl2.p
+00001eb0: 726f 6a2e 6c6f 6767 6572 2e6c 6f67 6765  roj.logger.logge
+00001ec0: 725f 7574 696c 7372 0800 0000 5a0f 746c  r_utilsr....Z.tl
+00001ed0: 322e 7072 6f6a 2e6c 6f67 6765 7272 0900  2.proj.loggerr..
+00001ee0: 0000 720a 0000 005a 1474 6c32 2e70 726f  ..r....Z.tl2.pro
+00001ef0: 6a2e 7079 746f 7263 682e 6464 7072 0b00  j.pytorch.ddpr..
+00001f00: 0000 5a12 746c 322e 7072 6f6a 2e61 7267  ..Z.tl2.proj.arg
+00001f10: 7061 7273 6572 720c 0000 00da 095f 6765  parserr......_ge
+00001f20: 7466 7261 6d65 da06 665f 636f 6465 da07  tframe..f_code..
+00001f30: 636f 5f6e 616d 65da 085f 5f66 696c 655f  co_name..__file_
+00001f40: 5f72 3300 0000 7235 0000 0072 4200 0000  _r3...r5...rB...
+00001f50: 7258 0000 0072 7400 0000 7288 0000 0072  rX...rt...r....r
+00001f60: 8900 0000 7294 0000 0072 9c00 0000 729f  ....r....r....r.
+00001f70: 0000 0072 5100 0000 72a4 0000 0072 3100  ...rQ...r....r1.
+00001f80: 0000 7231 0000 0072 3100 0000 7232 0000  ..r1...r1...r2..
+00001f90: 00da 083c 6d6f 6475 6c65 3e01 0000 0073  ...<module>....s
+00001fa0: 4a00 0000 0801 0801 0801 0801 0801 0801  J...............
+00001fb0: 0801 0c01 0802 0802 0801 0c02 0c01 1401  ................
+00001fc0: 0c01 1001 0c01 0c05 0a01 02fd 0a1b 0808  ................
+00001fd0: 0a12 0817 0829 0001 00fe 0a2c 080b 0816  .....).....,....
+00001fe0: 00fd 0a2f 0809 080c 0001 0000 00ff       .../..........
```

### Comparing `tl2-0.1.0/tl2/launch/tests/test_launch.py` & `tl2-0.1.1/tl2/launch/tests/test_launch.py`

 * *Files 2% similar despite different names*

```diff
@@ -59,14 +59,19 @@
                   python -c "from tl2.modelarts.tests.test_run import TestingRun;\
                         TestingRun().test_run_v2(number={os.environ['RUN_NUM']}, )" \
                         --tl_opts root_obs {cfg.root_obs}
                   """
       p = tl2_utils.Worker(name='Run', args=(run_command,))
       p.start()
 
+    os.environ['DNNLIB_CACHE_DIR'] = "cache_dnnlib"
+    os.environ['TORCH_EXTENSIONS_DIR'] = "cache_torch_extensions"
+    os.environ['PATH'] = f"{os.path.dirname(sys.executable)}:{os.environ['PATH']}"
+    os.environ['MAX_JOBS '] = "8"
+    
     n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
     PORT = os.environ.get('PORT', 8888)
 
     if n_gpus > 1:
       python_str = f"python -m torch.distributed.launch --nproc_per_node={n_gpus} --master_port={PORT} "
     else:
       python_str = "python "
```

### Comparing `tl2-0.1.0/tl2/modelarts/modelarts_utils.py` & `tl2-0.1.1/tl2/modelarts/modelarts_utils.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/modelarts/moxing_utils.py` & `tl2-0.1.1/tl2/modelarts/moxing_utils.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/modelarts/scripts/make_log_dirs.py` & `tl2-0.1.1/tl2/modelarts/scripts/make_log_dirs.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-import os
-import argparse
-
-
-
-
-def main(args):
-  for i in range(args.start_dir, args.num_dirs):
-    log_dir = f"{args.root_dir}/{i}"
-    print(f"Create dir in {log_dir}")
-    os.makedirs(log_dir, exist_ok=True)
-    with open(f"{log_dir}/test.txt", 'w') as f:
-      f.write('test')
-  pass
-
-
-
-if __name__ == '__main__':
-  """
-  python3 -m tl2.modelarts.scripts.make_log_dirs \
-    --num_dirs 500 --start_dir 0 --root_dir /home/z50017127/user/logs
-    
-  """
-  parser = argparse.ArgumentParser()
-  parser.add_argument('--num_dirs', type=int, default=100)
-  parser.add_argument('--start_dir', type=int, default=0)
-  parser.add_argument('--root_dir', default="/tmp/logs")
-  args = parser.parse_args()
+import os
+import argparse
+
+
+
+
+def main(args):
+  for i in range(args.start_dir, args.num_dirs):
+    log_dir = f"{args.root_dir}/{i}"
+    print(f"Create dir in {log_dir}")
+    os.makedirs(log_dir, exist_ok=True)
+    with open(f"{log_dir}/test.txt", 'w') as f:
+      f.write('test')
+  pass
+
+
+
+if __name__ == '__main__':
+  """
+  python3 -m tl2.modelarts.scripts.make_log_dirs \
+    --num_dirs 500 --start_dir 0 --root_dir /home/z50017127/user/logs
+    
+  """
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--num_dirs', type=int, default=100)
+  parser.add_argument('--start_dir', type=int, default=0)
+  parser.add_argument('--root_dir', default="/tmp/logs")
+  args = parser.parse_args()
   main(args)
```

### Comparing `tl2-0.1.0/tl2/modelarts/scripts/plot_results_obs.py` & `tl2-0.1.1/tl2/modelarts/scripts/plot_results_obs.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,173 +1,173 @@
-from IPython.core.debugger import set_trace
-
-
-class PlotResultsObs(object):
-
-  def __init__(self, ):
-    self.root_obs_dict = {
-      'beijing': 's3://bucket-cv-competition-bj4/ZhouPeng',
-      'huanan': 's3://bucket-1892/ZhouPeng',
-      'huabei': 's3://bucket-cv-competition',
-      '7001': "s3://bucket-7001/ZhouPeng",
-      '3690': "s3://bucket-3690/ZhouPeng"
-    }
-    PlotResultsObs.setup_env()
-
-    pass
-
-  @staticmethod
-  def setup_env():
-    import os
-    try:
-      import mpld3
-    except:
-      os.system('pip install mpld3')
-
-  def get_last_md_inter_time(self, filepath_obs):
-    import moxing as mox
-    from datetime import datetime, timedelta
-
-    statbuf = mox.file.stat(filepath_obs)
-    modi_time = datetime.fromtimestamp(statbuf.mtime_nsec / 1e9) + timedelta(hours=8)
-    modi_inter = datetime.now() - modi_time
-    modi_minutes = modi_inter.total_seconds() // 60
-    return int(modi_minutes)
-
-  def get_fig_axes(self, rows, cols, figsize_wh=(15, 7)):
-    import matplotlib.pyplot as plt
-    plt.style.use('seaborn-whitegrid')
-    plt.rcParams['axes.prop_cycle'] = plt.cycler(
-      color=['blue', 'green', 'red', 'cyan', 'magenta', 'black', 'orange', 'lime', 'tan', 'salmon', 'gold', 'darkred',
-             'darkblue'])
-    fig, axes = plt.subplots(rows, cols, figsize=(figsize_wh[0] * cols, figsize_wh[1] * rows))
-    if rows * cols > 1:
-      axes = axes.ravel()
-    else:
-      axes = [axes]
-    return fig, axes
-
-  def get_itr_val_str(self, data, ismax):
-    if ismax:
-      itr = int(data[:, 0][data[:, 1].argmax()])
-      val = data[:, 1].max()
-      return f'itr.{itr:06d}_maxv.{val:.3f}'
-    else:
-      itr = int(data[:, 0][data[:, 1].argmin()])
-      val = data[:, 1].min()
-      return f'itr.{itr:06d}_minv.{val:.3f}'
-
-  def _data_load_func(self, filepath):
-    import numpy as np
-    data = np.loadtxt(filepath, delimiter=':')
-    data = data.reshape(-1, 2)
-    return data
-
-  def plot_defaultdicts(self, default_dicts, show_max=True, bucket='huanan', figsize_wh=(15, 8), legend_size=12,
-                        data_load_func=None):
-    import matplotlib.pyplot as plt
-    % matplotlib inline
-    import numpy as np
-    import mpld3
-    mpld3.enable_notebook()
-    import os
-    import moxing as mox
-    import tempfile
-    assert isinstance(default_dicts, dict)
-
-    if not isinstance(show_max, list):
-      show_max = [show_max]
-    assert len(show_max) == len(default_dicts)
-
-    fig, axes = self.get_fig_axes(rows=len(default_dicts), cols=1, figsize_wh=figsize_wh)
-
-    if data_load_func is None:
-      data_load_func_list = [self._data_load_func, ] * len(default_dicts)
-    elif not isinstance(data_load_func, (list, tuple)):
-      data_load_func_list = [data_load_func, ] * len(default_dicts)
-    else:
-      data_load_func_list = data_load_func
-
-    bucket = self.root_obs_dict[bucket]
-    root_dir = os.path.expanduser('~/results')
-
-    label2datas_list = {}
-    for idx, (dict_name, default_dict) in enumerate(default_dicts.items()):
-      data_xlim = None
-      axes_prop = default_dict.get('properties')
-      if axes_prop is not None:
-        if 'xlim' in axes_prop:
-          data_xlim = axes_prop['xlim'][-1]
-
-      label2datas = {}
-      # for each result dir
-      for (result_dir, label2file) in default_dict.items():
-        if result_dir == 'properties':
-          continue
-        # for each texlog file
-        for label, file in label2file.items():
-          filepath = os.path.join(root_dir, result_dir, file)
-          filepath_obs = os.path.join(bucket, result_dir, file)
-          if not mox.file.exists(filepath_obs):
-            print("=> Not exist: '%s'" % filepath_obs)
-            continue
-          mox.file.copy(filepath_obs, filepath)
-          # get modified time
-          modi_minutes = self.get_last_md_inter_time(filepath_obs)
-
-          data = data_load_func_list[idx](filepath)
-          # data = np.loadtxt(filepath, delimiter=':')
-          # data = data.reshape(-1, 2)
-          # limit x in a range
-          if data_xlim:
-              data = data[data[:, 0] <= data_xlim]
-
-          itr_val_str = self.get_itr_val_str(data, show_max[idx])
-          label_str = f'{itr_val_str}' + f'-{modi_minutes:03d}m---' + label
-
-          axes[idx].plot(data[:, 0], data[:, 1], label=label_str, marker='.', linewidth='5', markersize='15', alpha=0.5)
-          label2datas[label] = data
-      axes[idx].legend(prop={'size': legend_size})
-      axes[idx].set(**default_dict['properties'])
-      axes[idx].grid(b=True, which='major', color='#666666', linestyle='--', alpha=0.2)
-
-      label2datas_list[dict_name] = label2datas
-
-    return label2datas_list
-
-
-import unittest
-class Testing_plot_results_obs(unittest.TestCase):
-
-  def test_plot_results(self, ):
-    import collections, os, functools
-
-    default_dicts = collections.OrderedDict()
-    show_max = []
-
-    FID = collections.defaultdict(dict)
-    title = 'FID'
-    log_file = 'textdir/evaltorch.ma2.FID.log'
-    dd = eval(title)
-    dd['results/Omni-GAN-ImageNet/OmniInrGAN_ImageNet256/train_ImageNet256-20210126_161550_248'] = \
-      {'20210126_161550_248-OmniInrGAN256-Gwd.1e-4-nd.2-bs.128x2': log_file, }
-
-    dd['properties'] = {'title': title, }
-    default_dicts[title] = dd
-    show_max.append(False)
-
-
-    plotobs = PlotResultsObs()
-    label2datas_list = plotobs.plot_defaultdicts(default_dicts=default_dicts, show_max=show_max, bucket='3690',
-                                                 figsize_wh=(16, 7.2))
-    pass
-
-  def test_save_results_list(self):
-    import moxing as mox
-    import pickle
-
-    obs_path = "s3://bucket-3690/ZhouPeng/results/Omni-GAN-ImageNet/data"
-    saved_data = 'OmniGAN_ImageNet128_results.pkl'
-    with open(saved_data, 'wb') as f:
-      pickle.dump(label2datas_list, f)
-    mox.file.copy(saved_data, f'{obs_path}/{saved_data}')
+from IPython.core.debugger import set_trace
+
+
+class PlotResultsObs(object):
+
+  def __init__(self, ):
+    self.root_obs_dict = {
+      'beijing': 's3://bucket-cv-competition-bj4/ZhouPeng',
+      'huanan': 's3://bucket-1892/ZhouPeng',
+      'huabei': 's3://bucket-cv-competition',
+      '7001': "s3://bucket-7001/ZhouPeng",
+      '3690': "s3://bucket-3690/ZhouPeng"
+    }
+    PlotResultsObs.setup_env()
+
+    pass
+
+  @staticmethod
+  def setup_env():
+    import os
+    try:
+      import mpld3
+    except:
+      os.system('pip install mpld3')
+
+  def get_last_md_inter_time(self, filepath_obs):
+    import moxing as mox
+    from datetime import datetime, timedelta
+
+    statbuf = mox.file.stat(filepath_obs)
+    modi_time = datetime.fromtimestamp(statbuf.mtime_nsec / 1e9) + timedelta(hours=8)
+    modi_inter = datetime.now() - modi_time
+    modi_minutes = modi_inter.total_seconds() // 60
+    return int(modi_minutes)
+
+  def get_fig_axes(self, rows, cols, figsize_wh=(15, 7)):
+    import matplotlib.pyplot as plt
+    plt.style.use('seaborn-whitegrid')
+    plt.rcParams['axes.prop_cycle'] = plt.cycler(
+      color=['blue', 'green', 'red', 'cyan', 'magenta', 'black', 'orange', 'lime', 'tan', 'salmon', 'gold', 'darkred',
+             'darkblue'])
+    fig, axes = plt.subplots(rows, cols, figsize=(figsize_wh[0] * cols, figsize_wh[1] * rows))
+    if rows * cols > 1:
+      axes = axes.ravel()
+    else:
+      axes = [axes]
+    return fig, axes
+
+  def get_itr_val_str(self, data, ismax):
+    if ismax:
+      itr = int(data[:, 0][data[:, 1].argmax()])
+      val = data[:, 1].max()
+      return f'itr.{itr:06d}_maxv.{val:.3f}'
+    else:
+      itr = int(data[:, 0][data[:, 1].argmin()])
+      val = data[:, 1].min()
+      return f'itr.{itr:06d}_minv.{val:.3f}'
+
+  def _data_load_func(self, filepath):
+    import numpy as np
+    data = np.loadtxt(filepath, delimiter=':')
+    data = data.reshape(-1, 2)
+    return data
+
+  def plot_defaultdicts(self, default_dicts, show_max=True, bucket='huanan', figsize_wh=(15, 8), legend_size=12,
+                        data_load_func=None):
+    import matplotlib.pyplot as plt
+    % matplotlib inline
+    import numpy as np
+    import mpld3
+    mpld3.enable_notebook()
+    import os
+    import moxing as mox
+    import tempfile
+    assert isinstance(default_dicts, dict)
+
+    if not isinstance(show_max, list):
+      show_max = [show_max]
+    assert len(show_max) == len(default_dicts)
+
+    fig, axes = self.get_fig_axes(rows=len(default_dicts), cols=1, figsize_wh=figsize_wh)
+
+    if data_load_func is None:
+      data_load_func_list = [self._data_load_func, ] * len(default_dicts)
+    elif not isinstance(data_load_func, (list, tuple)):
+      data_load_func_list = [data_load_func, ] * len(default_dicts)
+    else:
+      data_load_func_list = data_load_func
+
+    bucket = self.root_obs_dict[bucket]
+    root_dir = os.path.expanduser('~/results')
+
+    label2datas_list = {}
+    for idx, (dict_name, default_dict) in enumerate(default_dicts.items()):
+      data_xlim = None
+      axes_prop = default_dict.get('properties')
+      if axes_prop is not None:
+        if 'xlim' in axes_prop:
+          data_xlim = axes_prop['xlim'][-1]
+
+      label2datas = {}
+      # for each result dir
+      for (result_dir, label2file) in default_dict.items():
+        if result_dir == 'properties':
+          continue
+        # for each texlog file
+        for label, file in label2file.items():
+          filepath = os.path.join(root_dir, result_dir, file)
+          filepath_obs = os.path.join(bucket, result_dir, file)
+          if not mox.file.exists(filepath_obs):
+            print("=> Not exist: '%s'" % filepath_obs)
+            continue
+          mox.file.copy(filepath_obs, filepath)
+          # get modified time
+          modi_minutes = self.get_last_md_inter_time(filepath_obs)
+
+          data = data_load_func_list[idx](filepath)
+          # data = np.loadtxt(filepath, delimiter=':')
+          # data = data.reshape(-1, 2)
+          # limit x in a range
+          if data_xlim:
+              data = data[data[:, 0] <= data_xlim]
+
+          itr_val_str = self.get_itr_val_str(data, show_max[idx])
+          label_str = f'{itr_val_str}' + f'-{modi_minutes:03d}m---' + label
+
+          axes[idx].plot(data[:, 0], data[:, 1], label=label_str, marker='.', linewidth='5', markersize='15', alpha=0.5)
+          label2datas[label] = data
+      axes[idx].legend(prop={'size': legend_size})
+      axes[idx].set(**default_dict['properties'])
+      axes[idx].grid(b=True, which='major', color='#666666', linestyle='--', alpha=0.2)
+
+      label2datas_list[dict_name] = label2datas
+
+    return label2datas_list
+
+
+import unittest
+class Testing_plot_results_obs(unittest.TestCase):
+
+  def test_plot_results(self, ):
+    import collections, os, functools
+
+    default_dicts = collections.OrderedDict()
+    show_max = []
+
+    FID = collections.defaultdict(dict)
+    title = 'FID'
+    log_file = 'textdir/evaltorch.ma2.FID.log'
+    dd = eval(title)
+    dd['results/Omni-GAN-ImageNet/OmniInrGAN_ImageNet256/train_ImageNet256-20210126_161550_248'] = \
+      {'20210126_161550_248-OmniInrGAN256-Gwd.1e-4-nd.2-bs.128x2': log_file, }
+
+    dd['properties'] = {'title': title, }
+    default_dicts[title] = dd
+    show_max.append(False)
+
+
+    plotobs = PlotResultsObs()
+    label2datas_list = plotobs.plot_defaultdicts(default_dicts=default_dicts, show_max=show_max, bucket='3690',
+                                                 figsize_wh=(16, 7.2))
+    pass
+
+  def test_save_results_list(self):
+    import moxing as mox
+    import pickle
+
+    obs_path = "s3://bucket-3690/ZhouPeng/results/Omni-GAN-ImageNet/data"
+    saved_data = 'OmniGAN_ImageNet128_results.pkl'
+    with open(saved_data, 'wb') as f:
+      pickle.dump(label2datas_list, f)
+    mox.file.copy(saved_data, f'{obs_path}/{saved_data}')
     pass
```

### Comparing `tl2-0.1.0/tl2/modelarts/scripts/run.sh` & `tl2-0.1.1/tl2/modelarts/scripts/run.sh`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,72 +1,72 @@
-set -x
-# bash /home/work/run_train.sh bash /home/work/user-job-dir/pi-GAN-exp/exp/dev/nerf_inr/bash/run_v16_r128_grad96.sh
-# bash exp/dev/nerf_inr/bash/run_v16_r128_grad96.sh False
-# v2
-# "bash $PROJ_NAME/tl2_lib/tl2/modelarts/scripts/run.sh 0 run"
-
-# Env vars e.g.
-# PROJ_NAME=CIPS-exp
-
-
-#start_run=${1:-True}
-number=${1:-0}
-command=${2:-run}
-#command=${2:-run_v2}
-bucket=${3:-bucket-3690}
-cuda_devices=${4:-0,1,2,3,4,5,6,7}
-
-
-#curdir: /home/ma-user/modelarts/user-job-dir
-pwd
-ls -la
-
-proj_root=$PROJ_NAME
-
-############ copy code
-cd $proj_root
-## modelarts code
-python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-  -s s3://$bucket/ZhouPeng/codes/$proj_root \
-  -d ../$proj_root \
-  -t copytree -b ../$proj_root/code.zip
-## cache code
-python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-  -s s3://$bucket/ZhouPeng/codes/$proj_root \
-  -d /cache/$proj_root \
-  -t copytree -b /cache/$proj_root/code.zip
-
-cd /cache/$proj_root
-pwd
-############ Prepare envs
-#cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
-#cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
-        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-          -s s3://$bucket/ZhouPeng/pypi/torch182_cu101_py36 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-pip install -e tl2_lib
-#pip install --no-cache-dir torch==1.8.2 torchvision==0.9.2
-pip install --no-cache-dir easydict fvcore tensorboard tqdm opencv-python matplotlib scikit-video plyfile mrcfile pytorch-fid
-pip install --no-cache-dir streamlit ninja
-#pip install -e torch_fidelity_lib
-
-############ copy results
-#resume_dir=outdir/train_ffhq_r128_partial_grad-20210920_165510_097_grad96
-#python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-#  -s s3://$bucket/ZhouPeng/results/$proj_root/$resume_dir \
-#  -d /cache/$proj_root/results/$resume_dir -t copytree
-
-
-export CUDA_VISIBLE_DEVICES=$cuda_devices
-export TIME_STR=1
-export PORT=12345
-export PYTHONPATH=.:tl2_lib
-
-python tl2_lib/tl2/modelarts/scripts/run.py \
-  --tl_outdir=results/Run/run \
-  --tl_command=$command \
-  --tl_opts=root_obs s3://$bucket/ZhouPeng/ \
-  --number=$number
-
-
+set -x
+# bash /home/work/run_train.sh bash /home/work/user-job-dir/pi-GAN-exp/exp/dev/nerf_inr/bash/run_v16_r128_grad96.sh
+# bash exp/dev/nerf_inr/bash/run_v16_r128_grad96.sh False
+# v2
+# "bash $PROJ_NAME/tl2_lib/tl2/modelarts/scripts/run.sh 0 run"
+
+# Env vars e.g.
+# PROJ_NAME=CIPS-exp
+
+
+#start_run=${1:-True}
+number=${1:-0}
+command=${2:-run}
+#command=${2:-run_v2}
+bucket=${3:-bucket-3690}
+cuda_devices=${4:-0,1,2,3,4,5,6,7}
+
+
+#curdir: /home/ma-user/modelarts/user-job-dir
+pwd
+ls -la
+
+proj_root=$PROJ_NAME
+
+############ copy code
+cd $proj_root
+## modelarts code
+python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+  -s s3://$bucket/ZhouPeng/codes/$proj_root \
+  -d ../$proj_root \
+  -t copytree -b ../$proj_root/code.zip
+## cache code
+python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+  -s s3://$bucket/ZhouPeng/codes/$proj_root \
+  -d /cache/$proj_root \
+  -t copytree -b /cache/$proj_root/code.zip
+
+cd /cache/$proj_root
+pwd
+############ Prepare envs
+#cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
+#cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
+        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+          -s s3://$bucket/ZhouPeng/pypi/torch182_cu101_py36 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+pip install -e tl2_lib
+#pip install --no-cache-dir torch==1.8.2 torchvision==0.9.2
+pip install --no-cache-dir easydict fvcore tensorboard tqdm opencv-python matplotlib scikit-video plyfile mrcfile pytorch-fid
+pip install --no-cache-dir streamlit ninja
+#pip install -e torch_fidelity_lib
+
+############ copy results
+#resume_dir=outdir/train_ffhq_r128_partial_grad-20210920_165510_097_grad96
+#python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+#  -s s3://$bucket/ZhouPeng/results/$proj_root/$resume_dir \
+#  -d /cache/$proj_root/results/$resume_dir -t copytree
+
+
+export CUDA_VISIBLE_DEVICES=$cuda_devices
+export TIME_STR=1
+export PORT=12345
+export PYTHONPATH=.:tl2_lib
+
+python tl2_lib/tl2/modelarts/scripts/run.py \
+  --tl_outdir=results/Run/run \
+  --tl_command=$command \
+  --tl_opts=root_obs s3://$bucket/ZhouPeng/ \
+  --number=$number
+
+
```

### Comparing `tl2-0.1.0/tl2/modelarts/scripts/run_v2_modelarts.sh` & `tl2-0.1.1/tl2/modelarts/scripts/run_v2_modelarts.sh`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,83 +1,83 @@
-set -x
-
-# v2
-# bash = bash CIPS-3D/exp/cips3d/bash/afhq_exp/train_afhq_cat_r64.sh 0 bucket-3690
-
-# Env vars e.g.
-PROJ_NAME=CIPS-3D
-
-run_num=${1:-0}
-bucket=${2:-bucket-3690}
-#cuda_devices=${3:-0,1,2,3,4,5,6,7}
-cuda_devices=`python -c "import torch;print(','.join([str(i) for i in range(torch.cuda.device_count())]), end='')"`
-
-
-#curdir: /home/ma-user/modelarts/user-job-dir
-pwd && ls -la
-
-proj_root=$PROJ_NAME
-
-############ copy code
-cd $proj_root
-
-## modelarts code
-# copy tool
-pip install tl2
-
-python -m tl2.modelarts.scripts.copy_tool \
-  -s s3://$bucket/ZhouPeng/codes/$proj_root \
-  -d ../$proj_root \
-  -t copytree -b ../$proj_root/code.zip
-## cache code
-python -m tl2.modelarts.scripts.copy_tool \
-  -s s3://$bucket/ZhouPeng/codes/$proj_root \
-  -d /cache/$proj_root \
-  -t copytree -b /cache/$proj_root/code.zip
-
-cd /cache/$proj_root
-pwd
-
-############ copy results
-#resume_dir=encoder_inr_train/train_ffhq_r256_softplus-20211219_144749_467
-#python -m tl2.modelarts.scripts.copy_tool \
-#  -s s3://$bucket/ZhouPeng/results/$proj_root/$resume_dir \
-#  -d /cache/$proj_root/results/$resume_dir -t copytree
-
-#finetune_pkl=encoder_inr_train/train_ffhq_r256_softplus-20211217_175316_465/ckptdir/resume/snapshot_data.pkl
-#python -m tl2.modelarts.scripts.copy_tool \
-#  -s s3://$bucket/ZhouPeng/results/$proj_root/$finetune_pkl \
-#  -d /cache/$proj_root/results/$finetune_pkl -t copy
-
-############ Prepare envs
-bash exp/tests/setup_env.sh
-#pip uninstall -y tl2
-
-export ANSI_COLORS_DISABLED=1
-
-export CUDA_VISIBLE_DEVICES=$cuda_devices
-export RUN_NUM=$run_num
-
-export TIME_STR=1
-export PORT=12345
-#
-export PYTHONPATH=.:./tl2_lib
-
-python -c "from exp.tests.test_cips3d import Testing_afhq_exp;\
-  Testing_afhq_exp().test_train_afhq_cat(debug=False)" \
-  --tl_opts \
-    batch_size 4 img_size 64 total_iters 80000 \
-    warmup_D True fade_steps 10000 \
-    train_aux_img True G_kwargs.num_steps 48 \
-    load_finetune False
-
-
-
-
-
-
-
-
-
-
-
-
+set -x
+
+# v2
+# bash = bash CIPS-3D/exp/cips3d/bash/afhq_exp/train_afhq_cat_r64.sh 0 bucket-3690
+
+# Env vars e.g.
+PROJ_NAME=CIPS-3D
+
+run_num=${1:-0}
+bucket=${2:-bucket-3690}
+#cuda_devices=${3:-0,1,2,3,4,5,6,7}
+cuda_devices=`python -c "import torch;print(','.join([str(i) for i in range(torch.cuda.device_count())]), end='')"`
+
+
+#curdir: /home/ma-user/modelarts/user-job-dir
+pwd && ls -la
+
+proj_root=$PROJ_NAME
+
+############ copy code
+cd $proj_root
+
+## modelarts code
+# copy tool
+pip install tl2
+
+python -m tl2.modelarts.scripts.copy_tool \
+  -s s3://$bucket/ZhouPeng/codes/$proj_root \
+  -d ../$proj_root \
+  -t copytree -b ../$proj_root/code.zip
+## cache code
+python -m tl2.modelarts.scripts.copy_tool \
+  -s s3://$bucket/ZhouPeng/codes/$proj_root \
+  -d /cache/$proj_root \
+  -t copytree -b /cache/$proj_root/code.zip
+
+cd /cache/$proj_root
+pwd
+
+############ copy results
+#resume_dir=encoder_inr_train/train_ffhq_r256_softplus-20211219_144749_467
+#python -m tl2.modelarts.scripts.copy_tool \
+#  -s s3://$bucket/ZhouPeng/results/$proj_root/$resume_dir \
+#  -d /cache/$proj_root/results/$resume_dir -t copytree
+
+#finetune_pkl=encoder_inr_train/train_ffhq_r256_softplus-20211217_175316_465/ckptdir/resume/snapshot_data.pkl
+#python -m tl2.modelarts.scripts.copy_tool \
+#  -s s3://$bucket/ZhouPeng/results/$proj_root/$finetune_pkl \
+#  -d /cache/$proj_root/results/$finetune_pkl -t copy
+
+############ Prepare envs
+bash exp/tests/setup_env.sh
+#pip uninstall -y tl2
+
+export ANSI_COLORS_DISABLED=1
+
+export CUDA_VISIBLE_DEVICES=$cuda_devices
+export RUN_NUM=$run_num
+
+export TIME_STR=1
+export PORT=12345
+#
+export PYTHONPATH=.:./tl2_lib
+
+python -c "from exp.tests.test_cips3d import Testing_afhq_exp;\
+  Testing_afhq_exp().test_train_afhq_cat(debug=False)" \
+  --tl_opts \
+    batch_size 4 img_size 64 total_iters 80000 \
+    warmup_D True fade_steps 10000 \
+    train_aux_img True G_kwargs.num_steps 48 \
+    load_finetune False
+
+
+
+
+
+
+
+
+
+
+
+
```

### Comparing `tl2-0.1.0/tl2/modelarts/scripts/s3_downloader.py` & `tl2-0.1.1/tl2/modelarts/scripts/s3_uploader.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,438 +1,476 @@
-# encoding: utf-8
-# Author: Tao yuheng(t50018193).
-import os
-import sys
-import copy
-import base64
-import argparse
-from concurrent.futures import ThreadPoolExecutor, as_completed
-from urllib import request
-import json
-import time
-import queue
-import threading
-import platform
-import logging
-from datetime import datetime
-from collections import defaultdict
-
-import requests
-
-
-os.environ.pop('http_proxy', None)
-os.environ.pop('https_proxy', None)
-os.environ.pop('HTTP_PROXY', None)
-os.environ.pop('HTTPS_PROXY', None)
-
-logging.basicConfig(level=logging.ERROR, format='[%(asctime)s] [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
-
-endpoint = "http://roma.huawei.com"
-api = "/csb/rest/s3/bucket/endpoint"
-
-parser = argparse.ArgumentParser(description='Parameters of CSB-OBS',
-                                 formatter_class=
-                                 argparse.ArgumentDefaultsHelpFormatter)
-parser.add_argument("--show_speed", action='store_true',
-                    help="To display the current download speed, <psutil> is required")
-parser.add_argument("--single_file", action='store_true',
-                    help="if you download single file please add '--single_file' behind your cmd")
-parser.add_argument("--bucket_path", type=str, required=True,
-                    help="The default value of bucket path")
-parser.add_argument('--region', type=str, required=True,
-                    help='region of bucket.')
-parser.add_argument('--bucket_name', type=str, required=True,
-                    help='Please input you bucket_name.')
-parser.add_argument('--app_token', type=str, required=True,
-                    help='appToken of CSB.')
-parser.add_argument('--objects_storage_path', type=str, required=True,
-                    help='Output list of objectkeys.')
-parser.add_argument('--buffer_size', type=int, default=65536,
-                    help='The default value of buffer_size is 65536.')
-parser.add_argument("--retry_times", type=int, default=3,
-                    help='The default value of retry times.')
-parser.add_argument('--big_file', type=int, default=100*1024*1024,
-                    help='The default value of big file is 100M.')
-parser.add_argument('--thread_num', type=int, default=12,
-                    help='The default value of thread.')
-parser.add_argument("--package_size", type=int, default=50*1024*1024,
-                    help='The default value of package is 50M.')
-parser.add_argument('--vendor', type=str, default="HEC",
-                    help='vendor of bucket.')
-parser.add_argument('--queue_size', type=int, default=50000,
-                    help="max value of download_queue")
-parser.add_argument("--fail_json_storage_path", type=str, default="./fail.json",
-                    help="The default value of fail.json storage path")
-parser.add_argument("--time_wait", type=int, default=10,
-                    help="The default value of download thread waiting time (second)")
-
-result, _ = parser.parse_known_args()
-args = copy.deepcopy(result)
-
-
-if not args.single_file:
-    assert args.bucket_path.endswith("/"), "bucket_path must ends with /"
-    args.father_path = args.bucket_path.split("/")[-2] + "/"
-else:
-    args.father_path = args.bucket_path[args.bucket_path.rfind("/")+1:]
-
-print_lock = threading.Lock()
-file_lock = threading.Lock()
-download_queue = queue.Queue(args.queue_size)
-big_file_download_queue = queue.Queue()
-range_queue = queue.Queue()
-big_file_ls = []
-file_count = 0
-downloaded_count = 0
-big_file_process_count = 0
-big_file_process_total = 0
-error_count = 0
-
-current_speed = ""
-start_count_speed = True
-
-has_next = True
-
-retry_map = defaultdict(int)
-error_map = dict()
-next_marker = ""
-
-session = requests.Session()
-
-
-# 0. get endpoint
-def get_file_server_endpoint():
-    try:
-        url = endpoint + api
-        param = "?" + "bucketid=" + args.bucket_name + "&token=" \
-                + args.app_token + "&vendor=" + args.vendor + "&region=" + args.region
-        req = request.Request(url=url + param)
-        res = request.urlopen(req)
-        result = res.read().decode(encoding='utf-8')
-        result_dict = json.loads(result)
-        if result_dict["success"]:
-            return result_dict["result"]
-        else:
-            raise Exception("get endpoint failed")
-    except Exception as e:
-        import traceback
-        traceback.print_exc()
-
-
-# 1. bucket_auth
-def bucket_auth():
-    bucket_auth_endpoint = args.csb_file_server + '/rest/boto3/s3/bucket-auth?vendor=' + args.vendor + '&region=' + args.region + '&bucketid=' + args.bucket_name + '&apptoken=' + args.app_token
-    try:
-        req = request.Request(url=bucket_auth_endpoint)
-        res = request.urlopen(req)
-        result = res.read().decode(encoding='utf-8')
-        result_dict = json.loads(result)
-        if not result_dict["success"]:
-            raise Exception(result_dict["msg"])
-    except Exception as e:
-        import traceback
-        traceback.print_exc()
-        print("endpoint is", bucket_auth_endpoint)
-        raise Exception("bucket_auth error")
-
-
-def human_size(size, dot=3):
-    return str(round(size / pow(1024, 3), dot)) + ' GB'
-
-
-# 2. get objectKey
-def get_object_key(next_marker=""):
-    global has_next
-    global file_count
-    path = "/rest/boto3/s3/list/bucket/objectkeys?"
-    object_key = base64.urlsafe_b64encode(args.bucket_path.encode()).decode()
-    param = f"vendor={args.vendor}&region={args.region}&bucketid={args.bucket_name}" \
-            f"&apptoken={args.app_token}&objectkey={object_key}&nextmarker={next_marker}"
-    headers = {
-        "Content-Type": "application/json",
-        "csb-token": args.app_token
-    }
-    result = session.get(args.csb_file_server+path+param, headers=headers)
-    result_dict = json.loads(result.content)
-
-    if not result_dict["nextmarker"]:
-        has_next = False
-    if result_dict["success"] == "false":
-        logging.error(result.text)
-        error_map["next_marker" + "@" + next_marker] = result.text
-        raise Exception("get object key failed")
-    file_count += len(result_dict["objectKeys"])
-    for object_key in result_dict["objectKeys"]:
-        if int(object_key["size"]) > args.big_file:
-            big_file_download_queue.put(object_key)
-            file_count -= 1
-        else:
-            download_queue.put(object_key)
-    return result_dict
-
-
-def do_download_queue():
-    global file_count
-    result_dict = get_object_key()
-    while result_dict["nextmarker"]:
-        try:
-            result_dict = get_object_key(result_dict["nextmarker"])
-        except Exception:
-            error_map[result_dict["nextmarker"]] = time.time()
-            time.sleep(5)
-            result_dict = get_object_key(result_dict["nextmarker"])
-
-
-def get_object(object_name: str):
-    global error_count
-    if not args.single_file:
-        object_name_path = os.path.join(args.objects_storage_path, args.father_path, object_name.replace(args.bucket_path, ""))
-        local_objects_storage_path = os.path.join(args.objects_storage_path,
-                                                  object_name_path[:object_name_path.rfind("/")])
-    else:
-        object_name_path = os.path.join(args.objects_storage_path, args.father_path)
-        local_objects_storage_path = args.objects_storage_path
-    with print_lock:
-        if not os.path.exists(local_objects_storage_path):
-            os.makedirs(local_objects_storage_path)
-    b64_object_name = base64.urlsafe_b64encode(object_name.encode()).decode()
-    path = f"/rest/boto3/s3/{args.vendor}/{args.region}/{args.app_token}/{args.bucket_name}/{b64_object_name}"
-    try:
-        headers = {
-            "Content-Type": "application/json",
-            "csb-token": args.app_token,
-            'Connection': 'close'
-        }
-
-        with session.get(args.csb_file_server+path, headers=headers) as result:
-            with open(object_name_path, "wb") as f:
-                for content in result.iter_content(chunk_size=args.buffer_size):
-                    f.write(content)
-
-    except Exception as e:
-        import traceback
-        traceback.print_exc()
-        retry_map[object_name] += 1
-        if retry_map[object_name] > 3:
-            error_map[object_name] = traceback.format_exc()
-        else:
-            print(f"{object_name} try {args.retry_times} times failed")
-            error_count += 1
-            time.sleep(30)
-            get_object(object_name)
-
-
-def get_range_object(object_name: str, h_size, file_io):
-    global big_file_process_count
-    global big_file_process_total
-    global start_count_speed
-    global current_speed
-    while range_queue.qsize() > 0:
-        content_range = range_queue.get()
-        b64_object_name = base64.urlsafe_b64encode(object_name.encode()).decode()
-        path = f"/rest/boto3/s3/{args.vendor}/{args.region}/{args.app_token}/{args.bucket_name}/{b64_object_name}"
-        headers = {"Content-Type": "application/json",
-                "csb-token": args.app_token,
-                "Range": f"Range={content_range[0]}-{content_range[1]}",
-                "Date": datetime.utcnow().strftime('%a, %d %b %Y %H:%M:%S GMT'),
-                "Connection": "Keep-Alive"}
-        try:
-            with session.get(args.csb_file_server + path, headers=headers) as result:
-                with file_lock:
-                    file_io.seek(content_range[0])
-                    for content in result.iter_content(chunk_size=args.buffer_size):
-                        file_io.write(content)
-
-            big_file_process_count += 1
-            with print_lock:
-                if big_file_process_count > big_file_process_total:
-                    big_file_process_count = big_file_process_total
-                print("\r", end="")
-                print("Downloading <{}> size: {}: {}/{} - {}%: ".format(object_name, h_size,
-                                                                      big_file_process_count, big_file_process_total,
-                                                                int((big_file_process_count / big_file_process_total) * 100)),
-                      "▋" * (int(big_file_process_count / big_file_process_total * 50)) + current_speed, end="")
-                sys.stdout.flush()
-        except Exception:
-            import traceback
-            traceback.print_exc()
-            retry_map[str(content_range)] += 1
-            if retry_map[str(content_range)] > args.retry_times:
-                # log
-                print(f"{content_range} try {args.retry_times} times failed")
-                retry_map[str(content_range)] = traceback.format_exc()
-            else:
-                time.sleep(30)
-                range_queue.put(content_range)
-                get_range_object(object_name, h_size, file_io)
-    start_count_speed = False
-
-
-def do_range_object(pool: ThreadPoolExecutor):
-    global big_file_process_count
-    global big_file_process_total
-    global start_count_speed
-    while big_file_download_queue.qsize() > 0:
-        big_file_process_count = 0
-        download_content = big_file_download_queue.get()
-        total_size = int(download_content["size"])
-        object_name = download_content["objectKey"]
-        one_thread_range = args.package_size
-        range_list = [[i if i == 0 else i + 1, i + one_thread_range] for i in range(0, total_size + 1, one_thread_range)]
-        big_file_process_total = len(range_list)
-        for r in range_list:
-            range_queue.put(r)
-
-        if not args.single_file:
-            object_name_path = os.path.join(args.objects_storage_path, args.father_path,
-                                            object_name.replace(args.bucket_path, ""))
-            local_objects_storage_path = os.path.join(args.objects_storage_path,
-                                                      object_name_path[:object_name_path.rfind("/")])
-        else:
-            object_name_path = os.path.join(args.objects_storage_path, args.father_path)
-            local_objects_storage_path = args.objects_storage_path
-        with print_lock:
-            if not os.path.exists(local_objects_storage_path):
-                os.makedirs(local_objects_storage_path)
-        start_count_speed = True
-        thread_count_current_speed = pool.submit(get_delta)
-
-        file_io = open(object_name_path, "wb+")
-
-        ls_thread = [pool.submit(get_range_object, object_name,
-                                 human_size(total_size), file_io) for _ in range(args.thread_num)]
-        ls_thread.append(thread_count_current_speed)
-        for task in as_completed(ls_thread):
-            try:
-                task.result()
-            except Exception as e:
-                raise e
-        file_io.close()
-        print_split()
-
-
-def do_get():
-    global file_count
-    global downloaded_count
-    global current_speed
-    global start_count_speed
-    while download_queue.qsize() > 0 or has_next:
-        try:
-            download_content = download_queue.get(timeout=args.time_wait)
-        except Exception:
-            break
-        object_key = download_content["objectKey"]
-        try:
-            get_object(object_key)
-            downloaded_count += 1
-            with print_lock:
-                print("\r", end="")
-                print("Download progress: {}/{} - {}%: ".format(downloaded_count, file_count,
-                                                                int((downloaded_count / file_count) * 100)),
-                      "▋" * (int(downloaded_count / file_count * 50)) + current_speed, end="")
-                sys.stdout.flush()
-        except Exception:
-            import traceback
-            traceback.print_exc()
-
-            retry_map[str(download_content)] += 1
-            if retry_map[str(download_content)] > args.retry_times:
-                logging.error(f"{download_content} try {args.retry_times} times failed")
-                error_map[str(download_content)] = traceback.format_exc()
-                continue
-            download_queue.put(download_content)
-    start_count_speed = False
-
-
-def multi_thread_main():
-    print('begin downloading small file ...')
-    args.csb_file_server = get_file_server_endpoint()
-    bucket_auth()
-    full_thread_num = args.thread_num + 4
-    pool = ThreadPoolExecutor(max_workers=full_thread_num, thread_name_prefix="Python Downloader")
-
-    ls_thread = []
-    thread_do_download_queue = pool.submit(do_download_queue)
-    thread_count_current_speed = pool.submit(get_delta)
-    ls_thread.append(thread_do_download_queue)
-    for i in range(args.thread_num):
-        ls_thread.append(pool.submit(do_get))
-    ls_thread.append(thread_count_current_speed)
-    tic = time.time()
-    for task in as_completed(ls_thread):
-        try:
-            task.result()
-        except Exception as e:
-            raise e
-
-    toc = time.time()
-    print()
-    seconds = toc - tic
-    m, s = divmod(seconds, 60)
-    h, m = divmod(m, 60)
-    print("small file download completed, cost time : {:0>d} h {:0>2d} m {:0>2d} s".format(int(h), int(m), int(s)))
-    print(f'begin downloading {big_file_download_queue.qsize()} big files ...')
-
-    tic = time.time()
-    do_range_object(pool)
-    toc = time.time()
-    seconds = toc - tic
-    m, s = divmod(seconds, 60)
-    h, m = divmod(m, 60)
-    print("big file download completed, cost time : {:0>d} h {:0>2d} m {:0>2d} s".format(int(h), int(m), int(s)))
-
-
-def print_split():
-    print("\n\n--------------------------\n")
-
-
-def check_retry():
-    print(f"error_count =  {error_count}")
-    if error_map:
-        with open(args.fail_json_storage_path, "w", encoding="utf-8") as f:
-            json.dump(error_map, f)
-        logging.error(f"{len(error_map)} files download failed, written in {args.fail_json_storage_path}")
-    else:
-        print("all files download completed!")
-    if "Windows" in platform.platform():
-        set_display_required(False)
-
-
-def check_platform():
-    if "Windows" in platform.platform():
-        print("检测到使用Windows系统，在Windows系统下，该脚本会阻止系统休眠，以确保下载能够完成。", end='')
-        set_display_required(True)
-        print_split()
-
-
-def set_display_required(continuous: bool):
-    import ctypes
-    ES_CON = 0x80000000
-    ES_DIS = 0x00000002
-    if continuous:
-        ctypes.windll.kernel32.SetThreadExecutionState(ES_DIS | ES_CON)
-    else:
-        ctypes.windll.kernel32.SetThreadExecutionState(ES_DIS)
-
-
-def get_delta():
-    if args.show_speed:
-        import psutil
-        global current_speed
-        while start_count_speed:
-            before = psutil.net_io_counters().bytes_recv
-            time.sleep(0.1)
-            now = psutil.net_io_counters().bytes_recv
-            delta = (now-before) / (1024 * 102.4)
-            current_speed = "  {:.3f} MB/s".format(delta)
-
-
-if __name__ == '__main__':
-    r"""
-    pip install requests psutil
-
-    python -m tl2_lib.tl2.modelarts.scripts.s3_downloader --objects_storage_path=D:\user_data\datasets\AFHQv2\ --bucket_path=ZhouPeng/keras/AFHQv2/AFHQv2/ --app_token=0482fc70-f97c-49f4-878e-2eee5fe788e3 --region=cn-north-4 --bucket_name=bucket-3690 --show_speed   
-    --single_file
-    """
-    check_platform()
-    multi_thread_main()
-    check_retry()
+# encoding: utf-8
+# Author: Tao yuheng(t50018193).
+import logging
+import os
+import sys
+import copy
+import base64
+import argparse
+from concurrent.futures import ThreadPoolExecutor, as_completed
+from urllib import request
+import json
+import time
+import queue
+import threading
+import platform
+from xml.dom.minidom import parseString as parseXmlString
+from xml import etree
+from xml.etree.ElementTree import ElementTree
+from collections import defaultdict
+
+import requests
+
+
+os.environ.pop('http_proxy', None)
+os.environ.pop('https_proxy', None)
+os.environ.pop('HTTP_PROXY', None)
+os.environ.pop('HTTPS_PROXY', None)
+
+logging.basicConfig(level=logging.ERROR, format='[%(asctime)s] [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
+
+endpoint = "http://roma.huawei.com"
+api = "/csb/rest/s3/bucket/endpoint"
+
+parser = argparse.ArgumentParser(description='Parameters of CSB-OBS',
+                                 formatter_class=
+                                 argparse.ArgumentDefaultsHelpFormatter)
+parser.add_argument("--show_speed", action='store_true',
+                    help="To display the current upload speed, <psutil> is required")
+parser.add_argument("--bucket_path", type=str, required=True,
+                    help="The default value of bucket path")
+parser.add_argument('--region', type=str, required=True,
+                    help='region of bucket.')
+parser.add_argument('--bucket_name', type=str, required=True,
+                    help='Please input you bucket_name.')
+parser.add_argument('--app_token', type=str, required=True,
+                    help='appToken of CSB.')
+parser.add_argument('--local_folder_absolute_path', type=str, required=True,
+                    help='local_folder_absolute_path')
+parser.add_argument('--buffer_size', type=int, default=65536,
+                    help='The default value of buffer_size is 65536.')
+parser.add_argument("--retry_times", type=int, default=3,
+                    help='The default value of retry times.')
+parser.add_argument('--big_file', type=int, default=100*1024*1024,
+                    help='The default value of big file is 100M.')
+parser.add_argument('--thread_num', type=int, default=12,
+                    help='The default value of thread.')
+parser.add_argument("--package_size", type=int, default=50*1024*1024,
+                    help='The default value of package is 50M.')
+parser.add_argument('--vendor', type=str, default="HEC",
+                    help='vendor of bucket.')
+parser.add_argument("--fail_json_storage_path", type=str, default="./fail.json",
+                    help="The default value of fail.json storage path")
+result, _ = parser.parse_known_args()
+args = copy.deepcopy(result)
+
+cond = threading.Condition()
+
+print_lock = threading.Lock()
+file_lock = threading.Lock()
+upload_queue = queue.Queue()
+big_file_upload_queue = queue.Queue()
+range_queue = queue.Queue()
+big_file_ls = []
+file_count = 0
+uploaded_count = 0
+big_file_process_count = 0
+big_file_process_total = 0
+error_count = 0
+
+current_speed = ""
+start_count_speed = True
+
+session = requests.Session()
+
+is_file = False
+
+if os.path.isfile(args.local_folder_absolute_path):
+    is_file = True
+
+
+args.local_folder_absolute_path = args.local_folder_absolute_path.replace("\\", "/")
+args.father_path_name = args.local_folder_absolute_path[args.local_folder_absolute_path.rfind("/")+1:]
+
+retry_map = defaultdict(int)
+error_map = dict()
+
+
+# 1. get endpoint
+def get_file_server_endpoint():
+    try:
+        url = endpoint + api
+        param = "?" + "bucketid=" + args.bucket_name + "&token=" \
+                + args.app_token + "&vendor=" + args.vendor + "&region=" + args.region
+        req = request.Request(url=url + param)
+        res = request.urlopen(req)
+        result = res.read().decode(encoding='utf-8')
+        result_dict = json.loads(result)
+        if result_dict["success"]:
+            return result_dict["result"]
+        else:
+            raise Exception(result_dict["msg"])
+    except Exception as e:
+        import traceback
+        traceback.print_exc()
+
+
+# 1. bucket_auth
+def bucket_auth():
+    bucket_auth_endpoint = args.csb_file_server + '/rest/boto3/s3/bucket-auth?vendor=' + args.vendor + '&region=' + args.region + '&bucketid=' + args.bucket_name + '&apptoken=' + args.app_token
+    req = request.Request(url=bucket_auth_endpoint)
+    res = request.urlopen(req)
+    result = res.read().decode(encoding='utf-8')
+    result_dict = json.loads(result)
+    if not result_dict["success"]:
+        raise Exception(result_dict["msg"])
+
+
+def human_size(size, dot=3):
+    return str(round(size / pow(1024, 3), dot)) + ' GB'
+
+
+def do_upload_queue():
+    if not is_file:
+        for (rootDir, dirNames, filenames) in os.walk(args.local_folder_absolute_path):
+            for filename in filenames:
+                local_object_name = os.path.join(rootDir, filename).replace("\\", "/")
+                size = os.path.getsize(local_object_name)
+                object_name = args.bucket_path + args.father_path_name +  local_object_name.replace(args.local_folder_absolute_path, "").replace("\\", "/")
+                upload_content = {
+                    "local_object_name": local_object_name,
+                    "object_name": object_name,
+                    "size": size
+                }
+                if size > args.big_file:
+                    big_file_upload_queue.put(upload_content)
+                else:
+                    upload_queue.put(upload_content)
+    else:
+        file_name = args.local_folder_absolute_path[args.local_folder_absolute_path.rfind("/")+1:]
+        object_name = args.bucket_path + file_name
+        size = os.path.getsize(args.local_folder_absolute_path)
+        upload_content = {
+            "local_object_name": args.local_folder_absolute_path,
+            "object_name": object_name,
+            "size": size
+        }
+        if size > args.big_file:
+            big_file_upload_queue.put(upload_content)
+        else:
+            upload_queue.put(upload_content)
+
+
+# _ get-object
+def put_object(upload_content: dict):
+    global error_count
+
+    local_object_name = upload_content["local_object_name"]
+    object_name = upload_content["object_name"]
+
+    b64_object_name = base64.urlsafe_b64encode(object_name.encode()).decode()
+    path = f"/rest/boto3/s3/{args.vendor}/{args.region}/{args.app_token}/{args.bucket_name}/{b64_object_name}"
+
+    try:
+        headers = {
+            "Content-Type": "application/json",
+            "csb-token": args.app_token,
+            'Connection': 'close'
+        }
+
+        with open(local_object_name, "rb") as f:
+            session.put(args.csb_file_server+path, data=f.read(), headers=headers)
+
+    except Exception as e:
+        import traceback
+        traceback.print_exc()
+        error_count += 1
+        put_object(upload_content)
+
+
+# 3.1 do_get
+def do_put():
+    global file_count
+    global uploaded_count
+    global start_count_speed
+    while upload_queue.qsize() > 0:
+        try:
+            upload_content = upload_queue.get(timeout=3)
+        except Exception:
+            break
+
+        try:
+            put_object(upload_content)
+            uploaded_count += 1
+            with print_lock:
+                print("\r", end="")
+                print("Upload progress: {}/{} - {}%: ".format(uploaded_count, file_count,
+                                                                int((uploaded_count / file_count) * 100)),
+                      "▋" * (int(uploaded_count / file_count * 50)) + current_speed, end="")
+                sys.stdout.flush()
+        except Exception:
+            import traceback
+            traceback.print_exc()
+            retry_map[str(upload_content)] += 1
+
+            if retry_map[str(upload_content)] > args.retry_times:
+                # log
+                print(f"{upload_content} try {args.retry_times} times failed")
+                retry_map[str(upload_content)] = traceback.format_exc()
+                continue
+            upload_queue.put(upload_content)
+    start_count_speed = False
+
+
+def print_split():
+    print("\n\n--------------------------\n")
+
+
+def check_retry():
+    global error_count
+    print(f"error_count =  {error_count}")
+    if error_map:
+        with open(args.fail_json_storage_path, "w", encoding="utf-8") as f:
+            json.dump(error_map, f)
+        print(f"{len(error_map)} files upload failed, written in {args.fail_json_storage_path}")
+    else:
+        print("all files upload completed!")
+    if "Windows" in platform.platform():
+        set_display_required(False)
+
+
+def check_platform():
+    if "Windows" in platform.platform():
+        print("检测到使用Windows系统，在Windows系统下，该脚本会阻止系统休眠，以确保上传能够完成。", end='')
+        set_display_required(True)
+        print_split()
+
+
+def set_display_required(continuous: bool):
+    import ctypes
+    ES_CON = 0x80000000
+    ES_DIS = 0x00000002
+    if continuous:
+        ctypes.windll.kernel32.SetThreadExecutionState(ES_DIS | ES_CON)
+    else:
+        ctypes.windll.kernel32.SetThreadExecutionState(ES_DIS)
+
+
+def create_range_upload_task(object_name):
+    object_key = base64.urlsafe_b64encode(object_name.encode(encoding="utf-8"))
+    object_key = str(object_key, encoding="utf-8")
+    path = f"/rest/boto3/s3/{args.vendor}/{args.region}/{args.app_token}/{args.bucket_name}/{object_key}?uploads=uploads"
+    headers = {
+        "Content-Type": "text/plain"
+    }
+    resp = session.post(args.csb_file_server + path, headers=headers)
+    result_dict = parseXmlString(resp.content.decode(encoding="utf-8"))
+    upload_id = result_dict.getElementsByTagName("UploadId")[0].childNodes[0].data
+
+    return upload_id, object_key
+
+
+def range_put(tag_list, object_name, upload_id, h_size, file_io):
+    global big_file_process_count
+    global start_count_speed
+    object_key = base64.urlsafe_b64encode(object_name.encode(encoding="utf-8"))
+    object_key = str(object_key, encoding="utf-8")
+    headers = {
+        "Content-Type": "text/plain",
+        "Connection": "keep-alive"
+    }
+
+    while range_queue.qsize() > 0:
+        range_content = range_queue.get()
+        start = range_content[0]
+        end = range_content[1]
+        part_number = range_content[2]
+        path = f"/rest/boto3/s3/{args.vendor}/{args.region}/{args.app_token}/{args.bucket_name}/{object_key}?" \
+               f"partNumber={part_number}&uploadId={upload_id}"
+        try:
+            with file_lock:
+                file_io.seek(start)
+                # 这里一定要记得加 1
+                resp = session.put(args.csb_file_server + path, data=file_io.read(end-start+1), headers=headers)
+            if resp.status_code != 200:
+                print(resp.text)
+                raise Exception("Upload failed")
+            tag_content = {
+                "part_number": str(part_number),
+                "etag": resp.headers["ETag"]
+            }
+            tag_list.append(tag_content)
+            big_file_process_count += 1
+            with print_lock:
+                if big_file_process_count > big_file_process_total:
+                    big_file_process_count = big_file_process_total
+                print("\r", end="")
+                print("Uploading <{}> size: {}: {}/{} - {}%: ".format(object_name, h_size,
+                                                                      big_file_process_count, big_file_process_total,
+                                                                int((big_file_process_count / big_file_process_total) * 100)),
+                      "▋" * (int(big_file_process_count / big_file_process_total * 50)) + current_speed, end="")
+                sys.stdout.flush()
+        except Exception:
+            import traceback
+            traceback.print_exc()
+            key = object_name + "@" + str(range_content)
+            retry_map[key] += 1
+
+            if retry_map[key] > args.retry_times:
+                logging.error(f"{key} try {args.retry_times} times failed")
+                error_map[key] = traceback.format_exc()
+                continue
+            range_queue.put(range_content)
+            range_put(tag_list, object_name, upload_id, h_size, file_io)
+    start_count_speed = False
+
+
+def merge_range_put(tag_list, object_key, upload_id):
+    global error_count
+    path = f"/rest/boto3/s3/{args.vendor}/{args.region}/{args.app_token}/{args.bucket_name}/{object_key}?" \
+           f"uploadId={upload_id}"
+    headers = {
+        "Content-Type": "text/plain"
+    }
+    data = parse2xml(tag_list)
+    resp = session.post(args.csb_file_server + path, headers=headers, data=data)
+
+    if resp.status_code != 200:
+        try:
+            request_id = resp.headers["RequestId"]
+        except Exception:
+            request_id = "request_id is None"
+        error_count += 1
+        error_map[request_id + "@" + object_key] = str(resp.headers)
+
+
+def parse2xml(tag_list: list):
+    root = etree.ElementTree.Element("CompleteMultipartUpload")
+
+    for tag in tag_list:
+        part = etree.ElementTree.SubElement(root, "Part")
+        part_number = etree.ElementTree.SubElement(part, "PartNumber")
+        part_number.text = tag["part_number"]
+        etag = etree.ElementTree.SubElement(part, "ETag")
+        etag.text = tag["etag"]
+
+    return etree.ElementTree.tostring(root)
+
+
+def do_range_put(pool: ThreadPoolExecutor):
+    global big_file_process_count
+    global big_file_process_total
+    global start_count_speed
+    while big_file_upload_queue.qsize() > 0:
+        big_file_process_count = 0
+        upload_content = big_file_upload_queue.get()
+        local_object_name = upload_content["local_object_name"]
+        object_name = upload_content["object_name"]
+        total_size = upload_content["size"]
+        # 动态调节包的大小，防止文件过大（超过500G） 段超过10000
+        if total_size > args.package_size * 9500:
+            real_package_size = total_size // 9500
+        else:
+            real_package_size = args.package_size
+        range_list = [[i if i == 0 else i + 1, i + real_package_size]
+                      for i in range(0, total_size + 1, real_package_size)]
+        index = 1
+        if range_list[-1][0] > total_size:
+            del range_list[-1]
+        if range_list[-1][1] > total_size:
+            range_list[-1][1] = total_size
+        for r in range_list:
+            r.append(index)
+            index += 1
+        big_file_process_total = len(range_list)
+
+        for r in range_list:
+            range_queue.put(r)
+
+        upload_id, object_key = create_range_upload_task(object_name)
+        file_io = open(local_object_name, "rb")
+        tag_list = []
+        start_count_speed = True
+        thread_count_current_speed = pool.submit(get_delta)
+        ls_thread = [pool.submit(range_put, tag_list, object_name, upload_id, human_size(total_size), file_io)
+                     for _ in range(args.thread_num)]
+        ls_thread.append(thread_count_current_speed)
+        for task in as_completed(ls_thread):
+            try:
+                task.result()
+            except Exception as e:
+                raise e
+        file_io.close()
+        merge_range_put(tag_list, object_key, upload_id)
+        print_split()
+
+
+def multi_thread_main():
+    global file_count
+
+    sys.stdout.write('begin uploading small file ...\n')
+    args.csb_file_server = get_file_server_endpoint()
+    bucket_auth()
+    full_thread_num = args.thread_num + 4
+    do_upload_queue()
+    file_count = upload_queue.qsize()  # 上传文件总数
+    pool = ThreadPoolExecutor(max_workers=full_thread_num, thread_name_prefix="Python Uploader")
+
+    ls_thread = []
+    thread_count_current_speed = pool.submit(get_delta)
+    ls_thread.append(thread_count_current_speed)
+    for i in range(args.thread_num):
+        ls_thread.append(pool.submit(do_put))
+    tic = time.time()
+    for thread in ls_thread:
+        thread.result()
+
+    for task in as_completed(ls_thread):
+        try:
+            task.result()
+        except Exception as e:
+            raise e
+
+    toc = time.time()
+    seconds = toc - tic
+    m, s = divmod(seconds, 60)
+    h, m = divmod(m, 60)
+    print("\nsmall file upload completed, cost time : {:0>d} h {:0>2d} m {:0>2d} s".format(int(h), int(m), int(s)))
+
+    sys.stdout.write(f'begin uploading {big_file_upload_queue.qsize()} big files ...\n')
+
+    tic = time.time()
+    do_range_put(pool)
+    toc = time.time()
+    seconds = toc - tic
+    m, s = divmod(seconds, 60)
+    h, m = divmod(m, 60)
+    print("big file upload completed, cost time : {:0>d} h {:0>2d} m {:0>2d} s".format(int(h), int(m), int(s)))
+
+
+def get_delta():
+    if args.show_speed:
+        import psutil
+        global current_speed
+        while start_count_speed:
+            before = psutil.net_io_counters().bytes_sent
+            time.sleep(1)
+            now = psutil.net_io_counters().bytes_sent
+            delta = (now-before) / (1024 * 1024 * 1)
+            current_speed = "  {:.3f} MB/s".format(delta)
+
+
+if __name__ == '__main__':
+    r"""
+    pip install requests psutil
+    
+    python -m tl2.modelarts.scripts.s3_uploader --local_folder_absolute_path=C:\Users\z50017127\code\tl2_lib --bucket_path=ZhouPeng/codes/ --app_token=0482fc70-f97c-49f4-878e-2eee5fe788e3 --region=cn-north-4 --bucket_name=bucket-3690 --show_speed   
+                    
+    """
+    # print('test')
+    check_platform()
+    multi_thread_main()
+    check_retry()
+
+
+
+
```

### Comparing `tl2-0.1.0/tl2/modelarts/scripts/s3_uploader.py` & `tl2-0.1.1/tl2/modelarts/scripts/s3_downloader.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,476 +1,438 @@
-# encoding: utf-8
-# Author: Tao yuheng(t50018193).
-import logging
-import os
-import sys
-import copy
-import base64
-import argparse
-from concurrent.futures import ThreadPoolExecutor, as_completed
-from urllib import request
-import json
-import time
-import queue
-import threading
-import platform
-from xml.dom.minidom import parseString as parseXmlString
-from xml import etree
-from xml.etree.ElementTree import ElementTree
-from collections import defaultdict
-
-import requests
-
-
-os.environ.pop('http_proxy', None)
-os.environ.pop('https_proxy', None)
-os.environ.pop('HTTP_PROXY', None)
-os.environ.pop('HTTPS_PROXY', None)
-
-logging.basicConfig(level=logging.ERROR, format='[%(asctime)s] [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
-
-endpoint = "http://roma.huawei.com"
-api = "/csb/rest/s3/bucket/endpoint"
-
-parser = argparse.ArgumentParser(description='Parameters of CSB-OBS',
-                                 formatter_class=
-                                 argparse.ArgumentDefaultsHelpFormatter)
-parser.add_argument("--show_speed", action='store_true',
-                    help="To display the current upload speed, <psutil> is required")
-parser.add_argument("--bucket_path", type=str, required=True,
-                    help="The default value of bucket path")
-parser.add_argument('--region', type=str, required=True,
-                    help='region of bucket.')
-parser.add_argument('--bucket_name', type=str, required=True,
-                    help='Please input you bucket_name.')
-parser.add_argument('--app_token', type=str, required=True,
-                    help='appToken of CSB.')
-parser.add_argument('--local_folder_absolute_path', type=str, required=True,
-                    help='local_folder_absolute_path')
-parser.add_argument('--buffer_size', type=int, default=65536,
-                    help='The default value of buffer_size is 65536.')
-parser.add_argument("--retry_times", type=int, default=3,
-                    help='The default value of retry times.')
-parser.add_argument('--big_file', type=int, default=100*1024*1024,
-                    help='The default value of big file is 100M.')
-parser.add_argument('--thread_num', type=int, default=12,
-                    help='The default value of thread.')
-parser.add_argument("--package_size", type=int, default=50*1024*1024,
-                    help='The default value of package is 50M.')
-parser.add_argument('--vendor', type=str, default="HEC",
-                    help='vendor of bucket.')
-parser.add_argument("--fail_json_storage_path", type=str, default="./fail.json",
-                    help="The default value of fail.json storage path")
-result, _ = parser.parse_known_args()
-args = copy.deepcopy(result)
-
-cond = threading.Condition()
-
-print_lock = threading.Lock()
-file_lock = threading.Lock()
-upload_queue = queue.Queue()
-big_file_upload_queue = queue.Queue()
-range_queue = queue.Queue()
-big_file_ls = []
-file_count = 0
-uploaded_count = 0
-big_file_process_count = 0
-big_file_process_total = 0
-error_count = 0
-
-current_speed = ""
-start_count_speed = True
-
-session = requests.Session()
-
-is_file = False
-
-if os.path.isfile(args.local_folder_absolute_path):
-    is_file = True
-
-
-args.local_folder_absolute_path = args.local_folder_absolute_path.replace("\\", "/")
-args.father_path_name = args.local_folder_absolute_path[args.local_folder_absolute_path.rfind("/")+1:]
-
-retry_map = defaultdict(int)
-error_map = dict()
-
-
-# 1. get endpoint
-def get_file_server_endpoint():
-    try:
-        url = endpoint + api
-        param = "?" + "bucketid=" + args.bucket_name + "&token=" \
-                + args.app_token + "&vendor=" + args.vendor + "&region=" + args.region
-        req = request.Request(url=url + param)
-        res = request.urlopen(req)
-        result = res.read().decode(encoding='utf-8')
-        result_dict = json.loads(result)
-        if result_dict["success"]:
-            return result_dict["result"]
-        else:
-            raise Exception(result_dict["msg"])
-    except Exception as e:
-        import traceback
-        traceback.print_exc()
-
-
-# 1. bucket_auth
-def bucket_auth():
-    bucket_auth_endpoint = args.csb_file_server + '/rest/boto3/s3/bucket-auth?vendor=' + args.vendor + '&region=' + args.region + '&bucketid=' + args.bucket_name + '&apptoken=' + args.app_token
-    req = request.Request(url=bucket_auth_endpoint)
-    res = request.urlopen(req)
-    result = res.read().decode(encoding='utf-8')
-    result_dict = json.loads(result)
-    if not result_dict["success"]:
-        raise Exception(result_dict["msg"])
-
-
-def human_size(size, dot=3):
-    return str(round(size / pow(1024, 3), dot)) + ' GB'
-
-
-def do_upload_queue():
-    if not is_file:
-        for (rootDir, dirNames, filenames) in os.walk(args.local_folder_absolute_path):
-            for filename in filenames:
-                local_object_name = os.path.join(rootDir, filename).replace("\\", "/")
-                size = os.path.getsize(local_object_name)
-                object_name = args.bucket_path + args.father_path_name +  local_object_name.replace(args.local_folder_absolute_path, "").replace("\\", "/")
-                upload_content = {
-                    "local_object_name": local_object_name,
-                    "object_name": object_name,
-                    "size": size
-                }
-                if size > args.big_file:
-                    big_file_upload_queue.put(upload_content)
-                else:
-                    upload_queue.put(upload_content)
-    else:
-        file_name = args.local_folder_absolute_path[args.local_folder_absolute_path.rfind("/")+1:]
-        object_name = args.bucket_path + file_name
-        size = os.path.getsize(args.local_folder_absolute_path)
-        upload_content = {
-            "local_object_name": args.local_folder_absolute_path,
-            "object_name": object_name,
-            "size": size
-        }
-        if size > args.big_file:
-            big_file_upload_queue.put(upload_content)
-        else:
-            upload_queue.put(upload_content)
-
-
-# _ get-object
-def put_object(upload_content: dict):
-    global error_count
-
-    local_object_name = upload_content["local_object_name"]
-    object_name = upload_content["object_name"]
-
-    b64_object_name = base64.urlsafe_b64encode(object_name.encode()).decode()
-    path = f"/rest/boto3/s3/{args.vendor}/{args.region}/{args.app_token}/{args.bucket_name}/{b64_object_name}"
-
-    try:
-        headers = {
-            "Content-Type": "application/json",
-            "csb-token": args.app_token,
-            'Connection': 'close'
-        }
-
-        with open(local_object_name, "rb") as f:
-            session.put(args.csb_file_server+path, data=f.read(), headers=headers)
-
-    except Exception as e:
-        import traceback
-        traceback.print_exc()
-        error_count += 1
-        put_object(upload_content)
-
-
-# 3.1 do_get
-def do_put():
-    global file_count
-    global uploaded_count
-    global start_count_speed
-    while upload_queue.qsize() > 0:
-        try:
-            upload_content = upload_queue.get(timeout=3)
-        except Exception:
-            break
-
-        try:
-            put_object(upload_content)
-            uploaded_count += 1
-            with print_lock:
-                print("\r", end="")
-                print("Upload progress: {}/{} - {}%: ".format(uploaded_count, file_count,
-                                                                int((uploaded_count / file_count) * 100)),
-                      "▋" * (int(uploaded_count / file_count * 50)) + current_speed, end="")
-                sys.stdout.flush()
-        except Exception:
-            import traceback
-            traceback.print_exc()
-            retry_map[str(upload_content)] += 1
-
-            if retry_map[str(upload_content)] > args.retry_times:
-                # log
-                print(f"{upload_content} try {args.retry_times} times failed")
-                retry_map[str(upload_content)] = traceback.format_exc()
-                continue
-            upload_queue.put(upload_content)
-    start_count_speed = False
-
-
-def print_split():
-    print("\n\n--------------------------\n")
-
-
-def check_retry():
-    global error_count
-    print(f"error_count =  {error_count}")
-    if error_map:
-        with open(args.fail_json_storage_path, "w", encoding="utf-8") as f:
-            json.dump(error_map, f)
-        print(f"{len(error_map)} files upload failed, written in {args.fail_json_storage_path}")
-    else:
-        print("all files upload completed!")
-    if "Windows" in platform.platform():
-        set_display_required(False)
-
-
-def check_platform():
-    if "Windows" in platform.platform():
-        print("检测到使用Windows系统，在Windows系统下，该脚本会阻止系统休眠，以确保上传能够完成。", end='')
-        set_display_required(True)
-        print_split()
-
-
-def set_display_required(continuous: bool):
-    import ctypes
-    ES_CON = 0x80000000
-    ES_DIS = 0x00000002
-    if continuous:
-        ctypes.windll.kernel32.SetThreadExecutionState(ES_DIS | ES_CON)
-    else:
-        ctypes.windll.kernel32.SetThreadExecutionState(ES_DIS)
-
-
-def create_range_upload_task(object_name):
-    object_key = base64.urlsafe_b64encode(object_name.encode(encoding="utf-8"))
-    object_key = str(object_key, encoding="utf-8")
-    path = f"/rest/boto3/s3/{args.vendor}/{args.region}/{args.app_token}/{args.bucket_name}/{object_key}?uploads=uploads"
-    headers = {
-        "Content-Type": "text/plain"
-    }
-    resp = session.post(args.csb_file_server + path, headers=headers)
-    result_dict = parseXmlString(resp.content.decode(encoding="utf-8"))
-    upload_id = result_dict.getElementsByTagName("UploadId")[0].childNodes[0].data
-
-    return upload_id, object_key
-
-
-def range_put(tag_list, object_name, upload_id, h_size, file_io):
-    global big_file_process_count
-    global start_count_speed
-    object_key = base64.urlsafe_b64encode(object_name.encode(encoding="utf-8"))
-    object_key = str(object_key, encoding="utf-8")
-    headers = {
-        "Content-Type": "text/plain",
-        "Connection": "keep-alive"
-    }
-
-    while range_queue.qsize() > 0:
-        range_content = range_queue.get()
-        start = range_content[0]
-        end = range_content[1]
-        part_number = range_content[2]
-        path = f"/rest/boto3/s3/{args.vendor}/{args.region}/{args.app_token}/{args.bucket_name}/{object_key}?" \
-               f"partNumber={part_number}&uploadId={upload_id}"
-        try:
-            with file_lock:
-                file_io.seek(start)
-                # 这里一定要记得加 1
-                resp = session.put(args.csb_file_server + path, data=file_io.read(end-start+1), headers=headers)
-            if resp.status_code != 200:
-                print(resp.text)
-                raise Exception("Upload failed")
-            tag_content = {
-                "part_number": str(part_number),
-                "etag": resp.headers["ETag"]
-            }
-            tag_list.append(tag_content)
-            big_file_process_count += 1
-            with print_lock:
-                if big_file_process_count > big_file_process_total:
-                    big_file_process_count = big_file_process_total
-                print("\r", end="")
-                print("Uploading <{}> size: {}: {}/{} - {}%: ".format(object_name, h_size,
-                                                                      big_file_process_count, big_file_process_total,
-                                                                int((big_file_process_count / big_file_process_total) * 100)),
-                      "▋" * (int(big_file_process_count / big_file_process_total * 50)) + current_speed, end="")
-                sys.stdout.flush()
-        except Exception:
-            import traceback
-            traceback.print_exc()
-            key = object_name + "@" + str(range_content)
-            retry_map[key] += 1
-
-            if retry_map[key] > args.retry_times:
-                logging.error(f"{key} try {args.retry_times} times failed")
-                error_map[key] = traceback.format_exc()
-                continue
-            range_queue.put(range_content)
-            range_put(tag_list, object_name, upload_id, h_size, file_io)
-    start_count_speed = False
-
-
-def merge_range_put(tag_list, object_key, upload_id):
-    global error_count
-    path = f"/rest/boto3/s3/{args.vendor}/{args.region}/{args.app_token}/{args.bucket_name}/{object_key}?" \
-           f"uploadId={upload_id}"
-    headers = {
-        "Content-Type": "text/plain"
-    }
-    data = parse2xml(tag_list)
-    resp = session.post(args.csb_file_server + path, headers=headers, data=data)
-
-    if resp.status_code != 200:
-        try:
-            request_id = resp.headers["RequestId"]
-        except Exception:
-            request_id = "request_id is None"
-        error_count += 1
-        error_map[request_id + "@" + object_key] = str(resp.headers)
-
-
-def parse2xml(tag_list: list):
-    root = etree.ElementTree.Element("CompleteMultipartUpload")
-
-    for tag in tag_list:
-        part = etree.ElementTree.SubElement(root, "Part")
-        part_number = etree.ElementTree.SubElement(part, "PartNumber")
-        part_number.text = tag["part_number"]
-        etag = etree.ElementTree.SubElement(part, "ETag")
-        etag.text = tag["etag"]
-
-    return etree.ElementTree.tostring(root)
-
-
-def do_range_put(pool: ThreadPoolExecutor):
-    global big_file_process_count
-    global big_file_process_total
-    global start_count_speed
-    while big_file_upload_queue.qsize() > 0:
-        big_file_process_count = 0
-        upload_content = big_file_upload_queue.get()
-        local_object_name = upload_content["local_object_name"]
-        object_name = upload_content["object_name"]
-        total_size = upload_content["size"]
-        # 动态调节包的大小，防止文件过大（超过500G） 段超过10000
-        if total_size > args.package_size * 9500:
-            real_package_size = total_size // 9500
-        else:
-            real_package_size = args.package_size
-        range_list = [[i if i == 0 else i + 1, i + real_package_size]
-                      for i in range(0, total_size + 1, real_package_size)]
-        index = 1
-        if range_list[-1][0] > total_size:
-            del range_list[-1]
-        if range_list[-1][1] > total_size:
-            range_list[-1][1] = total_size
-        for r in range_list:
-            r.append(index)
-            index += 1
-        big_file_process_total = len(range_list)
-
-        for r in range_list:
-            range_queue.put(r)
-
-        upload_id, object_key = create_range_upload_task(object_name)
-        file_io = open(local_object_name, "rb")
-        tag_list = []
-        start_count_speed = True
-        thread_count_current_speed = pool.submit(get_delta)
-        ls_thread = [pool.submit(range_put, tag_list, object_name, upload_id, human_size(total_size), file_io)
-                     for _ in range(args.thread_num)]
-        ls_thread.append(thread_count_current_speed)
-        for task in as_completed(ls_thread):
-            try:
-                task.result()
-            except Exception as e:
-                raise e
-        file_io.close()
-        merge_range_put(tag_list, object_key, upload_id)
-        print_split()
-
-
-def multi_thread_main():
-    global file_count
-
-    sys.stdout.write('begin uploading small file ...\n')
-    args.csb_file_server = get_file_server_endpoint()
-    bucket_auth()
-    full_thread_num = args.thread_num + 4
-    do_upload_queue()
-    file_count = upload_queue.qsize()  # 上传文件总数
-    pool = ThreadPoolExecutor(max_workers=full_thread_num, thread_name_prefix="Python Uploader")
-
-    ls_thread = []
-    thread_count_current_speed = pool.submit(get_delta)
-    ls_thread.append(thread_count_current_speed)
-    for i in range(args.thread_num):
-        ls_thread.append(pool.submit(do_put))
-    tic = time.time()
-    for thread in ls_thread:
-        thread.result()
-
-    for task in as_completed(ls_thread):
-        try:
-            task.result()
-        except Exception as e:
-            raise e
-
-    toc = time.time()
-    seconds = toc - tic
-    m, s = divmod(seconds, 60)
-    h, m = divmod(m, 60)
-    print("\nsmall file upload completed, cost time : {:0>d} h {:0>2d} m {:0>2d} s".format(int(h), int(m), int(s)))
-
-    sys.stdout.write(f'begin uploading {big_file_upload_queue.qsize()} big files ...\n')
-
-    tic = time.time()
-    do_range_put(pool)
-    toc = time.time()
-    seconds = toc - tic
-    m, s = divmod(seconds, 60)
-    h, m = divmod(m, 60)
-    print("big file upload completed, cost time : {:0>d} h {:0>2d} m {:0>2d} s".format(int(h), int(m), int(s)))
-
-
-def get_delta():
-    if args.show_speed:
-        import psutil
-        global current_speed
-        while start_count_speed:
-            before = psutil.net_io_counters().bytes_sent
-            time.sleep(1)
-            now = psutil.net_io_counters().bytes_sent
-            delta = (now-before) / (1024 * 1024 * 1)
-            current_speed = "  {:.3f} MB/s".format(delta)
-
-
-if __name__ == '__main__':
-    r"""
-    pip install requests psutil
-    
-    python -m tl2.modelarts.scripts.s3_uploader --local_folder_absolute_path=C:\Users\z50017127\code\tl2_lib --bucket_path=ZhouPeng/codes/ --app_token=0482fc70-f97c-49f4-878e-2eee5fe788e3 --region=cn-north-4 --bucket_name=bucket-3690 --show_speed   
-                    
-    """
-    # print('test')
-    check_platform()
-    multi_thread_main()
-    check_retry()
-
-
-
-
+# encoding: utf-8
+# Author: Tao yuheng(t50018193).
+import os
+import sys
+import copy
+import base64
+import argparse
+from concurrent.futures import ThreadPoolExecutor, as_completed
+from urllib import request
+import json
+import time
+import queue
+import threading
+import platform
+import logging
+from datetime import datetime
+from collections import defaultdict
+
+import requests
+
+
+os.environ.pop('http_proxy', None)
+os.environ.pop('https_proxy', None)
+os.environ.pop('HTTP_PROXY', None)
+os.environ.pop('HTTPS_PROXY', None)
+
+logging.basicConfig(level=logging.ERROR, format='[%(asctime)s] [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
+
+endpoint = "http://roma.huawei.com"
+api = "/csb/rest/s3/bucket/endpoint"
+
+parser = argparse.ArgumentParser(description='Parameters of CSB-OBS',
+                                 formatter_class=
+                                 argparse.ArgumentDefaultsHelpFormatter)
+parser.add_argument("--show_speed", action='store_true',
+                    help="To display the current download speed, <psutil> is required")
+parser.add_argument("--single_file", action='store_true',
+                    help="if you download single file please add '--single_file' behind your cmd")
+parser.add_argument("--bucket_path", type=str, required=True,
+                    help="The default value of bucket path")
+parser.add_argument('--region', type=str, required=True,
+                    help='region of bucket.')
+parser.add_argument('--bucket_name', type=str, required=True,
+                    help='Please input you bucket_name.')
+parser.add_argument('--app_token', type=str, required=True,
+                    help='appToken of CSB.')
+parser.add_argument('--objects_storage_path', type=str, required=True,
+                    help='Output list of objectkeys.')
+parser.add_argument('--buffer_size', type=int, default=65536,
+                    help='The default value of buffer_size is 65536.')
+parser.add_argument("--retry_times", type=int, default=3,
+                    help='The default value of retry times.')
+parser.add_argument('--big_file', type=int, default=100*1024*1024,
+                    help='The default value of big file is 100M.')
+parser.add_argument('--thread_num', type=int, default=12,
+                    help='The default value of thread.')
+parser.add_argument("--package_size", type=int, default=50*1024*1024,
+                    help='The default value of package is 50M.')
+parser.add_argument('--vendor', type=str, default="HEC",
+                    help='vendor of bucket.')
+parser.add_argument('--queue_size', type=int, default=50000,
+                    help="max value of download_queue")
+parser.add_argument("--fail_json_storage_path", type=str, default="./fail.json",
+                    help="The default value of fail.json storage path")
+parser.add_argument("--time_wait", type=int, default=10,
+                    help="The default value of download thread waiting time (second)")
+
+result, _ = parser.parse_known_args()
+args = copy.deepcopy(result)
+
+
+if not args.single_file:
+    assert args.bucket_path.endswith("/"), "bucket_path must ends with /"
+    args.father_path = args.bucket_path.split("/")[-2] + "/"
+else:
+    args.father_path = args.bucket_path[args.bucket_path.rfind("/")+1:]
+
+print_lock = threading.Lock()
+file_lock = threading.Lock()
+download_queue = queue.Queue(args.queue_size)
+big_file_download_queue = queue.Queue()
+range_queue = queue.Queue()
+big_file_ls = []
+file_count = 0
+downloaded_count = 0
+big_file_process_count = 0
+big_file_process_total = 0
+error_count = 0
+
+current_speed = ""
+start_count_speed = True
+
+has_next = True
+
+retry_map = defaultdict(int)
+error_map = dict()
+next_marker = ""
+
+session = requests.Session()
+
+
+# 0. get endpoint
+def get_file_server_endpoint():
+    try:
+        url = endpoint + api
+        param = "?" + "bucketid=" + args.bucket_name + "&token=" \
+                + args.app_token + "&vendor=" + args.vendor + "&region=" + args.region
+        req = request.Request(url=url + param)
+        res = request.urlopen(req)
+        result = res.read().decode(encoding='utf-8')
+        result_dict = json.loads(result)
+        if result_dict["success"]:
+            return result_dict["result"]
+        else:
+            raise Exception("get endpoint failed")
+    except Exception as e:
+        import traceback
+        traceback.print_exc()
+
+
+# 1. bucket_auth
+def bucket_auth():
+    bucket_auth_endpoint = args.csb_file_server + '/rest/boto3/s3/bucket-auth?vendor=' + args.vendor + '&region=' + args.region + '&bucketid=' + args.bucket_name + '&apptoken=' + args.app_token
+    try:
+        req = request.Request(url=bucket_auth_endpoint)
+        res = request.urlopen(req)
+        result = res.read().decode(encoding='utf-8')
+        result_dict = json.loads(result)
+        if not result_dict["success"]:
+            raise Exception(result_dict["msg"])
+    except Exception as e:
+        import traceback
+        traceback.print_exc()
+        print("endpoint is", bucket_auth_endpoint)
+        raise Exception("bucket_auth error")
+
+
+def human_size(size, dot=3):
+    return str(round(size / pow(1024, 3), dot)) + ' GB'
+
+
+# 2. get objectKey
+def get_object_key(next_marker=""):
+    global has_next
+    global file_count
+    path = "/rest/boto3/s3/list/bucket/objectkeys?"
+    object_key = base64.urlsafe_b64encode(args.bucket_path.encode()).decode()
+    param = f"vendor={args.vendor}&region={args.region}&bucketid={args.bucket_name}" \
+            f"&apptoken={args.app_token}&objectkey={object_key}&nextmarker={next_marker}"
+    headers = {
+        "Content-Type": "application/json",
+        "csb-token": args.app_token
+    }
+    result = session.get(args.csb_file_server+path+param, headers=headers)
+    result_dict = json.loads(result.content)
+
+    if not result_dict["nextmarker"]:
+        has_next = False
+    if result_dict["success"] == "false":
+        logging.error(result.text)
+        error_map["next_marker" + "@" + next_marker] = result.text
+        raise Exception("get object key failed")
+    file_count += len(result_dict["objectKeys"])
+    for object_key in result_dict["objectKeys"]:
+        if int(object_key["size"]) > args.big_file:
+            big_file_download_queue.put(object_key)
+            file_count -= 1
+        else:
+            download_queue.put(object_key)
+    return result_dict
+
+
+def do_download_queue():
+    global file_count
+    result_dict = get_object_key()
+    while result_dict["nextmarker"]:
+        try:
+            result_dict = get_object_key(result_dict["nextmarker"])
+        except Exception:
+            error_map[result_dict["nextmarker"]] = time.time()
+            time.sleep(5)
+            result_dict = get_object_key(result_dict["nextmarker"])
+
+
+def get_object(object_name: str):
+    global error_count
+    if not args.single_file:
+        object_name_path = os.path.join(args.objects_storage_path, args.father_path, object_name.replace(args.bucket_path, ""))
+        local_objects_storage_path = os.path.join(args.objects_storage_path,
+                                                  object_name_path[:object_name_path.rfind("/")])
+    else:
+        object_name_path = os.path.join(args.objects_storage_path, args.father_path)
+        local_objects_storage_path = args.objects_storage_path
+    with print_lock:
+        if not os.path.exists(local_objects_storage_path):
+            os.makedirs(local_objects_storage_path)
+    b64_object_name = base64.urlsafe_b64encode(object_name.encode()).decode()
+    path = f"/rest/boto3/s3/{args.vendor}/{args.region}/{args.app_token}/{args.bucket_name}/{b64_object_name}"
+    try:
+        headers = {
+            "Content-Type": "application/json",
+            "csb-token": args.app_token,
+            'Connection': 'close'
+        }
+
+        with session.get(args.csb_file_server+path, headers=headers) as result:
+            with open(object_name_path, "wb") as f:
+                for content in result.iter_content(chunk_size=args.buffer_size):
+                    f.write(content)
+
+    except Exception as e:
+        import traceback
+        traceback.print_exc()
+        retry_map[object_name] += 1
+        if retry_map[object_name] > 3:
+            error_map[object_name] = traceback.format_exc()
+        else:
+            print(f"{object_name} try {args.retry_times} times failed")
+            error_count += 1
+            time.sleep(30)
+            get_object(object_name)
+
+
+def get_range_object(object_name: str, h_size, file_io):
+    global big_file_process_count
+    global big_file_process_total
+    global start_count_speed
+    global current_speed
+    while range_queue.qsize() > 0:
+        content_range = range_queue.get()
+        b64_object_name = base64.urlsafe_b64encode(object_name.encode()).decode()
+        path = f"/rest/boto3/s3/{args.vendor}/{args.region}/{args.app_token}/{args.bucket_name}/{b64_object_name}"
+        headers = {"Content-Type": "application/json",
+                "csb-token": args.app_token,
+                "Range": f"Range={content_range[0]}-{content_range[1]}",
+                "Date": datetime.utcnow().strftime('%a, %d %b %Y %H:%M:%S GMT'),
+                "Connection": "Keep-Alive"}
+        try:
+            with session.get(args.csb_file_server + path, headers=headers) as result:
+                with file_lock:
+                    file_io.seek(content_range[0])
+                    for content in result.iter_content(chunk_size=args.buffer_size):
+                        file_io.write(content)
+
+            big_file_process_count += 1
+            with print_lock:
+                if big_file_process_count > big_file_process_total:
+                    big_file_process_count = big_file_process_total
+                print("\r", end="")
+                print("Downloading <{}> size: {}: {}/{} - {}%: ".format(object_name, h_size,
+                                                                      big_file_process_count, big_file_process_total,
+                                                                int((big_file_process_count / big_file_process_total) * 100)),
+                      "▋" * (int(big_file_process_count / big_file_process_total * 50)) + current_speed, end="")
+                sys.stdout.flush()
+        except Exception:
+            import traceback
+            traceback.print_exc()
+            retry_map[str(content_range)] += 1
+            if retry_map[str(content_range)] > args.retry_times:
+                # log
+                print(f"{content_range} try {args.retry_times} times failed")
+                retry_map[str(content_range)] = traceback.format_exc()
+            else:
+                time.sleep(30)
+                range_queue.put(content_range)
+                get_range_object(object_name, h_size, file_io)
+    start_count_speed = False
+
+
+def do_range_object(pool: ThreadPoolExecutor):
+    global big_file_process_count
+    global big_file_process_total
+    global start_count_speed
+    while big_file_download_queue.qsize() > 0:
+        big_file_process_count = 0
+        download_content = big_file_download_queue.get()
+        total_size = int(download_content["size"])
+        object_name = download_content["objectKey"]
+        one_thread_range = args.package_size
+        range_list = [[i if i == 0 else i + 1, i + one_thread_range] for i in range(0, total_size + 1, one_thread_range)]
+        big_file_process_total = len(range_list)
+        for r in range_list:
+            range_queue.put(r)
+
+        if not args.single_file:
+            object_name_path = os.path.join(args.objects_storage_path, args.father_path,
+                                            object_name.replace(args.bucket_path, ""))
+            local_objects_storage_path = os.path.join(args.objects_storage_path,
+                                                      object_name_path[:object_name_path.rfind("/")])
+        else:
+            object_name_path = os.path.join(args.objects_storage_path, args.father_path)
+            local_objects_storage_path = args.objects_storage_path
+        with print_lock:
+            if not os.path.exists(local_objects_storage_path):
+                os.makedirs(local_objects_storage_path)
+        start_count_speed = True
+        thread_count_current_speed = pool.submit(get_delta)
+
+        file_io = open(object_name_path, "wb+")
+
+        ls_thread = [pool.submit(get_range_object, object_name,
+                                 human_size(total_size), file_io) for _ in range(args.thread_num)]
+        ls_thread.append(thread_count_current_speed)
+        for task in as_completed(ls_thread):
+            try:
+                task.result()
+            except Exception as e:
+                raise e
+        file_io.close()
+        print_split()
+
+
+def do_get():
+    global file_count
+    global downloaded_count
+    global current_speed
+    global start_count_speed
+    while download_queue.qsize() > 0 or has_next:
+        try:
+            download_content = download_queue.get(timeout=args.time_wait)
+        except Exception:
+            break
+        object_key = download_content["objectKey"]
+        try:
+            get_object(object_key)
+            downloaded_count += 1
+            with print_lock:
+                print("\r", end="")
+                print("Download progress: {}/{} - {}%: ".format(downloaded_count, file_count,
+                                                                int((downloaded_count / file_count) * 100)),
+                      "▋" * (int(downloaded_count / file_count * 50)) + current_speed, end="")
+                sys.stdout.flush()
+        except Exception:
+            import traceback
+            traceback.print_exc()
+
+            retry_map[str(download_content)] += 1
+            if retry_map[str(download_content)] > args.retry_times:
+                logging.error(f"{download_content} try {args.retry_times} times failed")
+                error_map[str(download_content)] = traceback.format_exc()
+                continue
+            download_queue.put(download_content)
+    start_count_speed = False
+
+
+def multi_thread_main():
+    print('begin downloading small file ...')
+    args.csb_file_server = get_file_server_endpoint()
+    bucket_auth()
+    full_thread_num = args.thread_num + 4
+    pool = ThreadPoolExecutor(max_workers=full_thread_num, thread_name_prefix="Python Downloader")
+
+    ls_thread = []
+    thread_do_download_queue = pool.submit(do_download_queue)
+    thread_count_current_speed = pool.submit(get_delta)
+    ls_thread.append(thread_do_download_queue)
+    for i in range(args.thread_num):
+        ls_thread.append(pool.submit(do_get))
+    ls_thread.append(thread_count_current_speed)
+    tic = time.time()
+    for task in as_completed(ls_thread):
+        try:
+            task.result()
+        except Exception as e:
+            raise e
+
+    toc = time.time()
+    print()
+    seconds = toc - tic
+    m, s = divmod(seconds, 60)
+    h, m = divmod(m, 60)
+    print("small file download completed, cost time : {:0>d} h {:0>2d} m {:0>2d} s".format(int(h), int(m), int(s)))
+    print(f'begin downloading {big_file_download_queue.qsize()} big files ...')
+
+    tic = time.time()
+    do_range_object(pool)
+    toc = time.time()
+    seconds = toc - tic
+    m, s = divmod(seconds, 60)
+    h, m = divmod(m, 60)
+    print("big file download completed, cost time : {:0>d} h {:0>2d} m {:0>2d} s".format(int(h), int(m), int(s)))
+
+
+def print_split():
+    print("\n\n--------------------------\n")
+
+
+def check_retry():
+    print(f"error_count =  {error_count}")
+    if error_map:
+        with open(args.fail_json_storage_path, "w", encoding="utf-8") as f:
+            json.dump(error_map, f)
+        logging.error(f"{len(error_map)} files download failed, written in {args.fail_json_storage_path}")
+    else:
+        print("all files download completed!")
+    if "Windows" in platform.platform():
+        set_display_required(False)
+
+
+def check_platform():
+    if "Windows" in platform.platform():
+        print("检测到使用Windows系统，在Windows系统下，该脚本会阻止系统休眠，以确保下载能够完成。", end='')
+        set_display_required(True)
+        print_split()
+
+
+def set_display_required(continuous: bool):
+    import ctypes
+    ES_CON = 0x80000000
+    ES_DIS = 0x00000002
+    if continuous:
+        ctypes.windll.kernel32.SetThreadExecutionState(ES_DIS | ES_CON)
+    else:
+        ctypes.windll.kernel32.SetThreadExecutionState(ES_DIS)
+
+
+def get_delta():
+    if args.show_speed:
+        import psutil
+        global current_speed
+        while start_count_speed:
+            before = psutil.net_io_counters().bytes_recv
+            time.sleep(0.1)
+            now = psutil.net_io_counters().bytes_recv
+            delta = (now-before) / (1024 * 102.4)
+            current_speed = "  {:.3f} MB/s".format(delta)
+
+
+if __name__ == '__main__':
+    r"""
+    pip install requests psutil
+
+    python -m tl2_lib.tl2.modelarts.scripts.s3_downloader --objects_storage_path=D:\user_data\datasets\AFHQv2\ --bucket_path=ZhouPeng/keras/AFHQv2/AFHQv2/ --app_token=0482fc70-f97c-49f4-878e-2eee5fe788e3 --region=cn-north-4 --bucket_name=bucket-3690 --show_speed   
+    --single_file
+    """
+    check_platform()
+    multi_thread_main()
+    check_retry()
```

### Comparing `tl2-0.1.0/tl2/modelarts/scripts/test_bash.py` & `tl2-0.1.1/tl2/modelarts/scripts/test_bash.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/modelarts/scripts/test_resnet.py` & `tl2-0.1.1/tl2/modelarts/scripts/test_resnet.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,136 +1,136 @@
-import os
-import random
-import time
-# from multiprocessing import Process
-from torch.multiprocessing import Process
-
-# from multiprocessing import set_start_method
-# try:
-#     set_start_method('spawn')
-# except RuntimeError:
-#     pass
-
-
-class TorchResnetWorker(Process):
-  def run(self):
-    bs, gpu, determine_bs, q = self._args
-    os.environ['CUDA_VISIBLE_DEVICES'] = gpu
-    import torch
-    import torch.nn.functional as F
-    import torchvision
-    net = torchvision.models.resnet152().cuda()
-    net = torch.nn.DataParallel(net).cuda()
-
-    if determine_bs:
-      self.determine_bs(net, q)
-    else:
-      self.train(net, bs)
-
-  def train(self, net, bs):
-    try:
-      import torch
-      import torch.nn.functional as F
-      rbs = bs
-      print(bs)
-      while True:
-        t = random.random()
-        time.sleep(t)
-
-        x = torch.rand(rbs, 3, 224, 224).cuda()
-        y = net(x)
-
-        tensor = torch.randint(0, 1000, (rbs,))  # tensor([0, 1, 2, 0, 1])
-        one_hot = F.one_hot(tensor, num_classes=1000).float().cuda()
-        loss = (y - one_hot).mean()
-        loss.backward()
-
-        t = random.random()
-        time.sleep(t)
-        rbs = random.randint(1, bs)
-    except RuntimeError:
-      torch.cuda.empty_cache()
-      pass
-
-  def determine_bs(self, net, q):
-    import torch
-    import torch.nn.functional as F
-    bs = 0
-    try:
-      while True:
-        bs += 1
-        print('%s' % bs)
-        x = torch.rand(bs, 3, 224, 224).cuda()
-        y = net(x)
-
-        tensor = torch.randint(0, 1000, (bs,))  # tensor([0, 1, 2, 0, 1])
-        one_hot = F.one_hot(tensor, num_classes=1000).float().cuda()
-        loss = (y - one_hot).mean()
-        loss.backward()
-    except RuntimeError:
-      torch.cuda.empty_cache()
-      q.put(bs - 1)
-      # import traceback
-      # print(traceback.format_exc())
-
-def run(localrank, bs, gpu, is_determine_bs, q):
-
-  os.environ['CUDA_VISIBLE_DEVICES'] = gpu
-  import torch
-  import torch.nn.functional as F
-  import torchvision
-  net = torchvision.models.resnet152().cuda()
-  net = torch.nn.DataParallel(net).cuda()
-
-  if is_determine_bs:
-    determine_bs(net, q)
-  else:
-    train(net, bs)
-
-def train(net, bs):
-  try:
-    import torch
-    import torch.nn.functional as F
-    rbs = bs
-    print(bs)
-    while True:
-      # t = random.random()
-      # time.sleep(t)
-
-      x = torch.rand(rbs, 3, 224, 224).cuda()
-      y = net(x)
-
-      tensor = torch.randint(0, 1000, (rbs,))  # tensor([0, 1, 2, 0, 1])
-      one_hot = F.one_hot(tensor, num_classes=1000).float().cuda()
-      loss = (y - one_hot).mean()
-      loss.backward()
-
-      # t = random.random()
-      # time.sleep(t)
-      rbs = random.randint(1, bs)
-  except RuntimeError:
-    torch.cuda.empty_cache()
-    pass
-
-def determine_bs(net, q):
-  import torch
-  import torch.nn.functional as F
-  bs = 0
-  try:
-    while True:
-      bs += 1
-      print('%s' % bs)
-      x = torch.rand(bs, 3, 224, 224).cuda()
-      y = net(x)
-
-      tensor = torch.randint(0, 1000, (bs,))  # tensor([0, 1, 2, 0, 1])
-      one_hot = F.one_hot(tensor, num_classes=1000).float().cuda()
-      loss = (y - one_hot).mean()
-      loss.backward()
-  except:
-    torch.cuda.empty_cache()
-    import traceback
-    print(traceback.format_exc())
-    os.makedirs('results', exist_ok=True)
-    with open('results/max_bs.txt', 'w') as f:
-      f.write(str(bs - 1))
-    # q.put(bs - 1)
+import os
+import random
+import time
+# from multiprocessing import Process
+from torch.multiprocessing import Process
+
+# from multiprocessing import set_start_method
+# try:
+#     set_start_method('spawn')
+# except RuntimeError:
+#     pass
+
+
+class TorchResnetWorker(Process):
+  def run(self):
+    bs, gpu, determine_bs, q = self._args
+    os.environ['CUDA_VISIBLE_DEVICES'] = gpu
+    import torch
+    import torch.nn.functional as F
+    import torchvision
+    net = torchvision.models.resnet152().cuda()
+    net = torch.nn.DataParallel(net).cuda()
+
+    if determine_bs:
+      self.determine_bs(net, q)
+    else:
+      self.train(net, bs)
+
+  def train(self, net, bs):
+    try:
+      import torch
+      import torch.nn.functional as F
+      rbs = bs
+      print(bs)
+      while True:
+        t = random.random()
+        time.sleep(t)
+
+        x = torch.rand(rbs, 3, 224, 224).cuda()
+        y = net(x)
+
+        tensor = torch.randint(0, 1000, (rbs,))  # tensor([0, 1, 2, 0, 1])
+        one_hot = F.one_hot(tensor, num_classes=1000).float().cuda()
+        loss = (y - one_hot).mean()
+        loss.backward()
+
+        t = random.random()
+        time.sleep(t)
+        rbs = random.randint(1, bs)
+    except RuntimeError:
+      torch.cuda.empty_cache()
+      pass
+
+  def determine_bs(self, net, q):
+    import torch
+    import torch.nn.functional as F
+    bs = 0
+    try:
+      while True:
+        bs += 1
+        print('%s' % bs)
+        x = torch.rand(bs, 3, 224, 224).cuda()
+        y = net(x)
+
+        tensor = torch.randint(0, 1000, (bs,))  # tensor([0, 1, 2, 0, 1])
+        one_hot = F.one_hot(tensor, num_classes=1000).float().cuda()
+        loss = (y - one_hot).mean()
+        loss.backward()
+    except RuntimeError:
+      torch.cuda.empty_cache()
+      q.put(bs - 1)
+      # import traceback
+      # print(traceback.format_exc())
+
+def run(localrank, bs, gpu, is_determine_bs, q):
+
+  os.environ['CUDA_VISIBLE_DEVICES'] = gpu
+  import torch
+  import torch.nn.functional as F
+  import torchvision
+  net = torchvision.models.resnet152().cuda()
+  net = torch.nn.DataParallel(net).cuda()
+
+  if is_determine_bs:
+    determine_bs(net, q)
+  else:
+    train(net, bs)
+
+def train(net, bs):
+  try:
+    import torch
+    import torch.nn.functional as F
+    rbs = bs
+    print(bs)
+    while True:
+      # t = random.random()
+      # time.sleep(t)
+
+      x = torch.rand(rbs, 3, 224, 224).cuda()
+      y = net(x)
+
+      tensor = torch.randint(0, 1000, (rbs,))  # tensor([0, 1, 2, 0, 1])
+      one_hot = F.one_hot(tensor, num_classes=1000).float().cuda()
+      loss = (y - one_hot).mean()
+      loss.backward()
+
+      # t = random.random()
+      # time.sleep(t)
+      rbs = random.randint(1, bs)
+  except RuntimeError:
+    torch.cuda.empty_cache()
+    pass
+
+def determine_bs(net, q):
+  import torch
+  import torch.nn.functional as F
+  bs = 0
+  try:
+    while True:
+      bs += 1
+      print('%s' % bs)
+      x = torch.rand(bs, 3, 224, 224).cuda()
+      y = net(x)
+
+      tensor = torch.randint(0, 1000, (bs,))  # tensor([0, 1, 2, 0, 1])
+      one_hot = F.one_hot(tensor, num_classes=1000).float().cuda()
+      loss = (y - one_hot).mean()
+      loss.backward()
+  except:
+    torch.cuda.empty_cache()
+    import traceback
+    print(traceback.format_exc())
+    os.makedirs('results', exist_ok=True)
+    with open('results/max_bs.txt', 'w') as f:
+      f.write(str(bs - 1))
+    # q.put(bs - 1)
```

### Comparing `tl2-0.1.0/tl2/modelarts/sources/sources.list.modelarts` & `tl2-0.1.1/tl2/modelarts/sources/sources.list.modelarts`

 * *Ordering differences only*

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-deb http://repo.myhuaweicloud.com/ubuntu/ bionic main restricted universe multiverse
-
-deb http://repo.myhuaweicloud.com/ubuntu/ bionic-security main restricted universe multiverse
-
-deb http://repo.myhuaweicloud.com/ubuntu/ bionic-updates main restricted universe multiverse
-
-deb http://repo.myhuaweicloud.com/ubuntu/ bionic-backports main restricted universe multiverse
-
-deb http://repo.myhuaweicloud.com/ubuntu/ bionic-proposed main restricted universe multiverse
-
-deb-src http://repo.myhuaweicloud.com/ubuntu/ bionic main restricted universe multiverse
-
-deb-src http://repo.myhuaweicloud.com/ubuntu/ bionic-security main restricted universe multiverse
-
-deb-src http://repo.myhuaweicloud.com/ubuntu/ bionic-updates main restricted universe multiverse
-
-deb-src http://repo.myhuaweicloud.com/ubuntu/ bionic-backports main restricted universe multiverse
-
-deb-src http://repo.myhuaweicloud.com/ubuntu/ bionic-proposed main restricted universe multiverse
-
+deb http://repo.myhuaweicloud.com/ubuntu/ bionic main restricted universe multiverse
+
+deb http://repo.myhuaweicloud.com/ubuntu/ bionic-security main restricted universe multiverse
+
+deb http://repo.myhuaweicloud.com/ubuntu/ bionic-updates main restricted universe multiverse
+
+deb http://repo.myhuaweicloud.com/ubuntu/ bionic-backports main restricted universe multiverse
+
+deb http://repo.myhuaweicloud.com/ubuntu/ bionic-proposed main restricted universe multiverse
+
+deb-src http://repo.myhuaweicloud.com/ubuntu/ bionic main restricted universe multiverse
+
+deb-src http://repo.myhuaweicloud.com/ubuntu/ bionic-security main restricted universe multiverse
+
+deb-src http://repo.myhuaweicloud.com/ubuntu/ bionic-updates main restricted universe multiverse
+
+deb-src http://repo.myhuaweicloud.com/ubuntu/ bionic-backports main restricted universe multiverse
+
+deb-src http://repo.myhuaweicloud.com/ubuntu/ bionic-proposed main restricted universe multiverse
+
```

### Comparing `tl2-0.1.0/tl2/modelarts/tests/test_run.py` & `tl2-0.1.1/tl2/modelarts/tests/test_run.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,201 +1,201 @@
-import os
-import sys
-import unittest
-import argparse
-
-
-class TestingRun(unittest.TestCase):
-
-  def test_run(self, *tmp):
-    """
-    Usage:
-        export ANSI_COLORS_DISABLED=1
-        export CUDA_VISIBLE_DEVICES=0
-        export TIME_STR=1
-        export PYTHONPATH=./
-        python template_lib/modelarts/scripts/run.py \
-          --tl_config_file template_lib/modelarts/tests/configs/run.yaml \
-          --tl_command run \
-          --tl_outdir results/Run/run \
-          --number 1
-
-        # default image
-        /bucket-8280/ZhouPeng/codes/Omni-GAN-ImageNet/template_lib/modelarts/scripts/run.py
-          number = 3
-          tl_outdir = results/Run/run
-          tl_config_file = template_lib/modelarts/tests/configs/run.yaml
-          tl_opts = root_obs s3://bucket-7001/ZhouPeng/
-          tl_command = run
-
-        # self defined image
-        bash /home/work/run_train.sh python /home/work/user-job-dir/Omni-GAN-ImageNet/template_lib/modelarts/scripts/run.py --tl_outdir=results/Run/run --tl_config_file=/home/work/user-job-dir/Omni-GAN-ImageNet/template_lib/modelarts/tests/configs/run.yaml --tl_command=run --tl_opts=root_obs s3://bucket-7001/ZhouPeng/ --number=2
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    # if 'TIME_STR' not in os.environ:
-    #   os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
-    os.environ['TIME_STR'] = '0'
-    if 'RESULTS_OBS' not in os.environ:
-      os.environ['RESULTS_OBS'] = 's3://bucket-xx/ZhouPeng/results'
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                    --tl_config_file tl2_lib/tl2/modelarts/configs/run.yaml
-                    --tl_command {command}
-                    --tl_outdir {outdir}
-                    """
-    args = setup_outdir_and_yaml(argv_str)
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-    cmd_str = f"""
-            python tl2_lib/tl2/modelarts/scripts/run.py
-            {get_append_cmd_str(args)}
-            --number 1
-            """
-    start_cmd_run(cmd_str)
-
-    pass
-
-  def test_run_v2(self, number=1, debug=False, **kwargs):
-    """
-    Usage:
-        export ANSI_COLORS_DISABLED=1
-        export CUDA_VISIBLE_DEVICES=0
-        export TIME_STR=1
-        export PYTHONPATH=./
-        python template_lib/modelarts/scripts/run.py \
-          --tl_config_file template_lib/modelarts/tests/configs/run.yaml \
-          --tl_command run \
-          --tl_outdir results/Run/run \
-          --number 1
-
-        # default image
-        /bucket-8280/ZhouPeng/codes/Omni-GAN-ImageNet/template_lib/modelarts/scripts/run.py
-          number = 3
-          tl_outdir = results/Run/run
-          tl_config_file = template_lib/modelarts/tests/configs/run.yaml
-          tl_opts = root_obs s3://bucket-7001/ZhouPeng/
-          tl_command = run
-
-        # self defined image
-        bash /home/work/run_train.sh python /home/work/user-job-dir/Omni-GAN-ImageNet/template_lib/modelarts/scripts/run.py --tl_outdir=results/Run/run --tl_config_file=/home/work/user-job-dir/Omni-GAN-ImageNet/template_lib/modelarts/tests/configs/run.yaml --tl_command=run --tl_opts=root_obs s3://bucket-7001/ZhouPeng/ --number=2
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    # if 'TIME_STR' not in os.environ:
-    #   os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
-    os.environ['TIME_STR'] = '1'
-    # if 'RESULTS_OBS' not in os.environ:
-    #   os.environ['RESULTS_OBS'] = 's3://bucket-xx/ZhouPeng/results'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                    --tl_config_file tl2_lib/tl2/modelarts/configs/run.yaml
-                    --tl_command {command}
-                    --tl_outdir {outdir}
-                    --tl_opts {tl_opts}
-                    """
-    args = setup_outdir_and_yaml(argv_str)
-
-    os.environ['TIME_STR'] = '0'
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-    cmd_str = f"""
-            python tl2_lib/tl2/modelarts/scripts/run.py
-            {get_append_cmd_str(args)}
-            --number {number}
-            --tl_opts {tl_opts}
-            """
-    start_cmd_run(cmd_str)
-
-    pass
-
-  def test_run_v2_modelarts(self, debug=True):
-    """
-    Usage:
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export RUN_NUM=0
-        export PYTHONPATH=.
-        python -c "from tl2_lib.tl2.modelarts.tests.test_run import TestingRun;\
-          TestingRun().test_run_v2_modelarts(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/modelarts/configs/run.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    if int(os.environ['RUN_NUM']) > 0:
-      run_command = f"""
-              python -c "from tl2.modelarts.tests.test_run import TestingRun;\
-                    TestingRun().test_run_v2(number={os.environ['RUN_NUM']}, )" \
-                    --tl_opts root_obs {cfg.root_obs}
-              """
-      p = tl2_utils.Worker(name='Run', args=(run_command,))
-      p.start()
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-    PORT = os.environ.get('PORT', 8888)
-
-    cmd_str = f"""
-        python -m tl2.modelarts.scripts.test_bash {os.environ['CUDA_VISIBLE_DEVICES']}
-        """
-
-    start_cmd_run(cmd_str)
-    # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
-    # from tl2.modelarts import modelarts_utils
-    # update_parser_defaults_from_yaml(parser)
-
-    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
-    #
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    pass
+import os
+import sys
+import unittest
+import argparse
+
+
+class TestingRun(unittest.TestCase):
+
+  def test_run(self, *tmp):
+    """
+    Usage:
+        export ANSI_COLORS_DISABLED=1
+        export CUDA_VISIBLE_DEVICES=0
+        export TIME_STR=1
+        export PYTHONPATH=./
+        python template_lib/modelarts/scripts/run.py \
+          --tl_config_file template_lib/modelarts/tests/configs/run.yaml \
+          --tl_command run \
+          --tl_outdir results/Run/run \
+          --number 1
+
+        # default image
+        /bucket-8280/ZhouPeng/codes/Omni-GAN-ImageNet/template_lib/modelarts/scripts/run.py
+          number = 3
+          tl_outdir = results/Run/run
+          tl_config_file = template_lib/modelarts/tests/configs/run.yaml
+          tl_opts = root_obs s3://bucket-7001/ZhouPeng/
+          tl_command = run
+
+        # self defined image
+        bash /home/work/run_train.sh python /home/work/user-job-dir/Omni-GAN-ImageNet/template_lib/modelarts/scripts/run.py --tl_outdir=results/Run/run --tl_config_file=/home/work/user-job-dir/Omni-GAN-ImageNet/template_lib/modelarts/tests/configs/run.yaml --tl_command=run --tl_opts=root_obs s3://bucket-7001/ZhouPeng/ --number=2
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    # if 'TIME_STR' not in os.environ:
+    #   os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
+    os.environ['TIME_STR'] = '0'
+    if 'RESULTS_OBS' not in os.environ:
+      os.environ['RESULTS_OBS'] = 's3://bucket-xx/ZhouPeng/results'
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                    --tl_config_file tl2_lib/tl2/modelarts/configs/run.yaml
+                    --tl_command {command}
+                    --tl_outdir {outdir}
+                    """
+    args = setup_outdir_and_yaml(argv_str)
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    cmd_str = f"""
+            python tl2_lib/tl2/modelarts/scripts/run.py
+            {get_append_cmd_str(args)}
+            --number 1
+            """
+    start_cmd_run(cmd_str)
+
+    pass
+
+  def test_run_v2(self, number=1, debug=False, **kwargs):
+    """
+    Usage:
+        export ANSI_COLORS_DISABLED=1
+        export CUDA_VISIBLE_DEVICES=0
+        export TIME_STR=1
+        export PYTHONPATH=./
+        python template_lib/modelarts/scripts/run.py \
+          --tl_config_file template_lib/modelarts/tests/configs/run.yaml \
+          --tl_command run \
+          --tl_outdir results/Run/run \
+          --number 1
+
+        # default image
+        /bucket-8280/ZhouPeng/codes/Omni-GAN-ImageNet/template_lib/modelarts/scripts/run.py
+          number = 3
+          tl_outdir = results/Run/run
+          tl_config_file = template_lib/modelarts/tests/configs/run.yaml
+          tl_opts = root_obs s3://bucket-7001/ZhouPeng/
+          tl_command = run
+
+        # self defined image
+        bash /home/work/run_train.sh python /home/work/user-job-dir/Omni-GAN-ImageNet/template_lib/modelarts/scripts/run.py --tl_outdir=results/Run/run --tl_config_file=/home/work/user-job-dir/Omni-GAN-ImageNet/template_lib/modelarts/tests/configs/run.yaml --tl_command=run --tl_opts=root_obs s3://bucket-7001/ZhouPeng/ --number=2
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    # if 'TIME_STR' not in os.environ:
+    #   os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
+    os.environ['TIME_STR'] = '1'
+    # if 'RESULTS_OBS' not in os.environ:
+    #   os.environ['RESULTS_OBS'] = 's3://bucket-xx/ZhouPeng/results'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                    --tl_config_file tl2_lib/tl2/modelarts/configs/run.yaml
+                    --tl_command {command}
+                    --tl_outdir {outdir}
+                    --tl_opts {tl_opts}
+                    """
+    args = setup_outdir_and_yaml(argv_str)
+
+    os.environ['TIME_STR'] = '0'
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    cmd_str = f"""
+            python tl2_lib/tl2/modelarts/scripts/run.py
+            {get_append_cmd_str(args)}
+            --number {number}
+            --tl_opts {tl_opts}
+            """
+    start_cmd_run(cmd_str)
+
+    pass
+
+  def test_run_v2_modelarts(self, debug=True):
+    """
+    Usage:
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export RUN_NUM=0
+        export PYTHONPATH=.
+        python -c "from tl2_lib.tl2.modelarts.tests.test_run import TestingRun;\
+          TestingRun().test_run_v2_modelarts(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/modelarts/configs/run.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    if int(os.environ['RUN_NUM']) > 0:
+      run_command = f"""
+              python -c "from tl2.modelarts.tests.test_run import TestingRun;\
+                    TestingRun().test_run_v2(number={os.environ['RUN_NUM']}, )" \
+                    --tl_opts root_obs {cfg.root_obs}
+              """
+      p = tl2_utils.Worker(name='Run', args=(run_command,))
+      p.start()
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    PORT = os.environ.get('PORT', 8888)
+
+    cmd_str = f"""
+        python -m tl2.modelarts.scripts.test_bash {os.environ['CUDA_VISIBLE_DEVICES']}
+        """
+
+    start_cmd_run(cmd_str)
+    # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
+    # from tl2.modelarts import modelarts_utils
+    # update_parser_defaults_from_yaml(parser)
+
+    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
+    #
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    pass
```

### Comparing `tl2-0.1.0/tl2/proj/GAN/eval_FID_given_imgdir.py` & `tl2-0.1.1/tl2/proj/GAN/eval_FID_given_imgdir.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,55 +1,55 @@
-import argparse
-
-from tl2 import tl2_utils
-from tl2.proj.logger import logging_utils_v2
-
-from torch_fidelity.datasets import ImagesPathDataset
-from torch_fidelity import calculate_metrics
-
-def main(outdir,
-         fake_dir,
-         dataset_root,
-         debug,
-         ):
-
-  logger = logging_utils_v2.get_logger(filename=f"{outdir}/log.txt")
-
-  fake_image_list = tl2_utils.get_filelist_recursive(fake_dir)
-  image_list = tl2_utils.get_filelist_recursive(dataset_root)
-
-  if debug:
-    fake_image_list = fake_image_list[:10]
-    image_list = image_list[:10]
-
-  fake_dataset = ImagesPathDataset(files=fake_image_list)
-  real_dataset = ImagesPathDataset(files=image_list)
-  logger.info(f"Number of fake images: {len(fake_image_list)}, {fake_dataset[0].shape}")
-  logger.info(f"Number of real images: {len(image_list)}, {real_dataset[0].shape}")
-
-  assert fake_dataset[0].shape == real_dataset[0].shape
-  metrics_dict = calculate_metrics(input1=fake_dataset,
-                                   input2=real_dataset,
-                                   cuda=True,
-                                   isc=True,
-                                   fid=True,
-                                   kid=True if not debug else False,
-                                   verbose=False)
-  logger.info(tl2_utils.dict2string(metrics_dict))
-
-  pass
-
-
-if __name__ == '__main__':
-  parser = argparse.ArgumentParser()
-  parser.add_argument("--outdir", default="")
-  parser.add_argument("--fake_dir", default="")
-  parser.add_argument("--dataset_root", default="")
-  parser.add_argument("--debug", action="store_true")
-  args, _ = parser.parse_known_args()
-
-  print(f"args: \n{tl2_utils.dict2string(vars(args))}")
-
-  main(outdir=args.outdir,
-       fake_dir=args.fake_dir,
-       dataset_root=args.dataset_root,
-       debug=args.debug)
+import argparse
+
+from tl2 import tl2_utils
+from tl2.proj.logger import logging_utils_v2
+
+from torch_fidelity.datasets import ImagesPathDataset
+from torch_fidelity import calculate_metrics
+
+def main(outdir,
+         fake_dir,
+         dataset_root,
+         debug,
+         ):
+
+  logger = logging_utils_v2.get_logger(filename=f"{outdir}/log.txt")
+
+  fake_image_list = tl2_utils.get_filelist_recursive(fake_dir)
+  image_list = tl2_utils.get_filelist_recursive(dataset_root)
+
+  if debug:
+    fake_image_list = fake_image_list[:10]
+    image_list = image_list[:10]
+
+  fake_dataset = ImagesPathDataset(files=fake_image_list)
+  real_dataset = ImagesPathDataset(files=image_list)
+  logger.info(f"Number of fake images: {len(fake_image_list)}, {fake_dataset[0].shape}")
+  logger.info(f"Number of real images: {len(image_list)}, {real_dataset[0].shape}")
+
+  assert fake_dataset[0].shape == real_dataset[0].shape
+  metrics_dict = calculate_metrics(input1=fake_dataset,
+                                   input2=real_dataset,
+                                   cuda=True,
+                                   isc=True,
+                                   fid=True,
+                                   kid=True if not debug else False,
+                                   verbose=False)
+  logger.info(tl2_utils.dict2string(metrics_dict))
+
+  pass
+
+
+if __name__ == '__main__':
+  parser = argparse.ArgumentParser()
+  parser.add_argument("--outdir", default="")
+  parser.add_argument("--fake_dir", default="")
+  parser.add_argument("--dataset_root", default="")
+  parser.add_argument("--debug", action="store_true")
+  args, _ = parser.parse_known_args()
+
+  print(f"args: \n{tl2_utils.dict2string(vars(args))}")
+
+  main(outdir=args.outdir,
+       fake_dir=args.fake_dir,
+       dataset_root=args.dataset_root,
+       debug=args.debug)
```

### Comparing `tl2-0.1.0/tl2/proj/GAN/frequency_spectrum.py` & `tl2-0.1.1/tl2/proj/GAN/frequency_spectrum.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,66 +1,66 @@
-from PIL import Image
-import numpy as np
-from scipy import ndimage
-
-from template_lib.proj.pil import pil_utils
-
-
-class FrequencySpectrum(object):
-  def __init__(self):
-
-    self.mean_gray_spectrum = None
-    self.sum_gray_spectrum = None
-    self.count = 0
-    pass
-
-  def to_pil(self, gray_np):
-    from matplotlib import cm
-    gray_np_norm = (gray_np - gray_np.min()) / (gray_np.max() - gray_np.min())
-    # img_pil = Image.fromarray(np.uint8(cm.gist_earth(gray_np_norm) * 255))
-    img_pil = Image.fromarray(np.uint8(cm.viridis(gray_np_norm) * 255))
-    return img_pil
-
-  def get_spectrum_pil(self, text=None):
-    spec_pil = self.to_pil(self.mean_gray_spectrum)
-    if text is not None:
-      pil_utils.add_text(spec_pil, text=text, size=spec_pil.size[0]//18)
-    return spec_pil
-
-  def get_frequency_spectrum(self, image_pil, ):
-    """
-
-    """
-
-    image_np = np.array(image_pil)
-
-    H, W, C = np.shape(image_np)
-    real_r, real_g, real_b = image_np[:, :, 0], image_np[:, :, 1], image_np[:, :, 2]
-    real_gray = 0.2989 * real_r + 0.5870 * real_g + 0.1140 * real_b
-
-    real_gray_f = np.fft.fft2(real_gray - ndimage.median_filter(real_gray, size=H // 8))
-    real_gray_f_shifted = np.fft.fftshift(real_gray_f)
-
-    real_gray_spectrum = 20 * np.log(np.abs(real_gray_f_shifted))
-
-    return real_gray_spectrum
-
-  def update(self, gray_spectrum):
-    if self.mean_gray_spectrum is None:
-      self.mean_gray_spectrum = gray_spectrum
-      self.sum_gray_spectrum = gray_spectrum
-      self.count += 1
-    else:
-      self.sum_gray_spectrum = self.sum_gray_spectrum + gray_spectrum
-      self.count += 1
-      self.mean_gray_spectrum = self.sum_gray_spectrum / self.count
-    pass
-
-  def get_spectrum_and_update(self, image_pil):
-    image_pil_spectrum = self.get_frequency_spectrum(image_pil)
-    self.update(image_pil_spectrum)
-
-  def reset(self):
-    self.mean_gray_spectrum = None
-    self.sum_gray_spectrum = None
-    self.count = 0
-
+from PIL import Image
+import numpy as np
+from scipy import ndimage
+
+from template_lib.proj.pil import pil_utils
+
+
+class FrequencySpectrum(object):
+  def __init__(self):
+
+    self.mean_gray_spectrum = None
+    self.sum_gray_spectrum = None
+    self.count = 0
+    pass
+
+  def to_pil(self, gray_np):
+    from matplotlib import cm
+    gray_np_norm = (gray_np - gray_np.min()) / (gray_np.max() - gray_np.min())
+    # img_pil = Image.fromarray(np.uint8(cm.gist_earth(gray_np_norm) * 255))
+    img_pil = Image.fromarray(np.uint8(cm.viridis(gray_np_norm) * 255))
+    return img_pil
+
+  def get_spectrum_pil(self, text=None):
+    spec_pil = self.to_pil(self.mean_gray_spectrum)
+    if text is not None:
+      pil_utils.add_text(spec_pil, text=text, size=spec_pil.size[0]//18)
+    return spec_pil
+
+  def get_frequency_spectrum(self, image_pil, ):
+    """
+
+    """
+
+    image_np = np.array(image_pil)
+
+    H, W, C = np.shape(image_np)
+    real_r, real_g, real_b = image_np[:, :, 0], image_np[:, :, 1], image_np[:, :, 2]
+    real_gray = 0.2989 * real_r + 0.5870 * real_g + 0.1140 * real_b
+
+    real_gray_f = np.fft.fft2(real_gray - ndimage.median_filter(real_gray, size=H // 8))
+    real_gray_f_shifted = np.fft.fftshift(real_gray_f)
+
+    real_gray_spectrum = 20 * np.log(np.abs(real_gray_f_shifted))
+
+    return real_gray_spectrum
+
+  def update(self, gray_spectrum):
+    if self.mean_gray_spectrum is None:
+      self.mean_gray_spectrum = gray_spectrum
+      self.sum_gray_spectrum = gray_spectrum
+      self.count += 1
+    else:
+      self.sum_gray_spectrum = self.sum_gray_spectrum + gray_spectrum
+      self.count += 1
+      self.mean_gray_spectrum = self.sum_gray_spectrum / self.count
+    pass
+
+  def get_spectrum_and_update(self, image_pil):
+    image_pil_spectrum = self.get_frequency_spectrum(image_pil)
+    self.update(image_pil_spectrum)
+
+  def reset(self):
+    self.mean_gray_spectrum = None
+    self.sum_gray_spectrum = None
+    self.count = 0
+
```

### Comparing `tl2-0.1.0/tl2/proj/GAN/plot_freq_spectrum.py` & `tl2-0.1.1/tl2/proj/GAN/plot_freq_spectrum.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,57 +1,57 @@
-from tqdm import tqdm
-
-from tl2 import tl2_utils
-from tl2.proj.logger import logging_utils_v2
-from tl2.proj.argparser import argparser_utils
-from tl2.proj.GAN import frequency_spectrum
-from tl2.proj.pil import pil_utils
-
-
-def main(outdir,
-         img_dir,
-         num_imgs,
-         debug,
-         ext=('*.jpg', '*.png')
-         ):
-
-  logger = logging_utils_v2.get_logger(filename=f"{outdir}/log.txt")
-
-  freq_spec = frequency_spectrum.FrequencySpectrum()
-
-  img_list = tl2_utils.get_filelist_recursive(img_dir, ext=ext)
-
-  if debug:
-    num_imgs = 10
-
-  pbar = tqdm(range(num_imgs))
-  for idx in pbar:
-    img_pil = pil_utils.pil_open_rgb(path=img_list[idx])
-    freq_spec.get_spectrum_and_update(image_pil=img_pil)
-    if idx % 100 == 0:
-      freq_spec_pil = freq_spec.get_spectrum_pil(text=None)
-      freq_spec_pil = pil_utils.convert_to_rgb(freq_spec_pil)
-      pil_utils.pil_save(freq_spec_pil, f"{outdir}/stylegan2_{idx:03d}_freq_spectrum.png")
-
-  freq_spec_pil = freq_spec.get_spectrum_pil(text=None)
-  freq_spec_pil = pil_utils.convert_to_rgb(freq_spec_pil)
-  pil_utils.pil_save(freq_spec_pil, f"{outdir}/stylegan2_{num_imgs:03d}_freq_spectrum.png")
-
-  pass
-
-
-if __name__ == '__main__':
-  parser = argparser_utils.get_parser()
-  argparser_utils.add_argument_str(parser, name="outdir", )
-  argparser_utils.add_argument_str(parser, name="img_dir", )
-  argparser_utils.add_argument_int(parser, name="num_imgs", default=100)
-  argparser_utils.add_argument_bool(parser, name="debug", )
-
-  args, _ = parser.parse_known_args()
-
-  argparser_utils.print_args(args)
-
-
-  main(outdir=args.outdir,
-       img_dir=args.img_dir,
-       num_imgs=args.num_imgs,
-       debug=args.debug)
+from tqdm import tqdm
+
+from tl2 import tl2_utils
+from tl2.proj.logger import logging_utils_v2
+from tl2.proj.argparser import argparser_utils
+from tl2.proj.GAN import frequency_spectrum
+from tl2.proj.pil import pil_utils
+
+
+def main(outdir,
+         img_dir,
+         num_imgs,
+         debug,
+         ext=('*.jpg', '*.png')
+         ):
+
+  logger = logging_utils_v2.get_logger(filename=f"{outdir}/log.txt")
+
+  freq_spec = frequency_spectrum.FrequencySpectrum()
+
+  img_list = tl2_utils.get_filelist_recursive(img_dir, ext=ext)
+
+  if debug:
+    num_imgs = 10
+
+  pbar = tqdm(range(num_imgs))
+  for idx in pbar:
+    img_pil = pil_utils.pil_open_rgb(path=img_list[idx])
+    freq_spec.get_spectrum_and_update(image_pil=img_pil)
+    if idx % 100 == 0:
+      freq_spec_pil = freq_spec.get_spectrum_pil(text=None)
+      freq_spec_pil = pil_utils.convert_to_rgb(freq_spec_pil)
+      pil_utils.pil_save(freq_spec_pil, f"{outdir}/stylegan2_{idx:03d}_freq_spectrum.png")
+
+  freq_spec_pil = freq_spec.get_spectrum_pil(text=None)
+  freq_spec_pil = pil_utils.convert_to_rgb(freq_spec_pil)
+  pil_utils.pil_save(freq_spec_pil, f"{outdir}/stylegan2_{num_imgs:03d}_freq_spectrum.png")
+
+  pass
+
+
+if __name__ == '__main__':
+  parser = argparser_utils.get_parser()
+  argparser_utils.add_argument_str(parser, name="outdir", )
+  argparser_utils.add_argument_str(parser, name="img_dir", )
+  argparser_utils.add_argument_int(parser, name="num_imgs", default=100)
+  argparser_utils.add_argument_bool(parser, name="debug", )
+
+  args, _ = parser.parse_known_args()
+
+  argparser_utils.print_args(args)
+
+
+  main(outdir=args.outdir,
+       img_dir=args.img_dir,
+       num_imgs=args.num_imgs,
+       debug=args.debug)
```

### Comparing `tl2-0.1.0/tl2/proj/argparser/__pycache__/argparser_utils.cpython-38.pyc` & `tl2-0.1.1/tl2/proj/argparser/__pycache__/argparser_utils.cpython-38.pyc`

 * *Format-specific differences are supported for Python .pyc files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: Byte-compiled Python module for CPython 3.8, timestamp-based, .py timestamp: Mon Oct  3 09:27:55 2022 UTC, .py size: 1997 bytes*

 * *Could not decompile bytecode: bad marshal data (unknown type code)*

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-00000000: 550d 0d0a 0000 0000 1bab 3a63 cd07 0000  U.........:c....
+00000000: 550d 0d0a 0000 0000 f13c 9362 1608 0000  U........<.b....
 00000010: e300 0000 0000 0000 0000 0000 0000 0000  ................
 00000020: 0003 0000 0040 0000 0073 6a00 0000 6400  .....@...sj...d.
 00000030: 6401 6c00 5a00 6400 6401 6c01 5a01 6402  d.l.Z.d.d.l.Z.d.
 00000040: 6403 8400 5a02 6404 6405 8400 5a03 6418  d...Z.d.d...Z.d.
 00000050: 6408 6409 8401 5a04 6419 640b 640c 8401  d.d...Z.d.d.d...
 00000060: 5a05 641a 640d 640e 8401 5a06 641b 640f  Z.d.d.d...Z.d.d.
 00000070: 6410 8401 5a07 641c 6411 6412 8401 5a08  d...Z.d.d.d...Z.
@@ -13,145 +13,143 @@
 000000c0: 6a01 7402 7c00 8301 6401 6402 8d02 7d01  j.t.|...d.d...}.
 000000d0: 7403 6403 7c01 9b00 9d02 8301 0100 7c01  t.d.|.........|.
 000000e0: 5300 2904 4ee9 0200 0000 2901 da06 696e  S.).N.....)...in
 000000f0: 6465 6e74 7a07 6172 6773 3a20 0a29 04da  dentz.args: .)..
 00000100: 046a 736f 6eda 0564 756d 7073 da04 7661  .json..dumps..va
 00000110: 7273 da05 7072 696e 7429 02da 0461 7267  rs..print)...arg
 00000120: 73da 0861 7267 735f 7374 72a9 0072 0a00  s..args_str..r..
-00000130: 0000 fa5f 2f68 6f6d 652f 6d61 2d75 7365  ..._/home/ma-use
-00000140: 722f 776f 726b 2f63 6f64 652f 7374 796c  r/work/code/styl
-00000150: 6567 616e 322d 6164 612d 7079 746f 7263  egan2-ada-pytorc
-00000160: 682d 6578 702f 746c 325f 6c69 622f 746c  h-exp/tl2_lib/tl
-00000170: 322f 7072 6f6a 2f61 7267 7061 7273 6572  2/proj/argparser
-00000180: 2f61 7267 7061 7273 6572 5f75 7469 6c73  /argparser_utils
-00000190: 2e70 79da 0a70 7269 6e74 5f61 7267 7306  .py..print_args.
-000001a0: 0000 0073 0600 0000 0001 1201 0e01 720c  ...s..........r.
-000001b0: 0000 0063 0000 0000 0000 0000 0000 0000  ...c............
-000001c0: 0100 0000 0200 0000 4300 0000 730c 0000  ........C...s...
-000001d0: 0074 00a0 01a1 007d 007c 0053 0029 014e  .t.....}.|.S.).N
-000001e0: 2902 da08 6172 6770 6172 7365 da0e 4172  )...argparse..Ar
-000001f0: 6775 6d65 6e74 5061 7273 6572 2901 da06  gumentParser)...
-00000200: 7061 7273 6572 720a 0000 0072 0a00 0000  parserr....r....
-00000210: 720b 0000 00da 0a67 6574 5f70 6172 7365  r......get_parse
-00000220: 720c 0000 0073 0400 0000 0001 0801 7210  r....s........r.
-00000230: 0000 0072 0a00 0000 da00 6306 0000 0000  ...r......c.....
-00000240: 0000 0000 0000 0006 0000 0007 0000 0043  ...............C
-00000250: 0000 0073 1e00 0000 7c00 6a00 6401 7c01  ...s....|.j.d.|.
-00000260: 9b00 9d02 7c02 7c03 7c04 7c05 6402 8d05  ....|.|.|.|.d...
-00000270: 0100 6403 5300 2904 7aba 0a0a 2020 3a70  ..d.S.).z...  :p
-00000280: 6172 616d 2070 6172 7365 723a 0a20 203a  aram parser:.  :
-00000290: 7061 7261 6d20 6e61 6d65 3a0a 2020 3a70  param name:.  :p
-000002a0: 6172 616d 2074 7970 653a 2074 7970 6520  aram type: type 
-000002b0: 6f66 2065 6c65 6d20 696e 2074 6865 206c  of elem in the l
-000002c0: 6973 740a 2020 3a70 6172 616d 206e 6172  ist.  :param nar
-000002d0: 6773 3a20 272b 2720 3d3d 2031 206f 7220  gs: '+' == 1 or 
-000002e0: 6d6f 7265 2c20 272a 2720 3d3d 2030 206f  more, '*' == 0 o
-000002f0: 7220 6d6f 7265 2c20 273f 2720 3d3d 2030  r more, '?' == 0
-00000300: 206f 7220 310a 2020 3a70 6172 616d 2064   or 1.  :param d
-00000310: 6566 6175 6c74 3a0a 2020 3a70 6172 616d  efault:.  :param
-00000320: 2068 656c 703a 0a20 203a 7265 7475 726e   help:.  :return
-00000330: 3a0a 2020 fa02 2d2d 2904 da04 7479 7065  :.  ..--)...type
-00000340: da05 6e61 7267 73da 0764 6566 6175 6c74  ..nargs..default
-00000350: da04 6865 6c70 4ea9 01da 0c61 6464 5f61  ..helpN....add_a
-00000360: 7267 756d 656e 74a9 0672 0f00 0000 da04  rgument..r......
-00000370: 6e61 6d65 7213 0000 0072 1400 0000 7215  namer....r....r.
-00000380: 0000 0072 1600 0000 720a 0000 0072 0a00  ...r....r....r..
-00000390: 0000 720b 0000 00da 1161 6464 5f61 7267  ..r......add_arg
-000003a0: 756d 656e 745f 6c69 7374 1100 0000 7304  ument_list....s.
-000003b0: 0000 0000 0c1a 0172 1b00 0000 da01 2a63  .......r......*c
-000003c0: 0500 0000 0000 0000 0000 0000 0500 0000  ................
-000003d0: 0800 0000 4300 0000 7318 0000 0074 007c  ....C...s....t.|
-000003e0: 007c 0174 017c 027c 037c 0464 018d 0601  .|.t.|.|.|.d....
-000003f0: 0064 0053 00a9 024e 7219 0000 0029 0272  .d.S...Nr....).r
-00000400: 1b00 0000 da03 696e 74a9 0572 0f00 0000  ......int..r....
-00000410: 721a 0000 0072 1400 0000 7215 0000 0072  r....r....r....r
-00000420: 1600 0000 720a 0000 0072 0a00 0000 720b  ....r....r....r.
-00000430: 0000 00da 1861 6464 5f61 7267 756d 656e  .....add_argumen
-00000440: 745f 6c69 7374 5f6f 665f 696e 7420 0000  t_list_of_int ..
-00000450: 0073 0400 0000 0001 1401 7220 0000 0063  .s........r ...c
-00000460: 0500 0000 0000 0000 0000 0000 0500 0000  ................
-00000470: 0800 0000 4300 0000 7318 0000 0074 007c  ....C...s....t.|
-00000480: 007c 0174 017c 027c 037c 0464 018d 0601  .|.t.|.|.|.d....
-00000490: 0064 0053 0072 1d00 0000 2902 721b 0000  .d.S.r....).r...
-000004a0: 00da 0566 6c6f 6174 721f 0000 0072 0a00  ...floatr....r..
-000004b0: 0000 720a 0000 0072 0b00 0000 da1a 6164  ..r....r......ad
-000004c0: 645f 6172 6775 6d65 6e74 5f6c 6973 745f  d_argument_list_
-000004d0: 6f66 5f66 6c6f 6174 2400 0000 7304 0000  of_float$...s...
-000004e0: 0000 0114 0172 2200 0000 6305 0000 0000  .....r"...c.....
-000004f0: 0000 0000 0000 0005 0000 0008 0000 0043  ...............C
-00000500: 0000 0073 1800 0000 7400 7c00 7c01 7401  ...s....t.|.|.t.
-00000510: 7c02 7c03 7c04 6401 8d06 0100 6400 5300  |.|.|.d.....d.S.
-00000520: 721d 0000 0029 0272 1b00 0000 da03 7374  r....).r......st
-00000530: 7272 1f00 0000 720a 0000 0072 0a00 0000  rr....r....r....
-00000540: 720b 0000 00da 1861 6464 5f61 7267 756d  r......add_argum
-00000550: 656e 745f 6c69 7374 5f6f 665f 7374 7228  ent_list_of_str(
-00000560: 0000 0073 0400 0000 0001 1401 7224 0000  ...s........r$..
-00000570: 0063 0500 0000 0000 0000 0000 0000 0500  .c..............
-00000580: 0000 0700 0000 4300 0000 733c 0000 007c  ......C...s<...|
-00000590: 0373 1e7c 006a 0064 017c 019b 009d 0274  .s.|.j.d.|.....t
-000005a0: 017c 027c 0464 028d 0401 006e 1a7c 006a  .|.|.d.....n.|.j
-000005b0: 0064 017c 019b 009d 0274 017c 037c 0464  .d.|.....t.|.|.d
-000005c0: 0364 048d 0501 0064 0053 0029 054e 7212  .d.....d.S.).Nr.
-000005d0: 0000 00a9 0372 1300 0000 7215 0000 0072  .....r....r....r
-000005e0: 1600 0000 5429 0472 1300 0000 da07 6368  ....T).r......ch
-000005f0: 6f69 6365 7372 1600 0000 da08 7265 7175  oicesr......requ
-00000600: 6972 6564 2902 7218 0000 0072 2300 0000  ired).r....r#...
-00000610: 2905 720f 0000 0072 1a00 0000 7215 0000  ).r....r....r...
-00000620: 0072 2600 0000 7216 0000 0072 0a00 0000  .r&...r....r....
-00000630: 720a 0000 0072 0b00 0000 da10 6164 645f  r....r......add_
-00000640: 6172 6775 6d65 6e74 5f73 7472 2d00 0000  argument_str-...
-00000650: 7308 0000 0000 0104 011a 021a 0272 2800  s............r(.
-00000660: 0000 6304 0000 0000 0000 0000 0000 0004  ..c.............
-00000670: 0000 0006 0000 0043 0000 0073 1c00 0000  .......C...s....
-00000680: 7c00 6a00 6401 7c01 9b00 9d02 7401 7c02  |.j.d.|.....t.|.
-00000690: 7c03 6402 8d04 0100 6400 5300 2903 4e72  |.d.....d.S.).Nr
-000006a0: 1200 0000 7225 0000 0029 0272 1800 0000  ....r%...).r....
-000006b0: 721e 0000 0029 0472 0f00 0000 721a 0000  r....).r....r...
-000006c0: 0072 1500 0000 7216 0000 0072 0a00 0000  .r....r....r....
-000006d0: 720a 0000 0072 0b00 0000 da10 6164 645f  r....r......add_
-000006e0: 6172 6775 6d65 6e74 5f69 6e74 3500 0000  argument_int5...
-000006f0: 7304 0000 0000 0118 0172 2900 0000 4663  s........r)...Fc
-00000700: 0400 0000 0000 0000 0000 0000 0500 0000  ................
-00000710: 0800 0000 4300 0000 7328 0000 0064 0164  ....C...s(...d.d
-00000720: 0284 007d 047c 006a 0064 037c 019b 009d  ...}.|.j.d.|....
-00000730: 027c 0464 0464 057c 027c 0364 068d 0601  .|.d.d.|.|.d....
-00000740: 0064 0053 0029 074e 6301 0000 0000 0000  .d.S.).Nc.......
-00000750: 0000 0000 0001 0000 0003 0000 0053 0000  .............S..
-00000760: 0073 3c00 0000 7400 7c00 7401 8302 720e  .s<...t.|.t...r.
-00000770: 7c00 5300 7c00 a002 a100 6401 6b06 721e  |.S.|.....d.k.r.
-00000780: 6402 5300 7c00 a002 a100 6403 6b06 722e  d.S.|.....d.k.r.
-00000790: 6404 5300 7403 a004 6405 a101 8201 6400  d.S.t...d.....d.
-000007a0: 5300 2906 4e29 05da 0379 6573 da04 7472  S.).N)...yes..tr
-000007b0: 7565 da01 74da 0179 da01 3154 2905 da02  ue..t..y..1T)...
-000007c0: 6e6f da05 6661 6c73 65da 0166 da01 6eda  no..false..f..n.
-000007d0: 0130 467a 1742 6f6f 6c65 616e 2076 616c  .0Fz.Boolean val
-000007e0: 7565 2065 7870 6563 7465 642e 2905 da0a  ue expected.)...
-000007f0: 6973 696e 7374 616e 6365 da04 626f 6f6c  isinstance..bool
-00000800: da05 6c6f 7765 7272 0d00 0000 da11 4172  ..lowerr......Ar
-00000810: 6775 6d65 6e74 5479 7065 4572 726f 7229  gumentTypeError)
-00000820: 01da 0176 720a 0000 0072 0a00 0000 720b  ...vr....r....r.
-00000830: 0000 00da 0873 7472 3262 6f6f 6c3a 0000  .....str2bool:..
-00000840: 0073 0e00 0000 0001 0a01 0401 0c01 0401  .s..............
-00000850: 0c01 0402 7a23 6164 645f 6172 6775 6d65  ....z#add_argume
-00000860: 6e74 5f62 6f6f 6c2e 3c6c 6f63 616c 733e  nt_bool.<locals>
-00000870: 2e73 7472 3262 6f6f 6c72 1200 0000 fa01  .str2boolr......
-00000880: 3f54 2905 7213 0000 0072 1400 0000 da05  ?T).r....r......
-00000890: 636f 6e73 7472 1500 0000 7216 0000 0072  constr....r....r
-000008a0: 1700 0000 2905 720f 0000 0072 1a00 0000  ....).r....r....
-000008b0: 7215 0000 0072 1600 0000 7239 0000 0072  r....r....r9...r
-000008c0: 0a00 0000 720a 0000 0072 0b00 0000 da11  ....r....r......
-000008d0: 6164 645f 6172 6775 6d65 6e74 5f62 6f6f  add_argument_boo
-000008e0: 6c39 0000 0073 0600 0000 0001 080a 1c01  l9...s..........
-000008f0: 723c 0000 0029 0272 0a00 0000 7211 0000  r<...).r....r...
-00000900: 0029 0372 1c00 0000 720a 0000 0072 1100  .).r....r....r..
-00000910: 0000 2903 721c 0000 0072 0a00 0000 7211  ..).r....r....r.
-00000920: 0000 0029 0372 1c00 0000 720a 0000 0072  ...).r....r....r
-00000930: 1100 0000 2903 7211 0000 0072 0a00 0000  ....).r....r....
-00000940: 7211 0000 0029 024e 7211 0000 0029 0246  r....).Nr....).F
-00000950: 7211 0000 0029 0b72 0400 0000 720d 0000  r....).r....r...
-00000960: 0072 0c00 0000 7210 0000 0072 1b00 0000  .r....r....r....
-00000970: 7220 0000 0072 2200 0000 7224 0000 0072  r ...r"...r$...r
-00000980: 2800 0000 7229 0000 0072 3c00 0000 720a  (...r)...r<...r.
-00000990: 0000 0072 0a00 0000 720a 0000 0072 0b00  ...r....r....r..
-000009a0: 0000 da08 3c6d 6f64 756c 653e 0100 0000  ....<module>....
-000009b0: 7314 0000 0008 0108 0408 0608 050a 0f0a  s...............
-000009c0: 040a 040a 050a 080a 04                   .........
+00000130: 0000 fa41 2f68 6f6d 652f 6d61 2d75 7365  ...A/home/ma-use
+00000140: 722f 776f 726b 2f63 6f64 652f 746c 322f  r/work/code/tl2/
+00000150: 746c 322f 7072 6f6a 2f61 7267 7061 7273  tl2/proj/argpars
+00000160: 6572 2f61 7267 7061 7273 6572 5f75 7469  er/argparser_uti
+00000170: 6c73 2e70 79da 0a70 7269 6e74 5f61 7267  ls.py..print_arg
+00000180: 7306 0000 0073 0600 0000 0001 1201 0e01  s....s..........
+00000190: 720c 0000 0063 0000 0000 0000 0000 0000  r....c..........
+000001a0: 0000 0100 0000 0200 0000 4300 0000 730c  ..........C...s.
+000001b0: 0000 0074 00a0 01a1 007d 007c 0053 0029  ...t.....}.|.S.)
+000001c0: 014e 2902 da08 6172 6770 6172 7365 da0e  .N)...argparse..
+000001d0: 4172 6775 6d65 6e74 5061 7273 6572 2901  ArgumentParser).
+000001e0: da06 7061 7273 6572 720a 0000 0072 0a00  ..parserr....r..
+000001f0: 0000 720b 0000 00da 0a67 6574 5f70 6172  ..r......get_par
+00000200: 7365 720c 0000 0073 0400 0000 0001 0801  ser....s........
+00000210: 7210 0000 0072 0a00 0000 da00 6306 0000  r....r......c...
+00000220: 0000 0000 0000 0000 0006 0000 0007 0000  ................
+00000230: 0043 0000 0073 1e00 0000 7c00 6a00 6401  .C...s....|.j.d.
+00000240: 7c01 9b00 9d02 7c02 7c03 7c04 7c05 6402  |.....|.|.|.|.d.
+00000250: 8d05 0100 6403 5300 2904 7aba 0a0a 2020  ....d.S.).z...  
+00000260: 3a70 6172 616d 2070 6172 7365 723a 0a20  :param parser:. 
+00000270: 203a 7061 7261 6d20 6e61 6d65 3a0a 2020   :param name:.  
+00000280: 3a70 6172 616d 2074 7970 653a 2074 7970  :param type: typ
+00000290: 6520 6f66 2065 6c65 6d20 696e 2074 6865  e of elem in the
+000002a0: 206c 6973 740a 2020 3a70 6172 616d 206e   list.  :param n
+000002b0: 6172 6773 3a20 272b 2720 3d3d 2031 206f  args: '+' == 1 o
+000002c0: 7220 6d6f 7265 2c20 272a 2720 3d3d 2030  r more, '*' == 0
+000002d0: 206f 7220 6d6f 7265 2c20 273f 2720 3d3d   or more, '?' ==
+000002e0: 2030 206f 7220 310a 2020 3a70 6172 616d   0 or 1.  :param
+000002f0: 2064 6566 6175 6c74 3a0a 2020 3a70 6172   default:.  :par
+00000300: 616d 2068 656c 703a 0a20 203a 7265 7475  am help:.  :retu
+00000310: 726e 3a0a 2020 fa02 2d2d 2904 da04 7479  rn:.  ..--)...ty
+00000320: 7065 da05 6e61 7267 73da 0764 6566 6175  pe..nargs..defau
+00000330: 6c74 da04 6865 6c70 4ea9 01da 0c61 6464  lt..helpN....add
+00000340: 5f61 7267 756d 656e 74a9 0672 0f00 0000  _argument..r....
+00000350: da04 6e61 6d65 7213 0000 0072 1400 0000  ..namer....r....
+00000360: 7215 0000 0072 1600 0000 720a 0000 0072  r....r....r....r
+00000370: 0a00 0000 720b 0000 00da 1161 6464 5f61  ....r......add_a
+00000380: 7267 756d 656e 745f 6c69 7374 1100 0000  rgument_list....
+00000390: 7304 0000 0000 0c1a 0172 1b00 0000 da01  s........r......
+000003a0: 2a63 0500 0000 0000 0000 0000 0000 0500  *c..............
+000003b0: 0000 0800 0000 4300 0000 7318 0000 0074  ......C...s....t
+000003c0: 007c 007c 0174 017c 027c 037c 0464 018d  .|.|.t.|.|.|.d..
+000003d0: 0601 0064 0053 00a9 024e 7219 0000 0029  ...d.S...Nr....)
+000003e0: 0272 1b00 0000 da03 696e 74a9 0572 0f00  .r......int..r..
+000003f0: 0000 721a 0000 0072 1400 0000 7215 0000  ..r....r....r...
+00000400: 0072 1600 0000 720a 0000 0072 0a00 0000  .r....r....r....
+00000410: 720b 0000 00da 1861 6464 5f61 7267 756d  r......add_argum
+00000420: 656e 745f 6c69 7374 5f6f 665f 696e 7420  ent_list_of_int 
+00000430: 0000 0073 0400 0000 0001 1401 7220 0000  ...s........r ..
+00000440: 0063 0500 0000 0000 0000 0000 0000 0500  .c..............
+00000450: 0000 0800 0000 4300 0000 7318 0000 0074  ......C...s....t
+00000460: 007c 007c 0174 017c 027c 037c 0464 018d  .|.|.t.|.|.|.d..
+00000470: 0601 0064 0053 0072 1d00 0000 2902 721b  ...d.S.r....).r.
+00000480: 0000 00da 0566 6c6f 6174 721f 0000 0072  .....floatr....r
+00000490: 0a00 0000 720a 0000 0072 0b00 0000 da1a  ....r....r......
+000004a0: 6164 645f 6172 6775 6d65 6e74 5f6c 6973  add_argument_lis
+000004b0: 745f 6f66 5f66 6c6f 6174 2400 0000 7304  t_of_float$...s.
+000004c0: 0000 0000 0114 0172 2200 0000 6305 0000  .......r"...c...
+000004d0: 0000 0000 0000 0000 0005 0000 0008 0000  ................
+000004e0: 0043 0000 0073 1800 0000 7400 7c00 7c01  .C...s....t.|.|.
+000004f0: 7401 7c02 7c03 7c04 6401 8d06 0100 6400  t.|.|.|.d.....d.
+00000500: 5300 721d 0000 0029 0272 1b00 0000 da03  S.r....).r......
+00000510: 7374 7272 1f00 0000 720a 0000 0072 0a00  strr....r....r..
+00000520: 0000 720b 0000 00da 1861 6464 5f61 7267  ..r......add_arg
+00000530: 756d 656e 745f 6c69 7374 5f6f 665f 7374  ument_list_of_st
+00000540: 7228 0000 0073 0400 0000 0001 1401 7224  r(...s........r$
+00000550: 0000 0063 0500 0000 0000 0000 0000 0000  ...c............
+00000560: 0500 0000 0700 0000 4300 0000 733c 0000  ........C...s<..
+00000570: 007c 0373 1e7c 006a 0064 017c 019b 009d  .|.s.|.j.d.|....
+00000580: 0274 017c 027c 0464 028d 0401 006e 1a7c  .t.|.|.d.....n.|
+00000590: 006a 0064 017c 019b 009d 0274 017c 037c  .j.d.|.....t.|.|
+000005a0: 0464 0364 048d 0501 0064 0053 0029 054e  .d.d.....d.S.).N
+000005b0: 7212 0000 00a9 0372 1300 0000 7215 0000  r......r....r...
+000005c0: 0072 1600 0000 5429 0472 1300 0000 da07  .r....T).r......
+000005d0: 6368 6f69 6365 7372 1600 0000 da08 7265  choicesr......re
+000005e0: 7175 6972 6564 2902 7218 0000 0072 2300  quired).r....r#.
+000005f0: 0000 2905 720f 0000 0072 1a00 0000 7215  ..).r....r....r.
+00000600: 0000 0072 2600 0000 7216 0000 0072 0a00  ...r&...r....r..
+00000610: 0000 720a 0000 0072 0b00 0000 da10 6164  ..r....r......ad
+00000620: 645f 6172 6775 6d65 6e74 5f73 7472 2d00  d_argument_str-.
+00000630: 0000 7308 0000 0000 0104 011a 021a 0272  ..s............r
+00000640: 2800 0000 6304 0000 0000 0000 0000 0000  (...c...........
+00000650: 0004 0000 0006 0000 0043 0000 0073 1c00  .........C...s..
+00000660: 0000 7c00 6a00 6401 7c01 9b00 9d02 7401  ..|.j.d.|.....t.
+00000670: 7c02 7c03 6402 8d04 0100 6400 5300 2903  |.|.d.....d.S.).
+00000680: 4e72 1200 0000 7225 0000 0029 0272 1800  Nr....r%...).r..
+00000690: 0000 721e 0000 0029 0472 0f00 0000 721a  ..r....).r....r.
+000006a0: 0000 0072 1500 0000 7216 0000 0072 0a00  ...r....r....r..
+000006b0: 0000 720a 0000 0072 0b00 0000 da10 6164  ..r....r......ad
+000006c0: 645f 6172 6775 6d65 6e74 5f69 6e74 3500  d_argument_int5.
+000006d0: 0000 7304 0000 0000 0118 0172 2900 0000  ..s........r)...
+000006e0: 4663 0400 0000 0000 0000 0000 0000 0500  Fc..............
+000006f0: 0000 0800 0000 4300 0000 7328 0000 0064  ......C...s(...d
+00000700: 0164 0284 007d 047c 006a 0064 037c 019b  .d...}.|.j.d.|..
+00000710: 009d 027c 0464 0464 057c 027c 0364 068d  ...|.d.d.|.|.d..
+00000720: 0601 0064 0053 0029 074e 6301 0000 0000  ...d.S.).Nc.....
+00000730: 0000 0000 0000 0001 0000 0003 0000 0053  ...............S
+00000740: 0000 0073 3c00 0000 7400 7c00 7401 8302  ...s<...t.|.t...
+00000750: 720e 7c00 5300 7c00 a002 a100 6401 6b06  r.|.S.|.....d.k.
+00000760: 721e 6402 5300 7c00 a002 a100 6403 6b06  r.d.S.|.....d.k.
+00000770: 722e 6404 5300 7403 a004 6405 a101 8201  r.d.S.t...d.....
+00000780: 6400 5300 2906 4e29 05da 0379 6573 da04  d.S.).N)...yes..
+00000790: 7472 7565 da01 74da 0179 da01 3154 2905  true..t..y..1T).
+000007a0: da02 6e6f da05 6661 6c73 65da 0166 da01  ..no..false..f..
+000007b0: 6eda 0130 467a 1742 6f6f 6c65 616e 2076  n..0Fz.Boolean v
+000007c0: 616c 7565 2065 7870 6563 7465 642e 2905  alue expected.).
+000007d0: da0a 6973 696e 7374 616e 6365 da04 626f  ..isinstance..bo
+000007e0: 6f6c da05 6c6f 7765 7272 0d00 0000 da11  ol..lowerr......
+000007f0: 4172 6775 6d65 6e74 5479 7065 4572 726f  ArgumentTypeErro
+00000800: 7229 01da 0176 720a 0000 0072 0a00 0000  r)...vr....r....
+00000810: 720b 0000 00da 0873 7472 3262 6f6f 6c3a  r......str2bool:
+00000820: 0000 0073 0e00 0000 0001 0a01 0401 0c01  ...s............
+00000830: 0401 0c01 0402 7a23 6164 645f 6172 6775  ......z#add_argu
+00000840: 6d65 6e74 5f62 6f6f 6c2e 3c6c 6f63 616c  ment_bool.<local
+00000850: 733e 2e73 7472 3262 6f6f 6c72 1200 0000  s>.str2boolr....
+00000860: fa01 3f54 2905 7213 0000 0072 1400 0000  ..?T).r....r....
+00000870: da05 636f 6e73 7472 1500 0000 7216 0000  ..constr....r...
+00000880: 0072 1700 0000 2905 720f 0000 0072 1a00  .r....).r....r..
+00000890: 0000 7215 0000 0072 1600 0000 7239 0000  ..r....r....r9..
+000008a0: 0072 0a00 0000 720a 0000 0072 0b00 0000  .r....r....r....
+000008b0: da11 6164 645f 6172 6775 6d65 6e74 5f62  ..add_argument_b
+000008c0: 6f6f 6c39 0000 0073 0600 0000 0001 080a  ool9...s........
+000008d0: 1c01 723c 0000 0029 0272 0a00 0000 7211  ..r<...).r....r.
+000008e0: 0000 0029 0372 1c00 0000 720a 0000 0072  ...).r....r....r
+000008f0: 1100 0000 2903 721c 0000 0072 0a00 0000  ....).r....r....
+00000900: 7211 0000 0029 0372 1c00 0000 720a 0000  r....).r....r...
+00000910: 0072 1100 0000 2903 7211 0000 0072 0a00  .r....).r....r..
+00000920: 0000 7211 0000 0029 024e 7211 0000 0029  ..r....).Nr....)
+00000930: 0246 7211 0000 0029 0b72 0400 0000 720d  .Fr....).r....r.
+00000940: 0000 0072 0c00 0000 7210 0000 0072 1b00  ...r....r....r..
+00000950: 0000 7220 0000 0072 2200 0000 7224 0000  ..r ...r"...r$..
+00000960: 0072 2800 0000 7229 0000 0072 3c00 0000  .r(...r)...r<...
+00000970: 720a 0000 0072 0a00 0000 720a 0000 0072  r....r....r....r
+00000980: 0b00 0000 da08 3c6d 6f64 756c 653e 0100  ......<module>..
+00000990: 0000 7314 0000 0008 0108 0408 0608 050a  ..s.............
+000009a0: 0f0a 040a 040a 050a 080a 04              ...........
```

### Comparing `tl2-0.1.0/tl2/proj/cv2/cv2_utils.py` & `tl2-0.1.1/tl2/proj/cv2/cv2_utils.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/dlib/configs/dlib_web.yaml` & `tl2-0.1.1/tl2/proj/dlib/configs/dlib_web.yaml`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-root_obs: &root_obs s3://bucket-3690/ZhouPeng
-
-align_face:
-  root_obs: *root_obs
-  port: 8561
-  sidebar:
-    sidebar_name: "align_face"
-  model_cfg:
-    register_modules:
-      - tl2_lib.tl2.proj.dlib.ffhq_face_align.align_face_stmodel
-    name: tl2_lib.tl2.proj.dlib.ffhq_face_align.align_face_stmodel.AlignFace
-  mode:
-    - align_face
-  align_face:
-    landmark_model: "cache_pretrained/dlib/shape_predictor_68_face_landmarks.dat"
-    image_list:
-      raw_img_list:
-        image_list_file:
-          - "tl2_lib/tl2/proj/dlib/datasets/raw_face_list.txt"
-    save_in_source_dir: false
-
-crop_face_by_landmarks:
-  base: align_face
-  mode:
-    - crop_face_by_landmarks
-  crop_face_by_landmarks:
-    landmark_model: "datasets/pretrained/shape_predictor_68_face_landmarks.dat"
-    image_list:
-      raw_img_list:
-        image_list_file:
-          - "tl2_lib/tl2/proj/dlib/datasets/raw_face_list.txt"
-
-
-
-
+root_obs: &root_obs s3://bucket-3690/ZhouPeng
+
+align_face:
+  root_obs: *root_obs
+  port: 8561
+  sidebar:
+    sidebar_name: "align_face"
+  model_cfg:
+    register_modules:
+      - tl2_lib.tl2.proj.dlib.ffhq_face_align.align_face_stmodel
+    name: tl2_lib.tl2.proj.dlib.ffhq_face_align.align_face_stmodel.AlignFace
+  mode:
+    - align_face
+  align_face:
+    landmark_model: "cache_pretrained/dlib/shape_predictor_68_face_landmarks.dat"
+    image_list:
+      raw_img_list:
+        image_list_file:
+          - "tl2_lib/tl2/proj/dlib/datasets/raw_face_list.txt"
+    save_in_source_dir: false
+
+crop_face_by_landmarks:
+  base: align_face
+  mode:
+    - crop_face_by_landmarks
+  crop_face_by_landmarks:
+    landmark_model: "datasets/pretrained/shape_predictor_68_face_landmarks.dat"
+    image_list:
+      raw_img_list:
+        image_list_file:
+          - "tl2_lib/tl2/proj/dlib/datasets/raw_face_list.txt"
+
+
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/dlib/datasets/dlib_landmarks_68.png` & `tl2-0.1.1/tl2/proj/dlib/datasets/dlib_landmarks_68.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/dlib/datasets/hand_convex_hull.jpg` & `tl2-0.1.1/tl2/proj/dlib/datasets/hand_convex_hull.jpg`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/21242213255_abde1622df_o.jpg` & `tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/21242213255_abde1622df_o.jpg`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/heben1.jfif` & `tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/heben1.jfif`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/korean1.jpeg` & `tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/korean1.jpeg`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/lecun1.png` & `tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/lecun1.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/liudehua1.jfif` & `tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/liudehua1.jfif`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/nanmingxing3.jfif` & `tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/nanmingxing3.jfif`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/peng.jpg` & `tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/peng.jpg`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/xiangnong1.jpeg` & `tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/xiangnong1.jpeg`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/dlib/datasets/raw_face/xinhengjieyi2.jpg` & `tl2-0.1.1/tl2/proj/dlib/datasets/raw_face/xinhengjieyi2.jpg`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/dlib/ffhq_face_align/align_face_stmodel.py` & `tl2-0.1.1/tl2/proj/dlib/ffhq_face_align/align_face_stmodel.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,219 +1,219 @@
-import numpy as np
-import cv2
-import dlib
-import collections
-from pathlib import Path
-import logging
-import os
-import sys
-from PIL import Image
-import streamlit as st
-
-sys.path.insert(0, os.getcwd())
-
-from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
-from tl2.proj.streamlit import st_utils
-from tl2 import tl2_utils
-from tl2.proj.fvcore import build_model, MODEL_REGISTRY
-from tl2.proj.cv2 import cv2_utils
-from tl2.proj.dlib import dlib_utils
-from tl2.proj.pil import pil_utils
-from tl2.modelarts import moxing_utils
-
-from . import align_images
-
-
-@MODEL_REGISTRY.register(name_prefix=__name__)
-class AlignFace(object):
-  def __init__(self):
-
-    pass
-
-  def align_face(self,
-                 cfg,
-                 outdir,
-                 saved_suffix_state=None,
-                 **kwargs):
-    st.write(f"landmark_model: {cfg.landmark_model}")
-    image_kwargs = collections.defaultdict(dict)
-    key_list = []
-    for k, v in cfg.image_list.items():
-      key1 = f"{k}1"
-      image_path = st_utils.parse_image_list(image_list_file=v.image_list_file, header=key1)
-      image_kwargs[key1]['image_path'] = image_path
-      key_list.extend([key1, ])
-
-    content_k = st_utils.radio(label='raw', options=key_list, sidebar=True)
-    content_kwargs = image_kwargs[content_k]
-    image_path = content_kwargs['image_path']
-
-    image_path = st_utils.text_input('image_path', image_path)
-    if not image_path:
-      image_path = content_kwargs['image_path']
-
-    save_in_source_dir = st_utils.checkbox('save_in_source_dir', cfg.save_in_source_dir, sidebar=True)
-
-    img_pil = Image.open(image_path)
-    st.image(img_pil, caption=f"{img_pil.size}")
-
-    if not global_cfg.tl_debug:
-      if not st.sidebar.button("run_web"):
-        return
-
-    if saved_suffix_state is not None:
-      saved_suffix_state.saved_suffix = saved_suffix_state.saved_suffix + 1
-
-    # img_ori_pil = Image.open(image_path)
-    # st.header(image_path)
-    # st.image(img_ori_pil, caption=f"{img_ori_pil.size}")
-
-    moxing_utils.copy_data(rank=0, global_cfg=global_cfg,
-                           datapath_obs=f"{cfg.landmark_model}.bz2",
-                           datapath=f"{cfg.landmark_model}.bz2")
-
-    saved_image_list = align_images.align_face(
-      image_path=image_path,
-      landmark_model=cfg.landmark_model,
-      outdir=os.path.dirname(image_path) if save_in_source_dir else outdir)
-    for image_file in saved_image_list:
-      img_pil = Image.open(image_file)
-      st.subheader('Aligned face:')
-      st.write(f'{image_file}')
-      st.image(img_pil, caption=f"{img_pil.size}", use_column_width=True)
-
-    # show alignment
-    detector = dlib.get_frontal_face_detector()  
-    shape_predictor = dlib.shape_predictor(cfg.landmark_model)
-    for image_file in saved_image_list:
-      imgs = []
-
-      img = dlib.load_rgb_image(image_file)
-      # imgs.append(img)
-      # cv2_utils.imshow_pil(img, is_bgr=False)
-
-      dets = detector(img, 1)
-      for detection in dets:
-        x, y, w, h = dlib_utils.rect_to_bb(rect=detection)
-        img_rect = cv2_utils.cv2_rectangle(img, x, y, w, h, thickness=3)
-        imgs.append(img_rect)
-        # cv2_utils.imshow_pil(img_rect, is_bgr=False)
-
-        face_landmarks = shape_predictor(img, detection)
-        face_landmarks = [(item.x, item.y) for item in face_landmarks.parts()]
-        img_landmarks = cv2_utils.cv2_landmarks(img_rect, landmarks=face_landmarks, radius=5)
-        imgs.append(img_landmarks)
-        # cv2_utils.imshow_pil(img_landmarks)
-        merged_pil = pil_utils.merge_image_np(imgs, nrow=len(imgs), pad=1, channel_first=False)
-        # pil_utils.imshow_pil(merged_pil)
-
-        st.subheader('Landmarks face:')
-        st.image(merged_pil, caption=f"{merged_pil.size}")
-    pass
-
-  def crop_face_by_landmarks(self,
-                             cfg,
-                             outdir,
-                             saved_suffix_state=None,
-                             **kwargs):
-    st.write(f"landmark_model: {cfg.landmark_model}")
-    image_kwargs = collections.defaultdict(dict)
-    key_list = []
-    for k, v in cfg.image_list.items():
-      key1 = f"{k}1"
-      image_path = st_utils.parse_image_list(image_list_file=v.image_list_file, header=key1)
-      image_kwargs[key1]['image_path'] = image_path
-      key_list.extend([key1, ])
-
-    content_k = st_utils.radio(label='raw', options=key_list, sidebar=True)
-    content_kwargs = image_kwargs[content_k]
-    image_path = content_kwargs['image_path']
-
-    # ****************************************************************************
-    if not global_cfg.tl_debug:
-      if not st.sidebar.button("run_web"):
-        return
-
-    if saved_suffix_state is not None:
-      saved_suffix_state.saved_suffix = saved_suffix_state.saved_suffix + 1
-
-
-    saved_image_list = align_images.align_face(
-      image_path=image_path, landmark_model=cfg.landmark_model, outdir=outdir)
-    for image_file in saved_image_list:
-      img_pil = Image.open(image_file)
-      st.subheader('Aligned face:')
-      st.write(f'{image_file}')
-      st.image(img_pil, caption=f"{img_pil.size}", use_column_width=True)
-
-    # show alignment
-    detector = dlib.get_frontal_face_detector()
-    shape_predictor = dlib.shape_predictor(cfg.landmark_model)
-    for image_file in saved_image_list:
-      imgs = []
-
-      img = dlib.load_rgb_image(image_file)
-      # imgs.append(img)
-      # cv2_utils.imshow_pil(img, is_bgr=False)
-
-      dets = detector(img, 1)
-      for detection in dets:
-        x, y, w, h = dlib_utils.rect_to_bb(rect=detection)
-        img_rect = cv2_utils.cv2_rectangle(img, x, y, w, h, thickness=3)
-        # imgs.append(img_rect)
-        # cv2_utils.imshow_pil(img_rect, is_bgr=False)
-
-        face_landmarks = shape_predictor(img, detection)
-        face_landmarks = [(item.x, item.y) for item in face_landmarks.parts()]
-        img_landmarks = cv2_utils.cv2_landmarks(img_rect, landmarks=face_landmarks, radius=5)
-        imgs.append(img_landmarks)
-        # cv2_utils.imshow_pil(img_landmarks)
-
-        # crop face
-        routes = {}
-        landmark_tuple = face_landmarks
-        for idx in range(16, -1, -1):
-          routes[idx] = landmark_tuple[idx]
-
-        for idx in range(17, 20): # 17, 18, 19
-          from_coordinate = landmark_tuple[idx]
-          to_coordinate = landmark_tuple[idx + 1]
-          routes[idx] = from_coordinate
-
-        for idx in range(24, 27): # 24, 25
-          from_coordinate = landmark_tuple[idx]
-          to_coordinate = landmark_tuple[idx + 1]
-          routes[idx] = from_coordinate
-
-        routes_line = list(routes.values())
-
-        img_line = img.copy()
-        cv2_utils.cv2_line_all_points(img_line, routes_line)
-
-        imgs.append(img_line)
-        merged_pil = pil_utils.merge_image_np(imgs, nrow=len(imgs), pad=1, channel_first=False)
-        st.subheader('Landmarks face:')
-        st_utils.st_image(merged_pil, caption=f"{merged_pil.size}", debug=global_cfg.tl_debug)
-
-        # mask
-        mask, img_mask = cv2_utils.get_mask_from_points(img, points=routes_line)
-
-        # convex hull
-        img_hull = img.copy()
-        imgs = [img_line, ]
-        face_landmarks = np.array(face_landmarks).reshape((-1, 1, 2))
-        hull = cv2.convexHull(face_landmarks)
-        hull = hull.squeeze()
-        cv2_utils.cv2_line_all_points(img_hull, points=hull)
-        imgs.append(img_hull)
-        merged_pil = pil_utils.merge_image_np(imgs, nrow=len(imgs), pad=1, channel_first=False)
-        st_utils.st_image(merged_pil, caption=f"{merged_pil.size}", debug=global_cfg.tl_debug)
-
-        mask, img_hull_mask = cv2_utils.get_mask_from_points(img, points=hull)
-        imgs = [img_mask, img_hull_mask]
-        merged_pil = pil_utils.merge_image_np(imgs, nrow=len(imgs), pad=1, channel_first=False)
-        st_utils.st_image(merged_pil, caption=f"{merged_pil.size}", debug=global_cfg.tl_debug)
-
-    pass
-
-
-
+import numpy as np
+import cv2
+import dlib
+import collections
+from pathlib import Path
+import logging
+import os
+import sys
+from PIL import Image
+import streamlit as st
+
+sys.path.insert(0, os.getcwd())
+
+from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
+from tl2.proj.streamlit import st_utils
+from tl2 import tl2_utils
+from tl2.proj.fvcore import build_model, MODEL_REGISTRY
+from tl2.proj.cv2 import cv2_utils
+from tl2.proj.dlib import dlib_utils
+from tl2.proj.pil import pil_utils
+from tl2.modelarts import moxing_utils
+
+from . import align_images
+
+
+@MODEL_REGISTRY.register(name_prefix=__name__)
+class AlignFace(object):
+  def __init__(self):
+
+    pass
+
+  def align_face(self,
+                 cfg,
+                 outdir,
+                 saved_suffix_state=None,
+                 **kwargs):
+    st.write(f"landmark_model: {cfg.landmark_model}")
+    image_kwargs = collections.defaultdict(dict)
+    key_list = []
+    for k, v in cfg.image_list.items():
+      key1 = f"{k}1"
+      image_path = st_utils.parse_image_list(image_list_file=v.image_list_file, header=key1)
+      image_kwargs[key1]['image_path'] = image_path
+      key_list.extend([key1, ])
+
+    content_k = st_utils.radio(label='raw', options=key_list, sidebar=True)
+    content_kwargs = image_kwargs[content_k]
+    image_path = content_kwargs['image_path']
+
+    image_path = st_utils.text_input('image_path', image_path)
+    if not image_path:
+      image_path = content_kwargs['image_path']
+
+    save_in_source_dir = st_utils.checkbox('save_in_source_dir', cfg.save_in_source_dir, sidebar=True)
+
+    img_pil = Image.open(image_path)
+    st.image(img_pil, caption=f"{img_pil.size}")
+
+    if not global_cfg.tl_debug:
+      if not st.sidebar.button("run_web"):
+        return
+
+    if saved_suffix_state is not None:
+      saved_suffix_state.saved_suffix = saved_suffix_state.saved_suffix + 1
+
+    # img_ori_pil = Image.open(image_path)
+    # st.header(image_path)
+    # st.image(img_ori_pil, caption=f"{img_ori_pil.size}")
+
+    moxing_utils.copy_data(rank=0, global_cfg=global_cfg,
+                           datapath_obs=f"{cfg.landmark_model}.bz2",
+                           datapath=f"{cfg.landmark_model}.bz2")
+
+    saved_image_list = align_images.align_face(
+      image_path=image_path,
+      landmark_model=cfg.landmark_model,
+      outdir=os.path.dirname(image_path) if save_in_source_dir else outdir)
+    for image_file in saved_image_list:
+      img_pil = Image.open(image_file)
+      st.subheader('Aligned face:')
+      st.write(f'{image_file}')
+      st.image(img_pil, caption=f"{img_pil.size}", use_column_width=True)
+
+    # show alignment
+    detector = dlib.get_frontal_face_detector()  
+    shape_predictor = dlib.shape_predictor(cfg.landmark_model)
+    for image_file in saved_image_list:
+      imgs = []
+
+      img = dlib.load_rgb_image(image_file)
+      # imgs.append(img)
+      # cv2_utils.imshow_pil(img, is_bgr=False)
+
+      dets = detector(img, 1)
+      for detection in dets:
+        x, y, w, h = dlib_utils.rect_to_bb(rect=detection)
+        img_rect = cv2_utils.cv2_rectangle(img, x, y, w, h, thickness=3)
+        imgs.append(img_rect)
+        # cv2_utils.imshow_pil(img_rect, is_bgr=False)
+
+        face_landmarks = shape_predictor(img, detection)
+        face_landmarks = [(item.x, item.y) for item in face_landmarks.parts()]
+        img_landmarks = cv2_utils.cv2_landmarks(img_rect, landmarks=face_landmarks, radius=5)
+        imgs.append(img_landmarks)
+        # cv2_utils.imshow_pil(img_landmarks)
+        merged_pil = pil_utils.merge_image_np(imgs, nrow=len(imgs), pad=1, channel_first=False)
+        # pil_utils.imshow_pil(merged_pil)
+
+        st.subheader('Landmarks face:')
+        st.image(merged_pil, caption=f"{merged_pil.size}")
+    pass
+
+  def crop_face_by_landmarks(self,
+                             cfg,
+                             outdir,
+                             saved_suffix_state=None,
+                             **kwargs):
+    st.write(f"landmark_model: {cfg.landmark_model}")
+    image_kwargs = collections.defaultdict(dict)
+    key_list = []
+    for k, v in cfg.image_list.items():
+      key1 = f"{k}1"
+      image_path = st_utils.parse_image_list(image_list_file=v.image_list_file, header=key1)
+      image_kwargs[key1]['image_path'] = image_path
+      key_list.extend([key1, ])
+
+    content_k = st_utils.radio(label='raw', options=key_list, sidebar=True)
+    content_kwargs = image_kwargs[content_k]
+    image_path = content_kwargs['image_path']
+
+    # ****************************************************************************
+    if not global_cfg.tl_debug:
+      if not st.sidebar.button("run_web"):
+        return
+
+    if saved_suffix_state is not None:
+      saved_suffix_state.saved_suffix = saved_suffix_state.saved_suffix + 1
+
+
+    saved_image_list = align_images.align_face(
+      image_path=image_path, landmark_model=cfg.landmark_model, outdir=outdir)
+    for image_file in saved_image_list:
+      img_pil = Image.open(image_file)
+      st.subheader('Aligned face:')
+      st.write(f'{image_file}')
+      st.image(img_pil, caption=f"{img_pil.size}", use_column_width=True)
+
+    # show alignment
+    detector = dlib.get_frontal_face_detector()
+    shape_predictor = dlib.shape_predictor(cfg.landmark_model)
+    for image_file in saved_image_list:
+      imgs = []
+
+      img = dlib.load_rgb_image(image_file)
+      # imgs.append(img)
+      # cv2_utils.imshow_pil(img, is_bgr=False)
+
+      dets = detector(img, 1)
+      for detection in dets:
+        x, y, w, h = dlib_utils.rect_to_bb(rect=detection)
+        img_rect = cv2_utils.cv2_rectangle(img, x, y, w, h, thickness=3)
+        # imgs.append(img_rect)
+        # cv2_utils.imshow_pil(img_rect, is_bgr=False)
+
+        face_landmarks = shape_predictor(img, detection)
+        face_landmarks = [(item.x, item.y) for item in face_landmarks.parts()]
+        img_landmarks = cv2_utils.cv2_landmarks(img_rect, landmarks=face_landmarks, radius=5)
+        imgs.append(img_landmarks)
+        # cv2_utils.imshow_pil(img_landmarks)
+
+        # crop face
+        routes = {}
+        landmark_tuple = face_landmarks
+        for idx in range(16, -1, -1):
+          routes[idx] = landmark_tuple[idx]
+
+        for idx in range(17, 20): # 17, 18, 19
+          from_coordinate = landmark_tuple[idx]
+          to_coordinate = landmark_tuple[idx + 1]
+          routes[idx] = from_coordinate
+
+        for idx in range(24, 27): # 24, 25
+          from_coordinate = landmark_tuple[idx]
+          to_coordinate = landmark_tuple[idx + 1]
+          routes[idx] = from_coordinate
+
+        routes_line = list(routes.values())
+
+        img_line = img.copy()
+        cv2_utils.cv2_line_all_points(img_line, routes_line)
+
+        imgs.append(img_line)
+        merged_pil = pil_utils.merge_image_np(imgs, nrow=len(imgs), pad=1, channel_first=False)
+        st.subheader('Landmarks face:')
+        st_utils.st_image(merged_pil, caption=f"{merged_pil.size}", debug=global_cfg.tl_debug)
+
+        # mask
+        mask, img_mask = cv2_utils.get_mask_from_points(img, points=routes_line)
+
+        # convex hull
+        img_hull = img.copy()
+        imgs = [img_line, ]
+        face_landmarks = np.array(face_landmarks).reshape((-1, 1, 2))
+        hull = cv2.convexHull(face_landmarks)
+        hull = hull.squeeze()
+        cv2_utils.cv2_line_all_points(img_hull, points=hull)
+        imgs.append(img_hull)
+        merged_pil = pil_utils.merge_image_np(imgs, nrow=len(imgs), pad=1, channel_first=False)
+        st_utils.st_image(merged_pil, caption=f"{merged_pil.size}", debug=global_cfg.tl_debug)
+
+        mask, img_hull_mask = cv2_utils.get_mask_from_points(img, points=hull)
+        imgs = [img_mask, img_hull_mask]
+        merged_pil = pil_utils.merge_image_np(imgs, nrow=len(imgs), pad=1, channel_first=False)
+        st_utils.st_image(merged_pil, caption=f"{merged_pil.size}", debug=global_cfg.tl_debug)
+
+    pass
+
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/dlib/ffhq_face_align/align_images.py` & `tl2-0.1.1/tl2/proj/dlib/ffhq_face_align/align_images.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,82 +1,82 @@
-from pathlib import Path
-import argparse
-import os
-import sys
-import bz2
-import tqdm
-# from tensorflow.keras.utils import get_file
-
-from tl2.proj.dlib.ffhq_face_align.face_alignment import image_align
-from tl2.proj.dlib.ffhq_face_align.landmarks_detector import LandmarksDetector
-
-
-def unpack_bz2(src_path, dst_path):
-    data = bz2.BZ2File(src_path).read()
-    with open(dst_path, 'wb') as fp:
-      fp.write(data)
-    print(f"Unzip {src_path} to \n{dst_path}")
-    return dst_path
-
-
-def get_saved_align_image_path(outdir, image_path, idx):
-    image_path = Path(image_path)
-    if outdir is None:
-        saved_image = f"{image_path.parent}/{image_path.stem}_aligned_{idx:02d}.png"
-    else:
-        saved_image = f"{outdir}/{image_path.stem}_aligned_{idx:02d}.png"
-    return saved_image
-
-
-def align_face(image_path,
-               outdir=None,
-               output_size=1024,
-               landmark_model="cache_pretrained/dlib/shape_predictor_68_face_landmarks.dat",
-               landmark_model_bz2="cache_pretrained/dlib/shape_predictor_68_face_landmarks.dat.bz2"):
-    """
-    Extracts and aligns all faces from images using DLib and a function from original FFHQ dataset preparation step
-
-    """
-    os.makedirs(outdir, exist_ok=True)
-
-    LANDMARKS_MODEL_URL = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'
-    # tmp = get_file(landmark_model, LANDMARKS_MODEL_URL)
-    if not os.path.exists(landmark_model):
-      landmark_model = unpack_bz2(src_path=landmark_model_bz2, dst_path=landmark_model)
-
-    landmarks_detector = LandmarksDetector(landmark_model)
-
-    image_path = Path(image_path)
-    landmarks = landmarks_detector.get_landmarks(str(image_path))
-    saved_image_list = []
-    det_success = False
-    for i, face_landmarks in enumerate(landmarks):
-        saved_image = get_saved_align_image_path(outdir=outdir, image_path=image_path, idx=i)
-        saved_image_list.append(saved_image)
-        image_align(image_path, saved_image, face_landmarks, output_size=output_size)
-        det_success = True
-        pass
-
-    if not det_success:
-        tqdm.tqdm.write(f"Fail to align: {image_path}")
-    return saved_image_list
-
-
-if __name__ == '__main__':
-    """
-    python -m tl2.proj.dlib.ffhq_face_align.align_images --image_path --outdir
-    """
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--landmark_model', type=str,
-                        default="datasets/pretrained/shape_predictor_68_face_landmarks.dat.bz2")
-    parser.add_argument('--image_path', type=str)
-    parser.add_argument('--outdir', type=str)
-
-    args, _ = parser.parse_known_args()
-
-    align_face(**vars(args))
-
-
-
-
-
-
+from pathlib import Path
+import argparse
+import os
+import sys
+import bz2
+import tqdm
+# from tensorflow.keras.utils import get_file
+
+from tl2.proj.dlib.ffhq_face_align.face_alignment import image_align
+from tl2.proj.dlib.ffhq_face_align.landmarks_detector import LandmarksDetector
+
+
+def unpack_bz2(src_path, dst_path):
+    data = bz2.BZ2File(src_path).read()
+    with open(dst_path, 'wb') as fp:
+      fp.write(data)
+    print(f"Unzip {src_path} to \n{dst_path}")
+    return dst_path
+
+
+def get_saved_align_image_path(outdir, image_path, idx):
+    image_path = Path(image_path)
+    if outdir is None:
+        saved_image = f"{image_path.parent}/{image_path.stem}_aligned_{idx:02d}.png"
+    else:
+        saved_image = f"{outdir}/{image_path.stem}_aligned_{idx:02d}.png"
+    return saved_image
+
+
+def align_face(image_path,
+               outdir=None,
+               output_size=1024,
+               landmark_model="cache_pretrained/dlib/shape_predictor_68_face_landmarks.dat",
+               landmark_model_bz2="cache_pretrained/dlib/shape_predictor_68_face_landmarks.dat.bz2"):
+    """
+    Extracts and aligns all faces from images using DLib and a function from original FFHQ dataset preparation step
+
+    """
+    os.makedirs(outdir, exist_ok=True)
+
+    LANDMARKS_MODEL_URL = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'
+    # tmp = get_file(landmark_model, LANDMARKS_MODEL_URL)
+    if not os.path.exists(landmark_model):
+      landmark_model = unpack_bz2(src_path=landmark_model_bz2, dst_path=landmark_model)
+
+    landmarks_detector = LandmarksDetector(landmark_model)
+
+    image_path = Path(image_path)
+    landmarks = landmarks_detector.get_landmarks(str(image_path))
+    saved_image_list = []
+    det_success = False
+    for i, face_landmarks in enumerate(landmarks):
+        saved_image = get_saved_align_image_path(outdir=outdir, image_path=image_path, idx=i)
+        saved_image_list.append(saved_image)
+        image_align(image_path, saved_image, face_landmarks, output_size=output_size)
+        det_success = True
+        pass
+
+    if not det_success:
+        tqdm.tqdm.write(f"Fail to align: {image_path}")
+    return saved_image_list
+
+
+if __name__ == '__main__':
+    """
+    python -m tl2.proj.dlib.ffhq_face_align.align_images --image_path --outdir
+    """
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--landmark_model', type=str,
+                        default="datasets/pretrained/shape_predictor_68_face_landmarks.dat.bz2")
+    parser.add_argument('--image_path', type=str)
+    parser.add_argument('--outdir', type=str)
+
+    args, _ = parser.parse_known_args()
+
+    align_face(**vars(args))
+
+
+
+
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/dlib/ffhq_face_align/face_alignment.py` & `tl2-0.1.1/tl2/proj/dlib/ffhq_face_align/face_alignment.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,105 +1,105 @@
-from pathlib import Path
-import numpy as np
-import scipy.ndimage
-import os
-import PIL.Image
-
-
-def image_align(src_file,
-                dst_file,
-                face_landmarks,
-                output_size=1024,
-                transform_size=4096,
-                enable_padding=True,
-                x_scale=1,
-                y_scale=1,
-                em_scale=0.1,
-                alpha=False):
-        # Align function from FFHQ dataset pre-processing step
-        # https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py
-
-        lm = np.array(face_landmarks)
-        lm_chin          = lm[0  : 17]  # left-right
-        lm_eyebrow_left  = lm[17 : 22]  # left-right
-        lm_eyebrow_right = lm[22 : 27]  # left-right
-        lm_nose          = lm[27 : 31]  # top-down
-        lm_nostrils      = lm[31 : 36]  # top-down
-        lm_eye_left      = lm[36 : 42]  # left-clockwise
-        lm_eye_right     = lm[42 : 48]  # left-clockwise
-        lm_mouth_outer   = lm[48 : 60]  # left-clockwise
-        lm_mouth_inner   = lm[60 : 68]  # left-clockwise
-
-        # Calculate auxiliary vectors.
-        eye_left     = np.mean(lm_eye_left, axis=0)
-        eye_right    = np.mean(lm_eye_right, axis=0)
-        eye_avg      = (eye_left + eye_right) * 0.5
-        eye_to_eye   = eye_right - eye_left
-        mouth_left   = lm_mouth_outer[0]
-        mouth_right  = lm_mouth_outer[6]
-        mouth_avg    = (mouth_left + mouth_right) * 0.5
-        eye_to_mouth = mouth_avg - eye_avg
-
-        # Choose oriented crop rectangle.
-        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]
-        x /= np.hypot(*x)
-        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)
-        x *= x_scale
-        y = np.flipud(x) * [-y_scale, y_scale]
-        c = eye_avg + eye_to_mouth * em_scale
-        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])
-        qsize = np.hypot(*x) * 2
-
-        # Load in-the-wild image.
-        if not os.path.isfile(src_file):
-            print('\nCannot find source image. Please run "--wilds" before "--align".')
-            return
-        img = PIL.Image.open(src_file).convert('RGBA').convert('RGB')
-
-        # Shrink.
-        shrink = int(np.floor(qsize / output_size * 0.5))
-        if shrink > 1:
-            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))
-            img = img.resize(rsize, PIL.Image.ANTIALIAS)
-            quad /= shrink
-            qsize /= shrink
-
-        # Crop.
-        border = max(int(np.rint(qsize * 0.1)), 3)
-        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))
-        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))
-        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:
-            img = img.crop(crop)
-            quad -= crop[0:2]
-
-        # Pad.
-        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))
-        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))
-        if enable_padding and max(pad) > border - 4:
-            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))
-            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')
-            h, w, _ = img.shape
-            y, x, _ = np.ogrid[:h, :w, :1]
-            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))
-            blur = qsize * 0.02
-            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)
-            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)
-            img = np.uint8(np.clip(np.rint(img), 0, 255))
-            if alpha:
-                mask = 1-np.clip(3.0 * mask, 0.0, 1.0)
-                mask = np.uint8(np.clip(np.rint(mask*255), 0, 255))
-                img = np.concatenate((img, mask), axis=2)
-                img = PIL.Image.fromarray(img, 'RGBA')
-            else:
-                img = PIL.Image.fromarray(img, 'RGB')
-            quad += pad[:2]
-
-        # Transform.
-        img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)
-        if output_size < transform_size:
-            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)
-
-        # Save aligned image.
-        img.save(dst_file, 'PNG')
-
-        dst_file = Path(dst_file)
-        # img.save(f"{dst_file.parent}/{dst_file.stem}.jpg")
+from pathlib import Path
+import numpy as np
+import scipy.ndimage
+import os
+import PIL.Image
+
+
+def image_align(src_file,
+                dst_file,
+                face_landmarks,
+                output_size=1024,
+                transform_size=4096,
+                enable_padding=True,
+                x_scale=1,
+                y_scale=1,
+                em_scale=0.1,
+                alpha=False):
+        # Align function from FFHQ dataset pre-processing step
+        # https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py
+
+        lm = np.array(face_landmarks)
+        lm_chin          = lm[0  : 17]  # left-right
+        lm_eyebrow_left  = lm[17 : 22]  # left-right
+        lm_eyebrow_right = lm[22 : 27]  # left-right
+        lm_nose          = lm[27 : 31]  # top-down
+        lm_nostrils      = lm[31 : 36]  # top-down
+        lm_eye_left      = lm[36 : 42]  # left-clockwise
+        lm_eye_right     = lm[42 : 48]  # left-clockwise
+        lm_mouth_outer   = lm[48 : 60]  # left-clockwise
+        lm_mouth_inner   = lm[60 : 68]  # left-clockwise
+
+        # Calculate auxiliary vectors.
+        eye_left     = np.mean(lm_eye_left, axis=0)
+        eye_right    = np.mean(lm_eye_right, axis=0)
+        eye_avg      = (eye_left + eye_right) * 0.5
+        eye_to_eye   = eye_right - eye_left
+        mouth_left   = lm_mouth_outer[0]
+        mouth_right  = lm_mouth_outer[6]
+        mouth_avg    = (mouth_left + mouth_right) * 0.5
+        eye_to_mouth = mouth_avg - eye_avg
+
+        # Choose oriented crop rectangle.
+        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]
+        x /= np.hypot(*x)
+        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)
+        x *= x_scale
+        y = np.flipud(x) * [-y_scale, y_scale]
+        c = eye_avg + eye_to_mouth * em_scale
+        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])
+        qsize = np.hypot(*x) * 2
+
+        # Load in-the-wild image.
+        if not os.path.isfile(src_file):
+            print('\nCannot find source image. Please run "--wilds" before "--align".')
+            return
+        img = PIL.Image.open(src_file).convert('RGBA').convert('RGB')
+
+        # Shrink.
+        shrink = int(np.floor(qsize / output_size * 0.5))
+        if shrink > 1:
+            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))
+            img = img.resize(rsize, PIL.Image.ANTIALIAS)
+            quad /= shrink
+            qsize /= shrink
+
+        # Crop.
+        border = max(int(np.rint(qsize * 0.1)), 3)
+        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))
+        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))
+        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:
+            img = img.crop(crop)
+            quad -= crop[0:2]
+
+        # Pad.
+        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))
+        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))
+        if enable_padding and max(pad) > border - 4:
+            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))
+            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')
+            h, w, _ = img.shape
+            y, x, _ = np.ogrid[:h, :w, :1]
+            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))
+            blur = qsize * 0.02
+            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)
+            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)
+            img = np.uint8(np.clip(np.rint(img), 0, 255))
+            if alpha:
+                mask = 1-np.clip(3.0 * mask, 0.0, 1.0)
+                mask = np.uint8(np.clip(np.rint(mask*255), 0, 255))
+                img = np.concatenate((img, mask), axis=2)
+                img = PIL.Image.fromarray(img, 'RGBA')
+            else:
+                img = PIL.Image.fromarray(img, 'RGB')
+            quad += pad[:2]
+
+        # Transform.
+        img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)
+        if output_size < transform_size:
+            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)
+
+        # Save aligned image.
+        img.save(dst_file, 'PNG')
+
+        dst_file = Path(dst_file)
+        # img.save(f"{dst_file.parent}/{dst_file.stem}.jpg")
```

### Comparing `tl2-0.1.0/tl2/proj/dlib/ffhq_face_align/landmarks_detector.py` & `tl2-0.1.1/tl2/proj/dlib/ffhq_face_align/landmarks_detector.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-import dlib
-
-
-class LandmarksDetector:
-    def __init__(self, predictor_model_path):
-        """
-        :param predictor_model_path: path to shape_predictor_68_face_landmarks.dat file
-        """
-        self.detector = dlib.get_frontal_face_detector() # cnn_face_detection_model_v1 also can be used
-        self.shape_predictor = dlib.shape_predictor(predictor_model_path)
-
-    def get_landmarks(self, image):
-        img = dlib.load_rgb_image(image)
-        dets = self.detector(img, 1)
-
-        for detection in dets:
-            try:
-                face_landmarks = [(item.x, item.y) for item in self.shape_predictor(img, detection).parts()]
-                yield face_landmarks
-            except:
-                print("Exception in get_landmarks()!")
+import dlib
+
+
+class LandmarksDetector:
+    def __init__(self, predictor_model_path):
+        """
+        :param predictor_model_path: path to shape_predictor_68_face_landmarks.dat file
+        """
+        self.detector = dlib.get_frontal_face_detector() # cnn_face_detection_model_v1 also can be used
+        self.shape_predictor = dlib.shape_predictor(predictor_model_path)
+
+    def get_landmarks(self, image):
+        img = dlib.load_rgb_image(image)
+        dets = self.detector(img, 1)
+
+        for detection in dets:
+            try:
+                face_landmarks = [(item.x, item.y) for item in self.shape_predictor(img, detection).parts()]
+                yield face_landmarks
+            except:
+                print("Exception in get_landmarks()!")
```

### Comparing `tl2-0.1.0/tl2/proj/dlib/tests/test_dlib.py` & `tl2-0.1.1/tl2/proj/dlib/tests/test_dlib.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,182 +1,182 @@
-import os
-import sys
-import unittest
-
-
-class Testing_dlib_web(unittest.TestCase):
-
-  def test_align_face(self, debug=True):
-    """
-    Usage:
-
-        export CUDA_VISIBLE_DEVICES=0
-        export TIME_STR=1
-        export PYTHONPATH=.:./tl2_lib
-        python -c "from tl2_lib.tl2.proj.dlib.tests.test_dlib import Testing_dlib_web;\
-          Testing_dlib_web().test_align_face(debug=False)" \
-          --tl_opts port 8503
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/dlib/configs/dlib_web.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-
-    script = "tl2_lib/tl2/proj/streamlit/scripts/run_web.py"
-    if debug:
-      cmd_str = f"""
-          python 
-            {script}
-            {get_append_cmd_str(args)}
-            --tl_debug
-            --tl_opts
-              """
-    else:
-      cmd_str_prefix = f"""
-              {os.path.dirname(sys.executable)}/streamlit run --server.port {cfg.port} 
-              {script}
-              --
-            """
-      cmd_str = f"""
-          {cmd_str_prefix}
-            {get_append_cmd_str(args)}
-            --tl_opts {tl_opts}
-        """
-    start_cmd_run(cmd_str)
-    pass
-
-  def test__convexHull(self, debug=True):
-    """
-    Usage:
-
-        export CUDA_VISIBLE_DEVICES=0
-        export TIME_STR=1
-        export PYTHONPATH=.:./tl2_lib
-        python -c "from tl2_lib.tl2.proj.dlib.tests.test_dlib import Testing_dlib_web;\
-          Testing_dlib_web().test_crop_face_by_landmarks(debug=False)" \
-          --tl_opts port 8561
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    import cv2
-    import numpy as np
-    from tl2.proj.cv2 import cv2_utils
-
-    import cv2
-
-    # 读取图片并转至灰度模式
-    imagepath = 'tl2_lib/tl2/proj/dlib/datasets/hand_convex_hull.jpg'
-    img = cv2.imread(imagepath, 1)
-    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
-
-    # 二值化，取阈值为235
-    ret, thresh = cv2.threshold(gray, 235, 255, cv2.THRESH_BINARY)
-
-    # 寻找图像中的轮廓
-    contours, hierarchy = cv2.findContours(thresh, 2, 1)
-
-    # 寻找物体的凸包并绘制凸包的轮廓
-    for cnt in contours:
-      # cnt: (num_points, 1, 2)
-      hull = cv2.convexHull(cnt)
-      length = len(hull)
-      # 如果凸包点集中的点个数大于5
-      if length > 5:
-        # 绘制图像凸包的轮廓
-        for i in range(length):
-          cv2.line(img, tuple(hull[i][0]), tuple(hull[(i + 1) % length][0]), (0, 0, 255), 2)
-
-    cv2_utils.imshow_pil(img, is_bgr=True)
-    pass
-
-  def test_crop_face_by_landmarks(self, debug=True):
-    """
-    Usage:
-
-        export CUDA_VISIBLE_DEVICES=0
-        export TIME_STR=1
-        export PYTHONPATH=.:./tl2_lib
-        python -c "from tl2_lib.tl2.proj.dlib.tests.test_dlib import Testing_dlib_web;\
-          Testing_dlib_web().test_crop_face_by_landmarks(debug=False)" \
-          --tl_opts port 8561
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/dlib/configs/dlib_web.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-
-    script = "tl2_lib/tl2/proj/streamlit/scripts/run_web.py"
-    if debug:
-      cmd_str = f"""
-          python 
-            {script}
-            {get_append_cmd_str(args)}
-            --tl_debug
-            --tl_opts
-              """
-    else:
-      cmd_str_prefix = f"""
-              {os.path.dirname(sys.executable)}/streamlit run --server.port {cfg.port} 
-              {script}
-              --
-            """
-      cmd_str = f"""
-          {cmd_str_prefix}
-            {get_append_cmd_str(args)}
-            --tl_opts {tl_opts}
-        """
-    start_cmd_run(cmd_str)
-    pass
-
-
+import os
+import sys
+import unittest
+
+
+class Testing_dlib_web(unittest.TestCase):
+
+  def test_align_face(self, debug=True):
+    """
+    Usage:
+
+        export CUDA_VISIBLE_DEVICES=0
+        export TIME_STR=1
+        export PYTHONPATH=.:./tl2_lib
+        python -c "from tl2_lib.tl2.proj.dlib.tests.test_dlib import Testing_dlib_web;\
+          Testing_dlib_web().test_align_face(debug=False)" \
+          --tl_opts port 8503
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/dlib/configs/dlib_web.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+
+    script = "tl2_lib/tl2/proj/streamlit/scripts/run_web.py"
+    if debug:
+      cmd_str = f"""
+          python 
+            {script}
+            {get_append_cmd_str(args)}
+            --tl_debug
+            --tl_opts
+              """
+    else:
+      cmd_str_prefix = f"""
+              {os.path.dirname(sys.executable)}/streamlit run --server.port {cfg.port} 
+              {script}
+              --
+            """
+      cmd_str = f"""
+          {cmd_str_prefix}
+            {get_append_cmd_str(args)}
+            --tl_opts {tl_opts}
+        """
+    start_cmd_run(cmd_str)
+    pass
+
+  def test__convexHull(self, debug=True):
+    """
+    Usage:
+
+        export CUDA_VISIBLE_DEVICES=0
+        export TIME_STR=1
+        export PYTHONPATH=.:./tl2_lib
+        python -c "from tl2_lib.tl2.proj.dlib.tests.test_dlib import Testing_dlib_web;\
+          Testing_dlib_web().test_crop_face_by_landmarks(debug=False)" \
+          --tl_opts port 8561
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    import cv2
+    import numpy as np
+    from tl2.proj.cv2 import cv2_utils
+
+    import cv2
+
+    # 读取图片并转至灰度模式
+    imagepath = 'tl2_lib/tl2/proj/dlib/datasets/hand_convex_hull.jpg'
+    img = cv2.imread(imagepath, 1)
+    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
+
+    # 二值化，取阈值为235
+    ret, thresh = cv2.threshold(gray, 235, 255, cv2.THRESH_BINARY)
+
+    # 寻找图像中的轮廓
+    contours, hierarchy = cv2.findContours(thresh, 2, 1)
+
+    # 寻找物体的凸包并绘制凸包的轮廓
+    for cnt in contours:
+      # cnt: (num_points, 1, 2)
+      hull = cv2.convexHull(cnt)
+      length = len(hull)
+      # 如果凸包点集中的点个数大于5
+      if length > 5:
+        # 绘制图像凸包的轮廓
+        for i in range(length):
+          cv2.line(img, tuple(hull[i][0]), tuple(hull[(i + 1) % length][0]), (0, 0, 255), 2)
+
+    cv2_utils.imshow_pil(img, is_bgr=True)
+    pass
+
+  def test_crop_face_by_landmarks(self, debug=True):
+    """
+    Usage:
+
+        export CUDA_VISIBLE_DEVICES=0
+        export TIME_STR=1
+        export PYTHONPATH=.:./tl2_lib
+        python -c "from tl2_lib.tl2.proj.dlib.tests.test_dlib import Testing_dlib_web;\
+          Testing_dlib_web().test_crop_face_by_landmarks(debug=False)" \
+          --tl_opts port 8561
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/dlib/configs/dlib_web.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+
+    script = "tl2_lib/tl2/proj/streamlit/scripts/run_web.py"
+    if debug:
+      cmd_str = f"""
+          python 
+            {script}
+            {get_append_cmd_str(args)}
+            --tl_debug
+            --tl_opts
+              """
+    else:
+      cmd_str_prefix = f"""
+              {os.path.dirname(sys.executable)}/streamlit run --server.port {cfg.port} 
+              {script}
+              --
+            """
+      cmd_str = f"""
+          {cmd_str_prefix}
+            {get_append_cmd_str(args)}
+            --tl_opts {tl_opts}
+        """
+    start_cmd_run(cmd_str)
+    pass
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/einops/resources/test_images.npy` & `tl2-0.1.1/tl2/proj/einops/resources/test_images.npy`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/einops/tests/test_einops.py` & `tl2-0.1.1/tl2/proj/einops/tests/test_einops.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,625 +1,625 @@
-import os
-import sys
-import unittest
-import argparse
-
-
-class Testing_einops(unittest.TestCase):
-
-  def test_rearrange(self, debug=True):
-    """
-    Usage:
-        proj_root=pi-GAN-exp
-        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
-        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
-        pip install -e tl2_lib
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts test0 10 test1 11 --test 1
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import numpy as np
-    from einops import rearrange, reduce, repeat
-    from tl2.proj.pil import pil_utils
-
-    ims = np.load('tl2_lib/tl2/proj/einops/resources/test_images.npy', allow_pickle=False)
-    # There are 6 images of shape 96x96 with 3 color channels packed into tensor
-    print(ims.shape, ims.dtype)
-    # (6, 96, 96, 3) float64
-
-    pil_utils.imshow_np(ims[0], range01=True, title='img[0]')
-
-    # rearrange, as its name suggests, rearranges elements
-    # below we swapped height and width.
-    # In other words, transposed first two axes (dimensions)
-    img = rearrange(ims[0], 'h w c -> w h c')
-    pil_utils.imshow_np(img, range01=True, title='h w c -> w h c')
-
-    # einops allows seamlessly composing batch and height to a new height dimension
-    # We just rendered all images by collapsing to 3d tensor!
-    img = rearrange(ims, 'b h w c -> (b h) w c')
-    pil_utils.imshow_np(img, range01=True, title='b h w c -> (b h) w c')
-
-    # or compose a new dimension of batch and width
-    img = rearrange(ims, 'b h w c -> h (b w) c')
-    pil_utils.imshow_np(img, range01=True, title='b h w c -> h (b w) c')
-
-    # finally, combine composition and decomposition:
-    img = rearrange(ims, '(b1 b2) h w c -> (b1 h) (b2 w) c', b1=2)
-    pil_utils.imshow_np(img, range01=True, title='(b1 b2) h w c -> (b1 h) (b2 w) c')
-
-    # slightly different composition: b1 is merged with width, b2 with height
-    # ... so letters are ordered by w then by h
-    img = rearrange(ims, '(b1 b2) h w c -> (b2 h) (b1 w) c ', b1=2)
-    pil_utils.imshow_np(img, range01=True, title='(b1 b2) h w c -> (b2 h) (b1 w) c')
-
-    # move part of width dimension to height.
-    # we should call this width-to-height as image width shrunk by 2 and height doubled.
-    # but all pixels are the same!
-    # Can you write reverse operation (height-to-width)?
-    img = rearrange(ims, 'b h (w w2) c -> (h w2) (b w) c', w2=2)
-    pil_utils.imshow_np(img, range01=True, title='b h (w w2) c -> (h w2) (b w) c')
-
-    # compare with the next example
-    img = rearrange(ims, 'b h w c -> h (w b) c')
-    pil_utils.imshow_np(img, range01=True, title='b h w c -> h (w b) c')
-
-    # what if b1 and b2 are reordered before composing to width?
-    img = rearrange(ims, '(b1 b2) h w c -> h (b1 b2 w) c ', b1=2)  # produces 'einops'
-    pil_utils.imshow_np(img, range01=True, title='(b1 b2) h w c -> h (b1 b2 w) c ')
-    img = rearrange(ims, '(b1 b2) h w c -> h (b2 b1 w) c ', b1=2)  # produces 'eoipns'
-    pil_utils.imshow_np(img, range01=True, title='(b1 b2) h w c -> h (b2 b1 w) c ')
-
-    pass
-
-  def test_reduce(self, debug=True):
-    """
-    Usage:
-        proj_root=pi-GAN-exp
-        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
-        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
-        pip install -e tl2_lib
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts test0 10 test1 11 --test 1
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import numpy as np
-    from einops import rearrange, reduce, repeat
-    from tl2.proj.pil import pil_utils
-
-    ims = np.load('tl2_lib/tl2/proj/einops/resources/test_images.npy', allow_pickle=False)
-    # There are 6 images of shape 96x96 with 3 color channels packed into tensor
-    print(ims.shape, ims.dtype)
-    # (6, 96, 96, 3) float64
-
-    pil_utils.imshow_np(ims[0], range01=True, title='img[0]')
-
-    # if axis is not present in the output — you guessed it — axis was reduced
-    # average over batch
-    img = reduce(ims, 'b h w c -> h w c', 'mean')
-    pil_utils.imshow_np(img, range01=True, title='b h w c -> h w c, mean')
-
-    # this is mean-pooling with 2x2 kernel
-    # image is split into 2x2 patches, each patch is averaged
-    img = reduce(ims, 'b (h h2) (w w2) c -> h (b w) c', 'mean', h2=2, w2=2)
-    pil_utils.imshow_np(img, range01=True, title='b (h h2) (w w2) c -> h (b w) c, mean_pooling 2x2')
-
-    # yet another example. Can you compute result shape?
-    img = reduce(ims, '(b1 b2) h w c -> (b2 h) (b1 w)', 'mean', b1=2)
-    pil_utils.imshow_np(img, range01=True, title='(b1 b2) h w c -> (b2 h) (b1 w)')
-
-    # compute max in each image individually, then show a difference
-    tmp = reduce(ims, 'b h w c -> b () () c', 'max')
-    x = reduce(ims, 'b h w c -> b () () c', 'max') - ims
-    img = rearrange(x, 'b h w c -> h (b w) c')
-    pil_utils.imshow_np(img, range01=True, title='b h w c -> h (b w) c, max - img')
-    pass
-
-  def test_stack_concatenate(self, debug=True):
-    """
-    Usage:
-        proj_root=pi-GAN-exp
-        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
-        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
-        pip install -e tl2_lib
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts test0 10 test1 11 --test 1
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import numpy as np
-    from einops import rearrange, reduce, repeat
-    from tl2.proj.pil import pil_utils
-
-    ims = np.load('tl2_lib/tl2/proj/einops/resources/test_images.npy', allow_pickle=False)
-    # There are 6 images of shape 96x96 with 3 color channels packed into tensor
-    print(ims.shape, ims.dtype)
-    # (6, 96, 96, 3) float64
-
-    pil_utils.imshow_np(ims[0], range01=True, title='img[0]')
-
-    # rearrange can also take care of lists of arrays with the same shape
-    x = list(ims)
-    print(type(x), 'with', len(x), 'tensors of shape', x[0].shape)
-    # that's how we can stack inputs
-    # "list axis" becomes first ("b" in this case), and we left it there
-    img = rearrange(x, 'b h w c -> b h w c')
-
-    # ... or we can concatenate along axes
-    img = rearrange(x, 'b h w c -> h (b w) c')
-    pass
-
-  def test_expand_squeeze(self, debug=True):
-    """
-    Usage:
-        proj_root=pi-GAN-exp
-        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
-        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
-        pip install -e tl2_lib
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts test0 10 test1 11 --test 1
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import numpy as np
-    from einops import rearrange, reduce, repeat
-    from tl2.proj.pil import pil_utils
-
-    ims = np.load('tl2_lib/tl2/proj/einops/resources/test_images.npy', allow_pickle=False)
-    # There are 6 images of shape 96x96 with 3 color channels packed into tensor
-    print(ims.shape, ims.dtype)
-    # (6, 96, 96, 3) float64
-
-    # functionality of numpy.expand_dims
-    x = rearrange(ims, 'b h w c -> b 1 h w 1 c')
-    print(x.shape)
-    # functionality of numpy.squeeze
-    print(rearrange(x, 'b 1 h w 1 c -> b h w c').shape)
-
-    pass
-
-  def test_repeat(self, debug=True):
-    """
-    Usage:
-        proj_root=pi-GAN-exp
-        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
-        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
-        pip install -e tl2_lib
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts test0 10 test1 11 --test 1
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import numpy as np
-    from einops import rearrange, reduce, repeat
-    from tl2.proj.pil import pil_utils
-
-    ims = np.load('tl2_lib/tl2/proj/einops/resources/test_images.npy', allow_pickle=False)
-    # There are 6 images of shape 96x96 with 3 color channels packed into tensor
-    print(ims.shape, ims.dtype)
-    # (6, 96, 96, 3) float64
-
-    # repeat along a new axis. New axis can be placed anywhere
-    img = repeat(ims[0], 'h w c -> h new_axis w c', new_axis=5)
-
-    # shortcut
-    img = repeat(ims[0], 'h w c -> h 5 w c')
-
-    # repeat along w (existing axis)
-    img = repeat(ims[0], 'h w c -> h (repeat w) c', repeat=3)
-    pil_utils.imshow_np(img, range01=True, title='h w c -> h (3 w) c')
-
-    # repeat along two existing axes
-    img = repeat(ims[0], 'h w c -> (2 h) (2 w) c')
-    pil_utils.imshow_np(img, range01=True, title='h w c -> (2 h) (2 w) c')
-
-    # order of axes matters as usual - you can repeat each element (pixel) 3 times
-    # by changing order in parenthesis
-    img = repeat(ims[0], 'h w c -> h (w repeat) c', repeat=3)
-    pil_utils.imshow_np(img, range01=True, title='h w c -> h (w 3) c')
-
-    pass
-
-
-class Testing_einops_DL(unittest.TestCase):
-
-  def test_common_ops(self, debug=True):
-    """
-    Usage:
-        proj_root=pi-GAN-exp
-        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
-        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
-        pip install -e tl2_lib
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts test0 10 test1 11 --test 1
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import numpy as np
-    from einops import rearrange, reduce, repeat
-    import torch
-    from tl2.proj.pil import pil_utils
-
-    x = np.random.RandomState(42).normal(size=[10, 32, 100, 200])
-    x = torch.from_numpy(x)
-    x.requires_grad = True
-
-    # flatten
-    y = rearrange(x, 'b c h w -> b (c h w)')
-
-    # space-to-depth
-    y = rearrange(x, 'b c (h h1) (w w1) -> b (h1 w1 c) h w', h1=2, w1=2)
-
-    # depth-to-space
-    y = rearrange(x, 'b (c h1 w1) h w -> b c (h h1) (w w1)', h1=2, w1=2)
-
-    # global average pooling
-    y = reduce(x, 'b c h w -> b c', reduction='mean')
-
-    # max-pooling with a kernel 2x2
-    y = reduce(x, 'b c (h h1) (w w1) -> b c h w', reduction='max', h1=2, w1=2)
-    y = reduce(x, 'b c (h 2) (w 2) -> b c h w', reduction='max')
-
-    # 1D
-    # reduce(x, '(t 2) b c -> t b c', reduction='max')
-
-    # volumetric
-    # reduce(x, 'b c (x 2) (y 2) (z 2) -> b c x y z', reduction='max')
-
-    # per-channel mean-normalization
-    y = x - reduce(x, 'b c h w -> b c 1 1', 'mean')
-
-    # per-channel mean-normalization for whole batch
-    y = x - reduce(y, 'b c h w -> 1 c 1 1', 'mean')
-
-    list_of_tensors = list(x)
-
-    # concatenate over the first dimension
-    tensors = rearrange(list_of_tensors, 'b c h w -> (b h) w c')
-    # or maybe concatenate along last dimension?
-    tensors = rearrange(list_of_tensors, 'b c h w -> h w (b c)')
-
-    # channel shuffle
-    y = rearrange(x, 'b (g1 g2 c) h w-> b (g2 g1 c) h w', g1=4, g2=4)
-    y = rearrange(x, 'b (g c) h w-> b (c g) h w', g=4)
-
-    # Split a dimension
-    # Assume we got 8 bboxes, 4 coordinates each.
-    # To get coordinated into 4 separate variables, you move corresponding dimension to front and unpack tuple
-    bbox_x, bbox_y, bbox_w, bbox_h = rearrange(x, 'b (coord bbox) h w -> coord b bbox h w', coord=4, bbox=8)
-    # now you can operate on individual variables
-    max_bbox_area = reduce(bbox_w * bbox_h, 'b bbox h w -> b h w', 'max')
-
-    pass
-
-  def test_parse_shape(self, debug=True):
-    """
-    Usage:
-        proj_root=pi-GAN-exp
-        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
-        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
-        pip install -e tl2_lib
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts test0 10 test1 11 --test 1
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import numpy as np
-    from einops import rearrange, reduce, repeat
-    from einops import parse_shape
-
-    x = np.random.RandomState(42).normal(size=[10, 32, 100, 200])
-
-    out = parse_shape(x, 'b c h w')
-    out = parse_shape(x, 'b c _ _')
-    pass
-
-  def test_Layers(self, debug=True):
-    """
-    Usage:
-        proj_root=pi-GAN-exp
-        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
-        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
-        pip install -e tl2_lib
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts test0 10 test1 11 --test 1
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import numpy as np
-    import torch
-    from torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU
-    from einops.layers.torch import Reduce, Rearrange
-    from einops import rearrange, reduce, repeat
-    from einops import parse_shape
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-
-    x = np.random.RandomState(42).normal(size=[10, 3, 20, 20])
-    x = x.astype(np.float32)
-    x = torch.from_numpy(x)
-
-    tmp = rearrange(x, "b c h w -> b c (h w)")
-
-    model = Sequential(
-      Conv2d(3, 6, kernel_size=3, padding=1),
-      MaxPool2d(kernel_size=2),
-      Conv2d(6, 16, kernel_size=3, padding=1),
-      # combined pooling and flattening in a single step
-      Reduce('b c (h 2) (w 2) -> b (c h w)', 'mean'),
-      Linear(16 * 5 * 5, 120),
-      ReLU(),
-      Linear(120, 10),
-    )
-
-    model_ver = VerboseModel(model, name_padding=15, input_padding=40, output_padding=40)
-    out = model_ver(x)
-
-    pass
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
+import os
+import sys
+import unittest
+import argparse
+
+
+class Testing_einops(unittest.TestCase):
+
+  def test_rearrange(self, debug=True):
+    """
+    Usage:
+        proj_root=pi-GAN-exp
+        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
+        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
+        pip install -e tl2_lib
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts test0 10 test1 11 --test 1
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import numpy as np
+    from einops import rearrange, reduce, repeat
+    from tl2.proj.pil import pil_utils
+
+    ims = np.load('tl2_lib/tl2/proj/einops/resources/test_images.npy', allow_pickle=False)
+    # There are 6 images of shape 96x96 with 3 color channels packed into tensor
+    print(ims.shape, ims.dtype)
+    # (6, 96, 96, 3) float64
+
+    pil_utils.imshow_np(ims[0], range01=True, title='img[0]')
+
+    # rearrange, as its name suggests, rearranges elements
+    # below we swapped height and width.
+    # In other words, transposed first two axes (dimensions)
+    img = rearrange(ims[0], 'h w c -> w h c')
+    pil_utils.imshow_np(img, range01=True, title='h w c -> w h c')
+
+    # einops allows seamlessly composing batch and height to a new height dimension
+    # We just rendered all images by collapsing to 3d tensor!
+    img = rearrange(ims, 'b h w c -> (b h) w c')
+    pil_utils.imshow_np(img, range01=True, title='b h w c -> (b h) w c')
+
+    # or compose a new dimension of batch and width
+    img = rearrange(ims, 'b h w c -> h (b w) c')
+    pil_utils.imshow_np(img, range01=True, title='b h w c -> h (b w) c')
+
+    # finally, combine composition and decomposition:
+    img = rearrange(ims, '(b1 b2) h w c -> (b1 h) (b2 w) c', b1=2)
+    pil_utils.imshow_np(img, range01=True, title='(b1 b2) h w c -> (b1 h) (b2 w) c')
+
+    # slightly different composition: b1 is merged with width, b2 with height
+    # ... so letters are ordered by w then by h
+    img = rearrange(ims, '(b1 b2) h w c -> (b2 h) (b1 w) c ', b1=2)
+    pil_utils.imshow_np(img, range01=True, title='(b1 b2) h w c -> (b2 h) (b1 w) c')
+
+    # move part of width dimension to height.
+    # we should call this width-to-height as image width shrunk by 2 and height doubled.
+    # but all pixels are the same!
+    # Can you write reverse operation (height-to-width)?
+    img = rearrange(ims, 'b h (w w2) c -> (h w2) (b w) c', w2=2)
+    pil_utils.imshow_np(img, range01=True, title='b h (w w2) c -> (h w2) (b w) c')
+
+    # compare with the next example
+    img = rearrange(ims, 'b h w c -> h (w b) c')
+    pil_utils.imshow_np(img, range01=True, title='b h w c -> h (w b) c')
+
+    # what if b1 and b2 are reordered before composing to width?
+    img = rearrange(ims, '(b1 b2) h w c -> h (b1 b2 w) c ', b1=2)  # produces 'einops'
+    pil_utils.imshow_np(img, range01=True, title='(b1 b2) h w c -> h (b1 b2 w) c ')
+    img = rearrange(ims, '(b1 b2) h w c -> h (b2 b1 w) c ', b1=2)  # produces 'eoipns'
+    pil_utils.imshow_np(img, range01=True, title='(b1 b2) h w c -> h (b2 b1 w) c ')
+
+    pass
+
+  def test_reduce(self, debug=True):
+    """
+    Usage:
+        proj_root=pi-GAN-exp
+        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
+        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
+        pip install -e tl2_lib
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts test0 10 test1 11 --test 1
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import numpy as np
+    from einops import rearrange, reduce, repeat
+    from tl2.proj.pil import pil_utils
+
+    ims = np.load('tl2_lib/tl2/proj/einops/resources/test_images.npy', allow_pickle=False)
+    # There are 6 images of shape 96x96 with 3 color channels packed into tensor
+    print(ims.shape, ims.dtype)
+    # (6, 96, 96, 3) float64
+
+    pil_utils.imshow_np(ims[0], range01=True, title='img[0]')
+
+    # if axis is not present in the output — you guessed it — axis was reduced
+    # average over batch
+    img = reduce(ims, 'b h w c -> h w c', 'mean')
+    pil_utils.imshow_np(img, range01=True, title='b h w c -> h w c, mean')
+
+    # this is mean-pooling with 2x2 kernel
+    # image is split into 2x2 patches, each patch is averaged
+    img = reduce(ims, 'b (h h2) (w w2) c -> h (b w) c', 'mean', h2=2, w2=2)
+    pil_utils.imshow_np(img, range01=True, title='b (h h2) (w w2) c -> h (b w) c, mean_pooling 2x2')
+
+    # yet another example. Can you compute result shape?
+    img = reduce(ims, '(b1 b2) h w c -> (b2 h) (b1 w)', 'mean', b1=2)
+    pil_utils.imshow_np(img, range01=True, title='(b1 b2) h w c -> (b2 h) (b1 w)')
+
+    # compute max in each image individually, then show a difference
+    tmp = reduce(ims, 'b h w c -> b () () c', 'max')
+    x = reduce(ims, 'b h w c -> b () () c', 'max') - ims
+    img = rearrange(x, 'b h w c -> h (b w) c')
+    pil_utils.imshow_np(img, range01=True, title='b h w c -> h (b w) c, max - img')
+    pass
+
+  def test_stack_concatenate(self, debug=True):
+    """
+    Usage:
+        proj_root=pi-GAN-exp
+        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
+        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
+        pip install -e tl2_lib
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts test0 10 test1 11 --test 1
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import numpy as np
+    from einops import rearrange, reduce, repeat
+    from tl2.proj.pil import pil_utils
+
+    ims = np.load('tl2_lib/tl2/proj/einops/resources/test_images.npy', allow_pickle=False)
+    # There are 6 images of shape 96x96 with 3 color channels packed into tensor
+    print(ims.shape, ims.dtype)
+    # (6, 96, 96, 3) float64
+
+    pil_utils.imshow_np(ims[0], range01=True, title='img[0]')
+
+    # rearrange can also take care of lists of arrays with the same shape
+    x = list(ims)
+    print(type(x), 'with', len(x), 'tensors of shape', x[0].shape)
+    # that's how we can stack inputs
+    # "list axis" becomes first ("b" in this case), and we left it there
+    img = rearrange(x, 'b h w c -> b h w c')
+
+    # ... or we can concatenate along axes
+    img = rearrange(x, 'b h w c -> h (b w) c')
+    pass
+
+  def test_expand_squeeze(self, debug=True):
+    """
+    Usage:
+        proj_root=pi-GAN-exp
+        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
+        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
+        pip install -e tl2_lib
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts test0 10 test1 11 --test 1
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import numpy as np
+    from einops import rearrange, reduce, repeat
+    from tl2.proj.pil import pil_utils
+
+    ims = np.load('tl2_lib/tl2/proj/einops/resources/test_images.npy', allow_pickle=False)
+    # There are 6 images of shape 96x96 with 3 color channels packed into tensor
+    print(ims.shape, ims.dtype)
+    # (6, 96, 96, 3) float64
+
+    # functionality of numpy.expand_dims
+    x = rearrange(ims, 'b h w c -> b 1 h w 1 c')
+    print(x.shape)
+    # functionality of numpy.squeeze
+    print(rearrange(x, 'b 1 h w 1 c -> b h w c').shape)
+
+    pass
+
+  def test_repeat(self, debug=True):
+    """
+    Usage:
+        proj_root=pi-GAN-exp
+        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
+        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
+        pip install -e tl2_lib
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts test0 10 test1 11 --test 1
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import numpy as np
+    from einops import rearrange, reduce, repeat
+    from tl2.proj.pil import pil_utils
+
+    ims = np.load('tl2_lib/tl2/proj/einops/resources/test_images.npy', allow_pickle=False)
+    # There are 6 images of shape 96x96 with 3 color channels packed into tensor
+    print(ims.shape, ims.dtype)
+    # (6, 96, 96, 3) float64
+
+    # repeat along a new axis. New axis can be placed anywhere
+    img = repeat(ims[0], 'h w c -> h new_axis w c', new_axis=5)
+
+    # shortcut
+    img = repeat(ims[0], 'h w c -> h 5 w c')
+
+    # repeat along w (existing axis)
+    img = repeat(ims[0], 'h w c -> h (repeat w) c', repeat=3)
+    pil_utils.imshow_np(img, range01=True, title='h w c -> h (3 w) c')
+
+    # repeat along two existing axes
+    img = repeat(ims[0], 'h w c -> (2 h) (2 w) c')
+    pil_utils.imshow_np(img, range01=True, title='h w c -> (2 h) (2 w) c')
+
+    # order of axes matters as usual - you can repeat each element (pixel) 3 times
+    # by changing order in parenthesis
+    img = repeat(ims[0], 'h w c -> h (w repeat) c', repeat=3)
+    pil_utils.imshow_np(img, range01=True, title='h w c -> h (w 3) c')
+
+    pass
+
+
+class Testing_einops_DL(unittest.TestCase):
+
+  def test_common_ops(self, debug=True):
+    """
+    Usage:
+        proj_root=pi-GAN-exp
+        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
+        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
+        pip install -e tl2_lib
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts test0 10 test1 11 --test 1
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import numpy as np
+    from einops import rearrange, reduce, repeat
+    import torch
+    from tl2.proj.pil import pil_utils
+
+    x = np.random.RandomState(42).normal(size=[10, 32, 100, 200])
+    x = torch.from_numpy(x)
+    x.requires_grad = True
+
+    # flatten
+    y = rearrange(x, 'b c h w -> b (c h w)')
+
+    # space-to-depth
+    y = rearrange(x, 'b c (h h1) (w w1) -> b (h1 w1 c) h w', h1=2, w1=2)
+
+    # depth-to-space
+    y = rearrange(x, 'b (c h1 w1) h w -> b c (h h1) (w w1)', h1=2, w1=2)
+
+    # global average pooling
+    y = reduce(x, 'b c h w -> b c', reduction='mean')
+
+    # max-pooling with a kernel 2x2
+    y = reduce(x, 'b c (h h1) (w w1) -> b c h w', reduction='max', h1=2, w1=2)
+    y = reduce(x, 'b c (h 2) (w 2) -> b c h w', reduction='max')
+
+    # 1D
+    # reduce(x, '(t 2) b c -> t b c', reduction='max')
+
+    # volumetric
+    # reduce(x, 'b c (x 2) (y 2) (z 2) -> b c x y z', reduction='max')
+
+    # per-channel mean-normalization
+    y = x - reduce(x, 'b c h w -> b c 1 1', 'mean')
+
+    # per-channel mean-normalization for whole batch
+    y = x - reduce(y, 'b c h w -> 1 c 1 1', 'mean')
+
+    list_of_tensors = list(x)
+
+    # concatenate over the first dimension
+    tensors = rearrange(list_of_tensors, 'b c h w -> (b h) w c')
+    # or maybe concatenate along last dimension?
+    tensors = rearrange(list_of_tensors, 'b c h w -> h w (b c)')
+
+    # channel shuffle
+    y = rearrange(x, 'b (g1 g2 c) h w-> b (g2 g1 c) h w', g1=4, g2=4)
+    y = rearrange(x, 'b (g c) h w-> b (c g) h w', g=4)
+
+    # Split a dimension
+    # Assume we got 8 bboxes, 4 coordinates each.
+    # To get coordinated into 4 separate variables, you move corresponding dimension to front and unpack tuple
+    bbox_x, bbox_y, bbox_w, bbox_h = rearrange(x, 'b (coord bbox) h w -> coord b bbox h w', coord=4, bbox=8)
+    # now you can operate on individual variables
+    max_bbox_area = reduce(bbox_w * bbox_h, 'b bbox h w -> b h w', 'max')
+
+    pass
+
+  def test_parse_shape(self, debug=True):
+    """
+    Usage:
+        proj_root=pi-GAN-exp
+        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
+        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
+        pip install -e tl2_lib
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts test0 10 test1 11 --test 1
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import numpy as np
+    from einops import rearrange, reduce, repeat
+    from einops import parse_shape
+
+    x = np.random.RandomState(42).normal(size=[10, 32, 100, 200])
+
+    out = parse_shape(x, 'b c h w')
+    out = parse_shape(x, 'b c _ _')
+    pass
+
+  def test_Layers(self, debug=True):
+    """
+    Usage:
+        proj_root=pi-GAN-exp
+        python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
+        cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
+        pip install -e tl2_lib
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts test0 10 test1 11 --test 1
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import numpy as np
+    import torch
+    from torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU
+    from einops.layers.torch import Reduce, Rearrange
+    from einops import rearrange, reduce, repeat
+    from einops import parse_shape
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+
+    x = np.random.RandomState(42).normal(size=[10, 3, 20, 20])
+    x = x.astype(np.float32)
+    x = torch.from_numpy(x)
+
+    tmp = rearrange(x, "b c h w -> b c (h w)")
+
+    model = Sequential(
+      Conv2d(3, 6, kernel_size=3, padding=1),
+      MaxPool2d(kernel_size=2),
+      Conv2d(6, 16, kernel_size=3, padding=1),
+      # combined pooling and flattening in a single step
+      Reduce('b c (h 2) (w 2) -> b (c h w)', 'mean'),
+      Linear(16 * 5 * 5, 120),
+      ReLU(),
+      Linear(120, 10),
+    )
+
+    model_ver = VerboseModel(model, name_padding=15, input_padding=40, output_padding=40)
+    out = model_ver(x)
+
+    pass
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/fvcore/__pycache__/config.cpython-38.pyc` & `tl2-0.1.1/tl2/proj/fvcore/__pycache__/config.cpython-38.pyc`

 * *Format-specific differences are supported for Python .pyc files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: Byte-compiled Python module for CPython 3.8, timestamp-based, .py timestamp: Mon Oct  3 09:27:55 2022 UTC, .py size: 4729 bytes*

 * *Could not decompile bytecode: bad marshal data (unknown type code)*

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-00000000: 550d 0d0a 0000 0000 1bab 3a63 7912 0000  U.........:cy...
+00000000: 550d 0d0a 0000 0000 f13c 9362 2e13 0000  U........<.b....
 00000010: e300 0000 0000 0000 0000 0000 0000 0000  ................
 00000020: 0004 0000 0040 0000 0073 a000 0000 6400  .....@...s....d.
 00000030: 6401 6c00 6d01 5a01 6d02 5a02 6d03 5a03  d.l.m.Z.m.Z.m.Z.
 00000040: 0100 6400 6402 6c04 5a04 6400 6402 6c05  ..d.d.l.Z.d.d.l.
 00000050: 5a05 6400 6402 6c06 5a06 6400 6402 6c07  Z.d.d.l.Z.d.d.l.
 00000060: 5a07 6400 6402 6c08 5a08 6400 6402 6c09  Z.d.d.l.Z.d.d.l.
 00000070: 5a0a 6400 6402 6c0b 5a0b 6400 6402 6c0c  Z.d.d.l.Z.d.d.l.
@@ -39,290 +39,288 @@
 00000260: 696e 6865 7269 7420 6b65 7920 277b 7d27  inherit key '{}'
 00000270: 2066 726f 6d20 6261 7365 2129 07da 0569   from base!)...i
 00000280: 7465 6d73 da0a 6973 696e 7374 616e 6365  tems..isinstance
 00000290: da04 6469 6374 da0e 4173 7365 7274 696f  ..dict..Assertio
 000002a0: 6e45 7272 6f72 da06 666f 726d 6174 7207  nError..formatr.
 000002b0: 0000 00da 0e6d 6572 6765 5f61 5f69 6e74  .....merge_a_int
 000002c0: 6f5f 6229 04da 0161 da01 62da 016b da01  o_b)...a..b..k..
-000002d0: 76a9 0072 1300 0000 fa53 2f68 6f6d 652f  v..r.....S/home/
+000002d0: 76a9 0072 1300 0000 fa35 2f68 6f6d 652f  v..r.....5/home/
 000002e0: 6d61 2d75 7365 722f 776f 726b 2f63 6f64  ma-user/work/cod
-000002f0: 652f 7374 796c 6567 616e 322d 6164 612d  e/stylegan2-ada-
-00000300: 7079 746f 7263 682d 6578 702f 746c 325f  pytorch-exp/tl2_
-00000310: 6c69 622f 746c 322f 7072 6f6a 2f66 7663  lib/tl2/proj/fvc
-00000320: 6f72 652f 636f 6e66 6967 2e70 7972 0e00  ore/config.pyr..
-00000330: 0000 1400 0000 7314 0000 0000 0310 0112  ......s.........
-00000340: 0102 0106 0002 ff06 0208 fe04 0312 027a  ...............z
-00000350: 1854 4c43 6667 4e6f 6465 2e6d 6572 6765  .TLCfgNode.merge
-00000360: 5f61 5f69 6e74 6f5f 6263 0200 0000 0000  _a_into_bc......
-00000370: 0000 0000 0000 0500 0000 0500 0000 4300  ..............C.
-00000380: 0000 737c 0000 007c 01a0 00a1 007d 017c  ..s|...|.....}.|
-00000390: 0044 005d 247d 027c 00a0 017c 02a1 017d  .D.]$}.|...|...}
-000003a0: 0374 027c 0374 0383 0272 0c74 04a0 057c  .t.|.t...r.t...|
-000003b0: 037c 01a1 0201 0071 0c74 067c 006b 0672  .|.....q.t.|.k.r
-000003c0: 787c 01a0 017c 0074 0619 00a1 017d 047c  x|...|.t.....}.|
-000003d0: 0074 063d 0074 04a0 057c 047c 01a1 0201  .t.=.t...|.|....
-000003e0: 0074 04a0 077c 007c 04a1 0201 007c 00a0  .t...|.|.....|..
-000003f0: 08a1 0001 007c 00a0 097c 04a1 0101 0064  .....|...|.....d
-00000400: 0053 00a9 014e 290a da05 636c 6f6e 65da  .S...N)...clone.
-00000410: 0367 6574 720a 0000 0072 0b00 0000 7207  .getr....r....r.
-00000420: 0000 00da 0f5f 6d65 7267 655f 6261 7365  ....._merge_base
-00000430: 5f63 6667 da08 4241 5345 5f4b 4559 720e  _cfg..BASE_KEYr.
-00000440: 0000 00da 0563 6c65 6172 da06 7570 6461  .....clear..upda
-00000450: 7465 2905 da03 6366 67da 0a6c 6f61 6465  te)...cfg..loade
-00000460: 645f 6366 675a 0773 7562 5f6b 6579 5a07  d_cfgZ.sub_keyZ.
-00000470: 7375 625f 6366 675a 0862 6173 655f 6366  sub_cfgZ.base_cf
-00000480: 6772 1300 0000 7213 0000 0072 1400 0000  gr....r....r....
-00000490: 7218 0000 0020 0000 0073 1800 0000 0002  r.... ...s......
-000004a0: 0802 0801 0a01 0a01 0e02 0801 0e01 0601  ................
-000004b0: 0c01 0c01 0801 7a19 544c 4366 674e 6f64  ......z.TLCfgNod
-000004c0: 652e 5f6d 6572 6765 5f62 6173 655f 6366  e._merge_base_cf
-000004d0: 6746 2901 da0c 6366 675f 6669 6c65 6e61  gF)...cfg_filena
-000004e0: 6d65 6302 0000 0000 0000 0000 0000 0004  mec.............
-000004f0: 0000 0004 0000 0043 0000 0073 1a00 0000  .......C...s....
-00000500: 7400 6a01 7c00 7c01 6401 8d02 7d02 7400  t.j.|.|.d...}.t.
-00000510: 7c02 8301 7d03 7c03 5300 2902 fa07 0a20  |...}.|.S.).... 
-00000520: 2020 2020 2029 01da 0c61 6c6c 6f77 5f75       )...allow_u
-00000530: 6e73 6166 6529 0272 0700 0000 5a13 6c6f  nsafe).r....Z.lo
-00000540: 6164 5f79 616d 6c5f 7769 7468 5f62 6173  ad_yaml_with_bas
-00000550: 6529 0472 1e00 0000 7220 0000 0072 1d00  e).r....r ...r..
-00000560: 0000 721c 0000 0072 1300 0000 7213 0000  ..r....r....r...
-00000570: 0072 1400 0000 da0e 6c6f 6164 5f79 616d  .r......load_yam
-00000580: 6c5f 6669 6c65 3200 0000 7306 0000 0000  l_file2...s.....
-00000590: 040e 0108 017a 1854 4c43 6667 4e6f 6465  .....z.TLCfgNode
-000005a0: 2e6c 6f61 645f 7961 6d6c 5f66 696c 6529  .load_yaml_file)
-000005b0: 0372 1e00 0000 da07 636f 6d6d 616e 6472  .r......commandr
-000005c0: 2000 0000 6303 0000 0000 0000 0000 0000   ...c...........
-000005d0: 0005 0000 0004 0000 0043 0000 0073 3000  .........C...s0.
-000005e0: 0000 7400 6a01 7c00 7c02 6401 8d02 7d03  ..t.j.|.|.d...}.
-000005f0: 7c03 a002 7c01 a101 7d04 7400 a003 7c04  |...|...}.t...|.
-00000600: 7c03 a102 0100 7400 7c04 8301 7d04 7c04  |.....t.|...}.|.
-00000610: 5300 2902 721f 0000 0029 0272 1e00 0000  S.).r....).r....
-00000620: 7220 0000 0029 0472 0700 0000 7221 0000  r ...).r....r!..
-00000630: 0072 1700 0000 7218 0000 0029 0572 1e00  .r....r....).r..
-00000640: 0000 7222 0000 0072 2000 0000 721d 0000  ..r"...r ...r...
-00000650: 00da 0b63 6f6d 6d61 6e64 5f63 6667 7213  ...command_cfgr.
-00000660: 0000 0072 1300 0000 7214 0000 00da 166c  ...r....r......l
-00000670: 6f61 645f 7961 6d6c 5f77 6974 685f 636f  oad_yaml_with_co
-00000680: 6d6d 616e 643a 0000 0073 0a00 0000 0007  mmand:...s......
-00000690: 0e02 0a01 0c02 0801 7a20 544c 4366 674e  ........z TLCfgN
-000006a0: 6f64 652e 6c6f 6164 5f79 616d 6c5f 7769  ode.load_yaml_wi
-000006b0: 7468 5f63 6f6d 6d61 6e64 6302 0000 0000  th_commandc.....
-000006c0: 0000 0000 0000 0003 0000 0009 0000 0043  ...............C
-000006d0: 0000 0073 2a00 0000 7400 7c01 6401 8302  ...s*...t.|.d...
-000006e0: 8f16 7d02 7c00 6a01 7c02 6402 6403 6404  ..}.|.j.|.d.d.d.
-000006f0: 8d03 0100 5700 3500 5100 5200 5800 6400  ....W.5.Q.R.X.d.
-00000700: 5300 2905 4eda 0177 46e9 0200 0000 a903  S.).N..wF.......
-00000710: da06 7374 7265 616d da09 736f 7274 5f6b  ..stream..sort_k
-00000720: 6579 73da 0669 6e64 656e 7429 02da 046f  eys..indent)...o
-00000730: 7065 6eda 0464 756d 7029 03da 0473 656c  pen..dump)...sel
-00000740: 66da 0a73 6176 6564 5f66 696c 65da 0166  f..saved_file..f
-00000750: 7213 0000 0072 1300 0000 7214 0000 00da  r....r....r.....
-00000760: 0c64 756d 705f 746f 5f66 696c 6549 0000  .dump_to_fileI..
-00000770: 0073 0400 0000 0001 0c01 7a16 544c 4366  .s........z.TLCf
-00000780: 674e 6f64 652e 6475 6d70 5f74 6f5f 6669  gNode.dump_to_fi
-00000790: 6c65 6303 0000 0000 0000 0000 0000 0005  lec.............
-000007a0: 0000 0009 0000 0043 0000 0073 4000 0000  .......C...s@...
-000007b0: 7400 6401 6402 8d01 7d03 7401 7c03 7c02  t.d.d...}.t.|.|.
-000007c0: 7c00 8303 0100 7402 7c01 6403 8302 8f16  |.....t.|.d.....
-000007d0: 7d04 7c03 6a03 7c04 6404 6405 6406 8d03  }.|.j.|.d.d.d...
-000007e0: 0100 5700 3500 5100 5200 5800 6400 5300  ..W.5.Q.R.X.d.S.
-000007f0: 2907 4e54 2901 da0b 6e65 775f 616c 6c6f  ).NT)...new_allo
-00000800: 7765 6472 2500 0000 4672 2600 0000 7227  wedr%...Fr&...r'
-00000810: 0000 0029 0472 0700 0000 da07 7365 7461  ...).r......seta
-00000820: 7474 7272 2b00 0000 722c 0000 0029 0572  ttrr+...r,...).r
-00000830: 2d00 0000 722e 0000 0072 2200 0000 7223  -...r....r"...r#
-00000840: 0000 0072 2f00 0000 7213 0000 0072 1300  ...r/...r....r..
-00000850: 0000 7214 0000 00da 1964 756d 705f 746f  ..r......dump_to
-00000860: 5f66 696c 655f 7769 7468 5f63 6f6d 6d61  _file_with_comma
-00000870: 6e64 4d00 0000 7308 0000 0000 010a 010c  ndM...s.........
-00000880: 010c 017a 2354 4c43 6667 4e6f 6465 2e64  ...z#TLCfgNode.d
-00000890: 756d 705f 746f 5f66 696c 655f 7769 7468  ump_to_file_with
-000008a0: 5f63 6f6d 6d61 6e64 6302 0000 0000 0000  _commandc.......
-000008b0: 0000 0000 0004 0000 0004 0000 000b 0000  ................
-000008c0: 0073 1e00 0000 7400 7401 7c00 8302 6a02  .s....t.t.|...j.
-000008d0: 6600 6401 6402 6901 7c02 9702 8e01 7d03  f.d.d.i.|.....}.
-000008e0: 7c03 5300 2903 4e72 2900 0000 4629 03da  |.S.).Nr)...F)..
-000008f0: 0573 7570 6572 7207 0000 0072 2c00 0000  .superr....r,...
-00000900: 2904 722d 0000 0072 2900 0000 da06 6b77  ).r-...r).....kw
-00000910: 6172 6773 5a08 6475 6d70 5f73 7472 a901  argsZ.dump_str..
-00000920: da09 5f5f 636c 6173 735f 5f72 1300 0000  ..__class__r....
-00000930: 7214 0000 0072 2c00 0000 5300 0000 7304  r....r,...S...s.
-00000940: 0000 0000 011a 017a 0e54 4c43 6667 4e6f  .......z.TLCfgNo
-00000950: 6465 2e64 756d 7063 0300 0000 0000 0000  de.dumpc........
-00000960: 0000 0000 0800 0000 0600 0000 0300 0000  ................
-00000970: 73a4 0000 007c 0272 907c 0164 0064 0064  s....|.r.|.d.d.d
-00000980: 0185 0319 0044 005d 7c7d 037c 03a0 0064  .....D.]|}.|...d
-00000990: 02a1 017d 047c 007d 0574 017c 0483 0144  ...}.|.}.t.|...D
-000009a0: 005d 605c 027d 067d 077c 077c 056b 0772  .]`\.}.}.|.|.k.r
-000009b0: 727c 0674 027c 0483 0164 0318 006b 0372  r|.t.|...d...k.r
-000009c0: 667c 05a0 037c 0774 0483 00a1 0201 007c  f|...|.t.......|
-000009d0: 05a0 057c 07a1 017d 0571 8c7c 05a0 037c  ...|...}.q.|...|
-000009e0: 07a1 0101 0071 2c7c 0674 027c 0483 0164  .....q,|.t.|...d
-000009f0: 0318 006b 0372 2c7c 05a0 057c 07a1 017d  ...k.r,|...|...}
-00000a00: 0571 2c71 1274 0674 047c 0083 02a0 077c  .q,q.t.t.|.....|
-00000a10: 01a1 0101 007c 0053 0029 044e 7226 0000  .....|.S.).Nr&..
-00000a20: 00da 012e e901 0000 0029 08da 0573 706c  .........)...spl
-00000a30: 6974 da09 656e 756d 6572 6174 65da 036c  it..enumerate..l
-00000a40: 656e da0a 7365 7464 6566 6175 6c74 7207  en..setdefaultr.
-00000a50: 0000 0072 1700 0000 7234 0000 00da 0f6d  ...r....r4.....m
-00000a60: 6572 6765 5f66 726f 6d5f 6c69 7374 2908  erge_from_list).
-00000a70: 722d 0000 005a 086f 7074 5f6c 6973 7472  r-...Z.opt_listr
-00000a80: 3100 0000 7211 0000 005a 0a73 7562 5f6b  1...r....Z.sub_k
-00000a90: 5f6c 6973 745a 0763 7572 5f63 6667 da03  _listZ.cur_cfg..
-00000aa0: 6964 785a 0573 7562 5f6b 7236 0000 0072  idxZ.sub_kr6...r
-00000ab0: 1300 0000 7214 0000 0072 3e00 0000 5700  ....r....r>...W.
-00000ac0: 0000 731c 0000 0000 0204 0112 010a 0104  ..s.............
-00000ad0: 0110 0108 0110 010e 010c 020c 0210 010e  ................
-00000ae0: 0110 017a 1954 4c43 6667 4e6f 6465 2e6d  ...z.TLCfgNode.m
-00000af0: 6572 6765 5f66 726f 6d5f 6c69 7374 6302  erge_from_listc.
-00000b00: 0000 0000 0000 0000 0000 0003 0000 0003  ................
-00000b10: 0000 0043 0000 0073 1600 0000 7400 7c01  ...C...s....t.|.
-00000b20: 8301 7d02 7c00 a001 7c02 a101 0100 6400  ..}.|...|.....d.
-00000b30: 5300 7215 0000 0029 0272 0700 0000 721b  S.r....).r....r.
-00000b40: 0000 0029 0372 2d00 0000 da08 6366 675f  ...).r-.....cfg_
-00000b50: 6469 6374 721c 0000 0072 1300 0000 7213  dictr....r....r.
-00000b60: 0000 0072 1400 0000 da0f 6d65 7267 655f  ...r......merge_
-00000b70: 6672 6f6d 5f64 6963 746a 0000 0073 0400  from_dictj...s..
-00000b80: 0000 0001 0801 7a19 544c 4366 674e 6f64  ......z.TLCfgNod
-00000b90: 652e 6d65 7267 655f 6672 6f6d 5f64 6963  e.merge_from_dic
-00000ba0: 7463 0100 0000 0000 0000 0000 0000 0300  tc..............
-00000bb0: 0000 0400 0000 4300 0000 731e 0000 0064  ......C...s....d
-00000bc0: 0164 026c 006d 017d 0101 007c 01a0 027c  .d.l.m.}...|...|
-00000bd0: 00a0 03a1 00a1 017d 027c 0253 0029 034e  .......}.|.S.).N
-00000be0: 7201 0000 0029 01da 094f 6d65 6761 436f  r....)...OmegaCo
-00000bf0: 6e66 2904 5a09 6f6d 6567 6163 6f6e 6672  nf).Z.omegaconfr
-00000c00: 4200 0000 da06 6372 6561 7465 722c 0000  B.....creater,..
-00000c10: 0029 0372 2d00 0000 7242 0000 0072 4000  .).r-...rB...r@.
-00000c20: 0000 7213 0000 0072 1300 0000 7214 0000  ..r....r....r...
-00000c30: 00da 0c64 756d 705f 746f 5f64 6963 746e  ...dump_to_dictn
-00000c40: 0000 0073 0600 0000 0001 0c01 0e01 7a16  ...s..........z.
-00000c50: 544c 4366 674e 6f64 652e 6475 6d70 5f74  TLCfgNode.dump_t
-00000c60: 6f5f 6469 6374 2901 da06 7265 7475 726e  o_dict)...return
-00000c70: 6301 0000 0000 0000 0000 0000 0003 0000  c...............
-00000c80: 0005 0000 0043 0000 0073 4800 0000 6900  .....C...sH...i.
-00000c90: 7d01 7c00 a000 a100 4400 5d36 7d02 7401  }.|.....D.]6}.t.
-00000ca0: 7c00 a002 7c02 a101 7403 8302 7234 7c00  |...|...t...r4|.
-00000cb0: a002 7c02 a101 a004 a100 7c01 7c02 3c00  ..|.......|.|.<.
-00000cc0: 710c 7c00 a002 7c02 a101 7c01 7c02 3c00  q.|...|...|.|.<.
-00000cd0: 710c 7c01 5300 7215 0000 0029 05da 046b  q.|.S.r....)...k
-00000ce0: 6579 7372 0a00 0000 7217 0000 0072 0700  eysr....r....r..
-00000cf0: 0000 da07 746f 5f64 6963 7429 0372 2d00  ....to_dict).r-.
-00000d00: 0000 da06 7265 7375 6c74 da03 6b65 7972  ....result..keyr
-00000d10: 1300 0000 7213 0000 0072 1400 0000 7247  ....r....r....rG
-00000d20: 0000 0073 0000 0073 0c00 0000 0001 0402  ...s...s........
-00000d30: 0c01 1001 1402 1002 7a11 544c 4366 674e  ........z.TLCfgN
-00000d40: 6f64 652e 746f 5f64 6963 74da 0374 6c5f  ode.to_dict..tl_
-00000d50: 6302 0000 0000 0000 0000 0000 0005 0000  c...............
-00000d60: 0004 0000 0043 0000 0073 3200 0000 6900  .....C...s2...i.
-00000d70: 7d02 7c00 a000 a100 7d03 7c03 4400 5d1c  }.|.....}.|.D.].
-00000d80: 7d04 7c04 a001 7c01 a101 7310 7c03 a002  }.|...|...s.|...
-00000d90: 7c04 a101 7c02 7c04 3c00 7110 7c02 5300  |...|.|.<.q.|.S.
-00000da0: 7215 0000 0029 0372 1600 0000 da0a 7374  r....).r......st
-00000db0: 6172 7473 7769 7468 7217 0000 0029 0572  artswithr....).r
-00000dc0: 2d00 0000 da06 7072 6566 6978 7235 0000  -.....prefixr5..
-00000dd0: 0072 1c00 0000 7211 0000 0072 1300 0000  .r....r....r....
-00000de0: 7213 0000 0072 1400 0000 da0d 6465 6c5f  r....r......del_
-00000df0: 746c 5f70 7265 6669 787e 0000 0073 0c00  tl_prefix~...s..
-00000e00: 0000 0001 0401 0801 0801 0a01 1001 7a17  ..............z.
-00000e10: 544c 4366 674e 6f64 652e 6465 6c5f 746c  TLCfgNode.del_tl
-00000e20: 5f70 7265 6669 7829 0146 2901 4629 0146  _prefix).F).F).F
-00000e30: 2901 4629 0172 4a00 0000 2915 da08 5f5f  ).F).rJ...)...__
-00000e40: 6e61 6d65 5f5f da0a 5f5f 6d6f 6475 6c65  name__..__module
-00000e50: 5f5f da0c 5f5f 7175 616c 6e61 6d65 5f5f  __..__qualname__
-00000e60: da07 5f5f 646f 635f 5fda 0c73 7461 7469  ..__doc__..stati
-00000e70: 636d 6574 686f 6472 0e00 0000 7218 0000  cmethodr....r...
-00000e80: 00da 0373 7472 7221 0000 00da 0462 6f6f  ...strr!.....boo
-00000e90: 6c72 2400 0000 7230 0000 0072 3300 0000  lr$...r0...r3...
-00000ea0: 722c 0000 0072 3e00 0000 7241 0000 0072  r,...r>...rA...r
-00000eb0: 4400 0000 7204 0000 0072 4700 0000 724d  D...r....rG...rM
-00000ec0: 0000 00da 0d5f 5f63 6c61 7373 6365 6c6c  .....__classcell
-00000ed0: 5f5f 7213 0000 0072 1300 0000 7236 0000  __r....r....r6..
-00000ee0: 0072 1400 0000 7207 0000 0011 0000 0073  .r....r........s
-00000ef0: 2c00 0000 0801 0402 0201 0a0b 0201 0a11  ,...............
-00000f00: 0201 1207 0204 00fd 0201 0201 0201 02fd  ................
-00000f10: 0e0e 0804 0806 0e04 0e13 0804 0805 0e0b  ................
-00000f20: 7207 0000 0029 0272 1c00 0000 7245 0000  r....).r....rE..
-00000f30: 0063 0100 0000 0000 0000 0000 0000 0100  .c..............
-00000f40: 0000 0300 0000 4300 0000 7316 0000 0074  ......C...s....t
-00000f50: 00a0 01a1 0001 0074 00a0 027c 00a1 0101  .......t...|....
-00000f60: 0064 0053 0072 1500 0000 2903 da0a 676c  .d.S.r....)...gl
-00000f70: 6f62 616c 5f63 6667 721a 0000 0072 1b00  obal_cfgr....r..
-00000f80: 0000 2901 721c 0000 0072 1300 0000 7213  ..).r....r....r.
-00000f90: 0000 0072 1400 0000 da0e 7365 745f 676c  ...r......set_gl
-00000fa0: 6f62 616c 5f63 6667 8a00 0000 7306 0000  obal_cfg....s...
-00000fb0: 0000 0208 010a 0172 5700 0000 6300 0000  .......rW...c...
-00000fc0: 0000 0000 0000 0000 0000 0000 0002 0000  ................
-00000fd0: 0040 0000 0073 1c00 0000 6500 5a01 6400  .@...s....e.Z.d.
-00000fe0: 5a02 6401 6402 8400 5a03 6403 6404 8400  Z.d.d...Z.d.d...
-00000ff0: 5a04 6405 5300 2906 da0e 5465 7374 5f54  Z.d.S.)...Test_T
-00001000: 4c43 6667 4e6f 6465 6301 0000 0000 0000  LCfgNodec.......
-00001010: 0000 0000 0004 0000 0004 0000 0043 0000  .............C..
-00001020: 0073 2600 0000 6401 7d01 6402 7d02 7400  .s&...d.}.d.}.t.
-00001030: 6a01 7c01 7c02 6403 8d02 7d03 7402 7c03  j.|.|.d...}.t.|.
-00001040: a003 a100 8301 0100 6404 5300 2905 7208  ........d.S.).r.
-00001050: 0000 00fa 2674 6c32 2f70 726f 6a2f 6676  ....&tl2/proj/fv
-00001060: 636f 7265 2f63 6f6e 6669 6773 2f54 4c43  core/configs/TLC
-00001070: 6667 4e6f 6465 2e79 616d 6cda 0574 6573  fgNode.yaml..tes
-00001080: 7432 a901 7222 0000 004e 2904 7207 0000  t2..r"...N).r...
-00001090: 0072 2400 0000 da05 7072 696e 7472 2c00  .r$.....printr,.
-000010a0: 0000 2904 722d 0000 00da 0b63 6f6e 6669  ..).r-.....confi
-000010b0: 675f 7961 6d6c 7222 0000 0072 1c00 0000  g_yamlr"...r....
-000010c0: 7213 0000 0072 1300 0000 7214 0000 00da  r....r....r.....
-000010d0: 1b74 6573 745f 6c6f 6164 5f79 616d 6c5f  .test_load_yaml_
-000010e0: 7769 7468 5f63 6f6d 6d61 6e64 9400 0000  with_command....
-000010f0: 730a 0000 0000 0304 0104 020e 010c 017a  s..............z
-00001100: 2a54 6573 745f 544c 4366 674e 6f64 652e  *Test_TLCfgNode.
-00001110: 7465 7374 5f6c 6f61 645f 7961 6d6c 5f77  test_load_yaml_w
-00001120: 6974 685f 636f 6d6d 616e 6463 0100 0000  ith_commandc....
-00001130: 0000 0000 0000 0000 0900 0000 0600 0000  ................
-00001140: 4300 0000 7386 0000 0064 017d 017c 01a0  C...s....d.}.|..
-00001150: 00a1 007d 0264 027d 0364 037d 0464 0464  ...}.d.}.d.}.d.d
-00001160: 056c 016d 027d 0501 007c 0583 007d 067c  .l.m.}...|...}.|
-00001170: 066a 0364 0674 0464 0767 0064 088d 0401  .j.d.t.d.g.d....
-00001180: 007c 066a 0364 0974 0464 0a8d 0201 007c  .|.j.d.t.d.....|
-00001190: 06a0 057c 02a1 017d 0774 066a 077c 037c  ...|...}.t.j.|.|
-000011a0: 0464 0b8d 027d 0874 087c 08a0 09a1 0083  .d...}.t.|......
-000011b0: 0101 007c 08a0 0a7c 076a 0ba1 0101 0074  ...|...|.j.....t
-000011c0: 087c 08a0 09a1 0083 0101 0064 0c53 0029  .|.........d.S.)
-000011d0: 0d72 0800 0000 7a24 2d2d 746c 5f6f 7074  .r....z$--tl_opt
-000011e0: 7320 7465 7374 312e 7465 7374 3020 3130  s test1.test0 10
-000011f0: 202d 2d74 6573 7420 7465 7374 7259 0000   --test testrY..
-00001200: 0072 5a00 0000 7201 0000 0029 01da 0e41  .rZ...r....)...A
-00001210: 7267 756d 656e 7450 6172 7365 727a 092d  rgumentParserz.-
-00001220: 2d74 6c5f 6f70 7473 da01 2a29 03da 0474  -tl_opts..*)...t
-00001230: 7970 65da 056e 6172 6773 da07 6465 6661  ype..nargs..defa
-00001240: 756c 747a 062d 2d74 6573 7429 0172 6100  ultz.--test).ra.
-00001250: 0000 725b 0000 004e 290c 723a 0000 00da  ..r[...N).r:....
-00001260: 0861 7267 7061 7273 6572 5f00 0000 da0c  .argparser_.....
-00001270: 6164 645f 6172 6775 6d65 6e74 7253 0000  add_argumentrS..
-00001280: 00da 0a70 6172 7365 5f61 7267 7372 0700  ...parse_argsr..
-00001290: 0000 7224 0000 0072 5c00 0000 722c 0000  ..r$...r\...r,..
-000012a0: 0072 3e00 0000 da07 746c 5f6f 7074 7329  .r>.....tl_opts)
-000012b0: 0972 2d00 0000 da08 6172 6776 5f73 7472  .r-.....argv_str
-000012c0: da09 6172 6776 5f6c 6973 7472 5d00 0000  ..argv_listr]...
-000012d0: 7222 0000 0072 5f00 0000 da06 7061 7273  r"...r_.....pars
-000012e0: 6572 da04 6172 6773 721c 0000 0072 1300  er..argsr....r..
-000012f0: 0000 7213 0000 0072 1400 0000 da0f 7465  ..r....r......te
-00001300: 7374 5f6d 6572 6765 5f6c 6973 749e 0000  st_merge_list...
-00001310: 0073 1c00 0000 0003 0401 0802 0401 0402  .s..............
-00001320: 0c01 0601 1201 0e01 0a02 0e01 0c02 0c01  ................
-00001330: 0c01 7a1e 5465 7374 5f54 4c43 6667 4e6f  ..z.Test_TLCfgNo
-00001340: 6465 2e74 6573 745f 6d65 7267 655f 6c69  de.test_merge_li
-00001350: 7374 4e29 0572 4e00 0000 724f 0000 0072  stN).rN...rO...r
-00001360: 5000 0000 725e 0000 0072 6c00 0000 7213  P...r^...rl...r.
-00001370: 0000 0072 1300 0000 7213 0000 0072 1400  ...r....r....r..
-00001380: 0000 7258 0000 0092 0000 0073 0400 0000  ..rX.......s....
-00001390: 0802 080a 7258 0000 0029 16da 0674 7970  ....rX...)...typ
-000013a0: 696e 6772 0200 0000 7203 0000 0072 0400  ingr....r....r..
-000013b0: 0000 da03 7379 73da 0479 616d 6cda 076c  ....sys..yaml..l
-000013c0: 6f67 6769 6e67 da02 6f73 da04 6d61 7468  ogging..os..math
-000013d0: da05 6e75 6d70 79da 026e 70da 0474 696d  ..numpy..np..tim
-000013e0: 65da 0875 6e69 7474 6573 745a 1466 7663  e..unittestZ.fvc
-000013f0: 6f72 652e 636f 6d6d 6f6e 2e63 6f6e 6669  ore.common.confi
-00001400: 6772 0500 0000 5a08 5f43 6667 4e6f 6465  gr....Z._CfgNode
-00001410: 7219 0000 0072 0700 0000 7256 0000 0072  r....r....rV...r
-00001420: 5700 0000 da08 5465 7374 4361 7365 7258  W.....TestCaserX
-00001430: 0000 0072 1300 0000 7213 0000 0072 1300  ...r....r....r..
-00001440: 0000 7214 0000 00da 083c 6d6f 6475 6c65  ..r......<module
-00001450: 3e01 0000 0073 1c00 0000 1401 0801 0801  >....s..........
-00001460: 0801 0801 0801 0801 0801 0802 0c03 0403  ................
-00001470: 1077 0602 1008                           .w....
+000002f0: 652f 746c 322f 746c 322f 7072 6f6a 2f66  e/tl2/tl2/proj/f
+00000300: 7663 6f72 652f 636f 6e66 6967 2e70 7972  vcore/config.pyr
+00000310: 0e00 0000 1400 0000 7314 0000 0000 0310  ........s.......
+00000320: 0112 0102 0106 0002 ff06 0208 fe04 0312  ................
+00000330: 027a 1854 4c43 6667 4e6f 6465 2e6d 6572  .z.TLCfgNode.mer
+00000340: 6765 5f61 5f69 6e74 6f5f 6263 0200 0000  ge_a_into_bc....
+00000350: 0000 0000 0000 0000 0500 0000 0500 0000  ................
+00000360: 4300 0000 737c 0000 007c 01a0 00a1 007d  C...s|...|.....}
+00000370: 017c 0044 005d 247d 027c 00a0 017c 02a1  .|.D.]$}.|...|..
+00000380: 017d 0374 027c 0374 0383 0272 0c74 04a0  .}.t.|.t...r.t..
+00000390: 057c 037c 01a1 0201 0071 0c74 067c 006b  .|.|.....q.t.|.k
+000003a0: 0672 787c 01a0 017c 0074 0619 00a1 017d  .rx|...|.t.....}
+000003b0: 047c 0074 063d 0074 04a0 057c 047c 01a1  .|.t.=.t...|.|..
+000003c0: 0201 0074 04a0 077c 007c 04a1 0201 007c  ...t...|.|.....|
+000003d0: 00a0 08a1 0001 007c 00a0 097c 04a1 0101  .......|...|....
+000003e0: 0064 0053 00a9 014e 290a da05 636c 6f6e  .d.S...N)...clon
+000003f0: 65da 0367 6574 720a 0000 0072 0b00 0000  e..getr....r....
+00000400: 7207 0000 00da 0f5f 6d65 7267 655f 6261  r......_merge_ba
+00000410: 7365 5f63 6667 da08 4241 5345 5f4b 4559  se_cfg..BASE_KEY
+00000420: 720e 0000 00da 0563 6c65 6172 da06 7570  r......clear..up
+00000430: 6461 7465 2905 da03 6366 67da 0a6c 6f61  date)...cfg..loa
+00000440: 6465 645f 6366 675a 0773 7562 5f6b 6579  ded_cfgZ.sub_key
+00000450: 5a07 7375 625f 6366 675a 0862 6173 655f  Z.sub_cfgZ.base_
+00000460: 6366 6772 1300 0000 7213 0000 0072 1400  cfgr....r....r..
+00000470: 0000 7218 0000 0020 0000 0073 1800 0000  ..r.... ...s....
+00000480: 0002 0802 0801 0a01 0a01 0e02 0801 0e01  ................
+00000490: 0601 0c01 0c01 0801 7a19 544c 4366 674e  ........z.TLCfgN
+000004a0: 6f64 652e 5f6d 6572 6765 5f62 6173 655f  ode._merge_base_
+000004b0: 6366 6746 2901 da0c 6366 675f 6669 6c65  cfgF)...cfg_file
+000004c0: 6e61 6d65 6302 0000 0000 0000 0000 0000  namec...........
+000004d0: 0004 0000 0004 0000 0043 0000 0073 1a00  .........C...s..
+000004e0: 0000 7400 6a01 7c00 7c01 6401 8d02 7d02  ..t.j.|.|.d...}.
+000004f0: 7400 7c02 8301 7d03 7c03 5300 2902 fa07  t.|...}.|.S.)...
+00000500: 0a20 2020 2020 2029 01da 0c61 6c6c 6f77  .      )...allow
+00000510: 5f75 6e73 6166 6529 0272 0700 0000 5a13  _unsafe).r....Z.
+00000520: 6c6f 6164 5f79 616d 6c5f 7769 7468 5f62  load_yaml_with_b
+00000530: 6173 6529 0472 1e00 0000 7220 0000 0072  ase).r....r ...r
+00000540: 1d00 0000 721c 0000 0072 1300 0000 7213  ....r....r....r.
+00000550: 0000 0072 1400 0000 da0e 6c6f 6164 5f79  ...r......load_y
+00000560: 616d 6c5f 6669 6c65 3200 0000 7306 0000  aml_file2...s...
+00000570: 0000 040e 0108 017a 1854 4c43 6667 4e6f  .......z.TLCfgNo
+00000580: 6465 2e6c 6f61 645f 7961 6d6c 5f66 696c  de.load_yaml_fil
+00000590: 6529 0372 1e00 0000 da07 636f 6d6d 616e  e).r......comman
+000005a0: 6472 2000 0000 6303 0000 0000 0000 0000  dr ...c.........
+000005b0: 0000 0005 0000 0004 0000 0043 0000 0073  ...........C...s
+000005c0: 3000 0000 7400 6a01 7c00 7c02 6401 8d02  0...t.j.|.|.d...
+000005d0: 7d03 7c03 a002 7c01 a101 7d04 7400 a003  }.|...|...}.t...
+000005e0: 7c04 7c03 a102 0100 7400 7c04 8301 7d04  |.|.....t.|...}.
+000005f0: 7c04 5300 2902 721f 0000 0029 0272 1e00  |.S.).r....).r..
+00000600: 0000 7220 0000 0029 0472 0700 0000 7221  ..r ...).r....r!
+00000610: 0000 0072 1700 0000 7218 0000 0029 0572  ...r....r....).r
+00000620: 1e00 0000 7222 0000 0072 2000 0000 721d  ....r"...r ...r.
+00000630: 0000 00da 0b63 6f6d 6d61 6e64 5f63 6667  .....command_cfg
+00000640: 7213 0000 0072 1300 0000 7214 0000 00da  r....r....r.....
+00000650: 166c 6f61 645f 7961 6d6c 5f77 6974 685f  .load_yaml_with_
+00000660: 636f 6d6d 616e 643a 0000 0073 0a00 0000  command:...s....
+00000670: 0007 0e02 0a01 0c02 0801 7a20 544c 4366  ..........z TLCf
+00000680: 674e 6f64 652e 6c6f 6164 5f79 616d 6c5f  gNode.load_yaml_
+00000690: 7769 7468 5f63 6f6d 6d61 6e64 6302 0000  with_commandc...
+000006a0: 0000 0000 0000 0000 0003 0000 0009 0000  ................
+000006b0: 0043 0000 0073 2a00 0000 7400 7c01 6401  .C...s*...t.|.d.
+000006c0: 8302 8f16 7d02 7c00 6a01 7c02 6402 6403  ....}.|.j.|.d.d.
+000006d0: 6404 8d03 0100 5700 3500 5100 5200 5800  d.....W.5.Q.R.X.
+000006e0: 6400 5300 2905 4eda 0177 46e9 0200 0000  d.S.).N..wF.....
+000006f0: a903 da06 7374 7265 616d da09 736f 7274  ....stream..sort
+00000700: 5f6b 6579 73da 0669 6e64 656e 7429 02da  _keys..indent)..
+00000710: 046f 7065 6eda 0464 756d 7029 03da 0473  .open..dump)...s
+00000720: 656c 66da 0a73 6176 6564 5f66 696c 65da  elf..saved_file.
+00000730: 0166 7213 0000 0072 1300 0000 7214 0000  .fr....r....r...
+00000740: 00da 0c64 756d 705f 746f 5f66 696c 6549  ...dump_to_fileI
+00000750: 0000 0073 0400 0000 0001 0c01 7a16 544c  ...s........z.TL
+00000760: 4366 674e 6f64 652e 6475 6d70 5f74 6f5f  CfgNode.dump_to_
+00000770: 6669 6c65 6303 0000 0000 0000 0000 0000  filec...........
+00000780: 0005 0000 0009 0000 0043 0000 0073 4000  .........C...s@.
+00000790: 0000 7400 6401 6402 8d01 7d03 7401 7c03  ..t.d.d...}.t.|.
+000007a0: 7c02 7c00 8303 0100 7402 7c01 6403 8302  |.|.....t.|.d...
+000007b0: 8f16 7d04 7c03 6a03 7c04 6404 6405 6406  ..}.|.j.|.d.d.d.
+000007c0: 8d03 0100 5700 3500 5100 5200 5800 6400  ....W.5.Q.R.X.d.
+000007d0: 5300 2907 4e54 2901 da0b 6e65 775f 616c  S.).NT)...new_al
+000007e0: 6c6f 7765 6472 2500 0000 4672 2600 0000  lowedr%...Fr&...
+000007f0: 7227 0000 0029 0472 0700 0000 da07 7365  r'...).r......se
+00000800: 7461 7474 7272 2b00 0000 722c 0000 0029  tattrr+...r,...)
+00000810: 0572 2d00 0000 722e 0000 0072 2200 0000  .r-...r....r"...
+00000820: 7223 0000 0072 2f00 0000 7213 0000 0072  r#...r/...r....r
+00000830: 1300 0000 7214 0000 00da 1964 756d 705f  ....r......dump_
+00000840: 746f 5f66 696c 655f 7769 7468 5f63 6f6d  to_file_with_com
+00000850: 6d61 6e64 4d00 0000 7308 0000 0000 010a  mandM...s.......
+00000860: 010c 010c 017a 2354 4c43 6667 4e6f 6465  .....z#TLCfgNode
+00000870: 2e64 756d 705f 746f 5f66 696c 655f 7769  .dump_to_file_wi
+00000880: 7468 5f63 6f6d 6d61 6e64 6302 0000 0000  th_commandc.....
+00000890: 0000 0000 0000 0004 0000 0004 0000 000b  ................
+000008a0: 0000 0073 1e00 0000 7400 7401 7c00 8302  ...s....t.t.|...
+000008b0: 6a02 6600 6401 6402 6901 7c02 9702 8e01  j.f.d.d.i.|.....
+000008c0: 7d03 7c03 5300 2903 4e72 2900 0000 4629  }.|.S.).Nr)...F)
+000008d0: 03da 0573 7570 6572 7207 0000 0072 2c00  ...superr....r,.
+000008e0: 0000 2904 722d 0000 0072 2900 0000 da06  ..).r-...r).....
+000008f0: 6b77 6172 6773 5a08 6475 6d70 5f73 7472  kwargsZ.dump_str
+00000900: a901 da09 5f5f 636c 6173 735f 5f72 1300  ....__class__r..
+00000910: 0000 7214 0000 0072 2c00 0000 5300 0000  ..r....r,...S...
+00000920: 7304 0000 0000 011a 017a 0e54 4c43 6667  s........z.TLCfg
+00000930: 4e6f 6465 2e64 756d 7063 0300 0000 0000  Node.dumpc......
+00000940: 0000 0000 0000 0800 0000 0600 0000 0300  ................
+00000950: 0000 73a4 0000 007c 0272 907c 0164 0064  ..s....|.r.|.d.d
+00000960: 0064 0185 0319 0044 005d 7c7d 037c 03a0  .d.....D.]|}.|..
+00000970: 0064 02a1 017d 047c 007d 0574 017c 0483  .d...}.|.}.t.|..
+00000980: 0144 005d 605c 027d 067d 077c 077c 056b  .D.]`\.}.}.|.|.k
+00000990: 0772 727c 0674 027c 0483 0164 0318 006b  .rr|.t.|...d...k
+000009a0: 0372 667c 05a0 037c 0774 0483 00a1 0201  .rf|...|.t......
+000009b0: 007c 05a0 057c 07a1 017d 0571 8c7c 05a0  .|...|...}.q.|..
+000009c0: 037c 07a1 0101 0071 2c7c 0674 027c 0483  .|.....q,|.t.|..
+000009d0: 0164 0318 006b 0372 2c7c 05a0 057c 07a1  .d...k.r,|...|..
+000009e0: 017d 0571 2c71 1274 0674 047c 0083 02a0  .}.q,q.t.t.|....
+000009f0: 077c 01a1 0101 007c 0053 0029 044e 7226  .|.....|.S.).Nr&
+00000a00: 0000 00da 012e e901 0000 0029 08da 0573  ...........)...s
+00000a10: 706c 6974 da09 656e 756d 6572 6174 65da  plit..enumerate.
+00000a20: 036c 656e da0a 7365 7464 6566 6175 6c74  .len..setdefault
+00000a30: 7207 0000 0072 1700 0000 7234 0000 00da  r....r....r4....
+00000a40: 0f6d 6572 6765 5f66 726f 6d5f 6c69 7374  .merge_from_list
+00000a50: 2908 722d 0000 005a 086f 7074 5f6c 6973  ).r-...Z.opt_lis
+00000a60: 7472 3100 0000 7211 0000 005a 0a73 7562  tr1...r....Z.sub
+00000a70: 5f6b 5f6c 6973 745a 0763 7572 5f63 6667  _k_listZ.cur_cfg
+00000a80: da03 6964 785a 0573 7562 5f6b 7236 0000  ..idxZ.sub_kr6..
+00000a90: 0072 1300 0000 7214 0000 0072 3e00 0000  .r....r....r>...
+00000aa0: 5700 0000 731c 0000 0000 0204 0112 010a  W...s...........
+00000ab0: 0104 0110 0108 0110 010e 010c 020c 0210  ................
+00000ac0: 010e 0110 017a 1954 4c43 6667 4e6f 6465  .....z.TLCfgNode
+00000ad0: 2e6d 6572 6765 5f66 726f 6d5f 6c69 7374  .merge_from_list
+00000ae0: 6302 0000 0000 0000 0000 0000 0003 0000  c...............
+00000af0: 0003 0000 0043 0000 0073 1600 0000 7400  .....C...s....t.
+00000b00: 7c01 8301 7d02 7c00 a001 7c02 a101 0100  |...}.|...|.....
+00000b10: 6400 5300 7215 0000 0029 0272 0700 0000  d.S.r....).r....
+00000b20: 721b 0000 0029 0372 2d00 0000 da08 6366  r....).r-.....cf
+00000b30: 675f 6469 6374 721c 0000 0072 1300 0000  g_dictr....r....
+00000b40: 7213 0000 0072 1400 0000 da0f 6d65 7267  r....r......merg
+00000b50: 655f 6672 6f6d 5f64 6963 746a 0000 0073  e_from_dictj...s
+00000b60: 0400 0000 0001 0801 7a19 544c 4366 674e  ........z.TLCfgN
+00000b70: 6f64 652e 6d65 7267 655f 6672 6f6d 5f64  ode.merge_from_d
+00000b80: 6963 7463 0100 0000 0000 0000 0000 0000  ictc............
+00000b90: 0300 0000 0400 0000 4300 0000 731e 0000  ........C...s...
+00000ba0: 0064 0164 026c 006d 017d 0101 007c 01a0  .d.d.l.m.}...|..
+00000bb0: 027c 00a0 03a1 00a1 017d 027c 0253 0029  .|.......}.|.S.)
+00000bc0: 034e 7201 0000 0029 01da 094f 6d65 6761  .Nr....)...Omega
+00000bd0: 436f 6e66 2904 5a09 6f6d 6567 6163 6f6e  Conf).Z.omegacon
+00000be0: 6672 4200 0000 da06 6372 6561 7465 722c  frB.....creater,
+00000bf0: 0000 0029 0372 2d00 0000 7242 0000 0072  ...).r-...rB...r
+00000c00: 4000 0000 7213 0000 0072 1300 0000 7214  @...r....r....r.
+00000c10: 0000 00da 0c64 756d 705f 746f 5f64 6963  .....dump_to_dic
+00000c20: 746e 0000 0073 0600 0000 0001 0c01 0e01  tn...s..........
+00000c30: 7a16 544c 4366 674e 6f64 652e 6475 6d70  z.TLCfgNode.dump
+00000c40: 5f74 6f5f 6469 6374 2901 da06 7265 7475  _to_dict)...retu
+00000c50: 726e 6301 0000 0000 0000 0000 0000 0003  rnc.............
+00000c60: 0000 0005 0000 0043 0000 0073 4800 0000  .......C...sH...
+00000c70: 6900 7d01 7c00 a000 a100 4400 5d36 7d02  i.}.|.....D.]6}.
+00000c80: 7401 7c00 a002 7c02 a101 7403 8302 7234  t.|...|...t...r4
+00000c90: 7c00 a002 7c02 a101 a004 a100 7c01 7c02  |...|.......|.|.
+00000ca0: 3c00 710c 7c00 a002 7c02 a101 7c01 7c02  <.q.|...|...|.|.
+00000cb0: 3c00 710c 7c01 5300 7215 0000 0029 05da  <.q.|.S.r....)..
+00000cc0: 046b 6579 7372 0a00 0000 7217 0000 0072  .keysr....r....r
+00000cd0: 0700 0000 da07 746f 5f64 6963 7429 0372  ......to_dict).r
+00000ce0: 2d00 0000 da06 7265 7375 6c74 da03 6b65  -.....result..ke
+00000cf0: 7972 1300 0000 7213 0000 0072 1400 0000  yr....r....r....
+00000d00: 7247 0000 0073 0000 0073 0c00 0000 0001  rG...s...s......
+00000d10: 0402 0c01 1001 1402 1002 7a11 544c 4366  ..........z.TLCf
+00000d20: 674e 6f64 652e 746f 5f64 6963 74da 0374  gNode.to_dict..t
+00000d30: 6c5f 6302 0000 0000 0000 0000 0000 0005  l_c.............
+00000d40: 0000 0004 0000 0043 0000 0073 3200 0000  .......C...s2...
+00000d50: 6900 7d02 7c00 a000 a100 7d03 7c03 4400  i.}.|.....}.|.D.
+00000d60: 5d1c 7d04 7c04 a001 7c01 a101 7310 7c03  ].}.|...|...s.|.
+00000d70: a002 7c04 a101 7c02 7c04 3c00 7110 7c02  ..|...|.|.<.q.|.
+00000d80: 5300 7215 0000 0029 0372 1600 0000 da0a  S.r....).r......
+00000d90: 7374 6172 7473 7769 7468 7217 0000 0029  startswithr....)
+00000da0: 0572 2d00 0000 da06 7072 6566 6978 7235  .r-.....prefixr5
+00000db0: 0000 0072 1c00 0000 7211 0000 0072 1300  ...r....r....r..
+00000dc0: 0000 7213 0000 0072 1400 0000 da0d 6465  ..r....r......de
+00000dd0: 6c5f 746c 5f70 7265 6669 787e 0000 0073  l_tl_prefix~...s
+00000de0: 0c00 0000 0001 0401 0801 0801 0a01 1001  ................
+00000df0: 7a17 544c 4366 674e 6f64 652e 6465 6c5f  z.TLCfgNode.del_
+00000e00: 746c 5f70 7265 6669 7829 0146 2901 4629  tl_prefix).F).F)
+00000e10: 0146 2901 4629 0172 4a00 0000 2915 da08  .F).F).rJ...)...
+00000e20: 5f5f 6e61 6d65 5f5f da0a 5f5f 6d6f 6475  __name__..__modu
+00000e30: 6c65 5f5f da0c 5f5f 7175 616c 6e61 6d65  le__..__qualname
+00000e40: 5f5f da07 5f5f 646f 635f 5fda 0c73 7461  __..__doc__..sta
+00000e50: 7469 636d 6574 686f 6472 0e00 0000 7218  ticmethodr....r.
+00000e60: 0000 00da 0373 7472 7221 0000 00da 0462  .....strr!.....b
+00000e70: 6f6f 6c72 2400 0000 7230 0000 0072 3300  oolr$...r0...r3.
+00000e80: 0000 722c 0000 0072 3e00 0000 7241 0000  ..r,...r>...rA..
+00000e90: 0072 4400 0000 7204 0000 0072 4700 0000  .rD...r....rG...
+00000ea0: 724d 0000 00da 0d5f 5f63 6c61 7373 6365  rM.....__classce
+00000eb0: 6c6c 5f5f 7213 0000 0072 1300 0000 7236  ll__r....r....r6
+00000ec0: 0000 0072 1400 0000 7207 0000 0011 0000  ...r....r.......
+00000ed0: 0073 2c00 0000 0801 0402 0201 0a0b 0201  .s,.............
+00000ee0: 0a11 0201 1207 0204 00fd 0201 0201 0201  ................
+00000ef0: 02fd 0e0e 0804 0806 0e04 0e13 0804 0805  ................
+00000f00: 0e0b 7207 0000 0029 0272 1c00 0000 7245  ..r....).r....rE
+00000f10: 0000 0063 0100 0000 0000 0000 0000 0000  ...c............
+00000f20: 0100 0000 0300 0000 4300 0000 7316 0000  ........C...s...
+00000f30: 0074 00a0 01a1 0001 0074 00a0 027c 00a1  .t.......t...|..
+00000f40: 0101 0064 0053 0072 1500 0000 2903 da0a  ...d.S.r....)...
+00000f50: 676c 6f62 616c 5f63 6667 721a 0000 0072  global_cfgr....r
+00000f60: 1b00 0000 2901 721c 0000 0072 1300 0000  ....).r....r....
+00000f70: 7213 0000 0072 1400 0000 da0e 7365 745f  r....r......set_
+00000f80: 676c 6f62 616c 5f63 6667 8a00 0000 7306  global_cfg....s.
+00000f90: 0000 0000 0208 010a 0172 5700 0000 6300  .........rW...c.
+00000fa0: 0000 0000 0000 0000 0000 0000 0000 0002  ................
+00000fb0: 0000 0040 0000 0073 1c00 0000 6500 5a01  ...@...s....e.Z.
+00000fc0: 6400 5a02 6401 6402 8400 5a03 6403 6404  d.Z.d.d...Z.d.d.
+00000fd0: 8400 5a04 6405 5300 2906 da0e 5465 7374  ..Z.d.S.)...Test
+00000fe0: 5f54 4c43 6667 4e6f 6465 6301 0000 0000  _TLCfgNodec.....
+00000ff0: 0000 0000 0000 0004 0000 0004 0000 0043  ...............C
+00001000: 0000 0073 2600 0000 6401 7d01 6402 7d02  ...s&...d.}.d.}.
+00001010: 7400 6a01 7c01 7c02 6403 8d02 7d03 7402  t.j.|.|.d...}.t.
+00001020: 7c03 a003 a100 8301 0100 6404 5300 2905  |.........d.S.).
+00001030: 7208 0000 00fa 2674 6c32 2f70 726f 6a2f  r.....&tl2/proj/
+00001040: 6676 636f 7265 2f63 6f6e 6669 6773 2f54  fvcore/configs/T
+00001050: 4c43 6667 4e6f 6465 2e79 616d 6cda 0574  LCfgNode.yaml..t
+00001060: 6573 7432 a901 7222 0000 004e 2904 7207  est2..r"...N).r.
+00001070: 0000 0072 2400 0000 da05 7072 696e 7472  ...r$.....printr
+00001080: 2c00 0000 2904 722d 0000 00da 0b63 6f6e  ,...).r-.....con
+00001090: 6669 675f 7961 6d6c 7222 0000 0072 1c00  fig_yamlr"...r..
+000010a0: 0000 7213 0000 0072 1300 0000 7214 0000  ..r....r....r...
+000010b0: 00da 1b74 6573 745f 6c6f 6164 5f79 616d  ...test_load_yam
+000010c0: 6c5f 7769 7468 5f63 6f6d 6d61 6e64 9400  l_with_command..
+000010d0: 0000 730a 0000 0000 0304 0104 020e 010c  ..s.............
+000010e0: 017a 2a54 6573 745f 544c 4366 674e 6f64  .z*Test_TLCfgNod
+000010f0: 652e 7465 7374 5f6c 6f61 645f 7961 6d6c  e.test_load_yaml
+00001100: 5f77 6974 685f 636f 6d6d 616e 6463 0100  _with_commandc..
+00001110: 0000 0000 0000 0000 0000 0900 0000 0600  ................
+00001120: 0000 4300 0000 7386 0000 0064 017d 017c  ..C...s....d.}.|
+00001130: 01a0 00a1 007d 0264 027d 0364 037d 0464  .....}.d.}.d.}.d
+00001140: 0464 056c 016d 027d 0501 007c 0583 007d  .d.l.m.}...|...}
+00001150: 067c 066a 0364 0674 0464 0767 0064 088d  .|.j.d.t.d.g.d..
+00001160: 0401 007c 066a 0364 0974 0464 0a8d 0201  ...|.j.d.t.d....
+00001170: 007c 06a0 057c 02a1 017d 0774 066a 077c  .|...|...}.t.j.|
+00001180: 037c 0464 0b8d 027d 0874 087c 08a0 09a1  .|.d...}.t.|....
+00001190: 0083 0101 007c 08a0 0a7c 076a 0ba1 0101  .....|...|.j....
+000011a0: 0074 087c 08a0 09a1 0083 0101 0064 0c53  .t.|.........d.S
+000011b0: 0029 0d72 0800 0000 7a24 2d2d 746c 5f6f  .).r....z$--tl_o
+000011c0: 7074 7320 7465 7374 312e 7465 7374 3020  pts test1.test0 
+000011d0: 3130 202d 2d74 6573 7420 7465 7374 7259  10 --test testrY
+000011e0: 0000 0072 5a00 0000 7201 0000 0029 01da  ...rZ...r....)..
+000011f0: 0e41 7267 756d 656e 7450 6172 7365 727a  .ArgumentParserz
+00001200: 092d 2d74 6c5f 6f70 7473 da01 2a29 03da  .--tl_opts..*)..
+00001210: 0474 7970 65da 056e 6172 6773 da07 6465  .type..nargs..de
+00001220: 6661 756c 747a 062d 2d74 6573 7429 0172  faultz.--test).r
+00001230: 6100 0000 725b 0000 004e 290c 723a 0000  a...r[...N).r:..
+00001240: 00da 0861 7267 7061 7273 6572 5f00 0000  ...argparser_...
+00001250: da0c 6164 645f 6172 6775 6d65 6e74 7253  ..add_argumentrS
+00001260: 0000 00da 0a70 6172 7365 5f61 7267 7372  .....parse_argsr
+00001270: 0700 0000 7224 0000 0072 5c00 0000 722c  ....r$...r\...r,
+00001280: 0000 0072 3e00 0000 da07 746c 5f6f 7074  ...r>.....tl_opt
+00001290: 7329 0972 2d00 0000 da08 6172 6776 5f73  s).r-.....argv_s
+000012a0: 7472 da09 6172 6776 5f6c 6973 7472 5d00  tr..argv_listr].
+000012b0: 0000 7222 0000 0072 5f00 0000 da06 7061  ..r"...r_.....pa
+000012c0: 7273 6572 da04 6172 6773 721c 0000 0072  rser..argsr....r
+000012d0: 1300 0000 7213 0000 0072 1400 0000 da0f  ....r....r......
+000012e0: 7465 7374 5f6d 6572 6765 5f6c 6973 749e  test_merge_list.
+000012f0: 0000 0073 1c00 0000 0003 0401 0802 0401  ...s............
+00001300: 0402 0c01 0601 1201 0e01 0a02 0e01 0c02  ................
+00001310: 0c01 0c01 7a1e 5465 7374 5f54 4c43 6667  ....z.Test_TLCfg
+00001320: 4e6f 6465 2e74 6573 745f 6d65 7267 655f  Node.test_merge_
+00001330: 6c69 7374 4e29 0572 4e00 0000 724f 0000  listN).rN...rO..
+00001340: 0072 5000 0000 725e 0000 0072 6c00 0000  .rP...r^...rl...
+00001350: 7213 0000 0072 1300 0000 7213 0000 0072  r....r....r....r
+00001360: 1400 0000 7258 0000 0092 0000 0073 0400  ....rX.......s..
+00001370: 0000 0802 080a 7258 0000 0029 16da 0674  ......rX...)...t
+00001380: 7970 696e 6772 0200 0000 7203 0000 0072  ypingr....r....r
+00001390: 0400 0000 da03 7379 73da 0479 616d 6cda  ......sys..yaml.
+000013a0: 076c 6f67 6769 6e67 da02 6f73 da04 6d61  .logging..os..ma
+000013b0: 7468 da05 6e75 6d70 79da 026e 70da 0474  th..numpy..np..t
+000013c0: 696d 65da 0875 6e69 7474 6573 745a 1466  ime..unittestZ.f
+000013d0: 7663 6f72 652e 636f 6d6d 6f6e 2e63 6f6e  vcore.common.con
+000013e0: 6669 6772 0500 0000 5a08 5f43 6667 4e6f  figr....Z._CfgNo
+000013f0: 6465 7219 0000 0072 0700 0000 7256 0000  der....r....rV..
+00001400: 0072 5700 0000 da08 5465 7374 4361 7365  .rW.....TestCase
+00001410: 7258 0000 0072 1300 0000 7213 0000 0072  rX...r....r....r
+00001420: 1300 0000 7214 0000 00da 083c 6d6f 6475  ....r......<modu
+00001430: 6c65 3e01 0000 0073 1c00 0000 1401 0801  le>....s........
+00001440: 0801 0801 0801 0801 0801 0801 0802 0c03  ................
+00001450: 0403 1077 0602 1008                      ...w....
```

### Comparing `tl2-0.1.0/tl2/proj/fvcore/__pycache__/registry.cpython-38.pyc` & `tl2-0.1.1/tl2/proj/fvcore/__pycache__/registry.cpython-38.pyc`

 * *Format-specific differences are supported for Python .pyc files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: Byte-compiled Python module for CPython 3.8, timestamp-based, .py timestamp: Mon Oct  3 09:35:07 2022 UTC, .py size: 4686 bytes*

 * *Could not decompile bytecode: bad marshal data (unknown type code)*

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-00000000: 550d 0d0a 0000 0000 cbac 3a63 4e12 0000  U.........:cN...
+00000000: 550d 0d0a 0000 0000 f13c 9362 ee12 0000  U........<.b....
 00000010: e300 0000 0000 0000 0000 0000 0000 0000  ................
 00000020: 0004 0000 0040 0000 0073 a000 0000 6400  .....@...s....d.
 00000030: 6401 6c00 5a00 6400 6401 6c01 5a01 6400  d.l.Z.d.d.l.Z.d.
 00000040: 6401 6c02 5a02 6400 6401 6c03 5a03 6400  d.l.Z.d.d.l.Z.d.
 00000050: 6401 6c04 5a04 6400 6401 6c05 5a05 6400  d.l.Z.d.d.l.Z.d.
 00000060: 6402 6c06 6d07 5a07 6d08 5a08 0100 6400  d.l.m.Z.m.Z...d.
 00000070: 6403 6c09 6d0a 5a0b 0100 4700 6404 6405  d.l.m.Z...G.d.d.
@@ -32,218 +32,216 @@
 000001f0: 2077 6173 2061 6c72 6561 6479 2072 6567   was already reg
 00000200: 6973 7465 7265 6420 696e 2027 7a18 2720  istered in 'z.' 
 00000210: 7265 6769 7374 7279 2120 0a55 7369 6e67  registry! .Using
 00000220: 206f 6c64 3a20 7a0d 0a49 676e 6f72 6520   old: z..Ignore 
 00000230: 6e65 773a 20da 010a 2903 da08 5f6f 626a  new: ...)..._obj
 00000240: 5f6d 6170 da05 7072 696e 74da 055f 6e61  _map..print.._na
 00000250: 6d65 2903 da04 7365 6c66 7205 0000 0072  me)...selfr....r
-00000260: 0600 0000 a900 720d 0000 00fa 552f 686f  ......r.....U/ho
+00000260: 0600 0000 a900 720d 0000 00fa 372f 686f  ......r.....7/ho
 00000270: 6d65 2f6d 612d 7573 6572 2f77 6f72 6b2f  me/ma-user/work/
-00000280: 636f 6465 2f73 7479 6c65 6761 6e32 2d61  code/stylegan2-a
-00000290: 6461 2d70 7974 6f72 6368 2d65 7870 2f74  da-pytorch-exp/t
-000002a0: 6c32 5f6c 6962 2f74 6c32 2f70 726f 6a2f  l2_lib/tl2/proj/
-000002b0: 6676 636f 7265 2f72 6567 6973 7472 792e  fvcore/registry.
-000002c0: 7079 da0c 5f64 6f5f 7265 6769 7374 6572  py.._do_register
-000002d0: 0d00 0000 730a 0000 0000 070a 012a 0304  ....s........*..
-000002e0: 020a 017a 1552 6567 6973 7472 792e 5f64  ...z.Registry._d
-000002f0: 6f5f 7265 6769 7374 6572 2902 7206 0000  o_register).r...
-00000300: 0072 0700 0000 6304 0000 0000 0000 0000  .r....c.........
-00000310: 0000 0005 0000 0005 0000 0003 0000 0073  ...............s
-00000320: 3600 0000 7c01 6401 6b08 7226 7c02 7c03  6...|.d.k.r&|.|.
-00000330: 6602 7400 7400 6402 9c02 8700 6601 6403  f.t.t.d.....f.d.
-00000340: 6404 840d 7d04 7c04 5300 8800 a001 7c02  d...}.|.S.....|.
-00000350: 7c01 a102 0100 6401 5300 2905 7a9b 0a20  |.....d.S.).z.. 
-00000360: 2020 2052 6567 6973 7465 7220 7468 6520     Register the 
-00000370: 6769 7665 6e20 6f62 6a65 6374 2075 6e64  given object und
-00000380: 6572 2074 6865 2074 6865 206e 616d 6520  er the the name 
-00000390: 606f 626a 2e5f 5f6e 616d 655f 5f60 2e0a  `obj.__name__`..
-000003a0: 2020 2020 4361 6e20 6265 2075 7365 6420      Can be used 
-000003b0: 6173 2065 6974 6865 7220 6120 6465 636f  as either a deco
-000003c0: 7261 746f 7220 6f72 206e 6f74 2e20 5365  rator or not. Se
-000003d0: 6520 646f 6373 7472 696e 6720 6f66 2074  e docstring of t
-000003e0: 6869 7320 636c 6173 7320 666f 7220 7573  his class for us
-000003f0: 6167 652e 0a20 2020 204e 2902 da0d 6675  age..    N)...fu
-00000400: 6e63 5f6f 725f 636c 6173 7372 0700 0000  nc_or_classr....
-00000410: 6303 0000 0000 0000 0000 0000 0003 0000  c...............
-00000420: 0004 0000 0013 0000 0073 4a00 0000 7c01  .........sJ...|.
-00000430: 6400 6b08 7236 7c02 6400 6b08 7224 7c00  d.k.r6|.d.k.r$|.
-00000440: 6a00 9b00 6401 7c00 6a01 9b00 9d03 7d01  j...d.|.j.....}.
-00000450: 713a 7c02 9b00 6401 7c00 6a01 9b00 9d03  q:|...d.|.j.....
-00000460: 7d01 6e04 7c01 7d01 8800 a002 7c01 7c00  }.n.|.}.....|.|.
-00000470: a102 0100 7c00 5300 2902 4eda 012e 2903  ....|.S.).N...).
-00000480: da0a 5f5f 6d6f 6475 6c65 5f5f da08 5f5f  ..__module__..__
-00000490: 6e61 6d65 5f5f 720f 0000 0029 0372 1000  name__r....).r..
-000004a0: 0000 7205 0000 00da 0b6e 616d 655f 7072  ..r......name_pr
-000004b0: 6566 6978 a901 720c 0000 0072 0d00 0000  efix..r....r....
-000004c0: 720e 0000 00da 0464 6563 6f24 0000 0073  r......deco$...s
-000004d0: 0e00 0000 0002 0801 0801 1402 1202 0401  ................
-000004e0: 0c01 7a1f 5265 6769 7374 7279 2e72 6567  ..z.Registry.reg
-000004f0: 6973 7465 722e 3c6c 6f63 616c 733e 2e64  ister.<locals>.d
-00000500: 6563 6f29 02da 066f 626a 6563 7472 0f00  eco)...objectr..
-00000510: 0000 2905 720c 0000 0072 0600 0000 7205  ..).r....r....r.
-00000520: 0000 0072 1400 0000 7216 0000 0072 0d00  ...r....r....r..
-00000530: 0000 7215 0000 0072 0e00 0000 da08 7265  ..r....r......re
-00000540: 6769 7374 6572 1d00 0000 730a 0000 0000  gister....s.....
-00000550: 0508 021a 0b04 020c 017a 1152 6567 6973  .........z.Regis
-00000560: 7472 792e 7265 6769 7374 6572 6301 0000  try.registerc...
-00000570: 0000 0000 0000 0000 0002 0000 0002 0000  ................
-00000580: 0043 0000 0073 1000 0000 7c00 6a00 7d01  .C...s....|.j.}.
-00000590: 6900 7c00 5f00 7c01 5300 a901 4e29 0172  i.|._.|.S...N).r
-000005a0: 0900 0000 a902 720c 0000 005a 076f 626a  ......r....Z.obj
-000005b0: 5f6d 6170 720d 0000 0072 0d00 0000 720e  _mapr....r....r.
-000005c0: 0000 00da 0563 6c65 6172 3400 0000 7306  .....clear4...s.
-000005d0: 0000 0000 0106 0106 017a 0e52 6567 6973  .........z.Regis
-000005e0: 7472 792e 636c 6561 7263 0200 0000 0000  try.clearc......
-000005f0: 0000 0000 0000 0200 0000 0300 0000 4300  ..............C.
-00000600: 0000 7310 0000 007c 006a 00a0 017c 01a1  ..s....|.j...|..
-00000610: 0101 0064 0053 0072 1900 0000 2902 7209  ...d.S.r....).r.
-00000620: 0000 00da 0675 7064 6174 6572 1a00 0000  .....updater....
-00000630: 720d 0000 0072 0d00 0000 720e 0000 0072  r....r....r....r
-00000640: 1c00 0000 3900 0000 7302 0000 0000 017a  ....9...s......z
-00000650: 0f52 6567 6973 7472 792e 7570 6461 7465  .Registry.update
-00000660: 2903 4e4e 4e29 0a72 1300 0000 7212 0000  ).NNN).r....r...
-00000670: 00da 0c5f 5f71 7561 6c6e 616d 655f 5fda  ...__qualname__.
-00000680: 0373 7472 7217 0000 0072 0f00 0000 7203  .strr....r....r.
-00000690: 0000 0072 1800 0000 721b 0000 0072 1c00  ...r....r....r..
-000006a0: 0000 720d 0000 0072 0d00 0000 720d 0000  ..r....r....r...
-000006b0: 0072 0e00 0000 7204 0000 000b 0000 0073  .r....r........s
-000006c0: 0800 0000 0802 1210 1617 0805 7204 0000  ............r...
-000006d0: 0063 0400 0000 0000 0000 0000 0000 0700  .c..............
-000006e0: 0000 0500 0000 4300 0000 73c6 0000 0074  ......C...s....t
-000006f0: 007c 0074 0183 0273 1c74 0264 0174 037c  .|.t...s.t.d.t.|
-00000700: 0083 019b 009d 0283 0182 0164 027c 006b  ...........d.|.k
-00000710: 0772 487c 0364 006b 0873 3464 037c 036b  .rH|.d.k.s4d.|.k
-00000720: 0772 4874 0464 047c 009b 0064 057c 039b  .rHt.d.|...d.|..
-00000730: 009d 0483 0182 0174 007c 0174 0583 0273  .......t.|.t...s
-00000740: 6474 0264 0674 037c 0183 019b 009d 0283  dt.d.t.|........
-00000750: 0182 0174 007c 0374 0183 0273 887c 0364  ...t.|.t...s.|.d
-00000760: 006b 0873 8874 0264 0774 037c 0383 019b  .k.s.t.d.t.|....
-00000770: 009d 0283 0182 0174 06a0 077c 00a1 017d  .......t...|...}
-00000780: 007c 0272 a07c 007c 0396 027d 046e 087c  .|.r.|.|...}.n.|
-00000790: 037c 0096 027d 047c 04a0 0864 02a1 017d  .|...}.|...d...}
-000007a0: 057c 01a0 097c 05a1 017d 067c 0666 007c  .|...|...}.|.f.|
-000007b0: 048e 0153 0029 084e 7a1c 6366 6720 6d75  ...S.).Nz.cfg mu
-000007c0: 7374 2062 6520 6120 6469 6374 2c20 6275  st be a dict, bu
-000007d0: 7420 676f 7420 7205 0000 00da 0474 7970  t got r......typ
-000007e0: 657a 3d60 6366 6760 206f 7220 6064 6566  ez=`cfg` or `def
-000007f0: 6175 6c74 5f61 7267 7360 206d 7573 7420  ault_args` must 
-00000800: 636f 6e74 6169 6e20 7468 6520 6b65 7920  contain the key 
-00000810: 2274 7970 6522 2c20 6275 7420 676f 7420  "type", but got 
-00000820: 7208 0000 007a 2d72 6567 6973 7472 7920  r....z-registry 
-00000830: 6d75 7374 2062 6520 616e 2052 6567 6973  must be an Regis
-00000840: 7472 7920 6f62 6a65 6374 2c20 6275 7420  try object, but 
-00000850: 676f 7420 7a2d 6465 6661 756c 745f 6172  got z-default_ar
-00000860: 6773 206d 7573 7420 6265 2061 2064 6963  gs must be a dic
-00000870: 7420 6f72 204e 6f6e 652c 2062 7574 2067  t or None, but g
-00000880: 6f74 2029 0ada 0a69 7369 6e73 7461 6e63  ot )...isinstanc
-00000890: 65da 0464 6963 74da 0954 7970 6545 7272  e..dict..TypeErr
-000008a0: 6f72 721f 0000 00da 084b 6579 4572 726f  orr......KeyErro
-000008b0: 7272 0400 0000 da04 636f 7079 da08 6465  rr......copy..de
-000008c0: 6570 636f 7079 da03 706f 70da 0367 6574  epcopy..pop..get
-000008d0: 2907 da03 6366 67da 0872 6567 6973 7472  )...cfg..registr
-000008e0: 79da 0f6b 7761 7267 735f 7072 696f 7269  y..kwargs_priori
-000008f0: 7479 da0c 6465 6661 756c 745f 6172 6773  ty..default_args
-00000900: da04 6172 6773 5a08 6f62 6a5f 7479 7065  ..argsZ.obj_type
-00000910: 5a07 6f62 6a5f 636c 7372 0d00 0000 720d  Z.obj_clsr....r.
-00000920: 0000 0072 0e00 0000 da0e 6275 696c 645f  ...r......build_
-00000930: 6672 6f6d 5f63 6667 3c00 0000 7324 0000  from_cfg<...s$..
-00000940: 0000 020a 0112 0108 0110 0102 010e ff04  ................
-00000950: 030a 0112 0212 0112 040a 0104 010a 0208  ................
-00000960: 020a 010a 0172 2d00 0000 da0e 4d4f 4445  .....r-.....MODE
-00000970: 4c5f 5245 4749 5354 5259 7a02 0a0a 6301  L_REGISTRYz...c.
-00000980: 0000 0000 0000 0000 0000 0002 0000 0004  ................
-00000990: 0000 0043 0000 0073 1800 0000 7c00 4400  ...C...s....|.D.
-000009a0: 5d0e 7d01 7400 a001 7c01 a101 0100 7104  ].}.t...|.....q.
-000009b0: 6400 5300 7219 0000 0029 02da 0969 6d70  d.S.r....)...imp
-000009c0: 6f72 746c 6962 da0d 696d 706f 7274 5f6d  ortlib..import_m
-000009d0: 6f64 756c 6529 02da 1072 6567 6973 7465  odule)...registe
-000009e0: 725f 6d6f 6475 6c65 73da 066d 6f64 756c  r_modules..modul
-000009f0: 6572 0d00 0000 720d 0000 0072 0e00 0000  er....r....r....
-00000a00: da11 5f72 6567 6973 7465 725f 6d6f 6475  .._register_modu
-00000a10: 6c65 735e 0000 0073 0600 0000 0001 0801  les^...s........
-00000a20: 0c03 7233 0000 0054 6304 0000 0000 0000  ..r3...Tc.......
-00000a30: 0000 0000 0007 0000 0006 0000 004b 0000  .............K..
-00000a40: 0073 7a00 0000 7c00 a000 a100 7d00 7401  .sz...|.....}.t.
-00000a50: 7c00 a002 6401 6700 a102 6402 8d01 0100  |...d.g...d.....
-00000a60: 7c03 7244 7403 a004 6403 a101 7d05 7c05  |.rDt...d...}.|.
-00000a70: a005 7406 a101 0100 7c05 a005 6404 7c00  ..t.....|...d.|.
-00000a80: 6a07 9b00 9d02 a101 0100 7c02 7366 7406  j.........|.sft.
-00000a90: a008 7c00 6a07 a101 6600 7c00 7c01 6405  ..|.j...f.|.|.d.
-00000aa0: 9c02 7c04 9702 8e01 7d06 6e10 7409 7c00  ..|.....}.n.t.|.
-00000ab0: 7406 7c01 7c04 6406 8d04 7d06 7c06 5300  t.|.|.d...}.|.S.
-00000ac0: 2907 4e72 3100 0000 2901 7231 0000 00da  ).Nr1...).r1....
-00000ad0: 0274 6c7a 0f42 7569 6c64 206d 6f64 656c  .tlz.Build model
-00000ae0: 3a0a 0920 2902 7228 0000 0072 2a00 0000  :.. ).r(...r*...
-00000af0: 2904 7228 0000 0072 2900 0000 722a 0000  ).r(...r)...r*..
-00000b00: 0072 2b00 0000 290a da05 636c 6f6e 6572  .r+...)...cloner
-00000b10: 3300 0000 7226 0000 00da 076c 6f67 6769  3...r&.....loggi
-00000b20: 6e67 da09 6765 744c 6f67 6765 72da 0469  ng..getLogger..i
-00000b30: 6e66 6fda 0852 4547 4953 5452 5972 0500  nfo..REGISTRYr..
-00000b40: 0000 7227 0000 0072 2d00 0000 2907 7228  ..r'...r-...).r(
-00000b50: 0000 0072 2a00 0000 da0d 6366 675f 746f  ...r*.....cfg_to
-00000b60: 5f6b 7761 7267 73da 0d62 7569 6c64 5f76  _kwargs..build_v
-00000b70: 6572 626f 7365 da06 6b77 6172 6773 da06  erbose..kwargs..
-00000b80: 6c6f 6767 6572 da03 7265 7472 0d00 0000  logger..retr....
-00000b90: 720d 0000 0072 0e00 0000 da06 5f62 7569  r....r......_bui
-00000ba0: 6c64 6600 0000 7314 0000 0000 0108 0212  ldf...s.........
-00000bb0: 0204 010a 010a 0112 0204 011e 0210 0172  ...............r
-00000bc0: 3f00 0000 4663 0400 0000 0000 0000 0000  ?...Fc..........
-00000bd0: 0000 0500 0000 0600 0000 4b00 0000 7318  ..........K...s.
-00000be0: 0000 0074 007c 0066 017c 017c 027c 0364  ...t.|.f.|.|.|.d
-00000bf0: 019c 037c 0497 028e 0153 0029 027a fe0a  ...|.....S.).z..
-00000c00: 2020 2020 2020 7265 6769 7374 6572 5f6d        register_m
-00000c10: 6f64 756c 6573 3a0a 2020 2020 2020 2020  odules:.        
-00000c20: 2d20 2265 7870 322e 6872 696e 7665 7273  - "exp2.hrinvers
-00000c30: 696f 6e2e 6d6f 6465 6c73 2e73 7479 6c65  ion.models.style
-00000c40: 5f69 6e72 5f67 616e 220a 2020 2020 2020  _inr_gan".      
-00000c50: 6e61 6d65 3a20 2265 7870 322e 6872 696e  name: "exp2.hrin
-00000c60: 7665 7273 696f 6e2e 6d6f 6465 6c73 2e73  version.models.s
-00000c70: 7479 6c65 5f69 6e72 5f67 616e 2e47 656e  tyle_inr_gan.Gen
-00000c80: 6572 6174 6f72 556c 7472 615a 5043 220a  eratorUltraZPC".
-00000c90: 2020 0a20 2020 203a 7061 7261 6d20 6366    .    :param cf
-00000ca0: 673a 0a20 2020 203a 7061 7261 6d20 6b77  g:.    :param kw
-00000cb0: 6172 6773 5f70 7269 6f72 6974 793a 0a20  args_priority:. 
-00000cc0: 2020 203a 7061 7261 6d20 6366 675f 746f     :param cfg_to
-00000cd0: 5f6b 7761 7267 733a 0a20 2020 203a 7061  _kwargs:.    :pa
-00000ce0: 7261 6d20 6b77 6172 6773 3a0a 2020 2020  ram kwargs:.    
-00000cf0: 3a72 6574 7572 6e3a 0a20 2020 2029 0372  :return:.    ).r
-00000d00: 2a00 0000 723a 0000 0072 3b00 0000 2901  *...r:...r;...).
-00000d10: 723f 0000 0029 0572 2800 0000 722a 0000  r?...).r(...r*..
-00000d20: 0072 3a00 0000 723b 0000 0072 3c00 0000  .r:...r;...r<...
-00000d30: 720d 0000 0072 0d00 0000 720e 0000 00da  r....r....r.....
-00000d40: 0b62 7569 6c64 5f6d 6f64 656c 7700 0000  .build_modelw...
-00000d50: 7302 0000 0000 1072 4000 0000 6301 0000  s......r@...c...
-00000d60: 0000 0000 0000 0000 0003 0000 0005 0000  ................
-00000d70: 0043 0000 0073 5400 0000 7400 a001 6401  .C...sT...t...d.
-00000d80: a101 7d01 7c01 a002 6402 7c00 9b00 6403  ..}.|...d.|...d.
-00000d90: 9d03 a101 0100 7c00 7403 6a04 6b07 723e  ......|.t.j.k.r>
-00000da0: 7405 a006 7c00 a101 7d02 7403 6a07 7c02  t...|...}.t.j.|.
-00000db0: 7c00 6404 8d02 0100 7c01 a002 7403 a101  |.d.....|...t...
-00000dc0: 0100 7403 6a08 7c00 6405 8d01 5300 2906  ..t.j.|.d...S.).
-00000dd0: 7a33 0a20 2072 6567 6973 7465 725f 6d6f  z3.  register_mo
-00000de0: 6475 6c65 3a20 2222 0a0a 2020 3a70 6172  dule: ""..  :par
-00000df0: 616d 2063 6667 3a0a 2020 3a72 6574 7572  am cfg:.  :retur
-00000e00: 6e3a 0a20 2072 3400 0000 7a09 4275 696c  n:.  r4...z.Buil
-00000e10: 6469 6e67 207a 0420 2e2e 2e29 0272 0600  ding z. ...).r..
-00000e20: 0000 7205 0000 0029 0172 0500 0000 2909  ..r....).r....).
-00000e30: 7236 0000 0072 3700 0000 7238 0000 0072  r6...r7...r8...r
-00000e40: 3900 0000 7209 0000 0072 2f00 0000 7230  9...r....r/...r0
-00000e50: 0000 0072 1800 0000 7227 0000 0029 0372  ...r....r'...).r
-00000e60: 0500 0000 723d 0000 0072 3200 0000 720d  ....r=...r2...r.
-00000e70: 0000 0072 0d00 0000 720e 0000 00da 0c62  ...r....r......b
-00000e80: 7569 6c64 5f6d 6f64 756c 658f 0000 0073  uild_module....s
-00000e90: 0e00 0000 0007 0a01 1202 0a01 0a01 0e02  ................
-00000ea0: 0a01 7241 0000 0029 014e 2901 5429 0346  ..rA...).N).T).F
-00000eb0: 5454 2914 da02 6f73 da03 7379 73da 0875  TT)...os..sys..u
-00000ec0: 6e69 7474 6573 7472 2f00 0000 7236 0000  nittestr/...r6..
-00000ed0: 0072 2400 0000 da06 7479 7069 6e67 7202  .r$.....typingr.
-00000ee0: 0000 0072 0300 0000 5a16 6676 636f 7265  ...r....Z.fvcore
-00000ef0: 2e63 6f6d 6d6f 6e2e 7265 6769 7374 7279  .common.registry
-00000f00: 7204 0000 005a 0d52 6567 6973 7472 795f  r....Z.Registry_
-00000f10: 6261 7365 722d 0000 0072 3900 0000 722e  baser-...r9...r.
-00000f20: 0000 00da 075f 5f64 6f63 5f5f 7233 0000  .....__doc__r3..
-00000f30: 0072 3f00 0000 7240 0000 0072 4100 0000  .r?...r@...rA...
-00000f40: 720d 0000 0072 0d00 0000 720d 0000 0072  r....r....r....r
-00000f50: 0e00 0000 da08 3c6d 6f64 756c 653e 0100  ......<module>..
-00000f60: 0000 7326 0000 0008 0108 0108 0108 0108  ..s&............
-00000f70: 0108 0110 010c 0310 310a 1c08 0104 0106  ........1.......
-00000f80: 0408 080a 1200 0100 0100 fd0a 18         .............
+00000280: 636f 6465 2f74 6c32 2f74 6c32 2f70 726f  code/tl2/tl2/pro
+00000290: 6a2f 6676 636f 7265 2f72 6567 6973 7472  j/fvcore/registr
+000002a0: 792e 7079 da0c 5f64 6f5f 7265 6769 7374  y.py.._do_regist
+000002b0: 6572 0d00 0000 730a 0000 0000 070a 012a  er....s........*
+000002c0: 0304 020a 017a 1552 6567 6973 7472 792e  .....z.Registry.
+000002d0: 5f64 6f5f 7265 6769 7374 6572 2902 7206  _do_register).r.
+000002e0: 0000 0072 0700 0000 6304 0000 0000 0000  ...r....c.......
+000002f0: 0000 0000 0005 0000 0005 0000 0003 0000  ................
+00000300: 0073 3600 0000 7c01 6401 6b08 7226 7c02  .s6...|.d.k.r&|.
+00000310: 7c03 6602 7400 7400 6402 9c02 8700 6601  |.f.t.t.d.....f.
+00000320: 6403 6404 840d 7d04 7c04 5300 8800 a001  d.d...}.|.S.....
+00000330: 7c02 7c01 a102 0100 6401 5300 2905 7a9b  |.|.....d.S.).z.
+00000340: 0a20 2020 2052 6567 6973 7465 7220 7468  .    Register th
+00000350: 6520 6769 7665 6e20 6f62 6a65 6374 2075  e given object u
+00000360: 6e64 6572 2074 6865 2074 6865 206e 616d  nder the the nam
+00000370: 6520 606f 626a 2e5f 5f6e 616d 655f 5f60  e `obj.__name__`
+00000380: 2e0a 2020 2020 4361 6e20 6265 2075 7365  ..    Can be use
+00000390: 6420 6173 2065 6974 6865 7220 6120 6465  d as either a de
+000003a0: 636f 7261 746f 7220 6f72 206e 6f74 2e20  corator or not. 
+000003b0: 5365 6520 646f 6373 7472 696e 6720 6f66  See docstring of
+000003c0: 2074 6869 7320 636c 6173 7320 666f 7220   this class for 
+000003d0: 7573 6167 652e 0a20 2020 204e 2902 da0d  usage..    N)...
+000003e0: 6675 6e63 5f6f 725f 636c 6173 7372 0700  func_or_classr..
+000003f0: 0000 6303 0000 0000 0000 0000 0000 0003  ..c.............
+00000400: 0000 0004 0000 0013 0000 0073 4a00 0000  ...........sJ...
+00000410: 7c01 6400 6b08 7236 7c02 6400 6b08 7224  |.d.k.r6|.d.k.r$
+00000420: 7c00 6a00 9b00 6401 7c00 6a01 9b00 9d03  |.j...d.|.j.....
+00000430: 7d01 713a 7c02 9b00 6401 7c00 6a01 9b00  }.q:|...d.|.j...
+00000440: 9d03 7d01 6e04 7c01 7d01 8800 a002 7c01  ..}.n.|.}.....|.
+00000450: 7c00 a102 0100 7c00 5300 2902 4eda 012e  |.....|.S.).N...
+00000460: 2903 da0a 5f5f 6d6f 6475 6c65 5f5f da08  )...__module__..
+00000470: 5f5f 6e61 6d65 5f5f 720f 0000 0029 0372  __name__r....).r
+00000480: 1000 0000 7205 0000 00da 0b6e 616d 655f  ....r......name_
+00000490: 7072 6566 6978 a901 720c 0000 0072 0d00  prefix..r....r..
+000004a0: 0000 720e 0000 00da 0464 6563 6f24 0000  ..r......deco$..
+000004b0: 0073 0e00 0000 0002 0801 0801 1402 1202  .s..............
+000004c0: 0401 0c01 7a1f 5265 6769 7374 7279 2e72  ....z.Registry.r
+000004d0: 6567 6973 7465 722e 3c6c 6f63 616c 733e  egister.<locals>
+000004e0: 2e64 6563 6f29 02da 066f 626a 6563 7472  .deco)...objectr
+000004f0: 0f00 0000 2905 720c 0000 0072 0600 0000  ....).r....r....
+00000500: 7205 0000 0072 1400 0000 7216 0000 0072  r....r....r....r
+00000510: 0d00 0000 7215 0000 0072 0e00 0000 da08  ....r....r......
+00000520: 7265 6769 7374 6572 1d00 0000 730a 0000  register....s...
+00000530: 0000 0508 021a 0b04 020c 017a 1152 6567  ...........z.Reg
+00000540: 6973 7472 792e 7265 6769 7374 6572 6301  istry.registerc.
+00000550: 0000 0000 0000 0000 0000 0002 0000 0002  ................
+00000560: 0000 0043 0000 0073 1000 0000 7c00 6a00  ...C...s....|.j.
+00000570: 7d01 6900 7c00 5f00 7c01 5300 a901 4e29  }.i.|._.|.S...N)
+00000580: 0172 0900 0000 a902 720c 0000 005a 076f  .r......r....Z.o
+00000590: 626a 5f6d 6170 720d 0000 0072 0d00 0000  bj_mapr....r....
+000005a0: 720e 0000 00da 0563 6c65 6172 3400 0000  r......clear4...
+000005b0: 7306 0000 0000 0106 0106 017a 0e52 6567  s..........z.Reg
+000005c0: 6973 7472 792e 636c 6561 7263 0200 0000  istry.clearc....
+000005d0: 0000 0000 0000 0000 0200 0000 0300 0000  ................
+000005e0: 4300 0000 7310 0000 007c 006a 00a0 017c  C...s....|.j...|
+000005f0: 01a1 0101 0064 0053 0072 1900 0000 2902  .....d.S.r....).
+00000600: 7209 0000 00da 0675 7064 6174 6572 1a00  r......updater..
+00000610: 0000 720d 0000 0072 0d00 0000 720e 0000  ..r....r....r...
+00000620: 0072 1c00 0000 3900 0000 7302 0000 0000  .r....9...s.....
+00000630: 017a 0f52 6567 6973 7472 792e 7570 6461  .z.Registry.upda
+00000640: 7465 2903 4e4e 4e29 0a72 1300 0000 7212  te).NNN).r....r.
+00000650: 0000 00da 0c5f 5f71 7561 6c6e 616d 655f  .....__qualname_
+00000660: 5fda 0373 7472 7217 0000 0072 0f00 0000  _..strr....r....
+00000670: 7203 0000 0072 1800 0000 721b 0000 0072  r....r....r....r
+00000680: 1c00 0000 720d 0000 0072 0d00 0000 720d  ....r....r....r.
+00000690: 0000 0072 0e00 0000 7204 0000 000b 0000  ...r....r.......
+000006a0: 0073 0800 0000 0802 1210 1617 0805 7204  .s............r.
+000006b0: 0000 0063 0400 0000 0000 0000 0000 0000  ...c............
+000006c0: 0700 0000 0500 0000 4300 0000 73c6 0000  ........C...s...
+000006d0: 0074 007c 0074 0183 0273 1c74 0264 0174  .t.|.t...s.t.d.t
+000006e0: 037c 0083 019b 009d 0283 0182 0164 027c  .|...........d.|
+000006f0: 006b 0772 487c 0364 006b 0873 3464 037c  .k.rH|.d.k.s4d.|
+00000700: 036b 0772 4874 0464 047c 009b 0064 057c  .k.rHt.d.|...d.|
+00000710: 039b 009d 0483 0182 0174 007c 0174 0583  .........t.|.t..
+00000720: 0273 6474 0264 0674 037c 0183 019b 009d  .sdt.d.t.|......
+00000730: 0283 0182 0174 007c 0374 0183 0273 887c  .....t.|.t...s.|
+00000740: 0364 006b 0873 8874 0264 0774 037c 0383  .d.k.s.t.d.t.|..
+00000750: 019b 009d 0283 0182 0174 06a0 077c 00a1  .........t...|..
+00000760: 017d 007c 0272 a07c 007c 0396 027d 046e  .}.|.r.|.|...}.n
+00000770: 087c 037c 0096 027d 047c 04a0 0864 02a1  .|.|...}.|...d..
+00000780: 017d 057c 01a0 097c 05a1 017d 067c 0666  .}.|...|...}.|.f
+00000790: 007c 048e 0153 0029 084e 7a1c 6366 6720  .|...S.).Nz.cfg 
+000007a0: 6d75 7374 2062 6520 6120 6469 6374 2c20  must be a dict, 
+000007b0: 6275 7420 676f 7420 7205 0000 00da 0474  but got r......t
+000007c0: 7970 657a 3d60 6366 6760 206f 7220 6064  ypez=`cfg` or `d
+000007d0: 6566 6175 6c74 5f61 7267 7360 206d 7573  efault_args` mus
+000007e0: 7420 636f 6e74 6169 6e20 7468 6520 6b65  t contain the ke
+000007f0: 7920 2274 7970 6522 2c20 6275 7420 676f  y "type", but go
+00000800: 7420 7208 0000 007a 2d72 6567 6973 7472  t r....z-registr
+00000810: 7920 6d75 7374 2062 6520 616e 2052 6567  y must be an Reg
+00000820: 6973 7472 7920 6f62 6a65 6374 2c20 6275  istry object, bu
+00000830: 7420 676f 7420 7a2d 6465 6661 756c 745f  t got z-default_
+00000840: 6172 6773 206d 7573 7420 6265 2061 2064  args must be a d
+00000850: 6963 7420 6f72 204e 6f6e 652c 2062 7574  ict or None, but
+00000860: 2067 6f74 2029 0ada 0a69 7369 6e73 7461   got )...isinsta
+00000870: 6e63 65da 0464 6963 74da 0954 7970 6545  nce..dict..TypeE
+00000880: 7272 6f72 721f 0000 00da 084b 6579 4572  rrorr......KeyEr
+00000890: 726f 7272 0400 0000 da04 636f 7079 da08  rorr......copy..
+000008a0: 6465 6570 636f 7079 da03 706f 70da 0367  deepcopy..pop..g
+000008b0: 6574 2907 da03 6366 67da 0872 6567 6973  et)...cfg..regis
+000008c0: 7472 79da 0f6b 7761 7267 735f 7072 696f  try..kwargs_prio
+000008d0: 7269 7479 da0c 6465 6661 756c 745f 6172  rity..default_ar
+000008e0: 6773 da04 6172 6773 5a08 6f62 6a5f 7479  gs..argsZ.obj_ty
+000008f0: 7065 5a07 6f62 6a5f 636c 7372 0d00 0000  peZ.obj_clsr....
+00000900: 720d 0000 0072 0e00 0000 da0e 6275 696c  r....r......buil
+00000910: 645f 6672 6f6d 5f63 6667 3c00 0000 7324  d_from_cfg<...s$
+00000920: 0000 0000 020a 0112 0108 0110 0102 010e  ................
+00000930: ff04 030a 0112 0212 0112 040a 0104 010a  ................
+00000940: 0208 020a 010a 0172 2d00 0000 da0e 4d4f  .......r-.....MO
+00000950: 4445 4c5f 5245 4749 5354 5259 7a02 0a0a  DEL_REGISTRYz...
+00000960: 6301 0000 0000 0000 0000 0000 0002 0000  c...............
+00000970: 0004 0000 0043 0000 0073 1800 0000 7c00  .....C...s....|.
+00000980: 4400 5d0e 7d01 7400 a001 7c01 a101 0100  D.].}.t...|.....
+00000990: 7104 6400 5300 7219 0000 0029 02da 0969  q.d.S.r....)...i
+000009a0: 6d70 6f72 746c 6962 da0d 696d 706f 7274  mportlib..import
+000009b0: 5f6d 6f64 756c 6529 02da 1072 6567 6973  _module)...regis
+000009c0: 7465 725f 6d6f 6475 6c65 73da 066d 6f64  ter_modules..mod
+000009d0: 756c 6572 0d00 0000 720d 0000 0072 0e00  uler....r....r..
+000009e0: 0000 da11 5f72 6567 6973 7465 725f 6d6f  ...._register_mo
+000009f0: 6475 6c65 735e 0000 0073 0600 0000 0001  dules^...s......
+00000a00: 0801 0c03 7233 0000 0054 6304 0000 0000  ....r3...Tc.....
+00000a10: 0000 0000 0000 0007 0000 0006 0000 004b  ...............K
+00000a20: 0000 0073 7a00 0000 7c00 a000 a100 7d00  ...sz...|.....}.
+00000a30: 7401 7c00 a002 6401 6700 a102 6402 8d01  t.|...d.g...d...
+00000a40: 0100 7c03 7244 7403 a004 6403 a101 7d05  ..|.rDt...d...}.
+00000a50: 7c05 a005 7406 a101 0100 7c05 a005 6404  |...t.....|...d.
+00000a60: 7c00 6a07 9b00 9d02 a101 0100 7c02 7366  |.j.........|.sf
+00000a70: 7406 a008 7c00 6a07 a101 6600 7c00 7c01  t...|.j...f.|.|.
+00000a80: 6405 9c02 7c04 9702 8e01 7d06 6e10 7409  d...|.....}.n.t.
+00000a90: 7c00 7406 7c01 7c04 6406 8d04 7d06 7c06  |.t.|.|.d...}.|.
+00000aa0: 5300 2907 4e72 3100 0000 2901 7231 0000  S.).Nr1...).r1..
+00000ab0: 00da 0274 6c7a 0f42 7569 6c64 206d 6f64  ...tlz.Build mod
+00000ac0: 656c 3a0a 0920 2902 7228 0000 0072 2a00  el:.. ).r(...r*.
+00000ad0: 0000 2904 7228 0000 0072 2900 0000 722a  ..).r(...r)...r*
+00000ae0: 0000 0072 2b00 0000 290a da05 636c 6f6e  ...r+...)...clon
+00000af0: 6572 3300 0000 7226 0000 00da 076c 6f67  er3...r&.....log
+00000b00: 6769 6e67 da09 6765 744c 6f67 6765 72da  ging..getLogger.
+00000b10: 0469 6e66 6fda 0852 4547 4953 5452 5972  .info..REGISTRYr
+00000b20: 0500 0000 7227 0000 0072 2d00 0000 2907  ....r'...r-...).
+00000b30: 7228 0000 0072 2a00 0000 da0d 6366 675f  r(...r*.....cfg_
+00000b40: 746f 5f6b 7761 7267 73da 0d62 7569 6c64  to_kwargs..build
+00000b50: 5f76 6572 626f 7365 da06 6b77 6172 6773  _verbose..kwargs
+00000b60: da06 6c6f 6767 6572 da03 7265 7472 0d00  ..logger..retr..
+00000b70: 0000 720d 0000 0072 0e00 0000 da06 5f62  ..r....r......_b
+00000b80: 7569 6c64 6600 0000 7314 0000 0000 0108  uildf...s.......
+00000b90: 0212 0204 010a 010a 0112 0204 011e 0210  ................
+00000ba0: 0172 3f00 0000 4663 0400 0000 0000 0000  .r?...Fc........
+00000bb0: 0000 0000 0500 0000 0600 0000 4b00 0000  ............K...
+00000bc0: 7318 0000 0074 007c 0066 017c 017c 027c  s....t.|.f.|.|.|
+00000bd0: 0364 019c 037c 0497 028e 0153 0029 027a  .d...|.....S.).z
+00000be0: fe0a 2020 2020 2020 7265 6769 7374 6572  ..      register
+00000bf0: 5f6d 6f64 756c 6573 3a0a 2020 2020 2020  _modules:.      
+00000c00: 2020 2d20 2265 7870 322e 6872 696e 7665    - "exp2.hrinve
+00000c10: 7273 696f 6e2e 6d6f 6465 6c73 2e73 7479  rsion.models.sty
+00000c20: 6c65 5f69 6e72 5f67 616e 220a 2020 2020  le_inr_gan".    
+00000c30: 2020 6e61 6d65 3a20 2265 7870 322e 6872    name: "exp2.hr
+00000c40: 696e 7665 7273 696f 6e2e 6d6f 6465 6c73  inversion.models
+00000c50: 2e73 7479 6c65 5f69 6e72 5f67 616e 2e47  .style_inr_gan.G
+00000c60: 656e 6572 6174 6f72 556c 7472 615a 5043  eneratorUltraZPC
+00000c70: 220a 2020 0a20 2020 203a 7061 7261 6d20  ".  .    :param 
+00000c80: 6366 673a 0a20 2020 203a 7061 7261 6d20  cfg:.    :param 
+00000c90: 6b77 6172 6773 5f70 7269 6f72 6974 793a  kwargs_priority:
+00000ca0: 0a20 2020 203a 7061 7261 6d20 6366 675f  .    :param cfg_
+00000cb0: 746f 5f6b 7761 7267 733a 0a20 2020 203a  to_kwargs:.    :
+00000cc0: 7061 7261 6d20 6b77 6172 6773 3a0a 2020  param kwargs:.  
+00000cd0: 2020 3a72 6574 7572 6e3a 0a20 2020 2029    :return:.    )
+00000ce0: 0372 2a00 0000 723a 0000 0072 3b00 0000  .r*...r:...r;...
+00000cf0: 2901 723f 0000 0029 0572 2800 0000 722a  ).r?...).r(...r*
+00000d00: 0000 0072 3a00 0000 723b 0000 0072 3c00  ...r:...r;...r<.
+00000d10: 0000 720d 0000 0072 0d00 0000 720e 0000  ..r....r....r...
+00000d20: 00da 0b62 7569 6c64 5f6d 6f64 656c 7700  ...build_modelw.
+00000d30: 0000 7302 0000 0000 1072 4000 0000 6301  ..s......r@...c.
+00000d40: 0000 0000 0000 0000 0000 0003 0000 0005  ................
+00000d50: 0000 0043 0000 0073 5400 0000 7400 a001  ...C...sT...t...
+00000d60: 6401 a101 7d01 7c01 a002 6402 7c00 9b00  d...}.|...d.|...
+00000d70: 6403 9d03 a101 0100 7c00 7403 6a04 6b07  d.......|.t.j.k.
+00000d80: 723e 7405 a006 7c00 a101 7d02 7403 6a07  r>t...|...}.t.j.
+00000d90: 7c02 7c00 6404 8d02 0100 7c01 a002 7403  |.|.d.....|...t.
+00000da0: a101 0100 7403 6a08 7c00 6405 8d01 5300  ....t.j.|.d...S.
+00000db0: 2906 7a33 0a20 2072 6567 6973 7465 725f  ).z3.  register_
+00000dc0: 6d6f 6475 6c65 3a20 2222 0a0a 2020 3a70  module: ""..  :p
+00000dd0: 6172 616d 2063 6667 3a0a 2020 3a72 6574  aram cfg:.  :ret
+00000de0: 7572 6e3a 0a20 2072 3400 0000 7a09 4275  urn:.  r4...z.Bu
+00000df0: 696c 6469 6e67 207a 0420 2e2e 2e29 0272  ilding z. ...).r
+00000e00: 0600 0000 7205 0000 0029 0172 0500 0000  ....r....).r....
+00000e10: 2909 7236 0000 0072 3700 0000 7238 0000  ).r6...r7...r8..
+00000e20: 0072 3900 0000 7209 0000 0072 2f00 0000  .r9...r....r/...
+00000e30: 7230 0000 0072 1800 0000 7227 0000 0029  r0...r....r'...)
+00000e40: 0372 0500 0000 723d 0000 0072 3200 0000  .r....r=...r2...
+00000e50: 720d 0000 0072 0d00 0000 720e 0000 00da  r....r....r.....
+00000e60: 0c62 7569 6c64 5f6d 6f64 756c 658f 0000  .build_module...
+00000e70: 0073 0e00 0000 0007 0a01 1202 0a01 0a01  .s..............
+00000e80: 0e02 0a01 7241 0000 0029 014e 2901 5429  ....rA...).N).T)
+00000e90: 0346 5454 2914 da02 6f73 da03 7379 73da  .FTT)...os..sys.
+00000ea0: 0875 6e69 7474 6573 7472 2f00 0000 7236  .unittestr/...r6
+00000eb0: 0000 0072 2400 0000 da06 7479 7069 6e67  ...r$.....typing
+00000ec0: 7202 0000 0072 0300 0000 5a16 6676 636f  r....r....Z.fvco
+00000ed0: 7265 2e63 6f6d 6d6f 6e2e 7265 6769 7374  re.common.regist
+00000ee0: 7279 7204 0000 005a 0d52 6567 6973 7472  ryr....Z.Registr
+00000ef0: 795f 6261 7365 722d 0000 0072 3900 0000  y_baser-...r9...
+00000f00: 722e 0000 00da 075f 5f64 6f63 5f5f 7233  r......__doc__r3
+00000f10: 0000 0072 3f00 0000 7240 0000 0072 4100  ...r?...r@...rA.
+00000f20: 0000 720d 0000 0072 0d00 0000 720d 0000  ..r....r....r...
+00000f30: 0072 0e00 0000 da08 3c6d 6f64 756c 653e  .r......<module>
+00000f40: 0100 0000 7326 0000 0008 0108 0108 0108  ....s&..........
+00000f50: 0108 0108 0110 010c 0310 310a 1c08 0104  ..........1.....
+00000f60: 0106 0408 080a 1200 0100 0100 fd0a 18    ...............
```

### Comparing `tl2-0.1.0/tl2/proj/fvcore/checkpoint.py` & `tl2-0.1.1/tl2/proj/fvcore/checkpoint.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,164 +1,164 @@
-import os
-import logging
-from typing import Any, Dict, Iterable, List, NamedTuple, Optional, Tuple
-
-import torch.nn as nn
-import torch
-
-from fvcore.common.checkpoint import Checkpointer as Checkpointer_base
-from fvcore.common.checkpoint import _IncompatibleKeys, _strip_prefix_if_present
-
-from .logger import setup_logger
-
-__all__ = ['Checkpointer', ]
-
-
-class Checkpointer(Checkpointer_base):
-  """
-  *: https://www.zhihu.com/question/287097169
-
-  """
-  def __init__(self,
-               model: nn.Module,
-               save_dir: str = "",
-               *,
-               distributed_rank=0,
-               save_to_disk: bool = True,
-               **checkpointables: object,
-               ):
-    """
-    Args:
-        model (nn.Module): model.
-        save_dir (str): a directory to save and find checkpoints.
-        save_to_disk (bool): if True, save checkpoint to disk, otherwise
-            disable saving for this checkpointer.
-        checkpointables (object): any checkpointable objects, i.e., objects
-            that have the `state_dict()` and `load_state_dict()` method. For
-            example, it can be used like
-            `Checkpointer(model, "dir", optimizer=optimizer)`.
-    """
-    self.distributed_rank = distributed_rank
-
-    if save_dir:
-      os.makedirs(save_dir, exist_ok=True)
-
-    self.logger = logging.getLogger('fvcore')
-    if len(self.logger.handlers) == 0:
-      self.logger = setup_logger(output=save_dir, name='fvcore', distributed_rank=distributed_rank)
-
-    super(Checkpointer, self).__init__(
-      model=model,
-      save_dir=save_dir,
-      save_to_disk=save_to_disk,
-      **checkpointables)
-    pass
-
-  def save(self, name: str, **kwargs: Dict[str, str]) -> None:
-    """
-    Dump model and checkpointables to a file.
-
-    Args:
-        name (str): name of the file.
-        kwargs (dict): extra arbitrary data to save.
-    """
-    # if self.distributed_rank != 0:
-    #   return
-
-    return super(Checkpointer, self).save(name=name, **kwargs)
-
-  def has_checkpoint(self) -> bool:
-    """
-    Returns:
-        bool: whether a checkpoint exists in the target directory.
-    """
-    return super(Checkpointer, self).has_checkpoint()
-
-  def get_checkpoint_file(self) -> str:
-    """
-    Returns:
-        str: The latest checkpoint file in target directory.
-    """
-    return super(Checkpointer, self).get_checkpoint_file()
-
-  def _load_state_dict(self, checkpoint_state_dict) -> _IncompatibleKeys:  # pyre-ignore
-    """
-    Load weights from a checkpoint.
-
-    Args:
-        checkpoint (Any): checkpoint contains the weights.
-
-    Returns:
-        ``NamedTuple`` with ``missing_keys``, ``unexpected_keys``,
-            and ``incorrect_shapes`` fields:
-            * **missing_keys** is a list of str containing the missing keys
-            * **unexpected_keys** is a list of str containing the unexpected keys
-            * **incorrect_shapes** is a list of (key, shape in checkpoint, shape in model)
-
-        This is just like the return value of
-        :func:`torch.nn.Module.load_state_dict`, but with extra support
-        for ``incorrect_shapes``.
-    """
-    self._convert_ndarray_to_tensor(checkpoint_state_dict)
-
-    # if the state_dict comes from a model that was wrapped in a
-    # DataParallel or DistributedDataParallel during serialization,
-    # remove the "module" prefix before performing the matching.
-    _strip_prefix_if_present(checkpoint_state_dict, "module.")
-
-    # work around https://github.com/pytorch/pytorch/issues/24139
-    model_state_dict = self.model.state_dict()
-    incorrect_shapes = []
-    for k in list(checkpoint_state_dict.keys()):
-      if k in model_state_dict:
-        shape_model = tuple(model_state_dict[k].shape)
-        shape_checkpoint = tuple(checkpoint_state_dict[k].shape)
-        if shape_model != shape_checkpoint:
-          incorrect_shapes.append((k, shape_checkpoint, shape_model))
-          checkpoint_state_dict.pop(k)
-    # pyre-ignore
-    incompatible = self.model.load_state_dict(checkpoint_state_dict, strict=False)
-    return _IncompatibleKeys(
-      missing_keys=incompatible.missing_keys,
-      unexpected_keys=incompatible.unexpected_keys,
-      incorrect_shapes=incorrect_shapes,
-    )
-
-  def load_state_dict(self,
-                      checkpoint_state_dict):
-    """
-    Load from the given checkpoint. When path points to network file, this
-    function has to be called on all ranks.
-
-    Args:
-        path (str): path or url to the checkpoint. If empty, will not load
-            anything.
-        checkpointables (list): List of checkpointable names to load. If not
-            specified (None), will load all the possible checkpointables.
-    Returns:
-        dict:
-            extra data loaded from the checkpoint that has not been
-            processed. For example, those saved with
-            :meth:`.save(**extra_data)`.
-    """
-
-    incompatible = self._load_state_dict(checkpoint_state_dict)
-    if (
-          incompatible is not None
-    ):  # handle some existing subclasses that returns None
-      self._log_incompatible_keys(incompatible)
-
-    # return any further checkpoint data
-    return incompatible
-
-  def load_state_dict_from_file(self,
-                                model_path,
-                                rank=0):
-    self.logger.info(f"Rank {rank} loading checkpoint from {model_path}")
-    if not os.path.isfile(model_path):
-      assert os.path.isfile(model_path), "Checkpoint {} not found!".format(model_path)
-
-    map_location = lambda storage, loc: storage.cuda(rank)
-    checkpoint_state_dict = torch.load(model_path, map_location=map_location)
-    return self.load_state_dict(checkpoint_state_dict)
-
-
+import os
+import logging
+from typing import Any, Dict, Iterable, List, NamedTuple, Optional, Tuple
+
+import torch.nn as nn
+import torch
+
+from fvcore.common.checkpoint import Checkpointer as Checkpointer_base
+from fvcore.common.checkpoint import _IncompatibleKeys, _strip_prefix_if_present
+
+from .logger import setup_logger
+
+__all__ = ['Checkpointer', ]
+
+
+class Checkpointer(Checkpointer_base):
+  """
+  *: https://www.zhihu.com/question/287097169
+
+  """
+  def __init__(self,
+               model: nn.Module,
+               save_dir: str = "",
+               *,
+               distributed_rank=0,
+               save_to_disk: bool = True,
+               **checkpointables: object,
+               ):
+    """
+    Args:
+        model (nn.Module): model.
+        save_dir (str): a directory to save and find checkpoints.
+        save_to_disk (bool): if True, save checkpoint to disk, otherwise
+            disable saving for this checkpointer.
+        checkpointables (object): any checkpointable objects, i.e., objects
+            that have the `state_dict()` and `load_state_dict()` method. For
+            example, it can be used like
+            `Checkpointer(model, "dir", optimizer=optimizer)`.
+    """
+    self.distributed_rank = distributed_rank
+
+    if save_dir:
+      os.makedirs(save_dir, exist_ok=True)
+
+    self.logger = logging.getLogger('fvcore')
+    if len(self.logger.handlers) == 0:
+      self.logger = setup_logger(output=save_dir, name='fvcore', distributed_rank=distributed_rank)
+
+    super(Checkpointer, self).__init__(
+      model=model,
+      save_dir=save_dir,
+      save_to_disk=save_to_disk,
+      **checkpointables)
+    pass
+
+  def save(self, name: str, **kwargs: Dict[str, str]) -> None:
+    """
+    Dump model and checkpointables to a file.
+
+    Args:
+        name (str): name of the file.
+        kwargs (dict): extra arbitrary data to save.
+    """
+    # if self.distributed_rank != 0:
+    #   return
+
+    return super(Checkpointer, self).save(name=name, **kwargs)
+
+  def has_checkpoint(self) -> bool:
+    """
+    Returns:
+        bool: whether a checkpoint exists in the target directory.
+    """
+    return super(Checkpointer, self).has_checkpoint()
+
+  def get_checkpoint_file(self) -> str:
+    """
+    Returns:
+        str: The latest checkpoint file in target directory.
+    """
+    return super(Checkpointer, self).get_checkpoint_file()
+
+  def _load_state_dict(self, checkpoint_state_dict) -> _IncompatibleKeys:  # pyre-ignore
+    """
+    Load weights from a checkpoint.
+
+    Args:
+        checkpoint (Any): checkpoint contains the weights.
+
+    Returns:
+        ``NamedTuple`` with ``missing_keys``, ``unexpected_keys``,
+            and ``incorrect_shapes`` fields:
+            * **missing_keys** is a list of str containing the missing keys
+            * **unexpected_keys** is a list of str containing the unexpected keys
+            * **incorrect_shapes** is a list of (key, shape in checkpoint, shape in model)
+
+        This is just like the return value of
+        :func:`torch.nn.Module.load_state_dict`, but with extra support
+        for ``incorrect_shapes``.
+    """
+    self._convert_ndarray_to_tensor(checkpoint_state_dict)
+
+    # if the state_dict comes from a model that was wrapped in a
+    # DataParallel or DistributedDataParallel during serialization,
+    # remove the "module" prefix before performing the matching.
+    _strip_prefix_if_present(checkpoint_state_dict, "module.")
+
+    # work around https://github.com/pytorch/pytorch/issues/24139
+    model_state_dict = self.model.state_dict()
+    incorrect_shapes = []
+    for k in list(checkpoint_state_dict.keys()):
+      if k in model_state_dict:
+        shape_model = tuple(model_state_dict[k].shape)
+        shape_checkpoint = tuple(checkpoint_state_dict[k].shape)
+        if shape_model != shape_checkpoint:
+          incorrect_shapes.append((k, shape_checkpoint, shape_model))
+          checkpoint_state_dict.pop(k)
+    # pyre-ignore
+    incompatible = self.model.load_state_dict(checkpoint_state_dict, strict=False)
+    return _IncompatibleKeys(
+      missing_keys=incompatible.missing_keys,
+      unexpected_keys=incompatible.unexpected_keys,
+      incorrect_shapes=incorrect_shapes,
+    )
+
+  def load_state_dict(self,
+                      checkpoint_state_dict):
+    """
+    Load from the given checkpoint. When path points to network file, this
+    function has to be called on all ranks.
+
+    Args:
+        path (str): path or url to the checkpoint. If empty, will not load
+            anything.
+        checkpointables (list): List of checkpointable names to load. If not
+            specified (None), will load all the possible checkpointables.
+    Returns:
+        dict:
+            extra data loaded from the checkpoint that has not been
+            processed. For example, those saved with
+            :meth:`.save(**extra_data)`.
+    """
+
+    incompatible = self._load_state_dict(checkpoint_state_dict)
+    if (
+          incompatible is not None
+    ):  # handle some existing subclasses that returns None
+      self._log_incompatible_keys(incompatible)
+
+    # return any further checkpoint data
+    return incompatible
+
+  def load_state_dict_from_file(self,
+                                model_path,
+                                rank=0):
+    self.logger.info(f"Rank {rank} loading checkpoint from {model_path}")
+    if not os.path.isfile(model_path):
+      assert os.path.isfile(model_path), "Checkpoint {} not found!".format(model_path)
+
+    map_location = lambda storage, loc: storage.cuda(rank)
+    checkpoint_state_dict = torch.load(model_path, map_location=map_location)
+    return self.load_state_dict(checkpoint_state_dict)
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/fvcore/dummy_model.py` & `tl2-0.1.1/tl2/proj/fvcore/dummy_model.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,61 +1,61 @@
-import unittest
-import os
-import sys
-from tl2.proj.fvcore import MODEL_REGISTRY, build_model
-
-
-@MODEL_REGISTRY.register(name_prefix=__name__)
-class DummyModel(object):
-  def __init__(self):
-    print(f"Create {self.__class__}")
-    pass
-
-
-class Testing_Registry(unittest.TestCase):
-
-  def test_build_model(self, debug=True):
-    """
-    Usage:
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts test0 10 test1 11 --test 1
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/fvcore/configs/Registry.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-    PORT = os.environ.get('PORT', 8888)
-
-    model = build_model(cfg.model)
-    pass
-
-
-
-
-
+import unittest
+import os
+import sys
+from tl2.proj.fvcore import MODEL_REGISTRY, build_model
+
+
+@MODEL_REGISTRY.register(name_prefix=__name__)
+class DummyModel(object):
+  def __init__(self):
+    print(f"Create {self.__class__}")
+    pass
+
+
+class Testing_Registry(unittest.TestCase):
+
+  def test_build_model(self, debug=True):
+    """
+    Usage:
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts test0 10 test1 11 --test 1
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/fvcore/configs/Registry.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    PORT = os.environ.get('PORT', 8888)
+
+    model = build_model(cfg.model)
+    pass
+
+
+
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/fvcore/logger.py` & `tl2-0.1.1/tl2/proj/fvcore/logger.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,104 +1,104 @@
-import os
-import sys
-import logging
-from termcolor import colored
-import functools
-import atexit
-
-
-# cache the opened file object, so that different calls to `setup_logger`
-# with the same file name can safely write to the same file.
-@functools.lru_cache(maxsize=None)
-def _cached_log_stream(filename):
-    # use 1K buffer if writing to cloud storage
-    # io = PathManager.open(filename, "a", buffering=1024 if "://" in filename else -1)
-    io = open(filename, "a")
-    atexit.register(io.close)
-    return io
-
-
-
-class _ColorfulFormatter(logging.Formatter):
-  def __init__(self, *args, **kwargs):
-    self._root_name = kwargs.pop("root_name") + "."
-    self._abbrev_name = kwargs.pop("abbrev_name", "")
-    if len(self._abbrev_name):
-      self._abbrev_name = self._abbrev_name + "."
-    super(_ColorfulFormatter, self).__init__(*args, **kwargs)
-
-  def formatMessage(self, record):
-    record.name = record.name.replace(self._root_name, self._abbrev_name)
-    log = super(_ColorfulFormatter, self).formatMessage(record)
-    if record.levelno == logging.WARNING:
-      prefix = colored("WARNING", "red", attrs=["blink"])
-    elif record.levelno == logging.ERROR or record.levelno == logging.CRITICAL:
-      prefix = colored("ERROR", "red", attrs=["blink", "underline"])
-    else:
-      return log
-    return prefix + " " + log
-
-
-def setup_logger(
-      output=None,
-      distributed_rank=0,
-      *,
-      color=True,
-      name="fvcore",
-      abbrev_name=None
-):
-  """
-  Initialize the fvcore logger and set its verbosity level to "DEBUG".
-
-  Args:
-      output (str): a file name or a directory to save log. If None, will not save log file.
-          If ends with ".txt" or ".log", assumed to be a file name.
-          Otherwise, logs will be saved to `output/log.txt`.
-      name (str): the root module name of this logger
-      abbrev_name (str): an abbreviation of the module, to avoid long names in logs.
-          Set to "" to not log the root module in logs.
-
-  Returns:
-      logging.Logger: a logger
-  """
-  logger = logging.getLogger(name)
-  logger.setLevel(logging.DEBUG)
-  logger.propagate = False
-
-  if abbrev_name is None:
-    abbrev_name = name
-
-  plain_formatter = logging.Formatter(
-    "[%(asctime)s] %(name)s %(levelname)s: %(message)s", datefmt="%m/%d %H:%M:%S"
-  )
-  # stdout logging: master only
-  if distributed_rank == 0:
-    ch = logging.StreamHandler(stream=sys.stdout)
-    ch.setLevel(logging.DEBUG)
-    if color:
-      formatter = _ColorfulFormatter(
-        colored("[%(asctime)s %(name)s]: ", "green") + "%(message)s",
-        datefmt="%m/%d %H:%M:%S",
-        root_name=name,
-        abbrev_name=str(abbrev_name),
-      )
-    else:
-      formatter = plain_formatter
-    ch.setFormatter(formatter)
-    logger.addHandler(ch)
-
-  # file logging: all workers
-  if output:
-    if output.endswith(".txt") or output.endswith(".log"):
-      filename = output
-    else:
-      filename = os.path.join(output, "log.txt")
-    if distributed_rank > 0:
-      filename = filename + ".rank{}".format(distributed_rank)
-    os.makedirs(os.path.dirname(filename), exist_ok=True)
-
-    fh = logging.StreamHandler(_cached_log_stream(filename))
-    fh.setLevel(logging.DEBUG)
-    fh.setFormatter(plain_formatter)
-    logger.addHandler(fh)
-
-  return logger
+import os
+import sys
+import logging
+from termcolor import colored
+import functools
+import atexit
+
+
+# cache the opened file object, so that different calls to `setup_logger`
+# with the same file name can safely write to the same file.
+@functools.lru_cache(maxsize=None)
+def _cached_log_stream(filename):
+    # use 1K buffer if writing to cloud storage
+    # io = PathManager.open(filename, "a", buffering=1024 if "://" in filename else -1)
+    io = open(filename, "a")
+    atexit.register(io.close)
+    return io
+
+
+
+class _ColorfulFormatter(logging.Formatter):
+  def __init__(self, *args, **kwargs):
+    self._root_name = kwargs.pop("root_name") + "."
+    self._abbrev_name = kwargs.pop("abbrev_name", "")
+    if len(self._abbrev_name):
+      self._abbrev_name = self._abbrev_name + "."
+    super(_ColorfulFormatter, self).__init__(*args, **kwargs)
+
+  def formatMessage(self, record):
+    record.name = record.name.replace(self._root_name, self._abbrev_name)
+    log = super(_ColorfulFormatter, self).formatMessage(record)
+    if record.levelno == logging.WARNING:
+      prefix = colored("WARNING", "red", attrs=["blink"])
+    elif record.levelno == logging.ERROR or record.levelno == logging.CRITICAL:
+      prefix = colored("ERROR", "red", attrs=["blink", "underline"])
+    else:
+      return log
+    return prefix + " " + log
+
+
+def setup_logger(
+      output=None,
+      distributed_rank=0,
+      *,
+      color=True,
+      name="fvcore",
+      abbrev_name=None
+):
+  """
+  Initialize the fvcore logger and set its verbosity level to "DEBUG".
+
+  Args:
+      output (str): a file name or a directory to save log. If None, will not save log file.
+          If ends with ".txt" or ".log", assumed to be a file name.
+          Otherwise, logs will be saved to `output/log.txt`.
+      name (str): the root module name of this logger
+      abbrev_name (str): an abbreviation of the module, to avoid long names in logs.
+          Set to "" to not log the root module in logs.
+
+  Returns:
+      logging.Logger: a logger
+  """
+  logger = logging.getLogger(name)
+  logger.setLevel(logging.DEBUG)
+  logger.propagate = False
+
+  if abbrev_name is None:
+    abbrev_name = name
+
+  plain_formatter = logging.Formatter(
+    "[%(asctime)s] %(name)s %(levelname)s: %(message)s", datefmt="%m/%d %H:%M:%S"
+  )
+  # stdout logging: master only
+  if distributed_rank == 0:
+    ch = logging.StreamHandler(stream=sys.stdout)
+    ch.setLevel(logging.DEBUG)
+    if color:
+      formatter = _ColorfulFormatter(
+        colored("[%(asctime)s %(name)s]: ", "green") + "%(message)s",
+        datefmt="%m/%d %H:%M:%S",
+        root_name=name,
+        abbrev_name=str(abbrev_name),
+      )
+    else:
+      formatter = plain_formatter
+    ch.setFormatter(formatter)
+    logger.addHandler(ch)
+
+  # file logging: all workers
+  if output:
+    if output.endswith(".txt") or output.endswith(".log"):
+      filename = output
+    else:
+      filename = os.path.join(output, "log.txt")
+    if distributed_rank > 0:
+      filename = filename + ".rank{}".format(distributed_rank)
+    os.makedirs(os.path.dirname(filename), exist_ok=True)
+
+    fh = logging.StreamHandler(_cached_log_stream(filename))
+    fh.setLevel(logging.DEBUG)
+    fh.setFormatter(plain_formatter)
+    logger.addHandler(fh)
+
+  return logger
```

### Comparing `tl2-0.1.0/tl2/proj/fvcore/tests/test_checkpoint.py` & `tl2-0.1.1/tl2/proj/fvcore/tests/test_checkpoint.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,323 +1,323 @@
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
-
-import copy
-import os
-import random
-import string
-import typing
-import unittest
-from collections import OrderedDict
-from tempfile import TemporaryDirectory
-from unittest.mock import MagicMock
-
-import torch
-from torch import nn
-
-from fvcore.common.checkpoint import PeriodicCheckpointer
-
-from tl2.proj.fvcore.checkpoint import Checkpointer
-
-
-class TestCheckpointer(unittest.TestCase):
-    def _create_model(self) -> nn.Module:
-        """
-        Create a simple model.
-        """
-        return nn.Sequential(nn.Linear(2, 3), nn.Linear(3, 1))
-
-    def _create_complex_model(
-        self,
-    ) -> typing.Tuple[nn.Module, typing.Dict[str, torch.Tensor]]:
-        """
-        Create a complex model.
-        """
-        m = nn.Module()
-        m.block1 = nn.Module()
-        m.block1.layer1 = nn.Linear(2, 3)
-        m.layer2 = nn.Linear(3, 2)
-        m.res = nn.Module()
-        m.res.layer2 = nn.Linear(3, 2)
-
-        state_dict = OrderedDict()
-        state_dict["layer1.weight"] = torch.rand(3, 2)
-        state_dict["layer1.bias"] = torch.rand(3)
-        state_dict["layer2.weight"] = torch.rand(2, 3)
-        state_dict["layer2.bias"] = torch.rand(2)
-        state_dict["res.layer2.weight"] = torch.rand(2, 3)
-        state_dict["res.layer2.bias"] = torch.rand(2)
-
-        return m, state_dict
-
-    def test_loading_objects_with_expected_shape_mismatches(self) -> None:
-
-        m1 = nn.Sequential(nn.Conv2d(2, 2, 1))
-        m2 = nn.Sequential(nn.Conv2d(2, 2, 3))
-
-        m1(torch.randn(4, 2, 4, 4))
-        # Load m1's checkpoint into m2.
-        with TemporaryDirectory() as f:
-            checkpointer = Checkpointer(m1, save_dir=f)
-            checkpointer.save("checkpoint_file")
-
-            # in the same folder
-            fresh_checkpointer = Checkpointer(m2, save_dir=f)
-            self.assertTrue(fresh_checkpointer.has_checkpoint())
-            self.assertEqual(
-                fresh_checkpointer.get_checkpoint_file(),
-                os.path.join(f, "checkpoint_file.pth"),
-            )
-            fresh_checkpointer.load(fresh_checkpointer.get_checkpoint_file())
-            pass
-
-    def test_from_last_checkpoint_model(self) -> None:
-        """
-        test that loading works even if they differ by a prefix.
-        """
-        for trained_model, fresh_model in [
-            (self._create_model(), self._create_model()),
-            (nn.DataParallel(self._create_model()), self._create_model()),
-            (self._create_model(), nn.DataParallel(self._create_model())),
-            (
-                nn.DataParallel(self._create_model()),
-                nn.DataParallel(self._create_model()),
-            ),
-        ]:
-
-            with TemporaryDirectory() as f:
-                checkpointer = Checkpointer(trained_model, save_dir=f)
-                checkpointer.save("checkpoint_file")
-
-                # in the same folder
-                fresh_checkpointer = Checkpointer(fresh_model, save_dir=f)
-                self.assertTrue(fresh_checkpointer.has_checkpoint())
-                self.assertEqual(
-                    fresh_checkpointer.get_checkpoint_file(),
-                    os.path.join(f, "checkpoint_file.pth"),
-                )
-                fresh_checkpointer.load(fresh_checkpointer.get_checkpoint_file())
-
-            for trained_p, loaded_p in zip(
-                trained_model.parameters(), fresh_model.parameters()
-            ):
-                # different tensor references
-                self.assertFalse(id(trained_p) == id(loaded_p))
-                # same content
-                self.assertTrue(trained_p.cpu().equal(loaded_p.cpu()))
-        pass
-
-    def test_from_name_file_model(self) -> None:
-        """
-        test that loading works even if they differ by a prefix.
-        """
-        for trained_model, fresh_model in [
-            (self._create_model(), self._create_model()),
-            (nn.DataParallel(self._create_model()), self._create_model()),
-            (self._create_model(), nn.DataParallel(self._create_model())),
-            (
-                nn.DataParallel(self._create_model()),
-                nn.DataParallel(self._create_model()),
-            ),
-        ]:
-            with TemporaryDirectory() as f:
-                checkpointer = Checkpointer(
-                    trained_model, save_dir=f, save_to_disk=True
-                )
-                checkpointer.save("checkpoint_file")
-
-                # on different folders.
-                with TemporaryDirectory() as g:
-                    fresh_checkpointer = Checkpointer(fresh_model, save_dir=g)
-                    self.assertFalse(fresh_checkpointer.has_checkpoint())
-                    self.assertEqual(fresh_checkpointer.get_checkpoint_file(), "")
-                    fresh_checkpointer.load(os.path.join(f, "checkpoint_file.pth"))
-
-            for trained_p, loaded_p in zip(
-                trained_model.parameters(), fresh_model.parameters()
-            ):
-                # different tensor references.
-                self.assertFalse(id(trained_p) == id(loaded_p))
-                # same content.
-                self.assertTrue(trained_p.cpu().equal(loaded_p.cpu()))
-        pass
-
-    def test_checkpointables(self) -> None:
-        """
-        Test saving and loading checkpointables.
-        """
-
-        class CheckpointableObj:
-            """
-            A dummy checkpointableObj class with state_dict and load_state_dict
-            methods.
-            """
-
-            def __init__(self):
-                self.state = {
-                    self.random_handle(): self.random_handle() for i in range(10)
-                }
-
-            def random_handle(self, str_len=100) -> str:
-                """
-                Generate a random string of fixed length.
-                Args:
-                    str_len (str): length of the output string.
-                Returns:
-                    (str): random generated handle.
-                """
-                letters = string.ascii_uppercase
-                return "".join(random.choice(letters) for i in range(str_len))
-
-            def state_dict(self):
-                """
-                Return the state.
-                Returns:
-                    (dict): return the state.
-                """
-                return self.state
-
-            def load_state_dict(self, state) -> None:
-                """
-                Load the state from a given state.
-                Args:
-                    state (dict): a key value dictionary.
-                """
-                self.state = copy.deepcopy(state)
-
-        trained_model, fresh_model = self._create_model(), self._create_model()
-        with TemporaryDirectory() as f:
-            checkpointables = CheckpointableObj()
-            checkpointer = Checkpointer(
-                trained_model,
-                save_dir=f,
-                save_to_disk=True,
-                checkpointables=checkpointables
-            )
-            checkpointer.save("checkpoint_file", epoch=0, itr=0)
-            # in the same folder
-            fresh_checkpointer = Checkpointer(fresh_model, save_dir=f)
-            self.assertTrue(fresh_checkpointer.has_checkpoint())
-            self.assertEqual(
-                fresh_checkpointer.get_checkpoint_file(),
-                os.path.join(f, "checkpoint_file.pth"),
-            )
-            checkpoint = fresh_checkpointer.load(
-                fresh_checkpointer.get_checkpoint_file()
-            )
-            state_dict = checkpointables.state_dict()
-            for key, _ in state_dict.items():
-                self.assertTrue(checkpoint["checkpointables"].get(key) is not None)
-                self.assertTrue(checkpoint["checkpointables"][key] == state_dict[key])
-        pass
-
-    def test_load_reused_params(self) -> None:
-        class Model(nn.Module):
-            def __init__(self, has_y: bool) -> None:
-                super().__init__()
-                self.x = nn.Linear(10, 10)
-                if has_y:
-                    self.y = self.x
-
-        model = Model(has_y=False)
-        model.x.bias.data.fill_(5.0)  # pyre-ignore
-        data = {"model": model.state_dict()}
-        new_model = Model(has_y=True)
-        chkpt = Checkpointer(new_model)
-        # chkpt.logger = logger = MagicMock()
-        incompatible = chkpt._load_model(data)
-        chkpt._log_incompatible_keys(incompatible)
-        self.assertTrue(
-            torch.allclose(new_model.y.bias - 5.0, torch.zeros_like(new_model.y.bias))
-        )
-        # logger.info.assert_not_called()
-        pass
-
-    @unittest.skipIf(  # pyre-fixme[56]
-        not hasattr(nn, "LazyLinear"), "LazyModule not supported"
-    )
-    def test_load_lazy_module(self) -> None:
-        def _get_model() -> nn.Sequential:  # pyre-fixme[11]
-            return nn.Sequential(nn.LazyLinear(10))
-
-        m1, m2 = _get_model(), _get_model()
-        m1(torch.randn(4, 2, 4, 4))  # initialize m1, but not m2
-        # Load m1's checkpoint into m2.
-        with TemporaryDirectory() as f:
-            checkpointer = Checkpointer(m1, save_dir=f)
-            checkpointer.save("checkpoint_file")
-
-            fresh_checkpointer = Checkpointer(m2, save_dir=f)
-            self.assertTrue(fresh_checkpointer.has_checkpoint())
-            self.assertEqual(
-                fresh_checkpointer.get_checkpoint_file(),
-                os.path.join(f, "checkpoint_file.pth"),
-            )
-            checkpointer.load(fresh_checkpointer.get_checkpoint_file())
-            # fresh_checkpointer.load(fresh_checkpointer.get_checkpoint_file())
-            self.assertTrue(torch.equal(m1[0].weight, m2[0].weight))
-        pass
-
-
-class TestPeriodicCheckpointer(unittest.TestCase):
-    def _create_model(self) -> nn.Module:
-        """
-        Create a simple model.
-        """
-        return nn.Sequential(nn.Linear(2, 3), nn.Linear(3, 1))
-
-    def test_periodic_checkpointer(self) -> None:
-        """
-        test that loading works even if they differ by a prefix.
-        """
-        _period = 10
-        _max_iter = 100
-        for trained_model in [
-            self._create_model(),
-            nn.DataParallel(self._create_model()),
-        ]:
-            with TemporaryDirectory() as f:
-                checkpointer = Checkpointer(
-                    trained_model, save_dir=f, save_to_disk=True
-                )
-                periodic_checkpointer = PeriodicCheckpointer(checkpointer, _period, 99)
-                for iteration in range(_max_iter):
-                    periodic_checkpointer.step(iteration)
-                    path = os.path.join(f, "model_{:07d}.pth".format(iteration))
-                    if (iteration + 1) % _period == 0:
-                        self.assertTrue(os.path.exists(path))
-                    else:
-                        self.assertFalse(os.path.exists(path))
-        pass
-
-    def test_periodic_checkpointer_max_to_keep(self) -> None:
-        """
-        Test parameter: max_to_keep
-        """
-        _period = 10
-        _max_iter = 100
-        _max_to_keep = 3
-        for trained_model in [
-            self._create_model(),
-            nn.DataParallel(self._create_model()),
-        ]:
-            with TemporaryDirectory() as f:
-                checkpointer = Checkpointer(
-                    trained_model, save_dir=f, save_to_disk=True
-                )
-                periodic_checkpointer = PeriodicCheckpointer(
-                    checkpointer, _period, 99, max_to_keep=_max_to_keep
-                )
-                for _ in range(2):
-                    checkpoint_paths = []
-
-                    for iteration in range(_max_iter):
-                        periodic_checkpointer.step(iteration)
-                        if (iteration + 1) % _period == 0:
-                            path = os.path.join(f, "model_{:07d}.pth".format(iteration))
-                            checkpoint_paths.append(path)
-
-                    for path in checkpoint_paths[:-_max_to_keep]:
-                        self.assertFalse(os.path.exists(path))
-
-                    for path in checkpoint_paths[-_max_to_keep:]:
-                        self.assertTrue(os.path.exists(path))
-
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+
+import copy
+import os
+import random
+import string
+import typing
+import unittest
+from collections import OrderedDict
+from tempfile import TemporaryDirectory
+from unittest.mock import MagicMock
+
+import torch
+from torch import nn
+
+from fvcore.common.checkpoint import PeriodicCheckpointer
+
+from tl2.proj.fvcore.checkpoint import Checkpointer
+
+
+class TestCheckpointer(unittest.TestCase):
+    def _create_model(self) -> nn.Module:
+        """
+        Create a simple model.
+        """
+        return nn.Sequential(nn.Linear(2, 3), nn.Linear(3, 1))
+
+    def _create_complex_model(
+        self,
+    ) -> typing.Tuple[nn.Module, typing.Dict[str, torch.Tensor]]:
+        """
+        Create a complex model.
+        """
+        m = nn.Module()
+        m.block1 = nn.Module()
+        m.block1.layer1 = nn.Linear(2, 3)
+        m.layer2 = nn.Linear(3, 2)
+        m.res = nn.Module()
+        m.res.layer2 = nn.Linear(3, 2)
+
+        state_dict = OrderedDict()
+        state_dict["layer1.weight"] = torch.rand(3, 2)
+        state_dict["layer1.bias"] = torch.rand(3)
+        state_dict["layer2.weight"] = torch.rand(2, 3)
+        state_dict["layer2.bias"] = torch.rand(2)
+        state_dict["res.layer2.weight"] = torch.rand(2, 3)
+        state_dict["res.layer2.bias"] = torch.rand(2)
+
+        return m, state_dict
+
+    def test_loading_objects_with_expected_shape_mismatches(self) -> None:
+
+        m1 = nn.Sequential(nn.Conv2d(2, 2, 1))
+        m2 = nn.Sequential(nn.Conv2d(2, 2, 3))
+
+        m1(torch.randn(4, 2, 4, 4))
+        # Load m1's checkpoint into m2.
+        with TemporaryDirectory() as f:
+            checkpointer = Checkpointer(m1, save_dir=f)
+            checkpointer.save("checkpoint_file")
+
+            # in the same folder
+            fresh_checkpointer = Checkpointer(m2, save_dir=f)
+            self.assertTrue(fresh_checkpointer.has_checkpoint())
+            self.assertEqual(
+                fresh_checkpointer.get_checkpoint_file(),
+                os.path.join(f, "checkpoint_file.pth"),
+            )
+            fresh_checkpointer.load(fresh_checkpointer.get_checkpoint_file())
+            pass
+
+    def test_from_last_checkpoint_model(self) -> None:
+        """
+        test that loading works even if they differ by a prefix.
+        """
+        for trained_model, fresh_model in [
+            (self._create_model(), self._create_model()),
+            (nn.DataParallel(self._create_model()), self._create_model()),
+            (self._create_model(), nn.DataParallel(self._create_model())),
+            (
+                nn.DataParallel(self._create_model()),
+                nn.DataParallel(self._create_model()),
+            ),
+        ]:
+
+            with TemporaryDirectory() as f:
+                checkpointer = Checkpointer(trained_model, save_dir=f)
+                checkpointer.save("checkpoint_file")
+
+                # in the same folder
+                fresh_checkpointer = Checkpointer(fresh_model, save_dir=f)
+                self.assertTrue(fresh_checkpointer.has_checkpoint())
+                self.assertEqual(
+                    fresh_checkpointer.get_checkpoint_file(),
+                    os.path.join(f, "checkpoint_file.pth"),
+                )
+                fresh_checkpointer.load(fresh_checkpointer.get_checkpoint_file())
+
+            for trained_p, loaded_p in zip(
+                trained_model.parameters(), fresh_model.parameters()
+            ):
+                # different tensor references
+                self.assertFalse(id(trained_p) == id(loaded_p))
+                # same content
+                self.assertTrue(trained_p.cpu().equal(loaded_p.cpu()))
+        pass
+
+    def test_from_name_file_model(self) -> None:
+        """
+        test that loading works even if they differ by a prefix.
+        """
+        for trained_model, fresh_model in [
+            (self._create_model(), self._create_model()),
+            (nn.DataParallel(self._create_model()), self._create_model()),
+            (self._create_model(), nn.DataParallel(self._create_model())),
+            (
+                nn.DataParallel(self._create_model()),
+                nn.DataParallel(self._create_model()),
+            ),
+        ]:
+            with TemporaryDirectory() as f:
+                checkpointer = Checkpointer(
+                    trained_model, save_dir=f, save_to_disk=True
+                )
+                checkpointer.save("checkpoint_file")
+
+                # on different folders.
+                with TemporaryDirectory() as g:
+                    fresh_checkpointer = Checkpointer(fresh_model, save_dir=g)
+                    self.assertFalse(fresh_checkpointer.has_checkpoint())
+                    self.assertEqual(fresh_checkpointer.get_checkpoint_file(), "")
+                    fresh_checkpointer.load(os.path.join(f, "checkpoint_file.pth"))
+
+            for trained_p, loaded_p in zip(
+                trained_model.parameters(), fresh_model.parameters()
+            ):
+                # different tensor references.
+                self.assertFalse(id(trained_p) == id(loaded_p))
+                # same content.
+                self.assertTrue(trained_p.cpu().equal(loaded_p.cpu()))
+        pass
+
+    def test_checkpointables(self) -> None:
+        """
+        Test saving and loading checkpointables.
+        """
+
+        class CheckpointableObj:
+            """
+            A dummy checkpointableObj class with state_dict and load_state_dict
+            methods.
+            """
+
+            def __init__(self):
+                self.state = {
+                    self.random_handle(): self.random_handle() for i in range(10)
+                }
+
+            def random_handle(self, str_len=100) -> str:
+                """
+                Generate a random string of fixed length.
+                Args:
+                    str_len (str): length of the output string.
+                Returns:
+                    (str): random generated handle.
+                """
+                letters = string.ascii_uppercase
+                return "".join(random.choice(letters) for i in range(str_len))
+
+            def state_dict(self):
+                """
+                Return the state.
+                Returns:
+                    (dict): return the state.
+                """
+                return self.state
+
+            def load_state_dict(self, state) -> None:
+                """
+                Load the state from a given state.
+                Args:
+                    state (dict): a key value dictionary.
+                """
+                self.state = copy.deepcopy(state)
+
+        trained_model, fresh_model = self._create_model(), self._create_model()
+        with TemporaryDirectory() as f:
+            checkpointables = CheckpointableObj()
+            checkpointer = Checkpointer(
+                trained_model,
+                save_dir=f,
+                save_to_disk=True,
+                checkpointables=checkpointables
+            )
+            checkpointer.save("checkpoint_file", epoch=0, itr=0)
+            # in the same folder
+            fresh_checkpointer = Checkpointer(fresh_model, save_dir=f)
+            self.assertTrue(fresh_checkpointer.has_checkpoint())
+            self.assertEqual(
+                fresh_checkpointer.get_checkpoint_file(),
+                os.path.join(f, "checkpoint_file.pth"),
+            )
+            checkpoint = fresh_checkpointer.load(
+                fresh_checkpointer.get_checkpoint_file()
+            )
+            state_dict = checkpointables.state_dict()
+            for key, _ in state_dict.items():
+                self.assertTrue(checkpoint["checkpointables"].get(key) is not None)
+                self.assertTrue(checkpoint["checkpointables"][key] == state_dict[key])
+        pass
+
+    def test_load_reused_params(self) -> None:
+        class Model(nn.Module):
+            def __init__(self, has_y: bool) -> None:
+                super().__init__()
+                self.x = nn.Linear(10, 10)
+                if has_y:
+                    self.y = self.x
+
+        model = Model(has_y=False)
+        model.x.bias.data.fill_(5.0)  # pyre-ignore
+        data = {"model": model.state_dict()}
+        new_model = Model(has_y=True)
+        chkpt = Checkpointer(new_model)
+        # chkpt.logger = logger = MagicMock()
+        incompatible = chkpt._load_model(data)
+        chkpt._log_incompatible_keys(incompatible)
+        self.assertTrue(
+            torch.allclose(new_model.y.bias - 5.0, torch.zeros_like(new_model.y.bias))
+        )
+        # logger.info.assert_not_called()
+        pass
+
+    @unittest.skipIf(  # pyre-fixme[56]
+        not hasattr(nn, "LazyLinear"), "LazyModule not supported"
+    )
+    def test_load_lazy_module(self) -> None:
+        def _get_model() -> nn.Sequential:  # pyre-fixme[11]
+            return nn.Sequential(nn.LazyLinear(10))
+
+        m1, m2 = _get_model(), _get_model()
+        m1(torch.randn(4, 2, 4, 4))  # initialize m1, but not m2
+        # Load m1's checkpoint into m2.
+        with TemporaryDirectory() as f:
+            checkpointer = Checkpointer(m1, save_dir=f)
+            checkpointer.save("checkpoint_file")
+
+            fresh_checkpointer = Checkpointer(m2, save_dir=f)
+            self.assertTrue(fresh_checkpointer.has_checkpoint())
+            self.assertEqual(
+                fresh_checkpointer.get_checkpoint_file(),
+                os.path.join(f, "checkpoint_file.pth"),
+            )
+            checkpointer.load(fresh_checkpointer.get_checkpoint_file())
+            # fresh_checkpointer.load(fresh_checkpointer.get_checkpoint_file())
+            self.assertTrue(torch.equal(m1[0].weight, m2[0].weight))
+        pass
+
+
+class TestPeriodicCheckpointer(unittest.TestCase):
+    def _create_model(self) -> nn.Module:
+        """
+        Create a simple model.
+        """
+        return nn.Sequential(nn.Linear(2, 3), nn.Linear(3, 1))
+
+    def test_periodic_checkpointer(self) -> None:
+        """
+        test that loading works even if they differ by a prefix.
+        """
+        _period = 10
+        _max_iter = 100
+        for trained_model in [
+            self._create_model(),
+            nn.DataParallel(self._create_model()),
+        ]:
+            with TemporaryDirectory() as f:
+                checkpointer = Checkpointer(
+                    trained_model, save_dir=f, save_to_disk=True
+                )
+                periodic_checkpointer = PeriodicCheckpointer(checkpointer, _period, 99)
+                for iteration in range(_max_iter):
+                    periodic_checkpointer.step(iteration)
+                    path = os.path.join(f, "model_{:07d}.pth".format(iteration))
+                    if (iteration + 1) % _period == 0:
+                        self.assertTrue(os.path.exists(path))
+                    else:
+                        self.assertFalse(os.path.exists(path))
+        pass
+
+    def test_periodic_checkpointer_max_to_keep(self) -> None:
+        """
+        Test parameter: max_to_keep
+        """
+        _period = 10
+        _max_iter = 100
+        _max_to_keep = 3
+        for trained_model in [
+            self._create_model(),
+            nn.DataParallel(self._create_model()),
+        ]:
+            with TemporaryDirectory() as f:
+                checkpointer = Checkpointer(
+                    trained_model, save_dir=f, save_to_disk=True
+                )
+                periodic_checkpointer = PeriodicCheckpointer(
+                    checkpointer, _period, 99, max_to_keep=_max_to_keep
+                )
+                for _ in range(2):
+                    checkpoint_paths = []
+
+                    for iteration in range(_max_iter):
+                        periodic_checkpointer.step(iteration)
+                        if (iteration + 1) % _period == 0:
+                            path = os.path.join(f, "model_{:07d}.pth".format(iteration))
+                            checkpoint_paths.append(path)
+
+                    for path in checkpoint_paths[:-_max_to_keep]:
+                        self.assertFalse(os.path.exists(path))
+
+                    for path in checkpoint_paths[-_max_to_keep:]:
+                        self.assertTrue(os.path.exists(path))
+
```

### Comparing `tl2-0.1.0/tl2/proj/logger/__pycache__/logger_utils.cpython-38.pyc` & `tl2-0.1.1/tl2/proj/logger/__pycache__/logger_utils.cpython-38.pyc`

 * *Format-specific differences are supported for Python .pyc files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: Byte-compiled Python module for CPython 3.8, timestamp-based, .py timestamp: Mon Oct  3 09:27:55 2022 UTC, .py size: 8239 bytes*

 * *Could not decompile bytecode: bad marshal data (unknown type code)*

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-00000000: 550d 0d0a 0000 0000 1bab 3a63 2f20 0000  U.........:c/ ..
+00000000: 550d 0d0a 0000 0000 f13c 9362 4921 0000  U........<.bI!..
 00000010: e300 0000 0000 0000 0000 0000 0000 0000  ................
 00000020: 0004 0000 0040 0000 0073 0001 0000 6400  .....@...s....d.
 00000030: 6401 6c00 5a00 6400 6401 6c01 5a02 6400  d.l.Z.d.d.l.Z.d.
 00000040: 6401 6c03 5a03 6400 6401 6c04 5a04 6400  d.l.Z.d.d.l.Z.d.
 00000050: 6401 6c05 5a05 6400 6401 6c06 5a06 6400  d.l.Z.d.d.l.Z.d.
 00000060: 6402 6c07 6d08 5a08 0100 6400 6401 6c09  d.l.m.Z...d.d.l.
 00000070: 5a09 6400 6401 6c0a 5a0a 6403 5a0b 6404  Z.d.d.l.Z.d.Z.d.
@@ -42,449 +42,447 @@
 00000290: da01 2eda 0b61 6262 7265 765f 6e61 6d65  .....abbrev_name
 000002a0: da00 2907 da03 706f 70da 0a5f 726f 6f74  ..)...pop.._root
 000002b0: 5f6e 616d 65da 0c5f 6162 6272 6576 5f6e  _name.._abbrev_n
 000002c0: 616d 65da 036c 656e da05 7375 7065 7272  ame..len..superr
 000002d0: 0300 0000 da08 5f5f 696e 6974 5f5f 2903  ......__init__).
 000002e0: da04 7365 6c66 da04 6172 6773 da06 6b77  ..self..args..kw
 000002f0: 6172 6773 a901 da09 5f5f 636c 6173 735f  args....__class_
-00000300: 5fa9 00fa 592f 686f 6d65 2f6d 612d 7573  _...Y/home/ma-us
-00000310: 6572 2f77 6f72 6b2f 636f 6465 2f73 7479  er/work/code/sty
-00000320: 6c65 6761 6e32 2d61 6461 2d70 7974 6f72  legan2-ada-pytor
-00000330: 6368 2d65 7870 2f74 6c32 5f6c 6962 2f74  ch-exp/tl2_lib/t
-00000340: 6c32 2f70 726f 6a2f 6c6f 6767 6572 2f6c  l2/proj/logger/l
-00000350: 6f67 6765 725f 7574 696c 732e 7079 720d  ogger_utils.pyr.
-00000360: 0000 000f 0000 0073 0a00 0000 0001 1001  .......s........
-00000370: 0e01 0a01 0c01 7a1b 5f43 6f6c 6f72 6675  ......z._Colorfu
-00000380: 6c46 6f72 6d61 7474 6572 2e5f 5f69 6e69  lFormatter.__ini
-00000390: 745f 5f63 0200 0000 0000 0000 0000 0000  t__c............
-000003a0: 0400 0000 0500 0000 0300 0000 737e 0000  ............s~..
-000003b0: 007c 016a 00a0 017c 006a 027c 006a 03a1  .|.j...|.j.|.j..
-000003c0: 027c 015f 0074 0474 057c 0083 02a0 067c  .|._.t.t.|.....|
-000003d0: 01a1 017d 027c 016a 0774 086a 096b 0272  ...}.|.j.t.j.k.r
-000003e0: 4274 0a64 0164 0264 0367 0164 048d 037d  Bt.d.d.d.g.d...}
-000003f0: 036e 307c 016a 0774 086a 0b6b 0273 5a7c  .n0|.j.t.j.k.sZ|
-00000400: 016a 0774 086a 0c6b 0272 6e74 0a64 0564  .j.t.j.k.rnt.d.d
-00000410: 0264 0364 0667 0264 048d 037d 036e 047c  .d.d.g.d...}.n.|
-00000420: 0253 007c 0364 0717 007c 0217 0053 0029  .S.|.d...|...S.)
-00000430: 084e da07 5741 524e 494e 47da 0372 6564  .N..WARNING..red
-00000440: 5a05 626c 696e 6b29 01da 0561 7474 7273  Z.blink)...attrs
-00000450: da05 4552 524f 525a 0975 6e64 6572 6c69  ..ERRORZ.underli
-00000460: 6e65 fa01 2029 0dda 046e 616d 65da 0772  ne.. )...name..r
-00000470: 6570 6c61 6365 7209 0000 0072 0a00 0000  eplacer....r....
-00000480: 720c 0000 0072 0300 0000 da0d 666f 726d  r....r......form
-00000490: 6174 4d65 7373 6167 65da 076c 6576 656c  atMessage..level
-000004a0: 6e6f da07 6c6f 6767 696e 6772 1500 0000  no..loggingr....
-000004b0: 7202 0000 0072 1800 0000 da08 4352 4954  r....r......CRIT
-000004c0: 4943 414c 2904 720e 0000 00da 0672 6563  ICAL).r......rec
-000004d0: 6f72 64da 036c 6f67 da06 7072 6566 6978  ord..log..prefix
-000004e0: 7211 0000 0072 1300 0000 7214 0000 0072  r....r....r....r
-000004f0: 1c00 0000 1600 0000 7310 0000 0000 0114  ........s.......
-00000500: 0110 010c 0112 0118 0114 0204 017a 205f  .............z _
-00000510: 436f 6c6f 7266 756c 466f 726d 6174 7465  ColorfulFormatte
-00000520: 722e 666f 726d 6174 4d65 7373 6167 6529  r.formatMessage)
-00000530: 06da 085f 5f6e 616d 655f 5fda 0a5f 5f6d  ...__name__..__m
-00000540: 6f64 756c 655f 5fda 0c5f 5f71 7561 6c6e  odule__..__qualn
-00000550: 616d 655f 5f72 0d00 0000 721c 0000 00da  ame__r....r.....
-00000560: 0d5f 5f63 6c61 7373 6365 6c6c 5f5f 7213  .__classcell__r.
-00000570: 0000 0072 1300 0000 7211 0000 0072 1400  ...r....r....r..
-00000580: 0000 7203 0000 000e 0000 0073 0400 0000  ..r........s....
-00000590: 0801 0c07 7203 0000 0046 6303 0000 0000  ....r....Fc.....
-000005a0: 0000 0000 0000 0006 0000 0007 0000 0003  ................
-000005b0: 0000 0073 8200 0000 6401 6402 8400 7d03  ...s....d.d...}.
-000005c0: 7c02 7214 7c03 7400 6a01 5f02 7400 6a03  |.r.|.t.j._.t.j.
-000005d0: 7c01 7404 7405 6400 6403 6404 8d05 0100  |.t.t.d.d.d.....
-000005e0: 7400 a006 a100 8900 7c00 726c 7400 6a07  t.......|.rlt.j.
-000005f0: 7c00 6403 6405 8d02 7d04 7c04 6a08 7c01  |.d.d...}.|.j.|.
-00000600: 6406 8d01 0100 7c04 a009 7400 6a01 7404  d.....|...t.j.t.
-00000610: 7405 6407 8d02 a101 0100 8800 a00a 7c04  t.d...........|.
-00000620: a101 0100 8700 6601 6408 6409 8408 7d05  ......f.d.d...}.
-00000630: 7c05 8800 5f0b 8800 5300 290a 4e63 0200  |..._...S.).Nc..
-00000640: 0000 0000 0000 0000 0000 0300 0000 0400  ................
-00000650: 0000 5300 0000 731e 0000 0074 006a 00a0  ..S...s....t.j..
-00000660: 01a1 0074 006a 0264 0164 028d 0117 007d  ...t.j.d.d.....}
-00000670: 027c 02a0 03a1 0053 0029 037a 1773 6563  .|.....S.).z.sec
-00000680: 2061 6e64 2077 6861 7420 6973 2075 6e75   and what is unu
-00000690: 7365 642e e908 0000 0029 01da 0568 6f75  sed......)...hou
-000006a0: 7273 2904 da08 6461 7465 7469 6d65 da03  rs)...datetime..
-000006b0: 6e6f 77da 0974 696d 6564 656c 7461 da09  now..timedelta..
-000006c0: 7469 6d65 7475 706c 6529 035a 0373 6563  timetuple).Z.sec
-000006d0: da04 7768 6174 5a0c 6265 696a 696e 675f  ..whatZ.beijing_
-000006e0: 7469 6d65 7213 0000 0072 1300 0000 7214  timer....r....r.
-000006f0: 0000 00da 0762 6569 6a69 6e67 2300 0000  .....beijing#...
-00000700: 7304 0000 0000 0216 017a 1d6c 6f67 6769  s........z.loggi
-00000710: 6e67 5f69 6e69 742e 3c6c 6f63 616c 733e  ng_init.<locals>
-00000720: 2e62 6569 6a69 6e67 da01 7729 05da 056c  .beijing..w)...l
-00000730: 6576 656c da06 666f 726d 6174 da07 6461  evel..format..da
-00000740: 7465 666d 74da 0866 696c 656e 616d 65da  tefmt..filename.
-00000750: 0866 696c 656d 6f64 65a9 0272 3300 0000  .filemode..r3...
-00000760: da04 6d6f 6465 a901 7230 0000 00a9 0172  ..mode..r0.....r
-00000770: 3200 0000 6300 0000 0000 0000 0000 0000  2...c...........
-00000780: 0004 0000 0006 0000 0017 0000 0073 5a00  .............sZ.
-00000790: 0000 6700 7d01 8800 6a00 4400 5d20 7d02  ..g.}...j.D.] }.
-000007a0: 7c01 a001 7c02 6a02 a101 0100 7c02 a003  |...|.j.....|...
-000007b0: 7404 a005 6401 a101 a101 0100 710a 8800  t...d.......q...
-000007c0: 6a06 7c00 8e00 0100 7407 8800 6a00 7c01  j.|.....t...j.|.
-000007d0: 8302 4400 5d12 5c02 7d02 7d03 7c02 a003  ..D.].\.}.}.|...
-000007e0: 7c03 a101 0100 7142 6400 5300 a902 4efa  |.....qBd.S...N.
-000007f0: 0b25 286d 6573 7361 6765 2973 a908 da08  .%(message)s....
-00000800: 6861 6e64 6c65 7273 da06 6170 7065 6e64  handlers..append
-00000810: da09 666f 726d 6174 7465 72da 0c73 6574  ..formatter..set
-00000820: 466f 726d 6174 7465 7272 1e00 0000 da09  Formatterr......
-00000830: 466f 726d 6174 7465 72da 0469 6e66 6fda  Formatter..info.
-00000840: 037a 6970 a904 da04 6172 6776 da0e 6f72  .zip....argv..or
-00000850: 675f 666f 726d 6174 7465 7273 da07 6861  g_formatters..ha
-00000860: 6e64 6c65 7272 3e00 0000 a901 da06 6c6f  ndlerr>.......lo
-00000870: 6767 6572 7213 0000 0072 1400 0000 da08  ggerr....r......
-00000880: 696e 666f 5f6d 7367 3a00 0000 730e 0000  info_msg:...s...
-00000890: 0000 0204 010a 010c 0112 020a 0314 017a  ...............z
-000008a0: 1e6c 6f67 6769 6e67 5f69 6e69 742e 3c6c  .logging_init.<l
-000008b0: 6f63 616c 733e 2e69 6e66 6f5f 6d73 6729  ocals>.info_msg)
-000008c0: 0c72 1e00 0000 7240 0000 00da 0963 6f6e  .r....r@.....con
-000008d0: 7665 7274 6572 da0b 6261 7369 6343 6f6e  verter..basicCon
-000008e0: 6669 67da 0646 4f52 4d41 54da 0744 4154  fig..FORMAT..DAT
-000008f0: 4546 4d54 da09 6765 744c 6f67 6765 72da  EFMT..getLogger.
-00000900: 0b46 696c 6548 616e 646c 6572 da08 7365  .FileHandler..se
-00000910: 744c 6576 656c 723f 0000 00da 0a61 6464  tLevelr?.....add
-00000920: 4861 6e64 6c65 7272 4900 0000 2906 7233  HandlerrI...).r3
-00000930: 0000 0072 3000 0000 5a0c 636f 7272 6563  ...r0...Z.correc
-00000940: 745f 7469 6d65 722e 0000 005a 0e6c 6f67  t_timer....Z.log
-00000950: 6765 725f 6861 6e64 6c65 7272 4900 0000  ger_handlerrI...
-00000960: 7213 0000 0072 4700 0000 7214 0000 00da  r....rG...r.....
-00000970: 0c6c 6f67 6769 6e67 5f69 6e69 7422 0000  .logging_init"..
-00000980: 0073 2400 0000 0001 0805 0401 0802 0601  .s$.............
-00000990: 0201 0201 0200 02fd 0604 0805 0401 0e01  ................
-000009a0: 0c01 1401 0a02 0c0d 0601 7252 0000 0054  ..........rR...T
-000009b0: 6303 0000 0000 0000 0000 0000 0004 0000  c...............
-000009c0: 0006 0000 0043 0000 0073 2600 0000 7400  .....C...s&...t.
-000009d0: a001 a100 7d03 7c03 a002 7c02 a101 0100  ....}.|...|.....
-000009e0: 7403 7c03 7c00 7c01 7c02 6401 8d04 0100  t.|.|.|.|.d.....
-000009f0: 7c03 5300 2902 4e29 0472 4800 0000 7233  |.S.).N).rH...r3
-00000a00: 0000 00da 0673 7472 6561 6d72 3000 0000  .....streamr0...
-00000a10: 2904 721e 0000 0072 4e00 0000 7250 0000  ).r....rN...rP..
-00000a20: 00da 0a73 6574 5f68 616e 6465 7229 0472  ...set_hander).r
-00000a30: 3300 0000 7253 0000 0072 3000 0000 7248  3...rS...r0...rH
-00000a40: 0000 0072 1300 0000 7213 0000 0072 1400  ...r....r....r..
-00000a50: 0000 da0f 6765 745f 726f 6f74 5f6c 6f67  ....get_root_log
-00000a60: 6765 724d 0000 0073 0800 0000 0001 0801  gerM...s........
-00000a70: 0a01 1001 7255 0000 00da 0c74 656d 706c  ....rU.....templ
-00000a80: 6174 655f 6c69 62da 0274 6cda 0161 6305  ate_lib..tl..ac.
-00000a90: 0000 0000 0000 0000 0000 0007 0000 0008  ................
-00000aa0: 0000 0043 0000 0073 4e00 0000 7400 a001  ...C...sN...t...
-00000ab0: 7c01 a101 7d01 7c01 7c00 6701 3700 7d01  |...}.|.|.g.7.}.
-00000ac0: 7c01 4400 5d30 7d05 7402 a003 7c05 a101  |.D.]0}.t...|...
-00000ad0: 7d06 7c06 a004 7c03 a101 0100 6401 7c06  }.|...|.....d.|.
-00000ae0: 5f05 7406 7c06 7c00 7c02 7c03 7c04 6402  _.t.|.|.|.|.|.d.
-00000af0: 8d05 0100 7118 7c06 5300 2903 7a4c 0a0a  ....q.|.S.).zL..
-00000b00: 2020 3a70 6172 616d 2066 696c 656e 616d    :param filenam
-00000b10: 653a 0a20 203a 7061 7261 6d20 7072 6f70  e:.  :param prop
-00000b20: 6167 6174 653a 2077 6865 7468 6572 206c  agate: whether l
-00000b30: 6f67 2074 6f20 7374 646f 7574 0a20 203a  og to stdout.  :
-00000b40: 7265 7475 726e 3a0a 2020 4629 0572 4800  return:.  F).rH.
-00000b50: 0000 7233 0000 0072 5300 0000 7230 0000  ..r3...rS...r0..
-00000b60: 0072 3600 0000 2907 da04 636f 7079 da08  .r6...)...copy..
-00000b70: 6465 6570 636f 7079 721e 0000 0072 4e00  deepcopyr....rN.
-00000b80: 0000 7250 0000 00da 0970 726f 7061 6761  ..rP.....propaga
-00000b90: 7465 7254 0000 0029 0772 3300 0000 da0c  terT...).r3.....
-00000ba0: 6c6f 6767 6572 5f6e 616d 6573 7253 0000  logger_namesrS..
-00000bb0: 0072 3000 0000 7236 0000 0072 1a00 0000  .r0...r6...r....
-00000bc0: 7248 0000 0072 1300 0000 7213 0000 0072  rH...r....r....r
-00000bd0: 1400 0000 da0a 6765 745f 6c6f 6767 6572  ......get_logger
-00000be0: 5400 0000 7310 0000 0000 0b0a 010a 0108  T...s...........
-00000bf0: 010a 010a 0106 0114 0172 5d00 0000 6301  .........r]...c.
-00000c00: 0000 0000 0000 0000 0000 0003 0000 0004  ................
-00000c10: 0000 0043 0000 0073 3200 0000 7400 7c00  ...C...s2...t.|.
-00000c20: 6a01 8301 7d01 7c01 4400 5d0e 7d02 7c00  j...}.|.D.].}.|.
-00000c30: a002 7c02 a101 0100 710e 7c02 a003 a100  ..|.....q.|.....
-00000c40: 0100 7c02 a004 a100 0100 6400 5300 a901  ..|.......d.S...
-00000c50: 4e29 05da 046c 6973 7472 3c00 0000 da0d  N)...listr<.....
-00000c60: 7265 6d6f 7665 4861 6e64 6c65 72da 0566  removeHandler..f
-00000c70: 6c75 7368 da05 636c 6f73 6529 0372 4800  lush..close).rH.
-00000c80: 0000 723c 0000 0072 4600 0000 7213 0000  ..r<...rF...r...
-00000c90: 0072 1300 0000 7214 0000 00da 1163 6c6f  .r....r......clo
-00000ca0: 7365 5f6c 6f67 6765 725f 6669 6c65 6900  se_logger_filei.
-00000cb0: 0000 730c 0000 0000 010a 0108 010c 0108  ..s.............
-00000cc0: 0108 0172 6300 0000 722f 0000 0063 0400  ...rc...r/...c..
-00000cd0: 0000 0000 0000 0000 0000 0500 0000 0600  ................
-00000ce0: 0000 4300 0000 7314 0000 0074 007c 007c  ..C...s....t.|.|
-00000cf0: 027c 037c 0164 018d 047d 047c 0453 0029  .|.|.d...}.|.S.)
-00000d00: 024e 2904 7233 0000 0072 5c00 0000 7253  .N).r3...r\...rS
-00000d10: 0000 0072 3600 0000 2901 725d 0000 0029  ...r6...).r]...)
-00000d20: 0572 3300 0000 7236 0000 0072 5c00 0000  .r3...r6...r\...
-00000d30: 7253 0000 0072 4800 0000 7213 0000 0072  rS...rH...r....r
-00000d40: 1300 0000 7214 0000 00da 0f67 6574 5f66  ....r......get_f
-00000d50: 696c 655f 6c6f 6767 6572 7100 0000 7304  ile_loggerq...s.
-00000d60: 0000 0000 0410 0172 6400 0000 6305 0000  .......rd...c...
-00000d70: 0000 0000 0000 0000 0009 0000 0006 0000  ................
-00000d80: 0003 0000 0073 a000 0000 7400 6a01 6401  .....s....t.j.d.
-00000d90: 6402 6403 8d02 7d05 7400 6a02 7c01 7c04  d.d...}.t.j.|.|.
-00000da0: 6404 8d02 7d06 7c06 6a03 7c03 6405 8d01  d...}.|.j.|.d...
-00000db0: 0100 7c06 a004 7c05 a101 0100 8800 a005  ..|...|.........
-00000dc0: 7c06 a101 0100 8700 6601 6406 6407 8408  |.......f.d.d...
-00000dd0: 7d07 7c07 8800 5f06 7c02 729c 7400 a007  }.|..._.|.r.t...
-00000de0: a100 7d08 7c08 a003 7c03 a101 0100 7408  ..}.|...|.....t.
-00000df0: 7409 6408 6409 8302 640a 1700 7409 640b  t.d.d...d...t.d.
-00000e00: 6409 8302 1700 6402 640c 640d 640e 8d04  d.....d.d.d.d...
-00000e10: 7d05 7c08 a004 7c05 a101 0100 8800 a005  }.|...|.........
-00000e20: 7c08 a101 0100 8800 5300 290f 4e7a 665b  |.......S.).Nzf[
-00000e30: 2528 6173 6374 696d 6529 735d 2025 286e  %(asctime)s] %(n
-00000e40: 616d 6529 733a 2528 6c69 6e65 6e6f 2973  ame)s:%(lineno)s
-00000e50: 2025 286c 6576 656c 6e61 6d65 2973 3a20   %(levelname)s: 
-00000e60: 2528 6d65 7373 6167 6529 7320 095b 2528  %(message)s .[%(
-00000e70: 6669 6c65 6e61 6d65 2973 3a25 2866 756e  filename)s:%(fun
-00000e80: 634e 616d 6529 7328 293a 2528 6c69 6e65  cName)s():%(line
-00000e90: 6e6f 2973 5d7a 0e25 6d2f 2564 2025 483a  no)s]z.%m/%d %H:
-00000ea0: 254d 3a25 5372 3800 0000 7235 0000 0072  %M:%Sr8...r5...r
-00000eb0: 3700 0000 6300 0000 0000 0000 0000 0000  7...c...........
-00000ec0: 0004 0000 0006 0000 0017 0000 0073 5a00  .............sZ.
-00000ed0: 0000 6700 7d01 8800 6a00 4400 5d20 7d02  ..g.}...j.D.] }.
-00000ee0: 7c01 a001 7c02 6a02 a101 0100 7c02 a003  |...|.j.....|...
-00000ef0: 7404 a005 6401 a101 a101 0100 710a 8800  t...d.......q...
-00000f00: 6a06 7c00 8e00 0100 7407 8800 6a00 7c01  j.|.....t...j.|.
-00000f10: 8302 4400 5d12 5c02 7d02 7d03 7c02 a003  ..D.].\.}.}.|...
-00000f20: 7c03 a101 0100 7142 6400 5300 7239 0000  |.....qBd.S.r9..
-00000f30: 0072 3b00 0000 7243 0000 0072 4700 0000  .r;...rC...rG...
-00000f40: 7213 0000 0072 1400 0000 7249 0000 0085  r....r....rI....
-00000f50: 0000 0073 0e00 0000 0002 0401 0a01 0c01  ...s............
-00000f60: 1202 0a03 1401 7a1c 7365 745f 6861 6e64  ......z.set_hand
-00000f70: 6572 2e3c 6c6f 6361 6c73 3e2e 696e 666f  er.<locals>.info
-00000f80: 5f6d 7367 7a25 5b25 2861 7363 7469 6d65  _msgz%[%(asctime
-00000f90: 2973 5d20 2528 6e61 6d65 2973 2025 286c  )s] %(name)s %(l
-00000fa0: 6576 656c 6e61 6d65 2973 3ada 0462 6c75  evelname)s:..blu
-00000fb0: 657a 0d25 286d 6573 7361 6765 2973 2009  ez.%(message)s .
-00000fc0: 7a28 5b25 2866 696c 656e 616d 6529 733a  z([%(filename)s:
-00000fd0: 2528 6675 6e63 4e61 6d65 2973 2829 3a25  %(funcName)s():%
-00000fe0: 286c 696e 656e 6f29 735d 7256 0000 0072  (lineno)s]rV...r
-00000ff0: 5700 0000 2903 7232 0000 0072 0400 0000  W...).r2...r....
-00001000: 7206 0000 0029 0a72 1e00 0000 7240 0000  r....).r....r@..
-00001010: 0072 4f00 0000 7250 0000 0072 3f00 0000  .rO...rP...r?...
-00001020: 7251 0000 0072 4900 0000 da0d 5374 7265  rQ...rI.....Stre
-00001030: 616d 4861 6e64 6c65 7272 0300 0000 7202  amHandlerr....r.
-00001040: 0000 0029 0972 4800 0000 7233 0000 0072  ...).rH...r3...r
-00001050: 5300 0000 7230 0000 0072 3600 0000 723e  S...r0...r6...r>
-00001060: 0000 005a 0b66 696c 655f 6861 6e64 6572  ...Z.file_hander
-00001070: 7249 0000 005a 0e73 7472 6561 6d5f 6861  rI...Z.stream_ha
-00001080: 6e64 6c65 7272 1300 0000 7247 0000 0072  ndlerr....rG...r
-00001090: 1400 0000 7254 0000 0079 0000 0073 3400  ....rT...y...s4.
-000010a0: 0000 0001 0401 0201 02fe 0606 0e01 0c01  ................
-000010b0: 0a01 0a02 0c0d 0602 0401 0801 0a02 0202  ................
-000010c0: 0801 02ff 0202 08fe 0203 0201 0201 02f9  ................
-000010d0: 0609 0a01 0a02 7254 0000 0063 0000 0000  ......rT...c....
-000010e0: 0000 0000 0000 0000 0000 0000 0200 0000  ................
-000010f0: 4000 0000 7338 0000 0065 005a 0164 005a  @...s8...e.Z.d.Z
-00001100: 0264 015a 0364 0264 0384 005a 0464 0464  .d.Z.d.d...Z.d.d
-00001110: 0584 005a 0564 0664 0784 005a 0664 0864  ...Z.d.d...Z.d.d
-00001120: 0984 005a 0764 0a64 0b84 005a 0864 0c53  ...Z.d.d...Z.d.S
-00001130: 0029 0dda 0e53 7472 6561 6d54 6f4c 6f67  .)...StreamToLog
-00001140: 6765 727a 4e0a 2020 4661 6b65 2066 696c  gerzN.  Fake fil
-00001150: 652d 6c69 6b65 2073 7472 6561 6d20 6f62  e-like stream ob
-00001160: 6a65 6374 2074 6861 7420 7265 6469 7265  ject that redire
-00001170: 6374 7320 7772 6974 6573 2074 6f20 6120  cts writes to a 
-00001180: 6c6f 6767 6572 2069 6e73 7461 6e63 652e  logger instance.
-00001190: 0a20 2063 0200 0000 0000 0000 0000 0000  .  c............
-000011a0: 0200 0000 0200 0000 4300 0000 7310 0000  ........C...s...
-000011b0: 007c 017c 005f 0064 017c 005f 0164 0053  .|.|._.d.|._.d.S
-000011c0: 0029 024e 7207 0000 0029 0272 4800 0000  .).Nr....).rH...
-000011d0: 5a07 6c69 6e65 6275 6629 0272 0e00 0000  Z.linebuf).r....
-000011e0: 7248 0000 0072 1300 0000 7213 0000 0072  rH...r....r....r
-000011f0: 1400 0000 720d 0000 00ac 0000 0073 0400  ....r........s..
-00001200: 0000 0001 0601 7a17 5374 7265 616d 546f  ......z.StreamTo
-00001210: 4c6f 6767 6572 2e5f 5f69 6e69 745f 5f63  Logger.__init__c
-00001220: 0200 0000 0000 0000 0000 0000 0500 0000  ................
-00001230: 0600 0000 4300 0000 737a 0000 007c 01a0  ....C...sz...|..
-00001240: 0064 01a1 017d 017c 0173 1264 0053 0064  .d...}.|.s.d.S.d
-00001250: 027c 0117 007d 0167 007d 027c 006a 016a  .|...}.g.}.|.j.j
-00001260: 0244 005d 207d 037c 02a0 037c 036a 04a1  .D.] }.|...|.j..
-00001270: 0101 007c 03a0 0574 06a0 0764 03a1 01a1  ...|...t...d....
-00001280: 0101 0071 267c 006a 01a0 087c 01a1 0101  ...q&|.j...|....
-00001290: 0074 097c 006a 016a 027c 0283 0244 005d  .t.|.j.j.|...D.]
-000012a0: 125c 027d 037d 047c 03a0 057c 04a1 0101  .\.}.}.|...|....
-000012b0: 0071 6264 0053 0029 044e da01 0a7a 033c  .qbd.S.).N...z.<
-000012c0: 3e20 723a 0000 0029 0ada 0672 7374 7269  > r:...)...rstri
-000012d0: 7072 4800 0000 723c 0000 0072 3d00 0000  prH...r<...r=...
-000012e0: 723e 0000 0072 3f00 0000 721e 0000 0072  r>...r?...r....r
-000012f0: 4000 0000 7241 0000 0072 4200 0000 2905  @...rA...rB...).
-00001300: 720e 0000 00da 0362 7566 7245 0000 0072  r......bufrE...r
-00001310: 4600 0000 723e 0000 0072 1300 0000 7213  F...r>...r....r.
-00001320: 0000 0072 1400 0000 da05 7772 6974 65b0  ...r......write.
-00001330: 0000 0073 1600 0000 0001 0a01 0401 0401  ...s............
-00001340: 0803 0401 0c01 0c01 1201 0c02 1601 7a14  ..............z.
-00001350: 5374 7265 616d 546f 4c6f 6767 6572 2e77  StreamToLogger.w
-00001360: 7269 7465 6301 0000 0000 0000 0000 0000  ritec...........
-00001370: 0001 0000 0001 0000 0043 0000 0073 0400  .........C...s..
-00001380: 0000 6400 5300 725e 0000 0072 1300 0000  ..d.S.r^...r....
-00001390: a901 720e 0000 0072 1300 0000 7213 0000  ..r....r....r...
-000013a0: 0072 1400 0000 7261 0000 00c0 0000 0073  .r....ra.......s
-000013b0: 0200 0000 0001 7a14 5374 7265 616d 546f  ......z.StreamTo
-000013c0: 4c6f 6767 6572 2e66 6c75 7368 6301 0000  Logger.flushc...
-000013d0: 0000 0000 0000 0000 0001 0000 0001 0000  ................
-000013e0: 0043 0000 0073 0400 0000 6400 5300 725e  .C...s....d.S.r^
-000013f0: 0000 0072 1300 0000 726c 0000 0072 1300  ...r....rl...r..
-00001400: 0000 7213 0000 0072 1400 0000 da08 6765  ..r....r......ge
-00001410: 7476 616c 7565 c300 0000 7302 0000 0000  tvalue....s.....
-00001420: 017a 1753 7472 6561 6d54 6f4c 6f67 6765  .z.StreamToLogge
-00001430: 722e 6765 7476 616c 7565 6301 0000 0000  r.getvaluec.....
-00001440: 0000 0000 0000 0001 0000 0001 0000 0043  ...............C
-00001450: 0000 0073 0400 0000 6400 5300 725e 0000  ...s....d.S.r^..
-00001460: 0072 1300 0000 726c 0000 0072 1300 0000  .r....rl...r....
-00001470: 7213 0000 0072 1400 0000 7262 0000 00c6  r....r....rb....
-00001480: 0000 0073 0200 0000 0001 7a14 5374 7265  ...s......z.Stre
-00001490: 616d 546f 4c6f 6767 6572 2e63 6c6f 7365  amToLogger.close
-000014a0: 4e29 0972 2300 0000 7224 0000 0072 2500  N).r#...r$...r%.
-000014b0: 0000 da07 5f5f 646f 635f 5f72 0d00 0000  ....__doc__r....
-000014c0: 726b 0000 0072 6100 0000 726d 0000 0072  rk...ra...rm...r
-000014d0: 6200 0000 7213 0000 0072 1300 0000 7213  b...r....r....r.
-000014e0: 0000 0072 1400 0000 7267 0000 00a7 0000  ...r....rg......
-000014f0: 0073 0c00 0000 0801 0404 0804 0810 0803  .s..............
-00001500: 0803 7267 0000 0063 0100 0000 0000 0000  ..rg...c........
-00001510: 0000 0000 0200 0000 0200 0000 4300 0000  ............C...
-00001520: 7318 0000 0074 007c 0083 017d 017c 0174  s....t.|...}.|.t
-00001530: 015f 027c 0174 015f 0364 0053 0072 5e00  ._.|.t._.d.S.r^.
-00001540: 0000 2904 7267 0000 00da 0373 7973 da06  ..).rg.....sys..
-00001550: 7374 646f 7574 da06 7374 6465 7272 2902  stdout..stderr).
-00001560: 7248 0000 00da 0273 6c72 1300 0000 7213  rH.....slr....r.
-00001570: 0000 0072 1400 0000 da18 7265 6469 7265  ...r......redire
-00001580: 6374 5f70 7269 6e74 5f74 6f5f 6c6f 6767  ct_print_to_logg
-00001590: 6572 ca00 0000 7308 0000 0000 0108 0106  er....s.........
-000015a0: 0106 0172 7300 0000 6301 0000 0000 0000  ...rs...c.......
-000015b0: 0000 0000 0005 0000 0006 0000 0047 0000  .............G..
-000015c0: 0073 5a00 0000 6700 7d02 7c00 6a00 4400  .sZ...g.}.|.j.D.
-000015d0: 5d20 7d03 7c02 a001 7c03 6a02 a101 0100  ] }.|...|.j.....
-000015e0: 7c03 a003 7404 a005 6401 a101 a101 0100  |...t...d.......
-000015f0: 710a 7c00 6a06 7c01 8e00 0100 7407 7c00  q.|.j.|.....t.|.
-00001600: 6a00 7c02 8302 4400 5d12 5c02 7d03 7d04  j.|...D.].\.}.}.
-00001610: 7c03 a003 7c04 a101 0100 7142 6400 5300  |...|.....qBd.S.
-00001620: 7239 0000 0072 3b00 0000 2905 7248 0000  r9...r;...).rH..
-00001630: 0072 4400 0000 7245 0000 0072 4600 0000  .rD...rE...rF...
-00001640: 723e 0000 0072 1300 0000 7213 0000 0072  r>...r....r....r
-00001650: 1400 0000 7249 0000 00d1 0000 0073 0e00  ....rI.......s..
-00001660: 0000 0002 0401 0a01 0c01 1202 0a03 1401  ................
-00001670: 7249 0000 0063 0000 0000 0000 0000 0000  rI...c..........
-00001680: 0000 0000 0000 0300 0000 4000 0000 7316  ..........@...s.
-00001690: 0000 0065 005a 0164 005a 0264 0564 0264  ...e.Z.d.Z.d.d.d
-000016a0: 0384 015a 0364 0453 0029 06da 0e54 6573  ...Z.d.S.)...Tes
-000016b0: 7469 6e67 5f4c 6f67 6765 7254 6302 0000  ting_LoggerTc...
-000016c0: 0000 0000 0000 0000 0016 0000 0005 0000  ................
-000016d0: 0043 0000 0073 6801 0000 6401 7400 6a01  .C...sh...d.t.j.
-000016e0: 6b07 7214 6402 7400 6a01 6401 3c00 6403  k.r.d.t.j.d.<.d.
-000016f0: 7400 6a01 6b07 7228 6402 7400 6a01 6403  t.j.k.r(d.t.j.d.
-00001700: 3c00 6404 6405 6c02 6d03 7d02 0100 6404  <.d.d.l.m.}...d.
-00001710: 6406 6c04 6d05 7d03 6d06 7d04 6d07 7d05  d.l.m.}.m.}.m.}.
-00001720: 6d08 7d06 0100 7c02 6a09 6407 740a 6a0b  m.}...|.j.d.t.j.
-00001730: 6408 6409 8d03 7d07 640a a00c 7c07 a101  d.d...}.d...|...
-00001740: 7d08 740d 640b 7c08 9b00 9d02 8301 0100  }.t.d.|.........
-00001750: 7c03 7c00 740a a00e a100 6a0f 6a10 7411  |.|.t.....j.j.t.
-00001760: 640c 8d03 5c02 7d09 7d0a 640d 7c0a 9b00  d...\.}.}.d.|...
-00001770: 640e 7c08 9b00 640f 9d05 7d0b 7c04 7c0b  d.|...d...}.|.|.
-00001780: 6410 6411 8d02 5c02 7d0c 7d0d 7412 7400  d.d...\.}.}.t.t.
-00001790: 6a01 6401 1900 a013 6412 a101 8301 7d0e  j.d.....d.....}.
-000017a0: 7400 6a01 a014 6413 6414 a102 7d0f 7415  t.j...d.d...}.t.
-000017b0: a016 6415 a101 7d10 7c0c 6a17 9b00 6416  ..d...}.|.j...d.
-000017c0: 9d02 7d11 7418 7c11 6410 6417 8d02 7d12  ..}.t.|.d.d...}.
-000017d0: 7c0c 6a17 9b00 6418 9d02 7d13 7418 7c13  |.j...d...}.t.|.
-000017e0: 6419 6417 8d02 7d14 7419 641a 8301 4400  d.d...}.t.d...D.
-000017f0: 5d16 7d15 7c10 a01a 641b 7c15 9b00 9d02  ].}.|...d.|.....
-00001800: a101 0100 9001 7116 7419 641c 8301 4400  ......q.t.d...D.
-00001810: 5d10 7d15 7c12 a01a 7c15 a101 0100 9001  ].}.|...|.......
-00001820: 7136 7419 641c 641d 8302 4400 5d10 7d15  q6t.d.d...D.].}.
-00001830: 7c14 a01a 7c15 a101 0100 9001 7152 641e  |...|.......qRd.
-00001840: 5300 291f 6142 0100 000a 2020 2020 5573  S.).aB....    Us
-00001850: 6167 653a 0a0a 2020 2020 2020 2020 6578  age:..        ex
-00001860: 706f 7274 2043 5544 415f 5649 5349 424c  port CUDA_VISIBL
-00001870: 455f 4445 5649 4345 533d 302c 312c 322c  E_DEVICES=0,1,2,
-00001880: 332c 342c 352c 362c 370a 2020 2020 2020  3,4,5,6,7.      
-00001890: 2020 6578 706f 7274 2054 494d 455f 5354    export TIME_ST
-000018a0: 523d 310a 2020 2020 2020 2020 6578 706f  R=1.        expo
-000018b0: 7274 2050 5954 484f 4e50 4154 483d 2e0a  rt PYTHONPATH=..
-000018c0: 2020 2020 2020 2020 7079 7468 6f6e 202d          python -
-000018d0: 6320 2266 726f 6d20 746c 322e 6c61 756e  c "from tl2.laun
-000018e0: 6368 2e74 6573 7473 2e74 6573 745f 6c61  ch.tests.test_la
-000018f0: 756e 6368 2069 6d70 6f72 7420 5465 7374  unch import Test
-00001900: 696e 675f 4c61 756e 6368 5f76 313b 2020  ing_Launch_v1;  
-00001910: 2020 2020 2020 2020 5465 7374 696e 675f          Testing_
-00001920: 4c61 756e 6368 5f76 3128 292e 7465 7374  Launch_v1().test
-00001930: 5f6c 6175 6e63 685f 6464 7028 6465 6275  _launch_ddp(debu
-00001940: 673d 4661 6c73 6529 2220 2020 2020 2020  g=False)"       
-00001950: 2020 2020 2d2d 746c 5f6f 7074 7320 7465      --tl_opts te
-00001960: 7374 3020 3130 2074 6573 7431 2031 3120  st0 10 test1 11 
-00001970: 2d2d 7465 7374 2031 0a0a 2020 2020 3a72  --test 1..    :r
-00001980: 6574 7572 6e3a 0a20 2020 20da 1443 5544  eturn:.    ..CUD
-00001990: 415f 5649 5349 424c 455f 4445 5649 4345  A_VISIBLE_DEVICE
-000019a0: 53da 0130 da08 5449 4d45 5f53 5452 7201  S..0..TIME_STRr.
-000019b0: 0000 0029 01da 0974 6c32 5f75 7469 6c73  ...)...tl2_utils
-000019c0: 2904 da16 6765 745f 636f 6d6d 616e 645f  )...get_command_
-000019d0: 616e 645f 6f75 7464 6972 da15 7365 7475  and_outdir..setu
-000019e0: 705f 6f75 7464 6972 5f61 6e64 5f79 616d  p_outdir_and_yam
-000019f0: 6cda 1267 6574 5f61 7070 656e 645f 636d  l..get_append_cm
-00001a00: 645f 7374 72da 0d73 7461 7274 5f63 6d64  d_str..start_cmd
-00001a10: 5f72 756e 7a09 2d2d 746c 5f6f 7074 7372  _runz.--tl_optsr
-00001a20: 5f00 0000 2903 721a 0000 00da 0961 7267  _...).r......arg
-00001a30: 765f 6c69 7374 da04 7479 7065 7219 0000  v_list..typer...
-00001a40: 007a 0a74 6c5f 6f70 7473 3a0a 2029 02da  .z.tl_opts:. )..
-00001a50: 0966 756e 635f 6e61 6d65 da04 6669 6c65  .func_name..file
-00001a60: 7a65 0a20 2020 2020 2020 2020 2020 2020  ze.             
-00001a70: 2020 202d 2d74 6c5f 636f 6e66 6967 5f66     --tl_config_f
-00001a80: 696c 6520 6e6f 6e65 0a20 2020 2020 2020  ile none.       
-00001a90: 2020 2020 2020 2020 202d 2d74 6c5f 636f           --tl_co
-00001aa0: 6d6d 616e 6420 6e6f 6e65 0a20 2020 2020  mmand none.     
-00001ab0: 2020 2020 2020 2020 2020 202d 2d74 6c5f             --tl_
-00001ac0: 6f75 7464 6972 207a 1b0a 2020 2020 2020  outdir z..      
-00001ad0: 2020 2020 2020 2020 2020 2d2d 746c 5f6f            --tl_o
-00001ae0: 7074 7320 7a11 0a20 2020 2020 2020 2020  pts z..         
-00001af0: 2020 2020 2020 2054 2901 da0a 7265 7475         T)...retu
-00001b00: 726e 5f63 6667 fa01 2cda 0450 4f52 5469  rn_cfg..,..PORTi
-00001b10: b822 0000 7257 0000 007a 0a2f 7465 7374  ."..rW...z./test
-00001b20: 312e 7478 7429 0172 5300 0000 7a0a 2f74  1.txt).rS...z./t
-00001b30: 6573 7432 2e74 7874 46e9 0500 0000 7a07  est2.txtF.....z.
-00001b40: 6c6f 6767 6572 20e9 0a00 0000 e914 0000  logger .........
-00001b50: 004e 291b da02 6f73 da07 656e 7669 726f  .N)...os..enviro
-00001b60: 6eda 0374 6c32 7278 0000 00da 1774 6c32  n..tl2rx.....tl2
-00001b70: 2e6c 6175 6e63 682e 6c61 756e 6368 5f75  .launch.launch_u
-00001b80: 7469 6c73 7279 0000 0072 7a00 0000 727b  tilsry...rz...r{
-00001b90: 0000 0072 7c00 0000 da15 7061 7273 6572  ...r|.....parser
-00001ba0: 5f61 7267 735f 6672 6f6d 5f6c 6973 7472  _args_from_listr
-00001bb0: 6f00 0000 7244 0000 00da 046a 6f69 6eda  o...rD.....join.
-00001bc0: 0570 7269 6e74 da09 5f67 6574 6672 616d  .print.._getfram
-00001bd0: 65da 0666 5f63 6f64 65da 0763 6f5f 6e61  e..f_code..co_na
-00001be0: 6d65 da08 5f5f 6669 6c65 5f5f 720b 0000  me..__file__r...
-00001bf0: 00da 0573 706c 6974 da03 6765 7472 1e00  ...split..getr..
-00001c00: 0000 724e 0000 00da 0974 6c5f 6f75 7464  ..rN.....tl_outd
-00001c10: 6972 7264 0000 00da 0572 616e 6765 7249  irrd.....rangerI
-00001c20: 0000 0029 1672 0e00 0000 da05 6465 6275  ...).r......debu
-00001c30: 6772 7800 0000 7279 0000 0072 7a00 0000  grx...ry...rz...
-00001c40: 727b 0000 0072 7c00 0000 da0c 746c 5f6f  r{...r|.....tl_o
-00001c50: 7074 735f 6c69 7374 da07 746c 5f6f 7074  pts_list..tl_opt
-00001c60: 73da 0763 6f6d 6d61 6e64 da06 6f75 7464  s..command..outd
-00001c70: 6972 da08 6172 6776 5f73 7472 720f 0000  ir..argv_strr...
-00001c80: 00da 0363 6667 da06 6e5f 6770 7573 7283  ...cfg..n_gpusr.
-00001c90: 0000 0072 4800 0000 5a05 6669 6c65 315a  ...rH...Z.file1Z
-00001ca0: 0766 696c 6531 5f66 5a05 6669 6c65 325a  .file1_fZ.file2Z
-00001cb0: 0766 696c 6532 5f66 da01 6972 1300 0000  .file2_f..ir....
-00001cc0: 7213 0000 0072 1400 0000 da14 7465 7374  r....r......test
-00001cd0: 5f67 6574 5f66 696c 655f 6c6f 6767 6572  _get_file_logger
-00001ce0: e100 0000 733c 0000 0000 0d0a 010a 010a  ....s<..........
-00001cf0: 010a 010c 0118 0312 010a 010e 041a 0102  ................
-00001d00: 0302 fd04 0402 fc08 0610 0214 010e 020a  ................
-00001d10: 020c 010c 010c 010c 020c 0114 020c 010e  ................
-00001d20: 010e 010e 027a 2354 6573 7469 6e67 5f4c  .....z#Testing_L
-00001d30: 6f67 6765 722e 7465 7374 5f67 6574 5f66  ogger.test_get_f
-00001d40: 696c 655f 6c6f 6767 6572 4e29 0154 2904  ile_loggerN).T).
-00001d50: 7223 0000 0072 2400 0000 7225 0000 0072  r#...r$...r%...r
-00001d60: 9f00 0000 7213 0000 0072 1300 0000 7213  ....r....r....r.
-00001d70: 0000 0072 1400 0000 7274 0000 00df 0000  ...r....rt......
-00001d80: 0073 0200 0000 0802 7274 0000 0029 1dda  .s......rt...)..
-00001d90: 0474 696d 65da 056e 756d 7079 da02 6e70  .time..numpy..np
-00001da0: 721e 0000 0072 8700 0000 7229 0000 0072  r....r....r)...r
-00001db0: 6f00 0000 da09 7465 726d 636f 6c6f 7272  o.....termcolorr
-00001dc0: 0200 0000 da08 756e 6974 7465 7374 7259  ......unittestrY
-00001dd0: 0000 0072 4c00 0000 724d 0000 0072 4000  ...rL...rM...r@.
-00001de0: 0000 7203 0000 00da 0449 4e46 4f72 5200  ..r......INFOrR.
-00001df0: 0000 7255 0000 00da 0544 4542 5547 725d  ..rU.....DEBUGr]
-00001e00: 0000 0072 6300 0000 7264 0000 0072 5400  ...rc...rd...rT.
-00001e10: 0000 da06 6f62 6a65 6374 7267 0000 0072  ....objectrg...r
-00001e20: 7300 0000 7249 0000 00da 0854 6573 7443  s...rI.....TestC
-00001e30: 6173 6572 7400 0000 7213 0000 0072 1300  asert...r....r..
-00001e40: 0000 7213 0000 0072 1400 0000 da08 3c6d  ..r....r......<m
-00001e50: 6f64 756c 653e 0100 0000 7336 0000 0008  odule>....s6....
-00001e60: 0108 0110 0108 0108 010c 0108 0108 0204  ................
-00001e70: 0104 0312 1412 2b10 0806 0102 0104 0102  ......+.........
-00001e80: fc0a 1508 0902 0102 0102 fd0a 0812 2e10  ................
-00001e90: 2308 0708 0e                             #....
+00000300: 5fa9 00fa 3b2f 686f 6d65 2f6d 612d 7573  _...;/home/ma-us
+00000310: 6572 2f77 6f72 6b2f 636f 6465 2f74 6c32  er/work/code/tl2
+00000320: 2f74 6c32 2f70 726f 6a2f 6c6f 6767 6572  /tl2/proj/logger
+00000330: 2f6c 6f67 6765 725f 7574 696c 732e 7079  /logger_utils.py
+00000340: 720d 0000 000f 0000 0073 0a00 0000 0001  r........s......
+00000350: 1001 0e01 0a01 0c01 7a1b 5f43 6f6c 6f72  ........z._Color
+00000360: 6675 6c46 6f72 6d61 7474 6572 2e5f 5f69  fulFormatter.__i
+00000370: 6e69 745f 5f63 0200 0000 0000 0000 0000  nit__c..........
+00000380: 0000 0400 0000 0500 0000 0300 0000 737e  ..............s~
+00000390: 0000 007c 016a 00a0 017c 006a 027c 006a  ...|.j...|.j.|.j
+000003a0: 03a1 027c 015f 0074 0474 057c 0083 02a0  ...|._.t.t.|....
+000003b0: 067c 01a1 017d 027c 016a 0774 086a 096b  .|...}.|.j.t.j.k
+000003c0: 0272 4274 0a64 0164 0264 0367 0164 048d  .rBt.d.d.d.g.d..
+000003d0: 037d 036e 307c 016a 0774 086a 0b6b 0273  .}.n0|.j.t.j.k.s
+000003e0: 5a7c 016a 0774 086a 0c6b 0272 6e74 0a64  Z|.j.t.j.k.rnt.d
+000003f0: 0564 0264 0364 0667 0264 048d 037d 036e  .d.d.d.g.d...}.n
+00000400: 047c 0253 007c 0364 0717 007c 0217 0053  .|.S.|.d...|...S
+00000410: 0029 084e da07 5741 524e 494e 47da 0372  .).N..WARNING..r
+00000420: 6564 5a05 626c 696e 6b29 01da 0561 7474  edZ.blink)...att
+00000430: 7273 da05 4552 524f 525a 0975 6e64 6572  rs..ERRORZ.under
+00000440: 6c69 6e65 fa01 2029 0dda 046e 616d 65da  line.. )...name.
+00000450: 0772 6570 6c61 6365 7209 0000 0072 0a00  .replacer....r..
+00000460: 0000 720c 0000 0072 0300 0000 da0d 666f  ..r....r......fo
+00000470: 726d 6174 4d65 7373 6167 65da 076c 6576  rmatMessage..lev
+00000480: 656c 6e6f da07 6c6f 6767 696e 6772 1500  elno..loggingr..
+00000490: 0000 7202 0000 0072 1800 0000 da08 4352  ..r....r......CR
+000004a0: 4954 4943 414c 2904 720e 0000 00da 0672  ITICAL).r......r
+000004b0: 6563 6f72 64da 036c 6f67 da06 7072 6566  ecord..log..pref
+000004c0: 6978 7211 0000 0072 1300 0000 7214 0000  ixr....r....r...
+000004d0: 0072 1c00 0000 1600 0000 7310 0000 0000  .r........s.....
+000004e0: 0114 0110 010c 0112 0118 0114 0204 017a  ...............z
+000004f0: 205f 436f 6c6f 7266 756c 466f 726d 6174   _ColorfulFormat
+00000500: 7465 722e 666f 726d 6174 4d65 7373 6167  ter.formatMessag
+00000510: 6529 06da 085f 5f6e 616d 655f 5fda 0a5f  e)...__name__.._
+00000520: 5f6d 6f64 756c 655f 5fda 0c5f 5f71 7561  _module__..__qua
+00000530: 6c6e 616d 655f 5f72 0d00 0000 721c 0000  lname__r....r...
+00000540: 00da 0d5f 5f63 6c61 7373 6365 6c6c 5f5f  ...__classcell__
+00000550: 7213 0000 0072 1300 0000 7211 0000 0072  r....r....r....r
+00000560: 1400 0000 7203 0000 000e 0000 0073 0400  ....r........s..
+00000570: 0000 0801 0c07 7203 0000 0046 6303 0000  ......r....Fc...
+00000580: 0000 0000 0000 0000 0006 0000 0007 0000  ................
+00000590: 0003 0000 0073 8200 0000 6401 6402 8400  .....s....d.d...
+000005a0: 7d03 7c02 7214 7c03 7400 6a01 5f02 7400  }.|.r.|.t.j._.t.
+000005b0: 6a03 7c01 7404 7405 6400 6403 6404 8d05  j.|.t.t.d.d.d...
+000005c0: 0100 7400 a006 a100 8900 7c00 726c 7400  ..t.......|.rlt.
+000005d0: 6a07 7c00 6403 6405 8d02 7d04 7c04 6a08  j.|.d.d...}.|.j.
+000005e0: 7c01 6406 8d01 0100 7c04 a009 7400 6a01  |.d.....|...t.j.
+000005f0: 7404 7405 6407 8d02 a101 0100 8800 a00a  t.t.d...........
+00000600: 7c04 a101 0100 8700 6601 6408 6409 8408  |.......f.d.d...
+00000610: 7d05 7c05 8800 5f0b 8800 5300 290a 4e63  }.|..._...S.).Nc
+00000620: 0200 0000 0000 0000 0000 0000 0300 0000  ................
+00000630: 0400 0000 5300 0000 731e 0000 0074 006a  ....S...s....t.j
+00000640: 00a0 01a1 0074 006a 0264 0164 028d 0117  .....t.j.d.d....
+00000650: 007d 027c 02a0 03a1 0053 0029 037a 1773  .}.|.....S.).z.s
+00000660: 6563 2061 6e64 2077 6861 7420 6973 2075  ec and what is u
+00000670: 6e75 7365 642e e908 0000 0029 01da 0568  nused......)...h
+00000680: 6f75 7273 2904 da08 6461 7465 7469 6d65  ours)...datetime
+00000690: da03 6e6f 77da 0974 696d 6564 656c 7461  ..now..timedelta
+000006a0: da09 7469 6d65 7475 706c 6529 035a 0373  ..timetuple).Z.s
+000006b0: 6563 da04 7768 6174 5a0c 6265 696a 696e  ec..whatZ.beijin
+000006c0: 675f 7469 6d65 7213 0000 0072 1300 0000  g_timer....r....
+000006d0: 7214 0000 00da 0762 6569 6a69 6e67 2300  r......beijing#.
+000006e0: 0000 7304 0000 0000 0216 017a 1d6c 6f67  ..s........z.log
+000006f0: 6769 6e67 5f69 6e69 742e 3c6c 6f63 616c  ging_init.<local
+00000700: 733e 2e62 6569 6a69 6e67 da01 7729 05da  s>.beijing..w)..
+00000710: 056c 6576 656c da06 666f 726d 6174 da07  .level..format..
+00000720: 6461 7465 666d 74da 0866 696c 656e 616d  datefmt..filenam
+00000730: 65da 0866 696c 656d 6f64 65a9 0272 3300  e..filemode..r3.
+00000740: 0000 da04 6d6f 6465 a901 7230 0000 00a9  ....mode..r0....
+00000750: 0172 3200 0000 6300 0000 0000 0000 0000  .r2...c.........
+00000760: 0000 0004 0000 0006 0000 0017 0000 0073  ...............s
+00000770: 5a00 0000 6700 7d01 8800 6a00 4400 5d20  Z...g.}...j.D.] 
+00000780: 7d02 7c01 a001 7c02 6a02 a101 0100 7c02  }.|...|.j.....|.
+00000790: a003 7404 a005 6401 a101 a101 0100 710a  ..t...d.......q.
+000007a0: 8800 6a06 7c00 8e00 0100 7407 8800 6a00  ..j.|.....t...j.
+000007b0: 7c01 8302 4400 5d12 5c02 7d02 7d03 7c02  |...D.].\.}.}.|.
+000007c0: a003 7c03 a101 0100 7142 6400 5300 a902  ..|.....qBd.S...
+000007d0: 4efa 0b25 286d 6573 7361 6765 2973 a908  N..%(message)s..
+000007e0: da08 6861 6e64 6c65 7273 da06 6170 7065  ..handlers..appe
+000007f0: 6e64 da09 666f 726d 6174 7465 72da 0c73  nd..formatter..s
+00000800: 6574 466f 726d 6174 7465 7272 1e00 0000  etFormatterr....
+00000810: da09 466f 726d 6174 7465 72da 0469 6e66  ..Formatter..inf
+00000820: 6fda 037a 6970 a904 da04 6172 6776 da0e  o..zip....argv..
+00000830: 6f72 675f 666f 726d 6174 7465 7273 da07  org_formatters..
+00000840: 6861 6e64 6c65 7272 3e00 0000 a901 da06  handlerr>.......
+00000850: 6c6f 6767 6572 7213 0000 0072 1400 0000  loggerr....r....
+00000860: da08 696e 666f 5f6d 7367 3a00 0000 730e  ..info_msg:...s.
+00000870: 0000 0000 0204 010a 010c 0112 020a 0314  ................
+00000880: 017a 1e6c 6f67 6769 6e67 5f69 6e69 742e  .z.logging_init.
+00000890: 3c6c 6f63 616c 733e 2e69 6e66 6f5f 6d73  <locals>.info_ms
+000008a0: 6729 0c72 1e00 0000 7240 0000 00da 0963  g).r....r@.....c
+000008b0: 6f6e 7665 7274 6572 da0b 6261 7369 6343  onverter..basicC
+000008c0: 6f6e 6669 67da 0646 4f52 4d41 54da 0744  onfig..FORMAT..D
+000008d0: 4154 4546 4d54 da09 6765 744c 6f67 6765  ATEFMT..getLogge
+000008e0: 72da 0b46 696c 6548 616e 646c 6572 da08  r..FileHandler..
+000008f0: 7365 744c 6576 656c 723f 0000 00da 0a61  setLevelr?.....a
+00000900: 6464 4861 6e64 6c65 7272 4900 0000 2906  ddHandlerrI...).
+00000910: 7233 0000 0072 3000 0000 5a0c 636f 7272  r3...r0...Z.corr
+00000920: 6563 745f 7469 6d65 722e 0000 005a 0e6c  ect_timer....Z.l
+00000930: 6f67 6765 725f 6861 6e64 6c65 7272 4900  ogger_handlerrI.
+00000940: 0000 7213 0000 0072 4700 0000 7214 0000  ..r....rG...r...
+00000950: 00da 0c6c 6f67 6769 6e67 5f69 6e69 7422  ...logging_init"
+00000960: 0000 0073 2400 0000 0001 0805 0401 0802  ...s$...........
+00000970: 0601 0201 0201 0200 02fd 0604 0805 0401  ................
+00000980: 0e01 0c01 1401 0a02 0c0d 0601 7252 0000  ............rR..
+00000990: 0054 6303 0000 0000 0000 0000 0000 0004  .Tc.............
+000009a0: 0000 0006 0000 0043 0000 0073 2600 0000  .......C...s&...
+000009b0: 7400 a001 a100 7d03 7c03 a002 7c02 a101  t.....}.|...|...
+000009c0: 0100 7403 7c03 7c00 7c01 7c02 6401 8d04  ..t.|.|.|.|.d...
+000009d0: 0100 7c03 5300 2902 4e29 0472 4800 0000  ..|.S.).N).rH...
+000009e0: 7233 0000 00da 0673 7472 6561 6d72 3000  r3.....streamr0.
+000009f0: 0000 2904 721e 0000 0072 4e00 0000 7250  ..).r....rN...rP
+00000a00: 0000 00da 0a73 6574 5f68 616e 6465 7229  .....set_hander)
+00000a10: 0472 3300 0000 7253 0000 0072 3000 0000  .r3...rS...r0...
+00000a20: 7248 0000 0072 1300 0000 7213 0000 0072  rH...r....r....r
+00000a30: 1400 0000 da0f 6765 745f 726f 6f74 5f6c  ......get_root_l
+00000a40: 6f67 6765 724d 0000 0073 0800 0000 0001  oggerM...s......
+00000a50: 0801 0a01 1001 7255 0000 00da 0c74 656d  ......rU.....tem
+00000a60: 706c 6174 655f 6c69 62da 0274 6cda 0161  plate_lib..tl..a
+00000a70: 6305 0000 0000 0000 0000 0000 0007 0000  c...............
+00000a80: 0008 0000 0043 0000 0073 4e00 0000 7400  .....C...sN...t.
+00000a90: a001 7c01 a101 7d01 7c01 7c00 6701 3700  ..|...}.|.|.g.7.
+00000aa0: 7d01 7c01 4400 5d30 7d05 7402 a003 7c05  }.|.D.]0}.t...|.
+00000ab0: a101 7d06 7c06 a004 7c03 a101 0100 6401  ..}.|...|.....d.
+00000ac0: 7c06 5f05 7406 7c06 7c00 7c02 7c03 7c04  |._.t.|.|.|.|.|.
+00000ad0: 6402 8d05 0100 7118 7c06 5300 2903 7a4c  d.....q.|.S.).zL
+00000ae0: 0a0a 2020 3a70 6172 616d 2066 696c 656e  ..  :param filen
+00000af0: 616d 653a 0a20 203a 7061 7261 6d20 7072  ame:.  :param pr
+00000b00: 6f70 6167 6174 653a 2077 6865 7468 6572  opagate: whether
+00000b10: 206c 6f67 2074 6f20 7374 646f 7574 0a20   log to stdout. 
+00000b20: 203a 7265 7475 726e 3a0a 2020 4629 0572   :return:.  F).r
+00000b30: 4800 0000 7233 0000 0072 5300 0000 7230  H...r3...rS...r0
+00000b40: 0000 0072 3600 0000 2907 da04 636f 7079  ...r6...)...copy
+00000b50: da08 6465 6570 636f 7079 721e 0000 0072  ..deepcopyr....r
+00000b60: 4e00 0000 7250 0000 00da 0970 726f 7061  N...rP.....propa
+00000b70: 6761 7465 7254 0000 0029 0772 3300 0000  gaterT...).r3...
+00000b80: da0c 6c6f 6767 6572 5f6e 616d 6573 7253  ..logger_namesrS
+00000b90: 0000 0072 3000 0000 7236 0000 0072 1a00  ...r0...r6...r..
+00000ba0: 0000 7248 0000 0072 1300 0000 7213 0000  ..rH...r....r...
+00000bb0: 0072 1400 0000 da0a 6765 745f 6c6f 6767  .r......get_logg
+00000bc0: 6572 5400 0000 7310 0000 0000 0b0a 010a  erT...s.........
+00000bd0: 0108 010a 010a 0106 0114 0172 5d00 0000  ...........r]...
+00000be0: 6301 0000 0000 0000 0000 0000 0003 0000  c...............
+00000bf0: 0004 0000 0043 0000 0073 3200 0000 7400  .....C...s2...t.
+00000c00: 7c00 6a01 8301 7d01 7c01 4400 5d0e 7d02  |.j...}.|.D.].}.
+00000c10: 7c00 a002 7c02 a101 0100 710e 7c02 a003  |...|.....q.|...
+00000c20: a100 0100 7c02 a004 a100 0100 6400 5300  ....|.......d.S.
+00000c30: a901 4e29 05da 046c 6973 7472 3c00 0000  ..N)...listr<...
+00000c40: da0d 7265 6d6f 7665 4861 6e64 6c65 72da  ..removeHandler.
+00000c50: 0566 6c75 7368 da05 636c 6f73 6529 0372  .flush..close).r
+00000c60: 4800 0000 723c 0000 0072 4600 0000 7213  H...r<...rF...r.
+00000c70: 0000 0072 1300 0000 7214 0000 00da 1163  ...r....r......c
+00000c80: 6c6f 7365 5f6c 6f67 6765 725f 6669 6c65  lose_logger_file
+00000c90: 6900 0000 730c 0000 0000 010a 0108 010c  i...s...........
+00000ca0: 0108 0108 0172 6300 0000 722f 0000 0063  .....rc...r/...c
+00000cb0: 0400 0000 0000 0000 0000 0000 0500 0000  ................
+00000cc0: 0600 0000 4300 0000 7314 0000 0074 007c  ....C...s....t.|
+00000cd0: 007c 027c 037c 0164 018d 047d 047c 0453  .|.|.|.d...}.|.S
+00000ce0: 0029 024e 2904 7233 0000 0072 5c00 0000  .).N).r3...r\...
+00000cf0: 7253 0000 0072 3600 0000 2901 725d 0000  rS...r6...).r]..
+00000d00: 0029 0572 3300 0000 7236 0000 0072 5c00  .).r3...r6...r\.
+00000d10: 0000 7253 0000 0072 4800 0000 7213 0000  ..rS...rH...r...
+00000d20: 0072 1300 0000 7214 0000 00da 0f67 6574  .r....r......get
+00000d30: 5f66 696c 655f 6c6f 6767 6572 7100 0000  _file_loggerq...
+00000d40: 7304 0000 0000 0410 0172 6400 0000 6305  s........rd...c.
+00000d50: 0000 0000 0000 0000 0000 0009 0000 0006  ................
+00000d60: 0000 0003 0000 0073 a000 0000 7400 6a01  .......s....t.j.
+00000d70: 6401 6402 6403 8d02 7d05 7400 6a02 7c01  d.d.d...}.t.j.|.
+00000d80: 7c04 6404 8d02 7d06 7c06 6a03 7c03 6405  |.d...}.|.j.|.d.
+00000d90: 8d01 0100 7c06 a004 7c05 a101 0100 8800  ....|...|.......
+00000da0: a005 7c06 a101 0100 8700 6601 6406 6407  ..|.......f.d.d.
+00000db0: 8408 7d07 7c07 8800 5f06 7c02 729c 7400  ..}.|..._.|.r.t.
+00000dc0: a007 a100 7d08 7c08 a003 7c03 a101 0100  ....}.|...|.....
+00000dd0: 7408 7409 6408 6409 8302 640a 1700 7409  t.t.d.d...d...t.
+00000de0: 640b 6409 8302 1700 6402 640c 640d 640e  d.d.....d.d.d.d.
+00000df0: 8d04 7d05 7c08 a004 7c05 a101 0100 8800  ..}.|...|.......
+00000e00: a005 7c08 a101 0100 8800 5300 290f 4e7a  ..|.......S.).Nz
+00000e10: 665b 2528 6173 6374 696d 6529 735d 2025  f[%(asctime)s] %
+00000e20: 286e 616d 6529 733a 2528 6c69 6e65 6e6f  (name)s:%(lineno
+00000e30: 2973 2025 286c 6576 656c 6e61 6d65 2973  )s %(levelname)s
+00000e40: 3a20 2528 6d65 7373 6167 6529 7320 095b  : %(message)s .[
+00000e50: 2528 6669 6c65 6e61 6d65 2973 3a25 2866  %(filename)s:%(f
+00000e60: 756e 634e 616d 6529 7328 293a 2528 6c69  uncName)s():%(li
+00000e70: 6e65 6e6f 2973 5d7a 0e25 6d2f 2564 2025  neno)s]z.%m/%d %
+00000e80: 483a 254d 3a25 5372 3800 0000 7235 0000  H:%M:%Sr8...r5..
+00000e90: 0072 3700 0000 6300 0000 0000 0000 0000  .r7...c.........
+00000ea0: 0000 0004 0000 0006 0000 0017 0000 0073  ...............s
+00000eb0: 5a00 0000 6700 7d01 8800 6a00 4400 5d20  Z...g.}...j.D.] 
+00000ec0: 7d02 7c01 a001 7c02 6a02 a101 0100 7c02  }.|...|.j.....|.
+00000ed0: a003 7404 a005 6401 a101 a101 0100 710a  ..t...d.......q.
+00000ee0: 8800 6a06 7c00 8e00 0100 7407 8800 6a00  ..j.|.....t...j.
+00000ef0: 7c01 8302 4400 5d12 5c02 7d02 7d03 7c02  |...D.].\.}.}.|.
+00000f00: a003 7c03 a101 0100 7142 6400 5300 7239  ..|.....qBd.S.r9
+00000f10: 0000 0072 3b00 0000 7243 0000 0072 4700  ...r;...rC...rG.
+00000f20: 0000 7213 0000 0072 1400 0000 7249 0000  ..r....r....rI..
+00000f30: 0085 0000 0073 0e00 0000 0002 0401 0a01  .....s..........
+00000f40: 0c01 1202 0a03 1401 7a1c 7365 745f 6861  ........z.set_ha
+00000f50: 6e64 6572 2e3c 6c6f 6361 6c73 3e2e 696e  nder.<locals>.in
+00000f60: 666f 5f6d 7367 7a25 5b25 2861 7363 7469  fo_msgz%[%(ascti
+00000f70: 6d65 2973 5d20 2528 6e61 6d65 2973 2025  me)s] %(name)s %
+00000f80: 286c 6576 656c 6e61 6d65 2973 3ada 0462  (levelname)s:..b
+00000f90: 6c75 657a 0d25 286d 6573 7361 6765 2973  luez.%(message)s
+00000fa0: 2009 7a28 5b25 2866 696c 656e 616d 6529   .z([%(filename)
+00000fb0: 733a 2528 6675 6e63 4e61 6d65 2973 2829  s:%(funcName)s()
+00000fc0: 3a25 286c 696e 656e 6f29 735d 7256 0000  :%(lineno)s]rV..
+00000fd0: 0072 5700 0000 2903 7232 0000 0072 0400  .rW...).r2...r..
+00000fe0: 0000 7206 0000 0029 0a72 1e00 0000 7240  ..r....).r....r@
+00000ff0: 0000 0072 4f00 0000 7250 0000 0072 3f00  ...rO...rP...r?.
+00001000: 0000 7251 0000 0072 4900 0000 da0d 5374  ..rQ...rI.....St
+00001010: 7265 616d 4861 6e64 6c65 7272 0300 0000  reamHandlerr....
+00001020: 7202 0000 0029 0972 4800 0000 7233 0000  r....).rH...r3..
+00001030: 0072 5300 0000 7230 0000 0072 3600 0000  .rS...r0...r6...
+00001040: 723e 0000 005a 0b66 696c 655f 6861 6e64  r>...Z.file_hand
+00001050: 6572 7249 0000 005a 0e73 7472 6561 6d5f  errI...Z.stream_
+00001060: 6861 6e64 6c65 7272 1300 0000 7247 0000  handlerr....rG..
+00001070: 0072 1400 0000 7254 0000 0079 0000 0073  .r....rT...y...s
+00001080: 3400 0000 0001 0401 0201 02fe 0606 0e01  4...............
+00001090: 0c01 0a01 0a02 0c0d 0602 0401 0801 0a02  ................
+000010a0: 0202 0801 02ff 0202 08fe 0203 0201 0201  ................
+000010b0: 02f9 0609 0a01 0a02 7254 0000 0063 0000  ........rT...c..
+000010c0: 0000 0000 0000 0000 0000 0000 0000 0200  ................
+000010d0: 0000 4000 0000 7338 0000 0065 005a 0164  ..@...s8...e.Z.d
+000010e0: 005a 0264 015a 0364 0264 0384 005a 0464  .Z.d.Z.d.d...Z.d
+000010f0: 0464 0584 005a 0564 0664 0784 005a 0664  .d...Z.d.d...Z.d
+00001100: 0864 0984 005a 0764 0a64 0b84 005a 0864  .d...Z.d.d...Z.d
+00001110: 0c53 0029 0dda 0e53 7472 6561 6d54 6f4c  .S.)...StreamToL
+00001120: 6f67 6765 727a 4e0a 2020 4661 6b65 2066  oggerzN.  Fake f
+00001130: 696c 652d 6c69 6b65 2073 7472 6561 6d20  ile-like stream 
+00001140: 6f62 6a65 6374 2074 6861 7420 7265 6469  object that redi
+00001150: 7265 6374 7320 7772 6974 6573 2074 6f20  rects writes to 
+00001160: 6120 6c6f 6767 6572 2069 6e73 7461 6e63  a logger instanc
+00001170: 652e 0a20 2063 0200 0000 0000 0000 0000  e..  c..........
+00001180: 0000 0200 0000 0200 0000 4300 0000 7310  ..........C...s.
+00001190: 0000 007c 017c 005f 0064 017c 005f 0164  ...|.|._.d.|._.d
+000011a0: 0053 0029 024e 7207 0000 0029 0272 4800  .S.).Nr....).rH.
+000011b0: 0000 5a07 6c69 6e65 6275 6629 0272 0e00  ..Z.linebuf).r..
+000011c0: 0000 7248 0000 0072 1300 0000 7213 0000  ..rH...r....r...
+000011d0: 0072 1400 0000 720d 0000 00ac 0000 0073  .r....r........s
+000011e0: 0400 0000 0001 0601 7a17 5374 7265 616d  ........z.Stream
+000011f0: 546f 4c6f 6767 6572 2e5f 5f69 6e69 745f  ToLogger.__init_
+00001200: 5f63 0200 0000 0000 0000 0000 0000 0500  _c..............
+00001210: 0000 0600 0000 4300 0000 737a 0000 007c  ......C...sz...|
+00001220: 01a0 0064 01a1 017d 017c 0173 1264 0053  ...d...}.|.s.d.S
+00001230: 0064 027c 0117 007d 0167 007d 027c 006a  .d.|...}.g.}.|.j
+00001240: 016a 0244 005d 207d 037c 02a0 037c 036a  .j.D.] }.|...|.j
+00001250: 04a1 0101 007c 03a0 0574 06a0 0764 03a1  .....|...t...d..
+00001260: 01a1 0101 0071 267c 006a 01a0 087c 01a1  .....q&|.j...|..
+00001270: 0101 0074 097c 006a 016a 027c 0283 0244  ...t.|.j.j.|...D
+00001280: 005d 125c 027d 037d 047c 03a0 057c 04a1  .].\.}.}.|...|..
+00001290: 0101 0071 6264 0053 0029 044e da01 0a7a  ...qbd.S.).N...z
+000012a0: 033c 3e20 723a 0000 0029 0ada 0672 7374  .<> r:...)...rst
+000012b0: 7269 7072 4800 0000 723c 0000 0072 3d00  riprH...r<...r=.
+000012c0: 0000 723e 0000 0072 3f00 0000 721e 0000  ..r>...r?...r...
+000012d0: 0072 4000 0000 7241 0000 0072 4200 0000  .r@...rA...rB...
+000012e0: 2905 720e 0000 00da 0362 7566 7245 0000  ).r......bufrE..
+000012f0: 0072 4600 0000 723e 0000 0072 1300 0000  .rF...r>...r....
+00001300: 7213 0000 0072 1400 0000 da05 7772 6974  r....r......writ
+00001310: 65b0 0000 0073 1600 0000 0001 0a01 0401  e....s..........
+00001320: 0401 0803 0401 0c01 0c01 1201 0c02 1601  ................
+00001330: 7a14 5374 7265 616d 546f 4c6f 6767 6572  z.StreamToLogger
+00001340: 2e77 7269 7465 6301 0000 0000 0000 0000  .writec.........
+00001350: 0000 0001 0000 0001 0000 0043 0000 0073  ...........C...s
+00001360: 0400 0000 6400 5300 725e 0000 0072 1300  ....d.S.r^...r..
+00001370: 0000 a901 720e 0000 0072 1300 0000 7213  ....r....r....r.
+00001380: 0000 0072 1400 0000 7261 0000 00c0 0000  ...r....ra......
+00001390: 0073 0200 0000 0001 7a14 5374 7265 616d  .s......z.Stream
+000013a0: 546f 4c6f 6767 6572 2e66 6c75 7368 6301  ToLogger.flushc.
+000013b0: 0000 0000 0000 0000 0000 0001 0000 0001  ................
+000013c0: 0000 0043 0000 0073 0400 0000 6400 5300  ...C...s....d.S.
+000013d0: 725e 0000 0072 1300 0000 726c 0000 0072  r^...r....rl...r
+000013e0: 1300 0000 7213 0000 0072 1400 0000 da08  ....r....r......
+000013f0: 6765 7476 616c 7565 c300 0000 7302 0000  getvalue....s...
+00001400: 0000 017a 1753 7472 6561 6d54 6f4c 6f67  ...z.StreamToLog
+00001410: 6765 722e 6765 7476 616c 7565 6301 0000  ger.getvaluec...
+00001420: 0000 0000 0000 0000 0001 0000 0001 0000  ................
+00001430: 0043 0000 0073 0400 0000 6400 5300 725e  .C...s....d.S.r^
+00001440: 0000 0072 1300 0000 726c 0000 0072 1300  ...r....rl...r..
+00001450: 0000 7213 0000 0072 1400 0000 7262 0000  ..r....r....rb..
+00001460: 00c6 0000 0073 0200 0000 0001 7a14 5374  .....s......z.St
+00001470: 7265 616d 546f 4c6f 6767 6572 2e63 6c6f  reamToLogger.clo
+00001480: 7365 4e29 0972 2300 0000 7224 0000 0072  seN).r#...r$...r
+00001490: 2500 0000 da07 5f5f 646f 635f 5f72 0d00  %.....__doc__r..
+000014a0: 0000 726b 0000 0072 6100 0000 726d 0000  ..rk...ra...rm..
+000014b0: 0072 6200 0000 7213 0000 0072 1300 0000  .rb...r....r....
+000014c0: 7213 0000 0072 1400 0000 7267 0000 00a7  r....r....rg....
+000014d0: 0000 0073 0c00 0000 0801 0404 0804 0810  ...s............
+000014e0: 0803 0803 7267 0000 0063 0100 0000 0000  ....rg...c......
+000014f0: 0000 0000 0000 0200 0000 0200 0000 4300  ..............C.
+00001500: 0000 7318 0000 0074 007c 0083 017d 017c  ..s....t.|...}.|
+00001510: 0174 015f 027c 0174 015f 0364 0053 0072  .t._.|.t._.d.S.r
+00001520: 5e00 0000 2904 7267 0000 00da 0373 7973  ^...).rg.....sys
+00001530: da06 7374 646f 7574 da06 7374 6465 7272  ..stdout..stderr
+00001540: 2902 7248 0000 00da 0273 6c72 1300 0000  ).rH.....slr....
+00001550: 7213 0000 0072 1400 0000 da18 7265 6469  r....r......redi
+00001560: 7265 6374 5f70 7269 6e74 5f74 6f5f 6c6f  rect_print_to_lo
+00001570: 6767 6572 ca00 0000 7308 0000 0000 0108  gger....s.......
+00001580: 0106 0106 0172 7300 0000 6301 0000 0000  .....rs...c.....
+00001590: 0000 0000 0000 0005 0000 0006 0000 0047  ...............G
+000015a0: 0000 0073 5a00 0000 6700 7d02 7c00 6a00  ...sZ...g.}.|.j.
+000015b0: 4400 5d20 7d03 7c02 a001 7c03 6a02 a101  D.] }.|...|.j...
+000015c0: 0100 7c03 a003 7404 a005 6401 a101 a101  ..|...t...d.....
+000015d0: 0100 710a 7c00 6a06 7c01 8e00 0100 7407  ..q.|.j.|.....t.
+000015e0: 7c00 6a00 7c02 8302 4400 5d12 5c02 7d03  |.j.|...D.].\.}.
+000015f0: 7d04 7c03 a003 7c04 a101 0100 7142 6400  }.|...|.....qBd.
+00001600: 5300 7239 0000 0072 3b00 0000 2905 7248  S.r9...r;...).rH
+00001610: 0000 0072 4400 0000 7245 0000 0072 4600  ...rD...rE...rF.
+00001620: 0000 723e 0000 0072 1300 0000 7213 0000  ..r>...r....r...
+00001630: 0072 1400 0000 7249 0000 00d1 0000 0073  .r....rI.......s
+00001640: 0e00 0000 0002 0401 0a01 0c01 1202 0a03  ................
+00001650: 1401 7249 0000 0063 0000 0000 0000 0000  ..rI...c........
+00001660: 0000 0000 0000 0000 0300 0000 4000 0000  ............@...
+00001670: 7316 0000 0065 005a 0164 005a 0264 0564  s....e.Z.d.Z.d.d
+00001680: 0264 0384 015a 0364 0453 0029 06da 0e54  .d...Z.d.S.)...T
+00001690: 6573 7469 6e67 5f4c 6f67 6765 7254 6302  esting_LoggerTc.
+000016a0: 0000 0000 0000 0000 0000 0016 0000 0005  ................
+000016b0: 0000 0043 0000 0073 6801 0000 6401 7400  ...C...sh...d.t.
+000016c0: 6a01 6b07 7214 6402 7400 6a01 6401 3c00  j.k.r.d.t.j.d.<.
+000016d0: 6403 7400 6a01 6b07 7228 6402 7400 6a01  d.t.j.k.r(d.t.j.
+000016e0: 6403 3c00 6404 6405 6c02 6d03 7d02 0100  d.<.d.d.l.m.}...
+000016f0: 6404 6406 6c04 6d05 7d03 6d06 7d04 6d07  d.d.l.m.}.m.}.m.
+00001700: 7d05 6d08 7d06 0100 7c02 6a09 6407 740a  }.m.}...|.j.d.t.
+00001710: 6a0b 6408 6409 8d03 7d07 640a a00c 7c07  j.d.d...}.d...|.
+00001720: a101 7d08 740d 640b 7c08 9b00 9d02 8301  ..}.t.d.|.......
+00001730: 0100 7c03 7c00 740a a00e a100 6a0f 6a10  ..|.|.t.....j.j.
+00001740: 7411 640c 8d03 5c02 7d09 7d0a 640d 7c0a  t.d...\.}.}.d.|.
+00001750: 9b00 640e 7c08 9b00 640f 9d05 7d0b 7c04  ..d.|...d...}.|.
+00001760: 7c0b 6410 6411 8d02 5c02 7d0c 7d0d 7412  |.d.d...\.}.}.t.
+00001770: 7400 6a01 6401 1900 a013 6412 a101 8301  t.j.d.....d.....
+00001780: 7d0e 7400 6a01 a014 6413 6414 a102 7d0f  }.t.j...d.d...}.
+00001790: 7415 a016 6415 a101 7d10 7c0c 6a17 9b00  t...d...}.|.j...
+000017a0: 6416 9d02 7d11 7418 7c11 6410 6417 8d02  d...}.t.|.d.d...
+000017b0: 7d12 7c0c 6a17 9b00 6418 9d02 7d13 7418  }.|.j...d...}.t.
+000017c0: 7c13 6419 6417 8d02 7d14 7419 641a 8301  |.d.d...}.t.d...
+000017d0: 4400 5d16 7d15 7c10 a01a 641b 7c15 9b00  D.].}.|...d.|...
+000017e0: 9d02 a101 0100 9001 7116 7419 641c 8301  ........q.t.d...
+000017f0: 4400 5d10 7d15 7c12 a01a 7c15 a101 0100  D.].}.|...|.....
+00001800: 9001 7136 7419 641c 641d 8302 4400 5d10  ..q6t.d.d...D.].
+00001810: 7d15 7c14 a01a 7c15 a101 0100 9001 7152  }.|...|.......qR
+00001820: 641e 5300 291f 6142 0100 000a 2020 2020  d.S.).aB....    
+00001830: 5573 6167 653a 0a0a 2020 2020 2020 2020  Usage:..        
+00001840: 6578 706f 7274 2043 5544 415f 5649 5349  export CUDA_VISI
+00001850: 424c 455f 4445 5649 4345 533d 302c 312c  BLE_DEVICES=0,1,
+00001860: 322c 332c 342c 352c 362c 370a 2020 2020  2,3,4,5,6,7.    
+00001870: 2020 2020 6578 706f 7274 2054 494d 455f      export TIME_
+00001880: 5354 523d 310a 2020 2020 2020 2020 6578  STR=1.        ex
+00001890: 706f 7274 2050 5954 484f 4e50 4154 483d  port PYTHONPATH=
+000018a0: 2e0a 2020 2020 2020 2020 7079 7468 6f6e  ..        python
+000018b0: 202d 6320 2266 726f 6d20 746c 322e 6c61   -c "from tl2.la
+000018c0: 756e 6368 2e74 6573 7473 2e74 6573 745f  unch.tests.test_
+000018d0: 6c61 756e 6368 2069 6d70 6f72 7420 5465  launch import Te
+000018e0: 7374 696e 675f 4c61 756e 6368 5f76 313b  sting_Launch_v1;
+000018f0: 2020 2020 2020 2020 2020 5465 7374 696e            Testin
+00001900: 675f 4c61 756e 6368 5f76 3128 292e 7465  g_Launch_v1().te
+00001910: 7374 5f6c 6175 6e63 685f 6464 7028 6465  st_launch_ddp(de
+00001920: 6275 673d 4661 6c73 6529 2220 2020 2020  bug=False)"     
+00001930: 2020 2020 2020 2d2d 746c 5f6f 7074 7320        --tl_opts 
+00001940: 7465 7374 3020 3130 2074 6573 7431 2031  test0 10 test1 1
+00001950: 3120 2d2d 7465 7374 2031 0a0a 2020 2020  1 --test 1..    
+00001960: 3a72 6574 7572 6e3a 0a20 2020 20da 1443  :return:.    ..C
+00001970: 5544 415f 5649 5349 424c 455f 4445 5649  UDA_VISIBLE_DEVI
+00001980: 4345 53da 0130 da08 5449 4d45 5f53 5452  CES..0..TIME_STR
+00001990: 7201 0000 0029 01da 0974 6c32 5f75 7469  r....)...tl2_uti
+000019a0: 6c73 2904 da16 6765 745f 636f 6d6d 616e  ls)...get_comman
+000019b0: 645f 616e 645f 6f75 7464 6972 da15 7365  d_and_outdir..se
+000019c0: 7475 705f 6f75 7464 6972 5f61 6e64 5f79  tup_outdir_and_y
+000019d0: 616d 6cda 1267 6574 5f61 7070 656e 645f  aml..get_append_
+000019e0: 636d 645f 7374 72da 0d73 7461 7274 5f63  cmd_str..start_c
+000019f0: 6d64 5f72 756e 7a09 2d2d 746c 5f6f 7074  md_runz.--tl_opt
+00001a00: 7372 5f00 0000 2903 721a 0000 00da 0961  sr_...).r......a
+00001a10: 7267 765f 6c69 7374 da04 7479 7065 7219  rgv_list..typer.
+00001a20: 0000 007a 0a74 6c5f 6f70 7473 3a0a 2029  ...z.tl_opts:. )
+00001a30: 02da 0966 756e 635f 6e61 6d65 da04 6669  ...func_name..fi
+00001a40: 6c65 7a65 0a20 2020 2020 2020 2020 2020  leze.           
+00001a50: 2020 2020 202d 2d74 6c5f 636f 6e66 6967       --tl_config
+00001a60: 5f66 696c 6520 6e6f 6e65 0a20 2020 2020  _file none.     
+00001a70: 2020 2020 2020 2020 2020 202d 2d74 6c5f             --tl_
+00001a80: 636f 6d6d 616e 6420 6e6f 6e65 0a20 2020  command none.   
+00001a90: 2020 2020 2020 2020 2020 2020 202d 2d74               --t
+00001aa0: 6c5f 6f75 7464 6972 207a 1b0a 2020 2020  l_outdir z..    
+00001ab0: 2020 2020 2020 2020 2020 2020 2d2d 746c              --tl
+00001ac0: 5f6f 7074 7320 7a11 0a20 2020 2020 2020  _opts z..       
+00001ad0: 2020 2020 2020 2020 2054 2901 da0a 7265           T)...re
+00001ae0: 7475 726e 5f63 6667 fa01 2cda 0450 4f52  turn_cfg..,..POR
+00001af0: 5469 b822 0000 7257 0000 007a 0a2f 7465  Ti."..rW...z./te
+00001b00: 7374 312e 7478 7429 0172 5300 0000 7a0a  st1.txt).rS...z.
+00001b10: 2f74 6573 7432 2e74 7874 46e9 0500 0000  /test2.txtF.....
+00001b20: 7a07 6c6f 6767 6572 20e9 0a00 0000 e914  z.logger .......
+00001b30: 0000 004e 291b da02 6f73 da07 656e 7669  ...N)...os..envi
+00001b40: 726f 6eda 0374 6c32 7278 0000 00da 1774  ron..tl2rx.....t
+00001b50: 6c32 2e6c 6175 6e63 682e 6c61 756e 6368  l2.launch.launch
+00001b60: 5f75 7469 6c73 7279 0000 0072 7a00 0000  _utilsry...rz...
+00001b70: 727b 0000 0072 7c00 0000 da15 7061 7273  r{...r|.....pars
+00001b80: 6572 5f61 7267 735f 6672 6f6d 5f6c 6973  er_args_from_lis
+00001b90: 7472 6f00 0000 7244 0000 00da 046a 6f69  tro...rD.....joi
+00001ba0: 6eda 0570 7269 6e74 da09 5f67 6574 6672  n..print.._getfr
+00001bb0: 616d 65da 0666 5f63 6f64 65da 0763 6f5f  ame..f_code..co_
+00001bc0: 6e61 6d65 da08 5f5f 6669 6c65 5f5f 720b  name..__file__r.
+00001bd0: 0000 00da 0573 706c 6974 da03 6765 7472  .....split..getr
+00001be0: 1e00 0000 724e 0000 00da 0974 6c5f 6f75  ....rN.....tl_ou
+00001bf0: 7464 6972 7264 0000 00da 0572 616e 6765  tdirrd.....range
+00001c00: 7249 0000 0029 1672 0e00 0000 da05 6465  rI...).r......de
+00001c10: 6275 6772 7800 0000 7279 0000 0072 7a00  bugrx...ry...rz.
+00001c20: 0000 727b 0000 0072 7c00 0000 da0c 746c  ..r{...r|.....tl
+00001c30: 5f6f 7074 735f 6c69 7374 da07 746c 5f6f  _opts_list..tl_o
+00001c40: 7074 73da 0763 6f6d 6d61 6e64 da06 6f75  pts..command..ou
+00001c50: 7464 6972 da08 6172 6776 5f73 7472 720f  tdir..argv_strr.
+00001c60: 0000 00da 0363 6667 da06 6e5f 6770 7573  .....cfg..n_gpus
+00001c70: 7283 0000 0072 4800 0000 5a05 6669 6c65  r....rH...Z.file
+00001c80: 315a 0766 696c 6531 5f66 5a05 6669 6c65  1Z.file1_fZ.file
+00001c90: 325a 0766 696c 6532 5f66 da01 6972 1300  2Z.file2_f..ir..
+00001ca0: 0000 7213 0000 0072 1400 0000 da14 7465  ..r....r......te
+00001cb0: 7374 5f67 6574 5f66 696c 655f 6c6f 6767  st_get_file_logg
+00001cc0: 6572 e100 0000 733c 0000 0000 0d0a 010a  er....s<........
+00001cd0: 010a 010a 010c 0118 0312 010a 010e 041a  ................
+00001ce0: 0102 0302 fd04 0402 fc08 0610 0214 010e  ................
+00001cf0: 020a 020c 010c 010c 010c 020c 0114 020c  ................
+00001d00: 010e 010e 010e 027a 2354 6573 7469 6e67  .......z#Testing
+00001d10: 5f4c 6f67 6765 722e 7465 7374 5f67 6574  _Logger.test_get
+00001d20: 5f66 696c 655f 6c6f 6767 6572 4e29 0154  _file_loggerN).T
+00001d30: 2904 7223 0000 0072 2400 0000 7225 0000  ).r#...r$...r%..
+00001d40: 0072 9f00 0000 7213 0000 0072 1300 0000  .r....r....r....
+00001d50: 7213 0000 0072 1400 0000 7274 0000 00df  r....r....rt....
+00001d60: 0000 0073 0200 0000 0802 7274 0000 0029  ...s......rt...)
+00001d70: 1dda 0474 696d 65da 056e 756d 7079 da02  ...time..numpy..
+00001d80: 6e70 721e 0000 0072 8700 0000 7229 0000  npr....r....r)..
+00001d90: 0072 6f00 0000 da09 7465 726d 636f 6c6f  .ro.....termcolo
+00001da0: 7272 0200 0000 da08 756e 6974 7465 7374  rr......unittest
+00001db0: 7259 0000 0072 4c00 0000 724d 0000 0072  rY...rL...rM...r
+00001dc0: 4000 0000 7203 0000 00da 0449 4e46 4f72  @...r......INFOr
+00001dd0: 5200 0000 7255 0000 00da 0544 4542 5547  R...rU.....DEBUG
+00001de0: 725d 0000 0072 6300 0000 7264 0000 0072  r]...rc...rd...r
+00001df0: 5400 0000 da06 6f62 6a65 6374 7267 0000  T.....objectrg..
+00001e00: 0072 7300 0000 7249 0000 00da 0854 6573  .rs...rI.....Tes
+00001e10: 7443 6173 6572 7400 0000 7213 0000 0072  tCasert...r....r
+00001e20: 1300 0000 7213 0000 0072 1400 0000 da08  ....r....r......
+00001e30: 3c6d 6f64 756c 653e 0100 0000 7336 0000  <module>....s6..
+00001e40: 0008 0108 0110 0108 0108 010c 0108 0108  ................
+00001e50: 0204 0104 0312 1412 2b10 0806 0102 0104  ........+.......
+00001e60: 0102 fc0a 1508 0902 0102 0102 fd0a 0812  ................
+00001e70: 2e10 2308 0708 0e                        ..#....
```

### Comparing `tl2-0.1.0/tl2/proj/logger/__pycache__/plot_utils.cpython-38.pyc` & `tl2-0.1.1/tl2/proj/logger/__pycache__/plot_utils.cpython-38.pyc`

 * *Format-specific differences are supported for Python .pyc files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: Byte-compiled Python module for CPython 3.8, timestamp-based, .py timestamp: Mon Oct  3 09:27:55 2022 UTC, .py size: 6454 bytes*

 * *Could not decompile bytecode: bad marshal data (unknown type code)*

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-00000000: 550d 0d0a 0000 0000 1bab 3a63 3619 0000  U.........:c6...
+00000000: 550d 0d0a 0000 0000 f13c 9362 fa19 0000  U........<.b....
 00000010: e300 0000 0000 0000 0000 0000 0000 0000  ................
 00000020: 0004 0000 0040 0000 0073 8600 0000 6400  .....@...s....d.
 00000030: 6401 6c00 5a01 6400 6401 6c02 5a02 6400  d.l.Z.d.d.l.Z.d.
 00000040: 6401 6c03 5a03 6400 6401 6c04 5a04 6400  d.l.Z.d.d.l.Z.d.
 00000050: 6401 6c05 5a05 4700 6402 6403 8400 6403  d.l.Z.G.d.d...d.
 00000060: 6506 8303 5a07 6404 6405 8400 5a08 6411  e...Z.d.d...Z.d.
 00000070: 6407 6408 8401 5a09 4700 6409 640a 8400  d.d...Z.G.d.d...
@@ -53,396 +53,395 @@
 00000340: 6461 726b 626c 7565 2901 da05 636f 6c6f  darkblue)...colo
 00000350: 727a 0f61 7865 732e 7072 6f70 5f63 7963  rz.axes.prop_cyc
 00000360: 6c65 2906 da11 6d61 7470 6c6f 746c 6962  le)...matplotlib
 00000370: 2e70 7970 6c6f 74da 0670 7970 6c6f 74da  .pyplot..pyplot.
 00000380: 0573 7479 6c65 da03 7573 655a 0663 7963  .style..useZ.cyc
 00000390: 6c65 72da 0872 6350 6172 616d 7329 03da  ler..rcParams)..
 000003a0: 0473 656c 6672 0e00 0000 da03 706c 74a9  .selfr......plt.
-000003b0: 0072 1300 0000 fa57 2f68 6f6d 652f 6d61  .r.....W/home/ma
+000003b0: 0072 1300 0000 fa39 2f68 6f6d 652f 6d61  .r.....9/home/ma
 000003c0: 2d75 7365 722f 776f 726b 2f63 6f64 652f  -user/work/code/
-000003d0: 7374 796c 6567 616e 322d 6164 612d 7079  stylegan2-ada-py
-000003e0: 746f 7263 682d 6578 702f 746c 325f 6c69  torch-exp/tl2_li
-000003f0: 622f 746c 322f 7072 6f6a 2f6c 6f67 6765  b/tl2/proj/logge
-00000400: 722f 706c 6f74 5f75 7469 6c73 2e70 79da  r/plot_utils.py.
-00000410: 085f 5f69 6e69 745f 5f08 0000 0073 1000  .__init__....s..
-00000420: 0000 0005 0c02 0c01 0401 1801 02ff 02ff  ................
-00000430: 0c04 7a10 4d61 7450 6c6f 742e 5f5f 696e  ..z.MatPlot.__in
-00000440: 6974 5f5f e901 0000 0046 a902 679a 9999  it__.....F..g...
-00000450: 9999 9919 4067 3333 3333 3333 1340 6305  ....@g333333.@c.
-00000460: 0000 0000 0000 0000 0000 0009 0000 0005  ................
-00000470: 0000 0043 0000 0073 6400 0000 6401 6402  ...C...sd...d.d.
-00000480: 6c00 6d01 7d05 0100 7c04 6401 1900 7c02  l.m.}...|.d...|.
-00000490: 1400 7c04 6403 1900 7c01 1400 6602 7d06  ..|.d...|...f.}.
-000004a0: 7c05 6a02 7c01 7c02 7c06 6404 8d03 5c02  |.j.|.|.|.d...\.
-000004b0: 7d07 7d08 7c03 725c 7c02 6403 6b02 7254  }.}.|.r\|.d.k.rT
-000004c0: 7c01 6403 6b02 7254 7c08 6701 7d08 6e08  |.d.k.rT|.g.}.n.
-000004d0: 7c08 a003 a100 7d08 7c07 7c08 6602 5300  |.....}.|.|.f.S.
-000004e0: 2905 7a1f 0a20 2020 2061 782e 6c65 6765  ).z..    ax.lege
-000004f0: 6e64 286c 6f63 3d27 6265 7374 2729 0a20  nd(loc='best'). 
-00000500: 2020 2072 0100 0000 4e72 1600 0000 2903     r....Nr....).
-00000510: da05 6e72 6f77 73da 056e 636f 6c73 da07  ..nrows..ncols..
-00000520: 6669 6773 697a 6529 0472 0c00 0000 720d  figsize).r....r.
-00000530: 0000 00da 0873 7562 706c 6f74 73da 0572  .....subplots..r
-00000540: 6176 656c 2909 7211 0000 0072 1800 0000  avel).r....r....
-00000550: 7219 0000 0072 1c00 0000 5a07 6669 675f  r....r....Z.fig_
-00000560: 775f 6872 1200 0000 721a 0000 00da 0366  w_hr....r......f
-00000570: 6967 da04 6178 6573 7213 0000 0072 1300  ig..axesr....r..
-00000580: 0000 7214 0000 00da 0e67 6574 5f66 6967  ..r......get_fig
-00000590: 5f61 6e64 5f61 7816 0000 0073 1000 0000  _and_ax....s....
-000005a0: 0004 0c01 1801 1401 0401 1001 0802 0801  ................
-000005b0: 7a16 4d61 7450 6c6f 742e 6765 745f 6669  z.MatPlot.get_fi
-000005c0: 675f 616e 645f 6178 e9e8 0300 00da 0574  g_and_ax.......t
-000005d0: 6967 6874 e79a 9999 9999 99b9 3f63 0600  ight........?c..
-000005e0: 0000 0000 0000 0000 0000 0600 0000 0600  ................
-000005f0: 0000 4300 0000 7324 0000 007c 02a0 0064  ..C...s$...|...d
-00000600: 01a1 0173 0e74 0182 017c 016a 027c 027c  ...s.t...|.j.|.|
-00000610: 037c 047c 0564 028d 0401 0064 0053 0029  .|.|.d.....d.S.)
-00000620: 034e fa04 2e70 6e67 2903 da03 6470 69da  .N...png)...dpi.
-00000630: 0b62 626f 785f 696e 6368 6573 da0a 7061  .bbox_inches..pa
-00000640: 645f 696e 6368 6573 2903 da08 656e 6473  d_inches)...ends
-00000650: 7769 7468 da0e 4173 7365 7274 696f 6e45  with..AssertionE
-00000660: 7272 6f72 da07 7361 7665 6669 6729 0672  rror..savefig).r
-00000670: 1100 0000 721d 0000 00da 0866 696c 6570  ....r......filep
-00000680: 6174 6872 2400 0000 7225 0000 0072 2600  athr$...r%...r&.
-00000690: 0000 7213 0000 0072 1300 0000 7214 0000  ..r....r....r...
-000006a0: 00da 0b73 6176 655f 746f 5f70 6e67 2400  ...save_to_png$.
-000006b0: 0000 730e 0000 0000 020e 0104 0102 0002  ..s.............
-000006c0: 0002 0002 ff7a 134d 6174 506c 6f74 2e73  .....z.MatPlot.s
-000006d0: 6176 655f 746f 5f70 6e67 6303 0000 0000  ave_to_pngc.....
-000006e0: 0000 0000 0000 0003 0000 0005 0000 0043  ...............C
-000006f0: 0000 0073 1400 0000 7c01 6a00 7c02 6401  ...s....|.j.|.d.
-00000700: 6402 6403 8d03 0100 6400 5300 2904 4e72  d.d.....d.S.).Nr
-00000710: 2100 0000 7201 0000 0029 0272 2500 0000  !...r....).r%...
-00000720: 7226 0000 0029 0172 2900 0000 2903 7211  r&...).r)...).r.
-00000730: 0000 0072 1d00 0000 722a 0000 0072 1300  ...r....r*...r..
-00000740: 0000 7213 0000 0072 1400 0000 da0b 7361  ..r....r......sa
-00000750: 7665 5f74 6f5f 7064 662a 0000 0073 0200  ve_to_pdf*...s..
-00000760: 0000 0001 7a13 4d61 7450 6c6f 742e 7361  ....z.MatPlot.sa
-00000770: 7665 5f74 6f5f 7064 6663 0300 0000 0000  ve_to_pdfc......
-00000780: 0000 0000 0000 0700 0000 0900 0000 4300  ..............C.
-00000790: 0000 7344 0000 0074 007c 0183 018f 2e7d  ..sD...t.|.....}
-000007a0: 037c 03a0 01a1 007d 0464 0164 0284 007c  .|.....}.d.d...|
-000007b0: 02a0 027c 04a1 0144 0083 017d 0574 0374  ...|...D...}.t.t
-000007c0: 047c 0583 0183 017d 0657 0035 0051 0052  .|.....}.W.5.Q.R
-000007d0: 0058 007c 067c 0566 0253 0029 037a 140a  .X.|.|.f.S.).z..
-000007e0: 2020 2020 696d 706f 7274 2072 650a 0a20      import re.. 
-000007f0: 2020 2063 0100 0000 0000 0000 0000 0000     c............
-00000800: 0200 0000 0400 0000 5300 0000 7314 0000  ........S...s...
-00000810: 0067 007c 005d 0c7d 0174 007c 0183 0191  .g.|.].}.t.|....
-00000820: 0271 0453 0072 1300 0000 2901 da05 666c  .q.S.r....)...fl
-00000830: 6f61 7429 02da 022e 30da 0178 7213 0000  oat)....0..xr...
-00000840: 0072 1300 0000 7214 0000 00da 0a3c 6c69  .r....r......<li
-00000850: 7374 636f 6d70 3e34 0000 0073 0400 0000  stcomp>4...s....
-00000860: 0600 0200 7a32 4d61 7450 6c6f 742e 7061  ....z2MatPlot.pa
-00000870: 7273 655f 6c6f 6766 696c 655f 7573 696e  rse_logfile_usin
-00000880: 675f 7265 2e3c 6c6f 6361 6c73 3e2e 3c6c  g_re.<locals>.<l
-00000890: 6973 7463 6f6d 703e 2905 da04 6f70 656e  istcomp>)...open
-000008a0: da04 7265 6164 da07 6669 6e64 616c 6cda  ..read..findall.
-000008b0: 0572 616e 6765 da03 6c65 6e29 0772 1100  .range..len).r..
-000008c0: 0000 da07 6c6f 6766 696c 65da 0672 655f  ....logfile..re_
-000008d0: 7374 72da 0166 da06 6c6f 6773 7472 da03  str..f..logstr..
-000008e0: 7661 6cda 0369 6478 7213 0000 0072 1300  val..idxr....r..
-000008f0: 0000 7214 0000 00da 1670 6172 7365 5f6c  ..r......parse_l
-00000900: 6f67 6669 6c65 5f75 7369 6e67 5f72 652d  ogfile_using_re-
-00000910: 0000 0073 0a00 0000 0005 0a01 0801 1401  ...s............
-00000920: 1601 7a1e 4d61 7450 6c6f 742e 7061 7273  ..z.MatPlot.pars
-00000930: 655f 6c6f 6766 696c 655f 7573 696e 675f  e_logfile_using_
-00000940: 7265 4e29 0172 0300 0000 2904 7216 0000  reN).r....).r...
-00000950: 0072 1600 0000 4672 1700 0000 2903 7220  .r....Fr....).r 
-00000960: 0000 0072 2100 0000 7222 0000 0029 08da  ...r!...r"...)..
-00000970: 085f 5f6e 616d 655f 5fda 0a5f 5f6d 6f64  .__name__..__mod
-00000980: 756c 655f 5fda 0c5f 5f71 7561 6c6e 616d  ule__..__qualnam
-00000990: 655f 5f72 1500 0000 721f 0000 0072 2b00  e__r....r....r+.
-000009a0: 0000 722c 0000 0072 3c00 0000 7213 0000  ..r,...r<...r...
-000009b0: 0072 1300 0000 7213 0000 0072 1400 0000  .r....r....r....
-000009c0: 7202 0000 0007 0000 0073 0e00 0000 0801  r........s......
-000009d0: 0a0e 0a0e 0001 00ff 0a06 0803 7202 0000  ............r...
-000009e0: 0063 0200 0000 0000 0000 0000 0000 0c00  .c..............
-000009f0: 0000 0700 0000 4300 0000 73ac 0000 0074  ......C...s....t
-00000a00: 007c 016a 017c 006a 0283 027d 0274 0383  .|.j.|.j...}.t..
-00000a10: 007d 037c 03a0 04a1 005c 027d 047d 0574  .}.|.....\.}.}.t
-00000a20: 057c 026a 0683 0164 016b 0272 3e7c 026a  .|.j...d.k.r>|.j
-00000a30: 0674 057c 026a 0783 0114 007d 0674 087c  .t.|.j.....}.t.|
-00000a40: 067c 026a 0783 0244 005d 345c 027d 077d  .|.j...D.]4\.}.}
-00000a50: 0874 09a0 0a7c 08a1 017d 097c 036a 0b7c  .t...|...}.|.j.|
-00000a60: 077c 0964 028d 025c 027d 0a7d 0b7c 056a  .|.d...\.}.}.|.j
-00000a70: 0c7c 0a7c 0b7c 0864 038d 0301 0071 4a7c  .|.|.|.d.....qJ|
-00000a80: 05a0 0da1 0001 007c 036a 0e7c 0474 0f6a  .......|.j.|.t.j
-00000a90: 10a0 117c 006a 127c 026a 1364 0417 00a1  ...|.j.|.j.d....
-00000aa0: 0264 058d 0201 0064 0053 0029 064e 7216  .d.....d.S.).Nr.
-00000ab0: 0000 0029 0272 3600 0000 7237 0000 0029  ...).r6...r7...)
-00000ac0: 01da 056c 6162 656c 7223 0000 0029 0172  ...labelr#...).r
-00000ad0: 2a00 0000 2914 da07 6765 7461 7474 72da  *...)...getattr.
-00000ae0: 0663 6f6e 6669 67da 0763 6f6d 6d61 6e64  .config..command
-00000af0: 7202 0000 0072 1f00 0000 7235 0000 00da  r....r....r5....
-00000b00: 086c 6f67 6669 6c65 735a 0772 655f 7374  .logfilesZ.re_st
-00000b10: 7273 da03 7a69 70da 0272 65da 0763 6f6d  rs..zip..re..com
-00000b20: 7069 6c65 723c 0000 00da 0470 6c6f 74da  piler<.....plot.
-00000b30: 066c 6567 656e 6472 2b00 0000 da02 6f73  .legendr+.....os
-00000b40: da04 7061 7468 da04 6a6f 696e da06 6f75  ..path..join..ou
-00000b50: 7464 6972 da05 7469 746c 6529 0cda 0461  tdir..title)...a
-00000b60: 7267 735a 066d 7961 7267 7372 4200 0000  rgsZ.myargsrB...
-00000b70: da07 6d61 7470 6c6f 7472 1d00 0000 da02  ..matplotr......
-00000b80: 6178 7244 0000 0072 3600 0000 7237 0000  axrD...r6...r7..
-00000b90: 005a 0652 455f 5354 5272 3b00 0000 723a  .Z.RE_STRr;...r:
-00000ba0: 0000 0072 1300 0000 7213 0000 0072 1400  ...r....r....r..
-00000bb0: 0000 da0d 7061 7273 655f 6c6f 6766 696c  ....parse_logfil
-00000bc0: 6539 0000 0073 1e00 0000 0001 0e01 0601  e9...s..........
-00000bd0: 0c01 0e01 1001 1401 0a01 1201 1201 0801  ................
-00000be0: 0401 0200 14ff 0602 7252 0000 0046 6304  ........rR...Fc.
-00000bf0: 0000 0000 0000 0000 0000 000f 0000 0008  ................
-00000c00: 0000 0043 0000 0073 6401 0000 6401 6400  ...C...sd...d.d.
-00000c10: 6c00 7d04 7c04 a001 6402 a101 0100 6401  l.}.|...d.....d.
-00000c20: 6400 6c02 6d03 7d05 0100 7404 7c01 8301  d.l.m.}...t.|...
-00000c30: 7404 7c00 8301 6b02 7332 7405 8201 7406  t.|...k.s2t...t.
-00000c40: 6a07 a008 7c02 6403 6404 a008 7c00 a101  j...|.d.d...|...
-00000c50: 1700 6405 1700 a102 7d06 7409 8300 7d07  ..d.....}.t...}.
-00000c60: 7c03 73b4 740a a00b 740a a00c 7404 7c00  |.s.t...t...t.|.
-00000c70: 8301 a101 a101 7d08 7404 7c00 8301 7c08  ......}.t.|...|.
-00000c80: 1700 6406 1800 7c08 1a00 7d09 7c07 6a0d  ..d...|...}.|.j.
-00000c90: 7c09 7c08 6407 8d02 5c02 7d0a 7d0b 7c08  |.|.d...\.}.}.|.
-00000ca0: 6406 6b02 72aa 7c09 6406 6b02 72aa 7c0b  d.k.r.|.d.k.r.|.
-00000cb0: 6701 7d0b 71dc 7c0b a00e a100 7d0b 6e28  g.}.q.|.....}.n(
-00000cc0: 6406 7d08 6406 7d09 7c07 6a0d 7c09 7c08  d.}.d.}.|.j.|.|.
-00000cd0: 6407 8d02 5c02 7d0a 7d0b 7c0b 6701 7404  d...\.}.}.|.g.t.
-00000ce0: 7c00 8301 1400 7d0b 740f 7410 7c00 7c01  |.....}.t.t.|.|.
-00000cf0: 8302 8301 4400 5d58 5c02 7d0c 5c02 7d0d  ....D.]X\.}.\.}.
-00000d00: 7d0e 7c0e a011 6408 6409 a102 7d0e 7c0b  }.|...d.d...}.|.
-00000d10: 7c0c 1900 6a12 7c0e 6400 6400 8502 6401  |...j.|.d.d...d.
-00000d20: 6602 1900 7c0e 6400 6400 8502 6406 6602  f...|.d.d...d.f.
-00000d30: 1900 640a 7c0d 640b 640c 8d05 0100 7c0b  ..d.|.d.d.....|.
-00000d40: 7c0c 1900 6a13 640d 640e 8d01 0100 71ea  |...j.d.d.....q.
-00000d50: 7c07 6a14 7c0a 7c06 6400 6400 640f 8d04  |.j.|.|.d.d.d...
-00000d60: 0100 7c05 a015 7c0a a101 0100 6400 5300  ..|...|.....d.S.
-00000d70: 2910 4e72 0100 0000 5a03 4167 675a 0570  ).Nr....Z.AggZ.p
-00000d80: 6c6f 745f da02 5f5f 7223 0000 0072 1600  lot_..__r#...r..
-00000d90: 0000 a902 7218 0000 0072 1900 0000 e9ff  ....r....r......
-00000da0: ffff ffe9 0200 0000 da01 2ee7 6666 6666  ............ffff
-00000db0: 6666 e63f a903 5a06 6d61 726b 6572 7240  ff.?..Z.markerr@
-00000dc0: 0000 00da 0561 6c70 6861 da04 6265 7374  .....alpha..best
-00000dd0: a901 da03 6c6f 63a9 0472 1d00 0000 722a  ....loc..r....r*
-00000de0: 0000 0072 2400 0000 7225 0000 0029 16da  ...r$...r%...)..
-00000df0: 0a6d 6174 706c 6f74 6c69 6272 0f00 0000  .matplotlibr....
-00000e00: 720c 0000 0072 0d00 0000 7235 0000 0072  r....r....r5...r
-00000e10: 2800 0000 724a 0000 0072 4b00 0000 724c  (...rJ...rK...rL
-00000e20: 0000 0072 0200 0000 da04 6d61 7468 da04  ...r......math..
-00000e30: 6365 696c da04 7371 7274 721f 0000 0072  ceil..sqrtr....r
-00000e40: 1c00 0000 da09 656e 756d 6572 6174 6572  ......enumerater
-00000e50: 4500 0000 da07 7265 7368 6170 6572 4800  E.....reshaperH.
-00000e60: 0000 7249 0000 0072 2b00 0000 da05 636c  ..rI...r+.....cl
-00000e70: 6f73 6529 0fda 056e 616d 6573 da05 6461  ose)...names..da
-00000e80: 7461 7372 4d00 0000 da0b 696e 5f6f 6e65  tasrM.....in_one
-00000e90: 5f61 7865 7372 5f00 0000 7212 0000 00da  _axesr_...r.....
-00000ea0: 0866 696c 656e 616d 6572 5000 0000 7219  .filenamerP...r.
-00000eb0: 0000 0072 1800 0000 721d 0000 0072 1e00  ...r....r....r..
-00000ec0: 0000 723b 0000 0072 4000 0000 da04 6461  ..r;...r@.....da
-00000ed0: 7461 7213 0000 0072 1300 0000 7214 0000  tar....r....r...
-00000ee0: 00da 0c5f 706c 6f74 5f66 6967 7572 6549  ..._plot_figureI
-00000ef0: 0000 0073 3000 0000 0001 0801 0a01 0c01  ...s0...........
-00000f00: 1401 1c01 0601 0401 1401 1401 1201 1001  ................
-00000f10: 0802 0a02 0401 0401 1201 0e02 1a01 0c01  ................
-00000f20: 3001 1202 1201 0a01 726b 0000 0063 0000  0.......rk...c..
-00000f30: 0000 0000 0000 0000 0000 0000 0000 0200  ................
-00000f40: 0000 4000 0000 7318 0000 0065 005a 0164  ..@...s....e.Z.d
-00000f50: 005a 0264 015a 0364 0264 0384 005a 0464  .Z.d.Z.d.d...Z.d
-00000f60: 0453 0029 05da 1450 6c6f 7446 6967 7572  .S.)...PlotFigur
-00000f70: 6550 726f 6365 7373 696e 677a 610a 2020  eProcessingza.  
-00000f80: 2020 776f 726b 6572 203d 2050 6c6f 7446    worker = PlotF
-00000f90: 6967 7572 6550 726f 6365 7373 696e 6728  igureProcessing(
-00000fa0: 6172 6773 3d28 732c 2064 2c20 636f 7079  args=(s, d, copy
-00000fb0: 7472 6565 2929 0a20 2020 2077 6f72 6b65  tree)).    worke
-00000fc0: 722e 7374 6172 7428 290a 2020 2020 776f  r.start().    wo
-00000fd0: 726b 6572 2e6a 6f69 6e28 290a 2020 6301  rker.join().  c.
-00000fe0: 0000 0000 0000 0000 0000 0008 0000 0006  ................
-00000ff0: 0000 0043 0000 0073 4800 0000 7c00 6a00  ...C...sH...|.j.
-00001000: 5c04 7d01 7d02 7d03 7d04 6700 7d05 7c02  \.}.}.}.}.g.}.|.
-00001010: 4400 5d1c 7d06 7401 6a02 7c06 6401 6402  D.].}.t.j.|.d.d.
-00001020: 8d02 7d07 7c05 a003 7c07 a101 0100 7116  ..}.|...|.....q.
-00001030: 7404 7c01 7c05 7c03 7c04 6403 8d04 0100  t.|.|.|.|.d.....
-00001040: 6400 5300 2904 4efa 013a a901 da09 6465  d.S.).N..:....de
-00001050: 6c69 6d69 7465 7229 0472 6600 0000 7267  limiter).rf...rg
-00001060: 0000 0072 4d00 0000 7268 0000 0029 05da  ...rM...rh...)..
-00001070: 055f 6172 6773 da02 6e70 da07 6c6f 6164  ._args..np..load
-00001080: 7478 74da 0661 7070 656e 6472 6b00 0000  txt..appendrk...
-00001090: 2908 7211 0000 0072 6600 0000 da09 6669  ).r....rf.....fi
-000010a0: 6c65 7061 7468 7372 4d00 0000 7268 0000  lepathsrM...rh..
-000010b0: 0072 6700 0000 722a 0000 0072 6a00 0000  .rg...r*...rj...
-000010c0: 7213 0000 0072 1300 0000 7214 0000 00da  r....r....r.....
-000010d0: 0372 756e 6e00 0000 7318 0000 0000 010e  .runn...s.......
-000010e0: 0104 0108 010e 010c 0102 0102 0002 0002  ................
-000010f0: 0002 ff06 027a 1850 6c6f 7446 6967 7572  .....z.PlotFigur
-00001100: 6550 726f 6365 7373 696e 672e 7275 6e4e  eProcessing.runN
-00001110: 2905 723d 0000 0072 3e00 0000 723f 0000  ).r=...r>...r?..
-00001120: 00da 075f 5f64 6f63 5f5f 7275 0000 0072  ...__doc__ru...r
-00001130: 1300 0000 7213 0000 0072 1300 0000 7214  ....r....r....r.
-00001140: 0000 0072 6c00 0000 6800 0000 7304 0000  ...rl...h...s...
-00001150: 0008 0104 0572 6c00 0000 6305 0000 0000  .....rl...c.....
-00001160: 0000 0000 0000 0006 0000 0005 0000 0043  ...............C
-00001170: 0000 0073 2a00 0000 7400 7c00 7c01 7c02  ...s*...t.|.|.|.
-00001180: 7c03 6604 6401 8d01 7d05 7c05 a001 a100  |.f.d...}.|.....
-00001190: 0100 7c04 7226 7c05 a002 a100 0100 6400  ..|.r&|.......d.
-000011a0: 5300 a902 4e29 0172 4f00 0000 2903 726c  S...N).rO...).rl
-000011b0: 0000 00da 0573 7461 7274 724c 0000 0029  .....startrL...)
-000011c0: 0672 6600 0000 7274 0000 0072 4d00 0000  .rf...rt...rM...
-000011d0: 7268 0000 0072 4c00 0000 da06 776f 726b  rh...rL.....work
-000011e0: 6572 7213 0000 0072 1300 0000 7214 0000  err....r....r...
-000011f0: 00da 0b70 6c6f 745f 6669 6775 7265 7800  ...plot_figurex.
-00001200: 0000 730a 0000 0000 0112 0108 0204 0108  ..s.............
-00001210: 0172 7a00 0000 6300 0000 0000 0000 0000  .rz...c.........
-00001220: 0000 0000 0000 0003 0000 0040 0000 0073  ...........@...s
-00001230: 3200 0000 6500 5a01 6400 5a02 6401 5a03  2...e.Z.d.Z.d.Z.
-00001240: 6402 6403 8400 5a04 640c 6405 6406 8401  d.d...Z.d.d.d...
-00001250: 5a05 6407 6408 8400 5a06 6409 640a 8400  Z.d.d...Z.d.d...
-00001260: 5a07 640b 5300 290d da16 506c 6f74 4465  Z.d.S.)...PlotDe
-00001270: 6661 756c 7464 6963 7432 6669 6775 7265  faultdict2figure
-00001280: 7a63 0a20 2020 2077 6f72 6b65 7220 3d20  zc.    worker = 
-00001290: 506c 6f74 4465 6661 756c 7464 6963 7432  PlotDefaultdict2
-000012a0: 6669 6775 7265 2861 7267 733d 2873 2c20  figure(args=(s, 
-000012b0: 642c 2063 6f70 7974 7265 6529 290a 2020  d, copytree)).  
-000012c0: 2020 776f 726b 6572 2e73 7461 7274 2829    worker.start()
-000012d0: 0a20 2020 2077 6f72 6b65 722e 6a6f 696e  .    worker.join
-000012e0: 2829 0a20 2063 0100 0000 0000 0000 0000  ().  c..........
-000012f0: 0000 0600 0000 0600 0000 4300 0000 7346  ..........C...sF
-00001300: 0000 007c 006a 005c 037d 017d 027d 0367  ...|.j.\.}.}.}.g
-00001310: 007d 047c 0144 005d 1c7d 057c 04a0 0164  .}.|.D.].}.|...d
-00001320: 0164 0284 007c 05a0 02a1 0044 0083 01a1  .d...|.....D....
-00001330: 0101 0071 147c 006a 037c 047c 027c 0364  ...q.|.j.|.|.|.d
-00001340: 038d 0301 0064 0053 0029 044e 6301 0000  .....d.S.).Nc...
-00001350: 0000 0000 0000 0000 0003 0000 0007 0000  ................
-00001360: 0053 0000 0073 2000 0000 6900 7c00 5d18  .S...s ...i.|.].
-00001370: 5c02 7d01 7d02 7c01 7400 6a01 7c02 6400  \.}.}.|.t.j.|.d.
-00001380: 6401 8d02 9302 7104 5300 2902 726d 0000  d.....q.S.).rm..
-00001390: 0072 6e00 0000 2902 7271 0000 0072 7200  .rn...).rq...rr.
-000013a0: 0000 2903 722e 0000 00da 016b 722a 0000  ..).r......kr*..
-000013b0: 0072 1300 0000 7213 0000 0072 1400 0000  .r....r....r....
-000013c0: da0a 3c64 6963 7463 6f6d 703e 8c00 0000  ..<dictcomp>....
-000013d0: 7306 0000 0006 0006 0002 007a 2e50 6c6f  s..........z.Plo
-000013e0: 7444 6566 6175 6c74 6469 6374 3266 6967  tDefaultdict2fig
-000013f0: 7572 652e 7275 6e2e 3c6c 6f63 616c 733e  ure.run.<locals>
-00001400: 2e3c 6469 6374 636f 6d70 3e29 03da 106c  .<dictcomp>)...l
-00001410: 6162 656c 3264 6174 6173 5f6c 6973 7472  abel2datas_listr
-00001420: 7400 0000 da0d 696e 5f6f 6e65 5f66 6967  t.....in_one_fig
-00001430: 7572 6529 0472 7000 0000 7273 0000 00da  ure).rp...rs....
-00001440: 0569 7465 6d73 726b 0000 0029 0672 1100  .itemsrk...).r..
-00001450: 0000 da14 6c61 6265 6c32 6669 6c65 7061  ....label2filepa
-00001460: 7468 735f 6c69 7374 7274 0000 0072 7f00  ths_listrt...r..
-00001470: 0000 727e 0000 005a 0f6c 6162 656c 3266  ..r~...Z.label2f
-00001480: 696c 6570 6174 6873 7213 0000 0072 1300  ilepathsr....r..
-00001490: 0000 7214 0000 0072 7500 0000 8700 0000  ..r....ru.......
-000014a0: 730c 0000 0000 010c 0204 0108 011a 0110  s...............
-000014b0: 017a 1a50 6c6f 7444 6566 6175 6c74 6469  .z.PlotDefaultdi
-000014c0: 6374 3266 6967 7572 652e 7275 6e46 6304  ct2figure.runFc.
-000014d0: 0000 0000 0000 0000 0000 0006 0000 0004  ................
-000014e0: 0000 0043 0000 0073 4200 0000 6401 6400  ...C...sB...d.d.
-000014f0: 6c00 7d04 6401 6400 6c01 6d02 7d05 0100  l.}.d.d.l.m.}...
-00001500: 7c05 a003 6402 a101 0100 7c03 7230 7c00  |...d.....|.r0|.
-00001510: a004 7c01 7c02 a102 0100 6e0e 7c00 6a05  ..|.|.....n.|.j.
-00001520: 7c01 7c02 6403 8d02 0100 6400 5300 2904  |.|.d.....d.S.).
-00001530: 4e72 0100 0000 5a03 6167 6729 0272 7e00  Nr....Z.agg).r~.
-00001540: 0000 7274 0000 0029 0672 5f00 0000 720c  ..rt...).r_...r.
-00001550: 0000 0072 0d00 0000 5a0e 7377 6974 6368  ...r....Z.switch
-00001560: 5f62 6163 6b65 6e64 da13 5f70 6c6f 745f  _backend.._plot_
-00001570: 696e 5f6f 6e65 5f66 6967 7572 65da 165f  in_one_figure.._
-00001580: 706c 6f74 5f69 6e5f 6d75 6c74 695f 6669  plot_in_multi_fi
-00001590: 6775 7265 7329 0672 1100 0000 727e 0000  gures).r....r~..
-000015a0: 0072 7400 0000 727f 0000 0072 5f00 0000  .rt...r....r_...
-000015b0: 7212 0000 0072 1300 0000 7213 0000 0072  r....r....r....r
-000015c0: 1400 0000 726b 0000 0090 0000 0073 0c00  ....rk.......s..
-000015d0: 0000 0001 0802 0c01 0a01 0401 0e02 7a23  ..............z#
-000015e0: 506c 6f74 4465 6661 756c 7464 6963 7432  PlotDefaultdict2
-000015f0: 6669 6775 7265 2e5f 706c 6f74 5f66 6967  figure._plot_fig
-00001600: 7572 6563 0300 0000 0000 0000 0000 0000  urec............
-00001610: 0d00 0000 0900 0000 4300 0000 73e2 0000  ........C...s...
-00001620: 0064 0164 006c 006d 017d 0301 0074 027c  .d.d.l.m.}...t.|
-00001630: 0283 0164 026b 0273 1c74 0382 0174 0483  ...d.k.s.t...t..
-00001640: 007d 0464 037d 0574 027c 0183 017c 0517  .}.d.}.t.|...|..
-00001650: 0064 0218 007c 051a 007d 067c 046a 057c  .d...|...}.|.j.|
-00001660: 067c 0564 0464 058d 035c 027d 077d 0874  .|.d.d...\.}.}.t
-00001670: 067c 0183 0144 005d 665c 027d 097d 0a7c  .|...D.]f\.}.}.|
-00001680: 0aa0 07a1 0044 005d 445c 027d 0b7d 0c7c  .....D.]D\.}.}.|
-00001690: 0ca0 0864 0664 03a1 027d 0c7c 087c 0919  ...d.d...}.|.|..
-000016a0: 006a 097c 0c64 0064 0085 0264 0166 0219  .j.|.d.d...d.f..
-000016b0: 007c 0c64 0064 0085 0264 0266 0219 0064  .|.d.d...d.f...d
-000016c0: 077c 0b64 0864 098d 0501 0071 667c 087c  .|.d.d.....qf|.|
-000016d0: 0919 006a 0a64 0a64 0b8d 0101 0071 567c  ...j.d.d.....qV|
-000016e0: 046a 0b7c 077c 0264 0119 0064 0064 0c64  .j.|.|.d...d.d.d
-000016f0: 0d8d 0401 007c 03a0 0c7c 07a1 0101 0064  .....|...|.....d
-00001700: 0053 0029 0e4e 7201 0000 0072 1600 0000  .S.).Nr....r....
-00001710: 7256 0000 0054 2903 7218 0000 0072 1900  rV...T).r....r..
-00001720: 0000 721c 0000 0072 5500 0000 7257 0000  ..r....rU...rW..
-00001730: 0072 5800 0000 7259 0000 0072 5b00 0000  .rX...rY...r[...
-00001740: 725c 0000 0072 2100 0000 725e 0000 0029  r\...r!...r^...)
-00001750: 0d72 0c00 0000 720d 0000 0072 3500 0000  .r....r....r5...
-00001760: 7228 0000 0072 0200 0000 721f 0000 0072  r(...r....r....r
-00001770: 6300 0000 7280 0000 0072 6400 0000 7248  c...r....rd...rH
-00001780: 0000 0072 4900 0000 722b 0000 0072 6500  ...rI...r+...re.
-00001790: 0000 290d 7211 0000 0072 7e00 0000 7274  ..).r....r~...rt
-000017a0: 0000 0072 1200 0000 7250 0000 0072 1900  ...r....rP...r..
-000017b0: 0000 7218 0000 0072 1d00 0000 721e 0000  ..r....r....r...
-000017c0: 0072 3b00 0000 da0b 6c61 6265 6c32 6461  .r;.....label2da
-000017d0: 7461 7372 4000 0000 726a 0000 0072 1300  tasr@...rj...r..
-000017e0: 0000 7213 0000 0072 1400 0000 7282 0000  ..r....r....r...
-000017f0: 009a 0000 0073 1c00 0000 0001 0c01 1001  .....s..........
-00001800: 0602 0401 1401 1403 1001 1001 0c01 3201  ..............2.
-00001810: 1202 1601 0a01 7a2a 506c 6f74 4465 6661  ......z*PlotDefa
-00001820: 756c 7464 6963 7432 6669 6775 7265 2e5f  ultdict2figure._
-00001830: 706c 6f74 5f69 6e5f 6f6e 655f 6669 6775  plot_in_one_figu
-00001840: 7265 6303 0000 0000 0000 0000 0000 000b  rec.............
-00001850: 0000 0009 0000 0043 0000 0073 c400 0000  .......C...s....
-00001860: 6401 6400 6c00 6d01 7d03 0100 7402 7c02  d.d.l.m.}...t.|.
-00001870: 8301 7402 7c01 8301 6b02 7320 7403 8201  ..t.|...k.s t...
-00001880: 7404 8300 7d04 7405 7c01 8301 4400 5d90  t...}.t.|...D.].
-00001890: 5c02 7d05 7d06 7c04 6a06 6402 6402 6403  \.}.}.|.j.d.d.d.
-000018a0: 8d02 5c02 7d07 7d08 7c06 a007 a100 4400  ..\.}.}.|.....D.
-000018b0: 5d40 5c02 7d09 7d0a 7c0a a008 6404 6405  ]@\.}.}.|...d.d.
-000018c0: a102 7d0a 7c08 6a09 7c0a 6400 6400 8502  ..}.|.j.|.d.d...
-000018d0: 6401 6602 1900 7c0a 6400 6400 8502 6402  d.f...|.d.d...d.
-000018e0: 6602 1900 6406 7c09 6407 6408 8d05 0100  f...d.|.d.d.....
-000018f0: 7150 7c08 6a0a 6409 640a 8d01 0100 7c04  qP|.j.d.d.....|.
-00001900: 6a0b 7c07 7c02 7c05 1900 6400 640b 640c  j.|.|.|...d.d.d.
-00001910: 8d04 0100 7c03 a00c 7c07 a101 0100 712e  ....|...|.....q.
-00001920: 6400 5300 290d 4e72 0100 0000 7216 0000  d.S.).Nr....r...
-00001930: 0072 5400 0000 7255 0000 0072 5600 0000  .rT...rU...rV...
-00001940: 7257 0000 0072 5800 0000 7259 0000 0072  rW...rX...rY...r
-00001950: 5b00 0000 725c 0000 0072 2100 0000 725e  [...r\...r!...r^
-00001960: 0000 0029 0d72 0c00 0000 720d 0000 0072  ...).r....r....r
-00001970: 3500 0000 7228 0000 0072 0200 0000 7263  5...r(...r....rc
-00001980: 0000 0072 1f00 0000 7280 0000 0072 6400  ...r....r....rd.
-00001990: 0000 7248 0000 0072 4900 0000 722b 0000  ..rH...rI...r+..
-000019a0: 0072 6500 0000 290b 7211 0000 0072 7e00  .re...).r....r~.
-000019b0: 0000 7274 0000 0072 1200 0000 7250 0000  ..rt...r....rP..
-000019c0: 0072 3b00 0000 7284 0000 0072 1d00 0000  .r;...r....r....
-000019d0: 721e 0000 0072 4000 0000 726a 0000 0072  r....r@...rj...r
-000019e0: 1300 0000 7213 0000 0072 1400 0000 7283  ....r....r....r.
-000019f0: 0000 00ae 0000 0073 1800 0000 0001 0c01  .......s........
-00001a00: 1401 0601 1001 1202 1001 0c01 2e01 0c02  ................
-00001a10: 1601 0c01 7a2d 506c 6f74 4465 6661 756c  ....z-PlotDefaul
-00001a20: 7464 6963 7432 6669 6775 7265 2e5f 706c  tdict2figure._pl
-00001a30: 6f74 5f69 6e5f 6d75 6c74 695f 6669 6775  ot_in_multi_figu
-00001a40: 7265 734e 2901 4629 0872 3d00 0000 723e  resN).F).r=...r>
-00001a50: 0000 0072 3f00 0000 7276 0000 0072 7500  ...r?...rv...ru.
-00001a60: 0000 726b 0000 0072 8200 0000 7283 0000  ..rk...r....r...
-00001a70: 0072 1300 0000 7213 0000 0072 1300 0000  .r....r....r....
-00001a80: 7214 0000 0072 7b00 0000 8100 0000 730a  r....r{.......s.
-00001a90: 0000 0008 0104 0508 090a 0a08 1472 7b00  .............r{.
-00001aa0: 0000 6304 0000 0000 0000 0000 0000 0005  ..c.............
-00001ab0: 0000 0004 0000 0043 0000 0073 2800 0000  .......C...s(...
-00001ac0: 7400 7c00 7c01 7c02 6603 6401 8d01 7d04  t.|.|.|.f.d...}.
-00001ad0: 7c04 a001 a100 0100 7c03 7224 7c04 a002  |.......|.r$|...
-00001ae0: a100 0100 6400 5300 7277 0000 0029 0372  ....d.S.rw...).r
-00001af0: 7b00 0000 7278 0000 0072 4c00 0000 2905  {...rx...rL...).
-00001b00: 7281 0000 0072 7400 0000 727f 0000 0072  r....rt...r....r
-00001b10: 4c00 0000 7279 0000 0072 1300 0000 7213  L...ry...r....r.
-00001b20: 0000 0072 1400 0000 da17 706c 6f74 5f64  ...r......plot_d
-00001b30: 6566 6175 6c74 6469 6374 3266 6967 7572  efaultdict2figur
-00001b40: 65bf 0000 0073 0a00 0000 0001 1001 0802  e....s..........
-00001b50: 0401 0801 7285 0000 0029 0146 2901 4629  ....r....).F).F)
-00001b60: 0146 290f da05 6e75 6d70 7972 7100 0000  .F)...numpyrq...
-00001b70: 7260 0000 0072 4600 0000 724a 0000 00da  r`...rF...rJ....
-00001b80: 0f6d 756c 7469 7072 6f63 6573 7369 6e67  .multiprocessing
-00001b90: da06 6f62 6a65 6374 7202 0000 0072 5200  ..objectr....rR.
-00001ba0: 0000 726b 0000 00da 0750 726f 6365 7373  ..rk.....Process
-00001bb0: 726c 0000 0072 7a00 0000 727b 0000 0072  rl...rz...r{...r
-00001bc0: 8500 0000 7213 0000 0072 1300 0000 7213  ....r....r....r.
-00001bd0: 0000 0072 1400 0000 da08 3c6d 6f64 756c  ...r......<modul
-00001be0: 653e 0100 0000 7314 0000 0008 0108 0110  e>....s.........
-00001bf0: 0108 0310 3208 100a 1f12 100a 0912 3e    ....2.........>
+000003d0: 746c 322f 746c 322f 7072 6f6a 2f6c 6f67  tl2/tl2/proj/log
+000003e0: 6765 722f 706c 6f74 5f75 7469 6c73 2e70  ger/plot_utils.p
+000003f0: 79da 085f 5f69 6e69 745f 5f08 0000 0073  y..__init__....s
+00000400: 1000 0000 0005 0c02 0c01 0401 1801 02ff  ................
+00000410: 02ff 0c04 7a10 4d61 7450 6c6f 742e 5f5f  ....z.MatPlot.__
+00000420: 696e 6974 5f5f e901 0000 0046 a902 679a  init__.....F..g.
+00000430: 9999 9999 9919 4067 3333 3333 3333 1340  ......@g333333.@
+00000440: 6305 0000 0000 0000 0000 0000 0009 0000  c...............
+00000450: 0005 0000 0043 0000 0073 6400 0000 6401  .....C...sd...d.
+00000460: 6402 6c00 6d01 7d05 0100 7c04 6401 1900  d.l.m.}...|.d...
+00000470: 7c02 1400 7c04 6403 1900 7c01 1400 6602  |...|.d...|...f.
+00000480: 7d06 7c05 6a02 7c01 7c02 7c06 6404 8d03  }.|.j.|.|.|.d...
+00000490: 5c02 7d07 7d08 7c03 725c 7c02 6403 6b02  \.}.}.|.r\|.d.k.
+000004a0: 7254 7c01 6403 6b02 7254 7c08 6701 7d08  rT|.d.k.rT|.g.}.
+000004b0: 6e08 7c08 a003 a100 7d08 7c07 7c08 6602  n.|.....}.|.|.f.
+000004c0: 5300 2905 7a1f 0a20 2020 2061 782e 6c65  S.).z..    ax.le
+000004d0: 6765 6e64 286c 6f63 3d27 6265 7374 2729  gend(loc='best')
+000004e0: 0a20 2020 2072 0100 0000 4e72 1600 0000  .    r....Nr....
+000004f0: 2903 da05 6e72 6f77 73da 056e 636f 6c73  )...nrows..ncols
+00000500: da07 6669 6773 697a 6529 0472 0c00 0000  ..figsize).r....
+00000510: 720d 0000 00da 0873 7562 706c 6f74 73da  r......subplots.
+00000520: 0572 6176 656c 2909 7211 0000 0072 1800  .ravel).r....r..
+00000530: 0000 7219 0000 0072 1c00 0000 5a07 6669  ..r....r....Z.fi
+00000540: 675f 775f 6872 1200 0000 721a 0000 00da  g_w_hr....r.....
+00000550: 0366 6967 da04 6178 6573 7213 0000 0072  .fig..axesr....r
+00000560: 1300 0000 7214 0000 00da 0e67 6574 5f66  ....r......get_f
+00000570: 6967 5f61 6e64 5f61 7816 0000 0073 1000  ig_and_ax....s..
+00000580: 0000 0004 0c01 1801 1401 0401 1001 0802  ................
+00000590: 0801 7a16 4d61 7450 6c6f 742e 6765 745f  ..z.MatPlot.get_
+000005a0: 6669 675f 616e 645f 6178 e9e8 0300 00da  fig_and_ax......
+000005b0: 0574 6967 6874 e79a 9999 9999 99b9 3f63  .tight........?c
+000005c0: 0600 0000 0000 0000 0000 0000 0600 0000  ................
+000005d0: 0600 0000 4300 0000 7324 0000 007c 02a0  ....C...s$...|..
+000005e0: 0064 01a1 0173 0e74 0182 017c 016a 027c  .d...s.t...|.j.|
+000005f0: 027c 037c 047c 0564 028d 0401 0064 0053  .|.|.|.d.....d.S
+00000600: 0029 034e fa04 2e70 6e67 2903 da03 6470  .).N...png)...dp
+00000610: 69da 0b62 626f 785f 696e 6368 6573 da0a  i..bbox_inches..
+00000620: 7061 645f 696e 6368 6573 2903 da08 656e  pad_inches)...en
+00000630: 6473 7769 7468 da0e 4173 7365 7274 696f  dswith..Assertio
+00000640: 6e45 7272 6f72 da07 7361 7665 6669 6729  nError..savefig)
+00000650: 0672 1100 0000 721d 0000 00da 0866 696c  .r....r......fil
+00000660: 6570 6174 6872 2400 0000 7225 0000 0072  epathr$...r%...r
+00000670: 2600 0000 7213 0000 0072 1300 0000 7214  &...r....r....r.
+00000680: 0000 00da 0b73 6176 655f 746f 5f70 6e67  .....save_to_png
+00000690: 2400 0000 730e 0000 0000 020e 0104 0102  $...s...........
+000006a0: 0002 0002 0002 ff7a 134d 6174 506c 6f74  .......z.MatPlot
+000006b0: 2e73 6176 655f 746f 5f70 6e67 6303 0000  .save_to_pngc...
+000006c0: 0000 0000 0000 0000 0003 0000 0005 0000  ................
+000006d0: 0043 0000 0073 1400 0000 7c01 6a00 7c02  .C...s....|.j.|.
+000006e0: 6401 6402 6403 8d03 0100 6400 5300 2904  d.d.d.....d.S.).
+000006f0: 4e72 2100 0000 7201 0000 0029 0272 2500  Nr!...r....).r%.
+00000700: 0000 7226 0000 0029 0172 2900 0000 2903  ..r&...).r)...).
+00000710: 7211 0000 0072 1d00 0000 722a 0000 0072  r....r....r*...r
+00000720: 1300 0000 7213 0000 0072 1400 0000 da0b  ....r....r......
+00000730: 7361 7665 5f74 6f5f 7064 662a 0000 0073  save_to_pdf*...s
+00000740: 0200 0000 0001 7a13 4d61 7450 6c6f 742e  ......z.MatPlot.
+00000750: 7361 7665 5f74 6f5f 7064 6663 0300 0000  save_to_pdfc....
+00000760: 0000 0000 0000 0000 0700 0000 0900 0000  ................
+00000770: 4300 0000 7344 0000 0074 007c 0183 018f  C...sD...t.|....
+00000780: 2e7d 037c 03a0 01a1 007d 0464 0164 0284  .}.|.....}.d.d..
+00000790: 007c 02a0 027c 04a1 0144 0083 017d 0574  .|...|...D...}.t
+000007a0: 0374 047c 0583 0183 017d 0657 0035 0051  .t.|.....}.W.5.Q
+000007b0: 0052 0058 007c 067c 0566 0253 0029 037a  .R.X.|.|.f.S.).z
+000007c0: 140a 2020 2020 696d 706f 7274 2072 650a  ..    import re.
+000007d0: 0a20 2020 2063 0100 0000 0000 0000 0000  .    c..........
+000007e0: 0000 0200 0000 0400 0000 5300 0000 7314  ..........S...s.
+000007f0: 0000 0067 007c 005d 0c7d 0174 007c 0183  ...g.|.].}.t.|..
+00000800: 0191 0271 0453 0072 1300 0000 2901 da05  ...q.S.r....)...
+00000810: 666c 6f61 7429 02da 022e 30da 0178 7213  float)....0..xr.
+00000820: 0000 0072 1300 0000 7214 0000 00da 0a3c  ...r....r......<
+00000830: 6c69 7374 636f 6d70 3e34 0000 0073 0400  listcomp>4...s..
+00000840: 0000 0600 0200 7a32 4d61 7450 6c6f 742e  ......z2MatPlot.
+00000850: 7061 7273 655f 6c6f 6766 696c 655f 7573  parse_logfile_us
+00000860: 696e 675f 7265 2e3c 6c6f 6361 6c73 3e2e  ing_re.<locals>.
+00000870: 3c6c 6973 7463 6f6d 703e 2905 da04 6f70  <listcomp>)...op
+00000880: 656e da04 7265 6164 da07 6669 6e64 616c  en..read..findal
+00000890: 6cda 0572 616e 6765 da03 6c65 6e29 0772  l..range..len).r
+000008a0: 1100 0000 da07 6c6f 6766 696c 65da 0672  ......logfile..r
+000008b0: 655f 7374 72da 0166 da06 6c6f 6773 7472  e_str..f..logstr
+000008c0: da03 7661 6cda 0369 6478 7213 0000 0072  ..val..idxr....r
+000008d0: 1300 0000 7214 0000 00da 1670 6172 7365  ....r......parse
+000008e0: 5f6c 6f67 6669 6c65 5f75 7369 6e67 5f72  _logfile_using_r
+000008f0: 652d 0000 0073 0a00 0000 0005 0a01 0801  e-...s..........
+00000900: 1401 1601 7a1e 4d61 7450 6c6f 742e 7061  ....z.MatPlot.pa
+00000910: 7273 655f 6c6f 6766 696c 655f 7573 696e  rse_logfile_usin
+00000920: 675f 7265 4e29 0172 0300 0000 2904 7216  g_reN).r....).r.
+00000930: 0000 0072 1600 0000 4672 1700 0000 2903  ...r....Fr....).
+00000940: 7220 0000 0072 2100 0000 7222 0000 0029  r ...r!...r"...)
+00000950: 08da 085f 5f6e 616d 655f 5fda 0a5f 5f6d  ...__name__..__m
+00000960: 6f64 756c 655f 5fda 0c5f 5f71 7561 6c6e  odule__..__qualn
+00000970: 616d 655f 5f72 1500 0000 721f 0000 0072  ame__r....r....r
+00000980: 2b00 0000 722c 0000 0072 3c00 0000 7213  +...r,...r<...r.
+00000990: 0000 0072 1300 0000 7213 0000 0072 1400  ...r....r....r..
+000009a0: 0000 7202 0000 0007 0000 0073 0e00 0000  ..r........s....
+000009b0: 0801 0a0e 0a0e 0001 00ff 0a06 0803 7202  ..............r.
+000009c0: 0000 0063 0200 0000 0000 0000 0000 0000  ...c............
+000009d0: 0c00 0000 0700 0000 4300 0000 73ac 0000  ........C...s...
+000009e0: 0074 007c 016a 017c 006a 0283 027d 0274  .t.|.j.|.j...}.t
+000009f0: 0383 007d 037c 03a0 04a1 005c 027d 047d  ...}.|.....\.}.}
+00000a00: 0574 057c 026a 0683 0164 016b 0272 3e7c  .t.|.j...d.k.r>|
+00000a10: 026a 0674 057c 026a 0783 0114 007d 0674  .j.t.|.j.....}.t
+00000a20: 087c 067c 026a 0783 0244 005d 345c 027d  .|.|.j...D.]4\.}
+00000a30: 077d 0874 09a0 0a7c 08a1 017d 097c 036a  .}.t...|...}.|.j
+00000a40: 0b7c 077c 0964 028d 025c 027d 0a7d 0b7c  .|.|.d...\.}.}.|
+00000a50: 056a 0c7c 0a7c 0b7c 0864 038d 0301 0071  .j.|.|.|.d.....q
+00000a60: 4a7c 05a0 0da1 0001 007c 036a 0e7c 0474  J|.......|.j.|.t
+00000a70: 0f6a 10a0 117c 006a 127c 026a 1364 0417  .j...|.j.|.j.d..
+00000a80: 00a1 0264 058d 0201 0064 0053 0029 064e  ...d.....d.S.).N
+00000a90: 7216 0000 0029 0272 3600 0000 7237 0000  r....).r6...r7..
+00000aa0: 0029 01da 056c 6162 656c 7223 0000 0029  .)...labelr#...)
+00000ab0: 0172 2a00 0000 2914 da07 6765 7461 7474  .r*...)...getatt
+00000ac0: 72da 0663 6f6e 6669 67da 0763 6f6d 6d61  r..config..comma
+00000ad0: 6e64 7202 0000 0072 1f00 0000 7235 0000  ndr....r....r5..
+00000ae0: 00da 086c 6f67 6669 6c65 735a 0772 655f  ...logfilesZ.re_
+00000af0: 7374 7273 da03 7a69 70da 0272 65da 0763  strs..zip..re..c
+00000b00: 6f6d 7069 6c65 723c 0000 00da 0470 6c6f  ompiler<.....plo
+00000b10: 74da 066c 6567 656e 6472 2b00 0000 da02  t..legendr+.....
+00000b20: 6f73 da04 7061 7468 da04 6a6f 696e da06  os..path..join..
+00000b30: 6f75 7464 6972 da05 7469 746c 6529 0cda  outdir..title)..
+00000b40: 0461 7267 735a 066d 7961 7267 7372 4200  .argsZ.myargsrB.
+00000b50: 0000 da07 6d61 7470 6c6f 7472 1d00 0000  ....matplotr....
+00000b60: da02 6178 7244 0000 0072 3600 0000 7237  ..axrD...r6...r7
+00000b70: 0000 005a 0652 455f 5354 5272 3b00 0000  ...Z.RE_STRr;...
+00000b80: 723a 0000 0072 1300 0000 7213 0000 0072  r:...r....r....r
+00000b90: 1400 0000 da0d 7061 7273 655f 6c6f 6766  ......parse_logf
+00000ba0: 696c 6539 0000 0073 1e00 0000 0001 0e01  ile9...s........
+00000bb0: 0601 0c01 0e01 1001 1401 0a01 1201 1201  ................
+00000bc0: 0801 0401 0200 14ff 0602 7252 0000 0046  ..........rR...F
+00000bd0: 6304 0000 0000 0000 0000 0000 000f 0000  c...............
+00000be0: 0008 0000 0043 0000 0073 6401 0000 6401  .....C...sd...d.
+00000bf0: 6400 6c00 7d04 7c04 a001 6402 a101 0100  d.l.}.|...d.....
+00000c00: 6401 6400 6c02 6d03 7d05 0100 7404 7c01  d.d.l.m.}...t.|.
+00000c10: 8301 7404 7c00 8301 6b02 7332 7405 8201  ..t.|...k.s2t...
+00000c20: 7406 6a07 a008 7c02 6403 6404 a008 7c00  t.j...|.d.d...|.
+00000c30: a101 1700 6405 1700 a102 7d06 7409 8300  ....d.....}.t...
+00000c40: 7d07 7c03 73b4 740a a00b 740a a00c 7404  }.|.s.t...t...t.
+00000c50: 7c00 8301 a101 a101 7d08 7404 7c00 8301  |.......}.t.|...
+00000c60: 7c08 1700 6406 1800 7c08 1a00 7d09 7c07  |...d...|...}.|.
+00000c70: 6a0d 7c09 7c08 6407 8d02 5c02 7d0a 7d0b  j.|.|.d...\.}.}.
+00000c80: 7c08 6406 6b02 72aa 7c09 6406 6b02 72aa  |.d.k.r.|.d.k.r.
+00000c90: 7c0b 6701 7d0b 71dc 7c0b a00e a100 7d0b  |.g.}.q.|.....}.
+00000ca0: 6e28 6406 7d08 6406 7d09 7c07 6a0d 7c09  n(d.}.d.}.|.j.|.
+00000cb0: 7c08 6407 8d02 5c02 7d0a 7d0b 7c0b 6701  |.d...\.}.}.|.g.
+00000cc0: 7404 7c00 8301 1400 7d0b 740f 7410 7c00  t.|.....}.t.t.|.
+00000cd0: 7c01 8302 8301 4400 5d58 5c02 7d0c 5c02  |.....D.]X\.}.\.
+00000ce0: 7d0d 7d0e 7c0e a011 6408 6409 a102 7d0e  }.}.|...d.d...}.
+00000cf0: 7c0b 7c0c 1900 6a12 7c0e 6400 6400 8502  |.|...j.|.d.d...
+00000d00: 6401 6602 1900 7c0e 6400 6400 8502 6406  d.f...|.d.d...d.
+00000d10: 6602 1900 640a 7c0d 640b 640c 8d05 0100  f...d.|.d.d.....
+00000d20: 7c0b 7c0c 1900 6a13 640d 640e 8d01 0100  |.|...j.d.d.....
+00000d30: 71ea 7c07 6a14 7c0a 7c06 6400 6400 640f  q.|.j.|.|.d.d.d.
+00000d40: 8d04 0100 7c05 a015 7c0a a101 0100 6400  ....|...|.....d.
+00000d50: 5300 2910 4e72 0100 0000 5a03 4167 675a  S.).Nr....Z.AggZ
+00000d60: 0570 6c6f 745f da02 5f5f 7223 0000 0072  .plot_..__r#...r
+00000d70: 1600 0000 a902 7218 0000 0072 1900 0000  ......r....r....
+00000d80: e9ff ffff ffe9 0200 0000 da01 2ee7 6666  ..............ff
+00000d90: 6666 6666 e63f a903 5a06 6d61 726b 6572  ffff.?..Z.marker
+00000da0: 7240 0000 00da 0561 6c70 6861 da04 6265  r@.....alpha..be
+00000db0: 7374 a901 da03 6c6f 63a9 0472 1d00 0000  st....loc..r....
+00000dc0: 722a 0000 0072 2400 0000 7225 0000 0029  r*...r$...r%...)
+00000dd0: 16da 0a6d 6174 706c 6f74 6c69 6272 0f00  ...matplotlibr..
+00000de0: 0000 720c 0000 0072 0d00 0000 7235 0000  ..r....r....r5..
+00000df0: 0072 2800 0000 724a 0000 0072 4b00 0000  .r(...rJ...rK...
+00000e00: 724c 0000 0072 0200 0000 da04 6d61 7468  rL...r......math
+00000e10: da04 6365 696c da04 7371 7274 721f 0000  ..ceil..sqrtr...
+00000e20: 0072 1c00 0000 da09 656e 756d 6572 6174  .r......enumerat
+00000e30: 6572 4500 0000 da07 7265 7368 6170 6572  erE.....reshaper
+00000e40: 4800 0000 7249 0000 0072 2b00 0000 da05  H...rI...r+.....
+00000e50: 636c 6f73 6529 0fda 056e 616d 6573 da05  close)...names..
+00000e60: 6461 7461 7372 4d00 0000 da0b 696e 5f6f  datasrM.....in_o
+00000e70: 6e65 5f61 7865 7372 5f00 0000 7212 0000  ne_axesr_...r...
+00000e80: 00da 0866 696c 656e 616d 6572 5000 0000  ...filenamerP...
+00000e90: 7219 0000 0072 1800 0000 721d 0000 0072  r....r....r....r
+00000ea0: 1e00 0000 723b 0000 0072 4000 0000 da04  ....r;...r@.....
+00000eb0: 6461 7461 7213 0000 0072 1300 0000 7214  datar....r....r.
+00000ec0: 0000 00da 0c5f 706c 6f74 5f66 6967 7572  ....._plot_figur
+00000ed0: 6549 0000 0073 3000 0000 0001 0801 0a01  eI...s0.........
+00000ee0: 0c01 1401 1c01 0601 0401 1401 1401 1201  ................
+00000ef0: 1001 0802 0a02 0401 0401 1201 0e02 1a01  ................
+00000f00: 0c01 3001 1202 1201 0a01 726b 0000 0063  ..0.......rk...c
+00000f10: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00000f20: 0200 0000 4000 0000 7318 0000 0065 005a  ....@...s....e.Z
+00000f30: 0164 005a 0264 015a 0364 0264 0384 005a  .d.Z.d.Z.d.d...Z
+00000f40: 0464 0453 0029 05da 1450 6c6f 7446 6967  .d.S.)...PlotFig
+00000f50: 7572 6550 726f 6365 7373 696e 677a 610a  ureProcessingza.
+00000f60: 2020 2020 776f 726b 6572 203d 2050 6c6f      worker = Plo
+00000f70: 7446 6967 7572 6550 726f 6365 7373 696e  tFigureProcessin
+00000f80: 6728 6172 6773 3d28 732c 2064 2c20 636f  g(args=(s, d, co
+00000f90: 7079 7472 6565 2929 0a20 2020 2077 6f72  pytree)).    wor
+00000fa0: 6b65 722e 7374 6172 7428 290a 2020 2020  ker.start().    
+00000fb0: 776f 726b 6572 2e6a 6f69 6e28 290a 2020  worker.join().  
+00000fc0: 6301 0000 0000 0000 0000 0000 0008 0000  c...............
+00000fd0: 0006 0000 0043 0000 0073 4800 0000 7c00  .....C...sH...|.
+00000fe0: 6a00 5c04 7d01 7d02 7d03 7d04 6700 7d05  j.\.}.}.}.}.g.}.
+00000ff0: 7c02 4400 5d1c 7d06 7401 6a02 7c06 6401  |.D.].}.t.j.|.d.
+00001000: 6402 8d02 7d07 7c05 a003 7c07 a101 0100  d...}.|...|.....
+00001010: 7116 7404 7c01 7c05 7c03 7c04 6403 8d04  q.t.|.|.|.|.d...
+00001020: 0100 6400 5300 2904 4efa 013a a901 da09  ..d.S.).N..:....
+00001030: 6465 6c69 6d69 7465 7229 0472 6600 0000  delimiter).rf...
+00001040: 7267 0000 0072 4d00 0000 7268 0000 0029  rg...rM...rh...)
+00001050: 05da 055f 6172 6773 da02 6e70 da07 6c6f  ..._args..np..lo
+00001060: 6164 7478 74da 0661 7070 656e 6472 6b00  adtxt..appendrk.
+00001070: 0000 2908 7211 0000 0072 6600 0000 da09  ..).r....rf.....
+00001080: 6669 6c65 7061 7468 7372 4d00 0000 7268  filepathsrM...rh
+00001090: 0000 0072 6700 0000 722a 0000 0072 6a00  ...rg...r*...rj.
+000010a0: 0000 7213 0000 0072 1300 0000 7214 0000  ..r....r....r...
+000010b0: 00da 0372 756e 6e00 0000 7318 0000 0000  ...runn...s.....
+000010c0: 010e 0104 0108 010e 010c 0102 0102 0002  ................
+000010d0: 0002 0002 ff06 027a 1850 6c6f 7446 6967  .......z.PlotFig
+000010e0: 7572 6550 726f 6365 7373 696e 672e 7275  ureProcessing.ru
+000010f0: 6e4e 2905 723d 0000 0072 3e00 0000 723f  nN).r=...r>...r?
+00001100: 0000 00da 075f 5f64 6f63 5f5f 7275 0000  .....__doc__ru..
+00001110: 0072 1300 0000 7213 0000 0072 1300 0000  .r....r....r....
+00001120: 7214 0000 0072 6c00 0000 6800 0000 7304  r....rl...h...s.
+00001130: 0000 0008 0104 0572 6c00 0000 6305 0000  .......rl...c...
+00001140: 0000 0000 0000 0000 0006 0000 0005 0000  ................
+00001150: 0043 0000 0073 2a00 0000 7400 7c00 7c01  .C...s*...t.|.|.
+00001160: 7c02 7c03 6604 6401 8d01 7d05 7c05 a001  |.|.f.d...}.|...
+00001170: a100 0100 7c04 7226 7c05 a002 a100 0100  ....|.r&|.......
+00001180: 6400 5300 a902 4e29 0172 4f00 0000 2903  d.S...N).rO...).
+00001190: 726c 0000 00da 0573 7461 7274 724c 0000  rl.....startrL..
+000011a0: 0029 0672 6600 0000 7274 0000 0072 4d00  .).rf...rt...rM.
+000011b0: 0000 7268 0000 0072 4c00 0000 da06 776f  ..rh...rL.....wo
+000011c0: 726b 6572 7213 0000 0072 1300 0000 7214  rkerr....r....r.
+000011d0: 0000 00da 0b70 6c6f 745f 6669 6775 7265  .....plot_figure
+000011e0: 7800 0000 730a 0000 0000 0112 0108 0204  x...s...........
+000011f0: 0108 0172 7a00 0000 6300 0000 0000 0000  ...rz...c.......
+00001200: 0000 0000 0000 0000 0003 0000 0040 0000  .............@..
+00001210: 0073 3200 0000 6500 5a01 6400 5a02 6401  .s2...e.Z.d.Z.d.
+00001220: 5a03 6402 6403 8400 5a04 640c 6405 6406  Z.d.d...Z.d.d.d.
+00001230: 8401 5a05 6407 6408 8400 5a06 6409 640a  ..Z.d.d...Z.d.d.
+00001240: 8400 5a07 640b 5300 290d da16 506c 6f74  ..Z.d.S.)...Plot
+00001250: 4465 6661 756c 7464 6963 7432 6669 6775  Defaultdict2figu
+00001260: 7265 7a63 0a20 2020 2077 6f72 6b65 7220  rezc.    worker 
+00001270: 3d20 506c 6f74 4465 6661 756c 7464 6963  = PlotDefaultdic
+00001280: 7432 6669 6775 7265 2861 7267 733d 2873  t2figure(args=(s
+00001290: 2c20 642c 2063 6f70 7974 7265 6529 290a  , d, copytree)).
+000012a0: 2020 2020 776f 726b 6572 2e73 7461 7274      worker.start
+000012b0: 2829 0a20 2020 2077 6f72 6b65 722e 6a6f  ().    worker.jo
+000012c0: 696e 2829 0a20 2063 0100 0000 0000 0000  in().  c........
+000012d0: 0000 0000 0600 0000 0600 0000 4300 0000  ............C...
+000012e0: 7346 0000 007c 006a 005c 037d 017d 027d  sF...|.j.\.}.}.}
+000012f0: 0367 007d 047c 0144 005d 1c7d 057c 04a0  .g.}.|.D.].}.|..
+00001300: 0164 0164 0284 007c 05a0 02a1 0044 0083  .d.d...|.....D..
+00001310: 01a1 0101 0071 147c 006a 037c 047c 027c  .....q.|.j.|.|.|
+00001320: 0364 038d 0301 0064 0053 0029 044e 6301  .d.....d.S.).Nc.
+00001330: 0000 0000 0000 0000 0000 0003 0000 0007  ................
+00001340: 0000 0053 0000 0073 2000 0000 6900 7c00  ...S...s ...i.|.
+00001350: 5d18 5c02 7d01 7d02 7c01 7400 6a01 7c02  ].\.}.}.|.t.j.|.
+00001360: 6400 6401 8d02 9302 7104 5300 2902 726d  d.d.....q.S.).rm
+00001370: 0000 0072 6e00 0000 2902 7271 0000 0072  ...rn...).rq...r
+00001380: 7200 0000 2903 722e 0000 00da 016b 722a  r...).r......kr*
+00001390: 0000 0072 1300 0000 7213 0000 0072 1400  ...r....r....r..
+000013a0: 0000 da0a 3c64 6963 7463 6f6d 703e 8c00  ....<dictcomp>..
+000013b0: 0000 7306 0000 0006 0006 0002 007a 2e50  ..s..........z.P
+000013c0: 6c6f 7444 6566 6175 6c74 6469 6374 3266  lotDefaultdict2f
+000013d0: 6967 7572 652e 7275 6e2e 3c6c 6f63 616c  igure.run.<local
+000013e0: 733e 2e3c 6469 6374 636f 6d70 3e29 03da  s>.<dictcomp>)..
+000013f0: 106c 6162 656c 3264 6174 6173 5f6c 6973  .label2datas_lis
+00001400: 7472 7400 0000 da0d 696e 5f6f 6e65 5f66  trt.....in_one_f
+00001410: 6967 7572 6529 0472 7000 0000 7273 0000  igure).rp...rs..
+00001420: 00da 0569 7465 6d73 726b 0000 0029 0672  ...itemsrk...).r
+00001430: 1100 0000 da14 6c61 6265 6c32 6669 6c65  ......label2file
+00001440: 7061 7468 735f 6c69 7374 7274 0000 0072  paths_listrt...r
+00001450: 7f00 0000 727e 0000 005a 0f6c 6162 656c  ....r~...Z.label
+00001460: 3266 696c 6570 6174 6873 7213 0000 0072  2filepathsr....r
+00001470: 1300 0000 7214 0000 0072 7500 0000 8700  ....r....ru.....
+00001480: 0000 730c 0000 0000 010c 0204 0108 011a  ..s.............
+00001490: 0110 017a 1a50 6c6f 7444 6566 6175 6c74  ...z.PlotDefault
+000014a0: 6469 6374 3266 6967 7572 652e 7275 6e46  dict2figure.runF
+000014b0: 6304 0000 0000 0000 0000 0000 0006 0000  c...............
+000014c0: 0004 0000 0043 0000 0073 4200 0000 6401  .....C...sB...d.
+000014d0: 6400 6c00 7d04 6401 6400 6c01 6d02 7d05  d.l.}.d.d.l.m.}.
+000014e0: 0100 7c05 a003 6402 a101 0100 7c03 7230  ..|...d.....|.r0
+000014f0: 7c00 a004 7c01 7c02 a102 0100 6e0e 7c00  |...|.|.....n.|.
+00001500: 6a05 7c01 7c02 6403 8d02 0100 6400 5300  j.|.|.d.....d.S.
+00001510: 2904 4e72 0100 0000 5a03 6167 6729 0272  ).Nr....Z.agg).r
+00001520: 7e00 0000 7274 0000 0029 0672 5f00 0000  ~...rt...).r_...
+00001530: 720c 0000 0072 0d00 0000 5a0e 7377 6974  r....r....Z.swit
+00001540: 6368 5f62 6163 6b65 6e64 da13 5f70 6c6f  ch_backend.._plo
+00001550: 745f 696e 5f6f 6e65 5f66 6967 7572 65da  t_in_one_figure.
+00001560: 165f 706c 6f74 5f69 6e5f 6d75 6c74 695f  ._plot_in_multi_
+00001570: 6669 6775 7265 7329 0672 1100 0000 727e  figures).r....r~
+00001580: 0000 0072 7400 0000 727f 0000 0072 5f00  ...rt...r....r_.
+00001590: 0000 7212 0000 0072 1300 0000 7213 0000  ..r....r....r...
+000015a0: 0072 1400 0000 726b 0000 0090 0000 0073  .r....rk.......s
+000015b0: 0c00 0000 0001 0802 0c01 0a01 0401 0e02  ................
+000015c0: 7a23 506c 6f74 4465 6661 756c 7464 6963  z#PlotDefaultdic
+000015d0: 7432 6669 6775 7265 2e5f 706c 6f74 5f66  t2figure._plot_f
+000015e0: 6967 7572 6563 0300 0000 0000 0000 0000  igurec..........
+000015f0: 0000 0d00 0000 0900 0000 4300 0000 73e2  ..........C...s.
+00001600: 0000 0064 0164 006c 006d 017d 0301 0074  ...d.d.l.m.}...t
+00001610: 027c 0283 0164 026b 0273 1c74 0382 0174  .|...d.k.s.t...t
+00001620: 0483 007d 0464 037d 0574 027c 0183 017c  ...}.d.}.t.|...|
+00001630: 0517 0064 0218 007c 051a 007d 067c 046a  ...d...|...}.|.j
+00001640: 057c 067c 0564 0464 058d 035c 027d 077d  .|.|.d.d...\.}.}
+00001650: 0874 067c 0183 0144 005d 665c 027d 097d  .t.|...D.]f\.}.}
+00001660: 0a7c 0aa0 07a1 0044 005d 445c 027d 0b7d  .|.....D.]D\.}.}
+00001670: 0c7c 0ca0 0864 0664 03a1 027d 0c7c 087c  .|...d.d...}.|.|
+00001680: 0919 006a 097c 0c64 0064 0085 0264 0166  ...j.|.d.d...d.f
+00001690: 0219 007c 0c64 0064 0085 0264 0266 0219  ...|.d.d...d.f..
+000016a0: 0064 077c 0b64 0864 098d 0501 0071 667c  .d.|.d.d.....qf|
+000016b0: 087c 0919 006a 0a64 0a64 0b8d 0101 0071  .|...j.d.d.....q
+000016c0: 567c 046a 0b7c 077c 0264 0119 0064 0064  V|.j.|.|.d...d.d
+000016d0: 0c64 0d8d 0401 007c 03a0 0c7c 07a1 0101  .d.....|...|....
+000016e0: 0064 0053 0029 0e4e 7201 0000 0072 1600  .d.S.).Nr....r..
+000016f0: 0000 7256 0000 0054 2903 7218 0000 0072  ..rV...T).r....r
+00001700: 1900 0000 721c 0000 0072 5500 0000 7257  ....r....rU...rW
+00001710: 0000 0072 5800 0000 7259 0000 0072 5b00  ...rX...rY...r[.
+00001720: 0000 725c 0000 0072 2100 0000 725e 0000  ..r\...r!...r^..
+00001730: 0029 0d72 0c00 0000 720d 0000 0072 3500  .).r....r....r5.
+00001740: 0000 7228 0000 0072 0200 0000 721f 0000  ..r(...r....r...
+00001750: 0072 6300 0000 7280 0000 0072 6400 0000  .rc...r....rd...
+00001760: 7248 0000 0072 4900 0000 722b 0000 0072  rH...rI...r+...r
+00001770: 6500 0000 290d 7211 0000 0072 7e00 0000  e...).r....r~...
+00001780: 7274 0000 0072 1200 0000 7250 0000 0072  rt...r....rP...r
+00001790: 1900 0000 7218 0000 0072 1d00 0000 721e  ....r....r....r.
+000017a0: 0000 0072 3b00 0000 da0b 6c61 6265 6c32  ...r;.....label2
+000017b0: 6461 7461 7372 4000 0000 726a 0000 0072  datasr@...rj...r
+000017c0: 1300 0000 7213 0000 0072 1400 0000 7282  ....r....r....r.
+000017d0: 0000 009a 0000 0073 1c00 0000 0001 0c01  .......s........
+000017e0: 1001 0602 0401 1401 1403 1001 1001 0c01  ................
+000017f0: 3201 1202 1601 0a01 7a2a 506c 6f74 4465  2.......z*PlotDe
+00001800: 6661 756c 7464 6963 7432 6669 6775 7265  faultdict2figure
+00001810: 2e5f 706c 6f74 5f69 6e5f 6f6e 655f 6669  ._plot_in_one_fi
+00001820: 6775 7265 6303 0000 0000 0000 0000 0000  gurec...........
+00001830: 000b 0000 0009 0000 0043 0000 0073 c400  .........C...s..
+00001840: 0000 6401 6400 6c00 6d01 7d03 0100 7402  ..d.d.l.m.}...t.
+00001850: 7c02 8301 7402 7c01 8301 6b02 7320 7403  |...t.|...k.s t.
+00001860: 8201 7404 8300 7d04 7405 7c01 8301 4400  ..t...}.t.|...D.
+00001870: 5d90 5c02 7d05 7d06 7c04 6a06 6402 6402  ].\.}.}.|.j.d.d.
+00001880: 6403 8d02 5c02 7d07 7d08 7c06 a007 a100  d...\.}.}.|.....
+00001890: 4400 5d40 5c02 7d09 7d0a 7c0a a008 6404  D.]@\.}.}.|...d.
+000018a0: 6405 a102 7d0a 7c08 6a09 7c0a 6400 6400  d...}.|.j.|.d.d.
+000018b0: 8502 6401 6602 1900 7c0a 6400 6400 8502  ..d.f...|.d.d...
+000018c0: 6402 6602 1900 6406 7c09 6407 6408 8d05  d.f...d.|.d.d...
+000018d0: 0100 7150 7c08 6a0a 6409 640a 8d01 0100  ..qP|.j.d.d.....
+000018e0: 7c04 6a0b 7c07 7c02 7c05 1900 6400 640b  |.j.|.|.|...d.d.
+000018f0: 640c 8d04 0100 7c03 a00c 7c07 a101 0100  d.....|...|.....
+00001900: 712e 6400 5300 290d 4e72 0100 0000 7216  q.d.S.).Nr....r.
+00001910: 0000 0072 5400 0000 7255 0000 0072 5600  ...rT...rU...rV.
+00001920: 0000 7257 0000 0072 5800 0000 7259 0000  ..rW...rX...rY..
+00001930: 0072 5b00 0000 725c 0000 0072 2100 0000  .r[...r\...r!...
+00001940: 725e 0000 0029 0d72 0c00 0000 720d 0000  r^...).r....r...
+00001950: 0072 3500 0000 7228 0000 0072 0200 0000  .r5...r(...r....
+00001960: 7263 0000 0072 1f00 0000 7280 0000 0072  rc...r....r....r
+00001970: 6400 0000 7248 0000 0072 4900 0000 722b  d...rH...rI...r+
+00001980: 0000 0072 6500 0000 290b 7211 0000 0072  ...re...).r....r
+00001990: 7e00 0000 7274 0000 0072 1200 0000 7250  ~...rt...r....rP
+000019a0: 0000 0072 3b00 0000 7284 0000 0072 1d00  ...r;...r....r..
+000019b0: 0000 721e 0000 0072 4000 0000 726a 0000  ..r....r@...rj..
+000019c0: 0072 1300 0000 7213 0000 0072 1400 0000  .r....r....r....
+000019d0: 7283 0000 00ae 0000 0073 1800 0000 0001  r........s......
+000019e0: 0c01 1401 0601 1001 1202 1001 0c01 2e01  ................
+000019f0: 0c02 1601 0c01 7a2d 506c 6f74 4465 6661  ......z-PlotDefa
+00001a00: 756c 7464 6963 7432 6669 6775 7265 2e5f  ultdict2figure._
+00001a10: 706c 6f74 5f69 6e5f 6d75 6c74 695f 6669  plot_in_multi_fi
+00001a20: 6775 7265 734e 2901 4629 0872 3d00 0000  guresN).F).r=...
+00001a30: 723e 0000 0072 3f00 0000 7276 0000 0072  r>...r?...rv...r
+00001a40: 7500 0000 726b 0000 0072 8200 0000 7283  u...rk...r....r.
+00001a50: 0000 0072 1300 0000 7213 0000 0072 1300  ...r....r....r..
+00001a60: 0000 7214 0000 0072 7b00 0000 8100 0000  ..r....r{.......
+00001a70: 730a 0000 0008 0104 0508 090a 0a08 1472  s..............r
+00001a80: 7b00 0000 6304 0000 0000 0000 0000 0000  {...c...........
+00001a90: 0005 0000 0004 0000 0043 0000 0073 2800  .........C...s(.
+00001aa0: 0000 7400 7c00 7c01 7c02 6603 6401 8d01  ..t.|.|.|.f.d...
+00001ab0: 7d04 7c04 a001 a100 0100 7c03 7224 7c04  }.|.......|.r$|.
+00001ac0: a002 a100 0100 6400 5300 7277 0000 0029  ......d.S.rw...)
+00001ad0: 0372 7b00 0000 7278 0000 0072 4c00 0000  .r{...rx...rL...
+00001ae0: 2905 7281 0000 0072 7400 0000 727f 0000  ).r....rt...r...
+00001af0: 0072 4c00 0000 7279 0000 0072 1300 0000  .rL...ry...r....
+00001b00: 7213 0000 0072 1400 0000 da17 706c 6f74  r....r......plot
+00001b10: 5f64 6566 6175 6c74 6469 6374 3266 6967  _defaultdict2fig
+00001b20: 7572 65bf 0000 0073 0a00 0000 0001 1001  ure....s........
+00001b30: 0802 0401 0801 7285 0000 0029 0146 2901  ......r....).F).
+00001b40: 4629 0146 290f da05 6e75 6d70 7972 7100  F).F)...numpyrq.
+00001b50: 0000 7260 0000 0072 4600 0000 724a 0000  ..r`...rF...rJ..
+00001b60: 00da 0f6d 756c 7469 7072 6f63 6573 7369  ...multiprocessi
+00001b70: 6e67 da06 6f62 6a65 6374 7202 0000 0072  ng..objectr....r
+00001b80: 5200 0000 726b 0000 00da 0750 726f 6365  R...rk.....Proce
+00001b90: 7373 726c 0000 0072 7a00 0000 727b 0000  ssrl...rz...r{..
+00001ba0: 0072 8500 0000 7213 0000 0072 1300 0000  .r....r....r....
+00001bb0: 7213 0000 0072 1400 0000 da08 3c6d 6f64  r....r......<mod
+00001bc0: 756c 653e 0100 0000 7314 0000 0008 0108  ule>....s.......
+00001bd0: 0110 0108 0310 3208 100a 1f12 100a 0912  ......2.........
+00001be0: 3e                                       >
```

### Comparing `tl2-0.1.0/tl2/proj/logger/__pycache__/textlogger.cpython-38.pyc` & `tl2-0.1.1/tl2/proj/logger/__pycache__/textlogger.cpython-38.pyc`

 * *Format-specific differences are supported for Python .pyc files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: Byte-compiled Python module for CPython 3.8, timestamp-based, .py timestamp: Mon Oct  3 09:27:55 2022 UTC, .py size: 5900 bytes*

 * *Could not decompile bytecode: bad marshal data (unknown type code)*

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-00000000: 550d 0d0a 0000 0000 1bab 3a63 0c17 0000  U.........:c....
+00000000: 550d 0d0a 0000 0000 f13c 9362 c317 0000  U........<.b....
 00000010: e300 0000 0000 0000 0000 0000 0000 0000  ................
 00000020: 0006 0000 0040 0000 0073 8e00 0000 6400  .....@...s....d.
 00000030: 6401 6c00 6d01 5a01 0100 6400 6402 6c02  d.l.m.Z...d.d.l.
 00000040: 5a02 6400 6402 6c03 5a03 6400 6402 6c04  Z.d.d.l.Z.d.d.l.
 00000050: 5a04 6400 6402 6c05 5a05 6400 6402 6c06  Z.d.d.l.Z.d.d.l.
 00000060: 5a06 6400 6402 6c07 5a07 6403 6404 6c08  Z.d.d.l.Z.d.d.l.
 00000070: 6d09 5a09 6d0a 5a0a 0100 6405 6406 8400  m.Z.m.Z...d.d...
@@ -21,333 +21,332 @@
 00000140: 6404 6405 8400 7c01 4400 8301 a101 7d02  d.d...|.D.....}.
 00000150: 7c02 5300 2906 4e7a 035f 7c2f 7203 0000  |.S.).Nz._|/r...
 00000160: 00da 0063 0100 0000 0000 0000 0000 0000  ...c............
 00000170: 0200 0000 0400 0000 5300 0000 7314 0000  ........S...s...
 00000180: 0067 007c 005d 0c7d 017c 0164 0019 0091  .g.|.].}.|.d....
 00000190: 0271 0453 0029 0172 0100 0000 a900 2902  .q.S.).r......).
 000001a0: da02 2e30 da01 6b72 0700 0000 7207 0000  ...0..kr....r...
-000001b0: 00fa 572f 686f 6d65 2f6d 612d 7573 6572  ..W/home/ma-user
-000001c0: 2f77 6f72 6b2f 636f 6465 2f73 7479 6c65  /work/code/style
-000001d0: 6761 6e32 2d61 6461 2d70 7974 6f72 6368  gan2-ada-pytorch
-000001e0: 2d65 7870 2f74 6c32 5f6c 6962 2f74 6c32  -exp/tl2_lib/tl2
-000001f0: 2f70 726f 6a2f 6c6f 6767 6572 2f74 6578  /proj/logger/tex
-00000200: 746c 6f67 6765 722e 7079 da0a 3c6c 6973  tlogger.py..<lis
-00000210: 7463 6f6d 703e 1100 0000 7304 0000 0006  tcomp>....s.....
-00000220: 0002 007a 2267 6574 5f70 7265 6669 785f  ...z"get_prefix_
-00000230: 6162 622e 3c6c 6f63 616c 733e 2e3c 6c69  abb.<locals>.<li
-00000240: 7374 636f 6d70 3e29 04da 0272 65da 0573  stcomp>)...re..s
-00000250: 706c 6974 da03 6c65 6eda 046a 6f69 6e29  plit..len..join)
-00000260: 03da 0670 7265 6669 785a 0c70 7265 6669  ...prefixZ.prefi
-00000270: 785f 7370 6c69 74da 0a70 7265 6669 785f  x_split..prefix_
-00000280: 6162 6272 0700 0000 7207 0000 0072 0a00  abbr....r....r..
-00000290: 0000 da0e 6765 745f 7072 6566 6978 5f61  ....get_prefix_a
-000002a0: 6262 0b00 0000 730a 0000 0000 020c 010c  bb....s.........
-000002b0: 0106 0214 0172 1200 0000 6300 0000 0000  .....r....c.....
-000002c0: 0000 0000 0000 0000 0000 0003 0000 0040  ...............@
-000002d0: 0000 0073 5e00 0000 6500 5a01 6400 5a02  ...s^...e.Z.d.Z.
-000002e0: 6401 5a03 641b 6404 6405 8401 5a04 6406  d.Z.d.d.d...Z.d.
-000002f0: 6407 8400 5a05 6408 6409 8400 5a06 640a  d...Z.d.d...Z.d.
-00000300: 640b 8400 5a07 640c 640d 8400 5a08 640e  d...Z.d.d...Z.d.
-00000310: 640f 8400 5a09 641c 6412 6413 8401 5a0a  d...Z.d.d.d...Z.
-00000320: 641d 6416 6417 8401 5a0b 6418 6419 8400  d.d.d...Z.d.d...
-00000330: 5a0c 641a 5300 291e da0a 5465 7874 4c6f  Z.d.S.)...TextLo
-00000340: 6767 6572 7a81 0a20 2023 204c 6f67 7374  ggerz..  # Logst
-00000350: 796c 6520 6973 2065 6974 6865 723a 0a20  yle is either:. 
-00000360: 2023 2027 2523 2e23 6627 2066 6f72 2066   # '%#.#f' for f
-00000370: 6c6f 6174 696e 6720 706f 696e 7420 7265  loating point re
-00000380: 7072 6573 656e 7461 7469 6f6e 2069 6e20  presentation in 
-00000390: 7465 7874 0a20 2023 2027 2523 2e23 6527  text.  # '%#.#e'
-000003a0: 2066 6f72 2065 7870 6f6e 656e 7420 7265   for exponent re
-000003b0: 7072 6573 656e 7461 7469 6f6e 2069 6e20  presentation in 
-000003c0: 7465 7874 0a20 2046 fa06 2531 302e 3666  text.  F..%10.6f
-000003d0: 6304 0000 0000 0000 0000 0000 0004 0000  c...............
-000003e0: 0003 0000 0043 0000 0073 3a00 0000 7c01  .....C...s:...|.
-000003f0: 7c00 5f00 7c01 7224 7401 6a02 a003 7c00  |._.|.r$t.j...|.
-00000400: 6a00 a101 7324 7401 a004 7c00 6a00 a101  j...s$t...|.j...
-00000410: 0100 7c02 7c00 5f05 6700 7c00 5f06 7c03  ..|.|._.g.|._.|.
-00000420: 7c00 5f07 6400 5300 a901 4e29 08da 0472  |._.d.S...N)...r
-00000430: 6f6f 74da 026f 73da 0470 6174 68da 0665  oot..os..path..e
-00000440: 7869 7374 73da 086d 616b 6564 6972 73da  xists..makedirs.
-00000450: 0c72 6569 6e69 7469 616c 697a 65da 076d  .reinitialize..m
-00000460: 6574 7269 6373 da08 6c6f 6773 7479 6c65  etrics..logstyle
-00000470: 2904 da04 7365 6c66 da08 6c6f 675f 726f  )...self..log_ro
-00000480: 6f74 721b 0000 0072 1d00 0000 7207 0000  otr....r....r...
-00000490: 0072 0700 0000 720a 0000 00da 085f 5f69  .r....r......__i
-000004a0: 6e69 745f 5f1b 0000 0073 0e00 0000 0001  nit__....s......
-000004b0: 0601 1201 0c01 0601 0602 0601 7a13 5465  ............z.Te
-000004c0: 7874 4c6f 6767 6572 2e5f 5f69 6e69 745f  xtLogger.__init_
-000004d0: 5f63 0200 0000 0000 0000 0000 0000 0200  _c..............
-000004e0: 0000 0200 0000 4300 0000 731c 0000 007c  ......C...s....|
-000004f0: 016a 007c 005f 007c 016a 017c 005f 017c  .j.|._.|.j.|._.|
-00000500: 016a 027c 005f 0264 0053 0072 1500 0000  .j.|._.d.S.r....
-00000510: 2903 7216 0000 0072 1b00 0000 721d 0000  ).r....r....r...
-00000520: 0029 0272 1e00 0000 da0a 7465 7874 6c6f  .).r......textlo
-00000530: 6767 6572 7207 0000 0072 0700 0000 720a  ggerr....r....r.
-00000540: 0000 00da 0675 7064 6174 6525 0000 0073  .....update%...s
-00000550: 0800 0000 0001 0801 0801 0801 7a11 5465  ............z.Te
-00000560: 7874 4c6f 6767 6572 2e75 7064 6174 6563  xtLogger.updatec
-00000570: 0200 0000 0000 0000 0000 0000 0200 0000  ................
-00000580: 0500 0000 4300 0000 732e 0000 0074 006a  ....C...s....t.j
-00000590: 01a0 0264 017c 006a 037c 0166 0216 00a1  ...d.|.j.|.f....
-000005a0: 0172 2a74 00a0 0464 017c 006a 037c 0166  .r*t...d.|.j.|.f
-000005b0: 0216 00a1 0101 0064 0253 0029 037a 3c0a  .......d.S.).z<.
-000005c0: 2020 2020 2020 4465 6c65 7465 206c 6f67        Delete log
-000005d0: 2069 6620 7265 2d73 7461 7274 696e 6720   if re-starting 
-000005e0: 616e 6420 6c6f 6720 616c 7265 6164 7920  and log already 
-000005f0: 6578 6973 7473 0a20 2020 20fa 0925 732f  exists.    ..%s/
-00000600: 2573 2e6c 6f67 4e29 0572 1700 0000 7218  %s.logN).r....r.
-00000610: 0000 0072 1900 0000 7216 0000 00da 0672  ...r....r......r
-00000620: 656d 6f76 6529 0272 1e00 0000 da04 6974  emove).r......it
-00000630: 656d 7207 0000 0072 0700 0000 720a 0000  emr....r....r...
-00000640: 00da 0672 6569 6e69 742b 0000 0073 0400  ...reinit+...s..
-00000650: 0000 0004 1601 7a11 5465 7874 4c6f 6767  ......z.TextLogg
-00000660: 6572 2e72 6569 6e69 7463 0200 0000 0000  er.reinitc......
-00000670: 0000 0000 0000 0600 0000 0a00 0000 4b00  ..............K.
-00000680: 0000 739e 0000 0074 006a 017c 006a 0264  ..s....t.j.|.j.d
-00000690: 0164 028d 0201 007c 0244 005d 847d 0364  .d.....|.D.].}.d
-000006a0: 037c 006a 027c 0366 0216 007d 047c 037c  .|.j.|.f...}.|.|
-000006b0: 006a 036b 0772 6674 006a 0174 006a 04a0  .j.k.rft.j.t.j..
-000006c0: 057c 04a1 0164 0164 028d 0201 007c 006a  .|...d.d.....|.j
-000006d0: 0672 567c 00a0 077c 03a1 0101 007c 0004  .rV|...|.....|..
-000006e0: 006a 037c 0367 0137 0002 005f 0374 087c  .j.|.g.7..._.t.|
-000006f0: 0464 0483 028f 227d 057c 05a0 0964 057c  .d...."}.|...d.|
-00000700: 017c 006a 0a7c 027c 0319 0016 0066 0216  .|.j.|.|.....f..
-00000710: 00a1 0101 0057 0035 0051 0052 0058 0071  .....W.5.Q.R.X.q
-00000720: 1464 0653 0029 077a 1b0a 2020 2020 4c6f  .d.S.).z..    Lo
-00000730: 6720 696e 2070 6c61 696e 7465 7874 3b0a  g in plaintext;.
-00000740: 2020 2020 5429 01da 0865 7869 7374 5f6f      T)...exist_o
-00000750: 6b72 2300 0000 da01 617a 0a25 332e 3466  kr#.....az.%3.4f
-00000760: 3a20 2573 0a4e 290b 7217 0000 0072 1a00  : %s.N).r....r..
-00000770: 0000 7216 0000 0072 1c00 0000 7218 0000  ..r....r....r...
-00000780: 00da 0764 6972 6e61 6d65 721b 0000 0072  ...dirnamer....r
-00000790: 2600 0000 da04 6f70 656e da05 7772 6974  &.....open..writ
-000007a0: 6572 1d00 0000 2906 721e 0000 00da 0369  er....).r......i
-000007b0: 7472 da06 6b77 6172 6773 da03 6172 67da  tr..kwargs..arg.
-000007c0: 0966 696c 655f 7061 7468 da01 6672 0700  .file_path..fr..
-000007d0: 0000 7207 0000 0072 0a00 0000 da03 6c6f  ..r....r......lo
-000007e0: 6733 0000 0073 1400 0000 0004 1001 0801  g3...s..........
-000007f0: 0e01 0a01 1601 0601 0a01 1001 0c01 7a0e  ..............z.
-00000800: 5465 7874 4c6f 6767 6572 2e6c 6f67 6301  TextLogger.logc.
-00000810: 0000 0000 0000 0000 0000 0006 0000 0006  ................
-00000820: 0000 004b 0000 0073 4a00 0000 6700 7d02  ...K...sJ...g.}.
-00000830: 6700 7d03 7c01 4400 5d26 7d04 6401 7c00  g.}.|.D.]&}.d.|.
-00000840: 6a00 7c04 6602 1600 7d05 7c02 a001 7c04  j.|.f...}.|...|.
-00000850: a101 0100 7c03 a001 7c05 a101 0100 710c  ....|...|.....q.
-00000860: 7402 7c02 7c03 7c00 6a00 6402 6403 8d04  t.|.|.|.j.d.d...
-00000870: 0100 6400 5300 2904 4e72 2300 0000 46a9  ..d.S.).Nr#...F.
-00000880: 04da 056e 616d 6573 da09 6669 6c65 7061  ...names..filepa
-00000890: 7468 73da 066f 7574 6469 725a 0b69 6e5f  ths..outdirZ.in_
-000008a0: 6f6e 655f 6178 6573 a903 7216 0000 00da  one_axes..r.....
-000008b0: 0661 7070 656e 6472 0400 0000 a906 721e  .appendr......r.
-000008c0: 0000 0072 2d00 0000 7233 0000 0072 3400  ...r-...r3...r4.
-000008d0: 0000 722e 0000 00da 0866 696c 656e 616d  ..r......filenam
-000008e0: 6572 0700 0000 7207 0000 0072 0a00 0000  er....r....r....
-000008f0: da08 6c6f 675f 6178 6573 4200 0000 7314  ..log_axesB...s.
-00000900: 0000 0000 0104 0104 0108 010e 010a 010c  ................
-00000910: 0206 0104 0002 ff7a 1354 6578 744c 6f67  .......z.TextLog
-00000920: 6765 722e 6c6f 675f 6178 6573 6301 0000  ger.log_axesc...
-00000930: 0000 0000 0000 0000 0006 0000 0006 0000  ................
-00000940: 004b 0000 0073 4a00 0000 6700 7d02 6700  .K...sJ...g.}.g.
-00000950: 7d03 7c01 4400 5d26 7d04 6401 7c00 6a00  }.|.D.]&}.d.|.j.
-00000960: 7c04 6602 1600 7d05 7c02 a001 7c04 a101  |.f...}.|...|...
-00000970: 0100 7c03 a001 7c05 a101 0100 710c 7402  ..|...|.....q.t.
-00000980: 7c02 7c03 7c00 6a00 6402 6403 8d04 0100  |.|.|.j.d.d.....
-00000990: 6400 5300 2904 4e72 2300 0000 5472 3200  d.S.).Nr#...Tr2.
-000009a0: 0000 7236 0000 0072 3800 0000 7207 0000  ..r6...r8...r...
-000009b0: 0072 0700 0000 720a 0000 00da 066c 6f67  .r....r......log
-000009c0: 5f61 784d 0000 0073 1400 0000 0001 0401  _axM...s........
-000009d0: 0401 0801 0e01 0a01 0c02 0601 0400 02ff  ................
-000009e0: 7a11 5465 7874 4c6f 6767 6572 2e6c 6f67  z.TextLogger.log
-000009f0: 5f61 78e9 6400 0000 fa04 2e70 6e67 6305  _ax.d......pngc.
-00000a00: 0000 0000 0000 0000 0000 000a 0000 0006  ................
-00000a10: 0000 0043 0000 0073 a200 0000 6700 7d05  ...C...s....g.}.
-00000a20: 7c02 7258 6700 7d06 7c01 4400 5d10 7d07  |.rXg.}.|.D.].}.
-00000a30: 7c06 7c07 a000 a100 3700 7d06 7110 6401  |.|.....7.}.q.d.
-00000a40: a001 7c06 a101 6400 7c03 8502 1900 7d08  ..|...d.|.....}.
-00000a50: 7402 6a03 a001 7c00 6a04 6402 7c08 1700  t.j...|.j.d.|...
-00000a60: 7c04 1700 a102 7d09 7c05 a005 7c09 a101  |.....}.|...|...
-00000a70: 0100 6e46 7c01 4400 5d40 7d07 7c07 a000  ..nF|.D.]@}.|...
-00000a80: a100 7d06 6401 a001 7c06 a101 6400 7c03  ..}.d...|...d.|.
-00000a90: 8502 1900 7d08 7402 6a03 a001 7c00 6a04  ....}.t.j...|.j.
-00000aa0: 6402 7c08 1700 7c04 1700 a102 7d09 7c05  d.|...|.....}.|.
-00000ab0: a005 7c09 a101 0100 715c 7c05 5300 2903  ..|.....q\|.S.).
-00000ac0: 4eda 025f 5f5a 0730 706c 6f74 5f5f 2906  N..__Z.0plot__).
-00000ad0: da04 6b65 7973 720f 0000 0072 1700 0000  ..keysr....r....
-00000ae0: 7218 0000 0072 1600 0000 7237 0000 0029  r....r....r7...)
-00000af0: 0a72 1e00 0000 da09 6469 6374 5f6c 6973  .r......dict_lis
-00000b00: 74da 0d69 6e5f 6f6e 655f 6669 6775 7265  t..in_one_figure
-00000b10: 5a06 4d41 584c 454e da03 6578 7472 3400  Z.MAXLEN..extr4.
-00000b20: 0000 da06 6c61 6265 6c73 da01 6472 3900  ....labels..dr9.
-00000b30: 0000 da08 6669 6c65 7061 7468 7207 0000  ....filepathr...
-00000b40: 0072 0700 0000 720a 0000 00da 1b5f 6765  .r....r......_ge
-00000b50: 745f 6669 6c65 7061 7468 5f66 726f 6d5f  t_filepath_from_
-00000b60: 6469 6374 6c69 7374 5800 0000 731c 0000  dictlistX...s...
-00000b70: 0000 0104 0104 0104 0108 010e 0112 0118  ................
-00000b80: 010c 0208 0108 0112 0118 010c 017a 2654  .............z&T
-00000b90: 6578 744c 6f67 6765 722e 5f67 6574 5f66  extLogger._get_f
-00000ba0: 696c 6570 6174 685f 6672 6f6d 5f64 6963  ilepath_from_dic
-00000bb0: 746c 6973 7454 e92c 0100 0063 0400 0000  tlistT.,...c....
-00000bc0: 0000 0000 0000 0000 0b00 0000 0600 0000  ................
-00000bd0: 0300 0000 7388 0000 0067 007d 047c 01a0  ....s....g.}.|..
-00000be0: 00a1 0044 005d 245c 027d 057d 067c 04a0  ...D.]$\.}.}.|..
-00000bf0: 0187 0066 0164 0164 0284 087c 06a0 00a1  ...f.d.d...|....
-00000c00: 0044 0083 01a1 0101 0071 0c88 006a 027c  .D.......q...j.|
-00000c10: 047c 0264 038d 027d 0764 04a0 037c 07a1  .|.d...}.d...|..
-00000c20: 017d 0874 04a0 04a1 007d 0974 0574 067c  .}.t.....}.t.t.|
-00000c30: 0864 0583 037d 0a7c 097c 0a18 007c 036b  .d...}.|.|...|.k
-00000c40: 0472 8474 077c 047c 077c 0264 068d 0301  .r.t.|.|.|.d....
-00000c50: 0074 0874 067c 087c 0983 0301 0064 0053  .t.t.|.|.....d.S
-00000c60: 0029 074e 6301 0000 0000 0000 0000 0000  .).Nc...........
-00000c70: 0003 0000 0006 0000 0013 0000 0073 2000  .............s .
-00000c80: 0000 6900 7c00 5d18 5c02 7d01 7d02 7c01  ..i.|.].\.}.}.|.
-00000c90: 6400 8800 6a00 7c01 6602 1600 9302 7104  d...j.|.f.....q.
-00000ca0: 5300 2901 7223 0000 0029 0172 1600 0000  S.).r#...).r....
-00000cb0: 2903 7208 0000 00da 0473 7562 6bda 015f  ).r......subk.._
-00000cc0: a901 721e 0000 0072 0700 0000 720a 0000  ..r....r....r...
-00000cd0: 00da 0a3c 6469 6374 636f 6d70 3e6c 0000  ...<dictcomp>l..
-00000ce0: 0073 0600 0000 0600 0600 0200 7a35 5465  .s..........z5Te
-00000cf0: 7874 4c6f 6767 6572 2e6c 6f67 5f64 6566  xtLogger.log_def
-00000d00: 6175 6c74 6469 6374 3266 6967 7572 652e  aultdict2figure.
-00000d10: 3c6c 6f63 616c 733e 2e3c 6469 6374 636f  <locals>.<dictco
-00000d20: 6d70 3e29 0272 4000 0000 7241 0000 0072  mp>).r@...rA...r
-00000d30: 4900 0000 7201 0000 0029 03da 146c 6162  I...r....)...lab
-00000d40: 656c 3266 696c 6570 6174 6873 5f6c 6973  el2filepaths_lis
-00000d50: 7472 3400 0000 7241 0000 0029 09da 0569  tr4...rA...)...i
-00000d60: 7465 6d73 7237 0000 0072 4600 0000 720f  temsr7...rF...r.
-00000d70: 0000 00da 0474 696d 65da 0767 6574 6174  .....time..getat
-00000d80: 7472 7213 0000 0072 0500 0000 da07 7365  trr....r......se
-00000d90: 7461 7474 7229 0b72 1e00 0000 da0c 6465  tattr).r......de
-00000da0: 6661 756c 745f 6469 6374 7241 0000 00da  fault_dictrA....
-00000db0: 0c73 6176 655f 6669 675f 7365 6372 4c00  .save_fig_secrL.
-00000dc0: 0000 7249 0000 00da 0176 7234 0000 00da  ..rI.....vr4....
-00000dd0: 0874 696d 655f 7374 72da 036e 6f77 5a09  .time_str..nowZ.
-00000de0: 6c61 7374 5f74 696d 6572 0700 0000 724a  last_timer....rJ
-00000df0: 0000 0072 0a00 0000 da16 6c6f 675f 6465  ...r......log_de
-00000e00: 6661 756c 7464 6963 7432 6669 6775 7265  faultdict2figure
-00000e10: 6900 0000 7318 0000 0000 0104 0110 011e  i...s...........
-00000e20: 010e 020a 0108 010c 010c 0106 0102 ff06  ................
-00000e30: 027a 2154 6578 744c 6f67 6765 722e 6c6f  .z!TextLogger.lo
-00000e40: 675f 6465 6661 756c 7464 6963 7432 6669  g_defaultdict2fi
-00000e50: 6775 7265 6302 0000 0000 0000 0000 0000  gurec...........
-00000e60: 0005 0000 000a 0000 004b 0000 0073 6e00  .........K...sn.
-00000e70: 0000 7c02 4400 5d64 7d03 7c03 7c00 6a00  ..|.D.]d}.|.|.j.
-00000e80: 6b07 7232 7c00 6a01 7222 7c00 a002 7c03  k.r2|.j.r"|...|.
-00000e90: a101 0100 7c00 0400 6a00 7c03 6701 3700  ....|...j.|.g.7.
-00000ea0: 0200 5f00 7403 6401 7c00 6a04 7c03 6602  .._.t.d.|.j.|.f.
-00000eb0: 1600 6402 8302 8f1c 7d04 7c04 a005 6403  ..d.....}.|...d.
-00000ec0: 7c01 7c02 7c03 1900 6602 1600 a101 0100  |.|.|...f.......
-00000ed0: 5700 3500 5100 5200 5800 7104 6400 5300  W.5.Q.R.X.q.d.S.
-00000ee0: 2904 4e72 2300 0000 7228 0000 007a 0825  ).Nr#...r(...z.%
-00000ef0: 3364 3a20 2573 0a29 0672 1c00 0000 721b  3d: %s.).r....r.
-00000f00: 0000 0072 2600 0000 722a 0000 0072 1600  ...r&...r*...r..
-00000f10: 0000 722b 0000 0029 0572 1e00 0000 722c  ..r+...).r....r,
-00000f20: 0000 0072 2d00 0000 722e 0000 0072 3000  ...r-...r....r0.
-00000f30: 0000 7207 0000 0072 0700 0000 720a 0000  ..r....r....r...
-00000f40: 00da 066c 6f67 7374 7278 0000 0073 0e00  ...logstrx...s..
-00000f50: 0000 0003 0801 0a01 0601 0a01 1001 1601  ................
-00000f60: 7a11 5465 7874 4c6f 6767 6572 2e6c 6f67  z.TextLogger.log
-00000f70: 7374 724e 2902 4672 1400 0000 2902 723c  strN).Fr....).r<
-00000f80: 0000 0072 3d00 0000 2902 5472 4700 0000  ...r=...).TrG...
-00000f90: 290d da08 5f5f 6e61 6d65 5f5f da0a 5f5f  )...__name__..__
-00000fa0: 6d6f 6475 6c65 5f5f da0c 5f5f 7175 616c  module__..__qual
-00000fb0: 6e61 6d65 5f5f da07 5f5f 646f 635f 5f72  name__..__doc__r
-00000fc0: 2000 0000 7222 0000 0072 2600 0000 7231   ...r"...r&...r1
-00000fd0: 0000 0072 3a00 0000 723b 0000 0072 4600  ...r:...r;...rF.
-00000fe0: 0000 7256 0000 0072 5700 0000 7207 0000  ..rV...rW...r...
-00000ff0: 0072 0700 0000 7207 0000 0072 0a00 0000  .r....r....r....
-00001000: 7213 0000 0015 0000 0073 1400 0000 0801  r........s......
-00001010: 0405 0a0a 0806 0808 080f 080b 080b 0a11  ................
-00001020: 0a0f 7213 0000 0054 723c 0000 0063 0900  ..r....Tr<...c..
-00001030: 0000 0000 0000 0000 0000 0b00 0000 0500  ................
-00001040: 0000 0300 0000 739a 0000 007c 0873 0864  ......s....|.s.d
-00001050: 0053 007c 0364 006b 0972 8e74 007c 0164  .S.|.d.k.r.t.|.d
-00001060: 018d 0189 0174 0174 0283 017d 097c 00a0  .....t.t...}.|..
-00001070: 03a1 0044 005d 245c 0289 007d 0a87 0087  ...D.]$\...}....
-00001080: 0166 0264 0264 0384 087c 0aa0 03a1 0044  .f.d.d...|.....D
-00001090: 0083 017c 0988 003c 0071 2a7c 097d 007c  ...|...<.q*|.}.|
-000010a0: 0572 787c 00a0 03a1 0044 005d 165c 0289  .rx|.....D.].\..
-000010b0: 007d 0a7c 036a 047c 0266 017c 0a8e 0101  .}.|.j.|.f.|....
-000010c0: 0071 607c 0672 967c 036a 057c 007c 047c  .q`|.r.|.j.|.|.|
-000010d0: 0764 048d 0301 006e 0874 0664 0583 0101  .d.....n.t.d....
-000010e0: 0064 0053 0029 064e 2901 7210 0000 0063  .d.S.).N).r....c
-000010f0: 0100 0000 0000 0000 0000 0000 0300 0000  ................
-00001100: 0400 0000 1300 0000 7326 0000 0069 007c  ........s&...i.|
-00001110: 005d 1e5c 027d 017d 0288 0164 0017 0088  .].\.}.}...d....
-00001120: 0017 0064 0017 007c 0117 007c 0293 0271  ...d...|...|...q
-00001130: 0453 0029 01da 012e 7207 0000 0029 0372  .S.)....r....).r
-00001140: 0800 0000 7248 0000 005a 0473 7562 76a9  ....rH...Z.subv.
-00001150: 0272 0900 0000 7211 0000 0072 0700 0000  .r....r....r....
-00001160: 720a 0000 0072 4b00 0000 8e00 0000 7306  r....rK.......s.
-00001170: 0000 0006 0006 0012 007a 2e73 756d 6d61  .........z.summa
-00001180: 7279 5f64 6566 6175 6c74 6469 6374 3274  ry_defaultdict2t
-00001190: 7874 6669 672e 3c6c 6f63 616c 733e 2e3c  xtfig.<locals>.<
-000011a0: 6469 6374 636f 6d70 3e29 0272 4100 0000  dictcomp>).rA...
-000011b0: 7252 0000 007a 1474 6578 746c 6f67 6765  rR...z.textlogge
-000011c0: 7220 6172 6520 4e6f 6e65 2129 0772 1200  r are None!).r..
-000011d0: 0000 7202 0000 00da 0464 6963 7472 4d00  ..r......dictrM.
-000011e0: 0000 7231 0000 0072 5600 0000 da05 7072  ..r1...rV.....pr
-000011f0: 696e 7429 0b72 5100 0000 7210 0000 00da  int).rQ...r.....
-00001200: 0473 7465 7072 2100 0000 7241 0000 00da  .stepr!...rA....
-00001210: 076c 6f67 5f74 7874 da07 6c6f 675f 6669  .log_txt..log_fi
-00001220: 6772 5200 0000 da0f 6973 5f6d 6169 6e5f  grR.....is_main_
-00001230: 7072 6f63 6573 735a 1164 6566 6175 6c74  processZ.default
-00001240: 5f64 6963 745f 636f 7079 7253 0000 0072  _dict_copyrS...r
-00001250: 0700 0000 725d 0000 0072 0a00 0000 da1a  ....r]...r......
-00001260: 7375 6d6d 6172 795f 6465 6661 756c 7464  summary_defaultd
-00001270: 6963 7432 7478 7466 6967 8400 0000 731c  ict2txtfig....s.
-00001280: 0000 0000 0304 0104 0108 010a 0108 0210  ................
-00001290: 011e 0104 0204 0110 0110 0104 0112 0272  ...............r
-000012a0: 6400 0000 4663 0900 0000 0000 0000 0000  d...Fc..........
-000012b0: 0000 1000 0000 0a00 0000 4300 0000 73a2  ..........C...s.
-000012c0: 0000 007c 0873 0864 0053 0069 007d 097c  ...|.s.d.S.i.}.|
-000012d0: 00a0 00a1 0044 005d 1c5c 027d 0a7d 0b7c  .....D.].\.}.}.|
-000012e0: 0aa0 0164 0164 02a1 027d 0c7c 0b7c 097c  ...d.d...}.|.|.|
-000012f0: 0c3c 0071 147c 097d 007c 0472 5074 0274  .<.q.|.}.|.rPt.t
-00001300: 0383 017d 0d64 037d 0e7c 007c 0d7c 0e3c  ...}.d.}.|.|.|.<
-00001310: 006e 3674 0274 0383 017d 0d64 047d 0e74  .n6t.t...}.d.}.t
-00001320: 047c 00a0 00a1 0083 0144 005d 1c5c 027d  .|.......D.].\.}
-00001330: 0f5c 027d 0a7d 0b7c 0a7c 0b69 017c 0d7c  .\.}.}.|.|.i.|.|
-00001340: 0e7c 0f16 003c 0071 6874 057c 0d7c 017c  .|...<.qht.|.|.|
-00001350: 027c 0364 057c 057c 067c 0764 068d 0801  .|.d.|.|.|.d....
-00001360: 0064 0053 0029 074e fa01 2f7a 022d 2dda  .d.S.).N../z.--.
-00001370: 0273 617a 046d 6125 6454 2908 7251 0000  .saz.ma%dT).rQ..
-00001380: 0072 1000 0000 7260 0000 0072 2100 0000  .r....r`...r!...
-00001390: 7241 0000 0072 6100 0000 7262 0000 0072  rA...ra...rb...r
-000013a0: 5200 0000 2906 724d 0000 00da 0772 6570  R...).rM.....rep
-000013b0: 6c61 6365 7202 0000 0072 5e00 0000 da09  lacer....r^.....
-000013c0: 656e 756d 6572 6174 6572 6400 0000 2910  enumeraterd...).
-000013d0: 5a09 6469 6374 5f64 6174 6172 1000 0000  Z.dict_datar....
-000013e0: 7260 0000 0072 2100 0000 5a0a 696e 5f6f  r`...r!...Z.in_o
-000013f0: 6e65 5f61 7865 7261 0000 0072 6200 0000  ne_axera...rb...
-00001400: 7252 0000 0072 6300 0000 5a11 6e65 775f  rR...rc...Z.new_
-00001410: 6b65 795f 6469 6374 5f64 6174 6172 0900  key_dict_datar..
-00001420: 0000 7253 0000 005a 056e 6577 5f6b 7251  ..rS...Z.new_krQ
-00001430: 0000 0072 3f00 0000 da01 6972 0700 0000  ...r?.....ir....
-00001440: 7207 0000 0072 0a00 0000 da13 7375 6d6d  r....r......summ
-00001450: 6172 795f 6469 6374 3274 7874 6669 679a  ary_dict2txtfig.
-00001460: 0000 0073 2c00 0000 0003 0401 0401 0401  ...s,...........
-00001470: 1001 0c01 0a01 0402 0401 0801 0401 0a02  ................
-00001480: 0801 0401 1801 1201 0801 0200 0201 0200  ................
-00001490: 0200 02fe 726a 0000 0029 0172 1f00 0000  ....rj...).r....
-000014a0: 6301 0000 0000 0000 0000 0000 0001 0000  c...............
-000014b0: 0003 0000 0043 0000 0073 0e00 0000 7400  .....C...s....t.
-000014c0: a001 7c00 a101 0100 6400 5300 7215 0000  ..|.....d.S.r...
-000014d0: 0029 02da 1167 6c6f 6261 6c5f 7465 7874  .)...global_text
-000014e0: 6c6f 6767 6572 7222 0000 0029 0172 2100  loggerr"...).r!.
-000014f0: 0000 7207 0000 0072 0700 0000 720a 0000  ..r....r....r...
-00001500: 00da 1573 6574 5f67 6c6f 6261 6c5f 7465  ...set_global_te
-00001510: 7874 6c6f 6767 6572 b500 0000 7304 0000  xtlogger....s...
-00001520: 0000 020a 0172 6c00 0000 2906 4e54 5454  .....rl...).NTTT
-00001530: 723c 0000 0054 2906 4e46 5454 723c 0000  r<...T).NFTTr<..
-00001540: 0054 2912 da0b 636f 6c6c 6563 7469 6f6e  .T)...collection
-00001550: 7372 0200 0000 724e 0000 0072 0c00 0000  sr....rN...r....
-00001560: da07 6c6f 6767 696e 6772 1700 0000 da08  ..loggingr......
-00001570: 6461 7465 7469 6d65 da03 7379 735a 0a70  datetime..sysZ.p
-00001580: 6c6f 745f 7574 696c 7372 0400 0000 7205  lot_utilsr....r.
-00001590: 0000 0072 1200 0000 da06 6f62 6a65 6374  ...r......object
-000015a0: 7213 0000 0072 6400 0000 726a 0000 0072  r....rd...rj...r
-000015b0: 6b00 0000 726c 0000 0072 0700 0000 7207  k...rl...r....r.
-000015c0: 0000 0072 0700 0000 720a 0000 00da 083c  ...r....r......<
-000015d0: 6d6f 6475 6c65 3e01 0000 0073 3000 0000  module>....s0...
-000015e0: 0c01 0801 0801 1001 0801 0802 1003 080a  ................
-000015f0: 1070 0000 0001 0000 0000 0000 00fe 0a17  .p..............
-00001600: 0000 0001 0000 0000 0000 00fe 0a19 0a02  ................
+000001b0: 00fa 392f 686f 6d65 2f6d 612d 7573 6572  ..9/home/ma-user
+000001c0: 2f77 6f72 6b2f 636f 6465 2f74 6c32 2f74  /work/code/tl2/t
+000001d0: 6c32 2f70 726f 6a2f 6c6f 6767 6572 2f74  l2/proj/logger/t
+000001e0: 6578 746c 6f67 6765 722e 7079 da0a 3c6c  extlogger.py..<l
+000001f0: 6973 7463 6f6d 703e 1100 0000 7304 0000  istcomp>....s...
+00000200: 0006 0002 007a 2267 6574 5f70 7265 6669  .....z"get_prefi
+00000210: 785f 6162 622e 3c6c 6f63 616c 733e 2e3c  x_abb.<locals>.<
+00000220: 6c69 7374 636f 6d70 3e29 04da 0272 65da  listcomp>)...re.
+00000230: 0573 706c 6974 da03 6c65 6eda 046a 6f69  .split..len..joi
+00000240: 6e29 03da 0670 7265 6669 785a 0c70 7265  n)...prefixZ.pre
+00000250: 6669 785f 7370 6c69 74da 0a70 7265 6669  fix_split..prefi
+00000260: 785f 6162 6272 0700 0000 7207 0000 0072  x_abbr....r....r
+00000270: 0a00 0000 da0e 6765 745f 7072 6566 6978  ......get_prefix
+00000280: 5f61 6262 0b00 0000 730a 0000 0000 020c  _abb....s.......
+00000290: 010c 0106 0214 0172 1200 0000 6300 0000  .......r....c...
+000002a0: 0000 0000 0000 0000 0000 0000 0003 0000  ................
+000002b0: 0040 0000 0073 5e00 0000 6500 5a01 6400  .@...s^...e.Z.d.
+000002c0: 5a02 6401 5a03 641b 6404 6405 8401 5a04  Z.d.Z.d.d.d...Z.
+000002d0: 6406 6407 8400 5a05 6408 6409 8400 5a06  d.d...Z.d.d...Z.
+000002e0: 640a 640b 8400 5a07 640c 640d 8400 5a08  d.d...Z.d.d...Z.
+000002f0: 640e 640f 8400 5a09 641c 6412 6413 8401  d.d...Z.d.d.d...
+00000300: 5a0a 641d 6416 6417 8401 5a0b 6418 6419  Z.d.d.d...Z.d.d.
+00000310: 8400 5a0c 641a 5300 291e da0a 5465 7874  ..Z.d.S.)...Text
+00000320: 4c6f 6767 6572 7a81 0a20 2023 204c 6f67  Loggerz..  # Log
+00000330: 7374 796c 6520 6973 2065 6974 6865 723a  style is either:
+00000340: 0a20 2023 2027 2523 2e23 6627 2066 6f72  .  # '%#.#f' for
+00000350: 2066 6c6f 6174 696e 6720 706f 696e 7420   floating point 
+00000360: 7265 7072 6573 656e 7461 7469 6f6e 2069  representation i
+00000370: 6e20 7465 7874 0a20 2023 2027 2523 2e23  n text.  # '%#.#
+00000380: 6527 2066 6f72 2065 7870 6f6e 656e 7420  e' for exponent 
+00000390: 7265 7072 6573 656e 7461 7469 6f6e 2069  representation i
+000003a0: 6e20 7465 7874 0a20 2046 fa06 2531 302e  n text.  F..%10.
+000003b0: 3666 6304 0000 0000 0000 0000 0000 0004  6fc.............
+000003c0: 0000 0003 0000 0043 0000 0073 3a00 0000  .......C...s:...
+000003d0: 7c01 7c00 5f00 7c01 7224 7401 6a02 a003  |.|._.|.r$t.j...
+000003e0: 7c00 6a00 a101 7324 7401 a004 7c00 6a00  |.j...s$t...|.j.
+000003f0: a101 0100 7c02 7c00 5f05 6700 7c00 5f06  ....|.|._.g.|._.
+00000400: 7c03 7c00 5f07 6400 5300 a901 4e29 08da  |.|._.d.S...N)..
+00000410: 0472 6f6f 74da 026f 73da 0470 6174 68da  .root..os..path.
+00000420: 0665 7869 7374 73da 086d 616b 6564 6972  .exists..makedir
+00000430: 73da 0c72 6569 6e69 7469 616c 697a 65da  s..reinitialize.
+00000440: 076d 6574 7269 6373 da08 6c6f 6773 7479  .metrics..logsty
+00000450: 6c65 2904 da04 7365 6c66 da08 6c6f 675f  le)...self..log_
+00000460: 726f 6f74 721b 0000 0072 1d00 0000 7207  rootr....r....r.
+00000470: 0000 0072 0700 0000 720a 0000 00da 085f  ...r....r......_
+00000480: 5f69 6e69 745f 5f1b 0000 0073 0e00 0000  _init__....s....
+00000490: 0001 0601 1201 0c01 0601 0602 0601 7a13  ..............z.
+000004a0: 5465 7874 4c6f 6767 6572 2e5f 5f69 6e69  TextLogger.__ini
+000004b0: 745f 5f63 0200 0000 0000 0000 0000 0000  t__c............
+000004c0: 0200 0000 0200 0000 4300 0000 731c 0000  ........C...s...
+000004d0: 007c 016a 007c 005f 007c 016a 017c 005f  .|.j.|._.|.j.|._
+000004e0: 017c 016a 027c 005f 0264 0053 0072 1500  .|.j.|._.d.S.r..
+000004f0: 0000 2903 7216 0000 0072 1b00 0000 721d  ..).r....r....r.
+00000500: 0000 0029 0272 1e00 0000 da0a 7465 7874  ...).r......text
+00000510: 6c6f 6767 6572 7207 0000 0072 0700 0000  loggerr....r....
+00000520: 720a 0000 00da 0675 7064 6174 6525 0000  r......update%..
+00000530: 0073 0800 0000 0001 0801 0801 0801 7a11  .s............z.
+00000540: 5465 7874 4c6f 6767 6572 2e75 7064 6174  TextLogger.updat
+00000550: 6563 0200 0000 0000 0000 0000 0000 0200  ec..............
+00000560: 0000 0500 0000 4300 0000 732e 0000 0074  ......C...s....t
+00000570: 006a 01a0 0264 017c 006a 037c 0166 0216  .j...d.|.j.|.f..
+00000580: 00a1 0172 2a74 00a0 0464 017c 006a 037c  ...r*t...d.|.j.|
+00000590: 0166 0216 00a1 0101 0064 0253 0029 037a  .f.......d.S.).z
+000005a0: 3c0a 2020 2020 2020 4465 6c65 7465 206c  <.      Delete l
+000005b0: 6f67 2069 6620 7265 2d73 7461 7274 696e  og if re-startin
+000005c0: 6720 616e 6420 6c6f 6720 616c 7265 6164  g and log alread
+000005d0: 7920 6578 6973 7473 0a20 2020 20fa 0925  y exists.    ..%
+000005e0: 732f 2573 2e6c 6f67 4e29 0572 1700 0000  s/%s.logN).r....
+000005f0: 7218 0000 0072 1900 0000 7216 0000 00da  r....r....r.....
+00000600: 0672 656d 6f76 6529 0272 1e00 0000 da04  .remove).r......
+00000610: 6974 656d 7207 0000 0072 0700 0000 720a  itemr....r....r.
+00000620: 0000 00da 0672 6569 6e69 742b 0000 0073  .....reinit+...s
+00000630: 0400 0000 0004 1601 7a11 5465 7874 4c6f  ........z.TextLo
+00000640: 6767 6572 2e72 6569 6e69 7463 0200 0000  gger.reinitc....
+00000650: 0000 0000 0000 0000 0600 0000 0a00 0000  ................
+00000660: 4b00 0000 739e 0000 0074 006a 017c 006a  K...s....t.j.|.j
+00000670: 0264 0164 028d 0201 007c 0244 005d 847d  .d.d.....|.D.].}
+00000680: 0364 037c 006a 027c 0366 0216 007d 047c  .d.|.j.|.f...}.|
+00000690: 037c 006a 036b 0772 6674 006a 0174 006a  .|.j.k.rft.j.t.j
+000006a0: 04a0 057c 04a1 0164 0164 028d 0201 007c  ...|...d.d.....|
+000006b0: 006a 0672 567c 00a0 077c 03a1 0101 007c  .j.rV|...|.....|
+000006c0: 0004 006a 037c 0367 0137 0002 005f 0374  ...j.|.g.7..._.t
+000006d0: 087c 0464 0483 028f 227d 057c 05a0 0964  .|.d...."}.|...d
+000006e0: 057c 017c 006a 0a7c 027c 0319 0016 0066  .|.|.j.|.|.....f
+000006f0: 0216 00a1 0101 0057 0035 0051 0052 0058  .......W.5.Q.R.X
+00000700: 0071 1464 0653 0029 077a 1b0a 2020 2020  .q.d.S.).z..    
+00000710: 4c6f 6720 696e 2070 6c61 696e 7465 7874  Log in plaintext
+00000720: 3b0a 2020 2020 5429 01da 0865 7869 7374  ;.    T)...exist
+00000730: 5f6f 6b72 2300 0000 da01 617a 0a25 332e  _okr#.....az.%3.
+00000740: 3466 3a20 2573 0a4e 290b 7217 0000 0072  4f: %s.N).r....r
+00000750: 1a00 0000 7216 0000 0072 1c00 0000 7218  ....r....r....r.
+00000760: 0000 00da 0764 6972 6e61 6d65 721b 0000  .....dirnamer...
+00000770: 0072 2600 0000 da04 6f70 656e da05 7772  .r&.....open..wr
+00000780: 6974 6572 1d00 0000 2906 721e 0000 00da  iter....).r.....
+00000790: 0369 7472 da06 6b77 6172 6773 da03 6172  .itr..kwargs..ar
+000007a0: 67da 0966 696c 655f 7061 7468 da01 6672  g..file_path..fr
+000007b0: 0700 0000 7207 0000 0072 0a00 0000 da03  ....r....r......
+000007c0: 6c6f 6733 0000 0073 1400 0000 0004 1001  log3...s........
+000007d0: 0801 0e01 0a01 1601 0601 0a01 1001 0c01  ................
+000007e0: 7a0e 5465 7874 4c6f 6767 6572 2e6c 6f67  z.TextLogger.log
+000007f0: 6301 0000 0000 0000 0000 0000 0006 0000  c...............
+00000800: 0006 0000 004b 0000 0073 4a00 0000 6700  .....K...sJ...g.
+00000810: 7d02 6700 7d03 7c01 4400 5d26 7d04 6401  }.g.}.|.D.]&}.d.
+00000820: 7c00 6a00 7c04 6602 1600 7d05 7c02 a001  |.j.|.f...}.|...
+00000830: 7c04 a101 0100 7c03 a001 7c05 a101 0100  |.....|...|.....
+00000840: 710c 7402 7c02 7c03 7c00 6a00 6402 6403  q.t.|.|.|.j.d.d.
+00000850: 8d04 0100 6400 5300 2904 4e72 2300 0000  ....d.S.).Nr#...
+00000860: 46a9 04da 056e 616d 6573 da09 6669 6c65  F....names..file
+00000870: 7061 7468 73da 066f 7574 6469 725a 0b69  paths..outdirZ.i
+00000880: 6e5f 6f6e 655f 6178 6573 a903 7216 0000  n_one_axes..r...
+00000890: 00da 0661 7070 656e 6472 0400 0000 a906  ...appendr......
+000008a0: 721e 0000 0072 2d00 0000 7233 0000 0072  r....r-...r3...r
+000008b0: 3400 0000 722e 0000 00da 0866 696c 656e  4...r......filen
+000008c0: 616d 6572 0700 0000 7207 0000 0072 0a00  amer....r....r..
+000008d0: 0000 da08 6c6f 675f 6178 6573 4200 0000  ....log_axesB...
+000008e0: 7314 0000 0000 0104 0104 0108 010e 010a  s...............
+000008f0: 010c 0206 0104 0002 ff7a 1354 6578 744c  .........z.TextL
+00000900: 6f67 6765 722e 6c6f 675f 6178 6573 6301  ogger.log_axesc.
+00000910: 0000 0000 0000 0000 0000 0006 0000 0006  ................
+00000920: 0000 004b 0000 0073 4a00 0000 6700 7d02  ...K...sJ...g.}.
+00000930: 6700 7d03 7c01 4400 5d26 7d04 6401 7c00  g.}.|.D.]&}.d.|.
+00000940: 6a00 7c04 6602 1600 7d05 7c02 a001 7c04  j.|.f...}.|...|.
+00000950: a101 0100 7c03 a001 7c05 a101 0100 710c  ....|...|.....q.
+00000960: 7402 7c02 7c03 7c00 6a00 6402 6403 8d04  t.|.|.|.j.d.d...
+00000970: 0100 6400 5300 2904 4e72 2300 0000 5472  ..d.S.).Nr#...Tr
+00000980: 3200 0000 7236 0000 0072 3800 0000 7207  2...r6...r8...r.
+00000990: 0000 0072 0700 0000 720a 0000 00da 066c  ...r....r......l
+000009a0: 6f67 5f61 784d 0000 0073 1400 0000 0001  og_axM...s......
+000009b0: 0401 0401 0801 0e01 0a01 0c02 0601 0400  ................
+000009c0: 02ff 7a11 5465 7874 4c6f 6767 6572 2e6c  ..z.TextLogger.l
+000009d0: 6f67 5f61 78e9 6400 0000 fa04 2e70 6e67  og_ax.d......png
+000009e0: 6305 0000 0000 0000 0000 0000 000a 0000  c...............
+000009f0: 0006 0000 0043 0000 0073 a200 0000 6700  .....C...s....g.
+00000a00: 7d05 7c02 7258 6700 7d06 7c01 4400 5d10  }.|.rXg.}.|.D.].
+00000a10: 7d07 7c06 7c07 a000 a100 3700 7d06 7110  }.|.|.....7.}.q.
+00000a20: 6401 a001 7c06 a101 6400 7c03 8502 1900  d...|...d.|.....
+00000a30: 7d08 7402 6a03 a001 7c00 6a04 6402 7c08  }.t.j...|.j.d.|.
+00000a40: 1700 7c04 1700 a102 7d09 7c05 a005 7c09  ..|.....}.|...|.
+00000a50: a101 0100 6e46 7c01 4400 5d40 7d07 7c07  ....nF|.D.]@}.|.
+00000a60: a000 a100 7d06 6401 a001 7c06 a101 6400  ....}.d...|...d.
+00000a70: 7c03 8502 1900 7d08 7402 6a03 a001 7c00  |.....}.t.j...|.
+00000a80: 6a04 6402 7c08 1700 7c04 1700 a102 7d09  j.d.|...|.....}.
+00000a90: 7c05 a005 7c09 a101 0100 715c 7c05 5300  |...|.....q\|.S.
+00000aa0: 2903 4eda 025f 5f5a 0730 706c 6f74 5f5f  ).N..__Z.0plot__
+00000ab0: 2906 da04 6b65 7973 720f 0000 0072 1700  )...keysr....r..
+00000ac0: 0000 7218 0000 0072 1600 0000 7237 0000  ..r....r....r7..
+00000ad0: 0029 0a72 1e00 0000 da09 6469 6374 5f6c  .).r......dict_l
+00000ae0: 6973 74da 0d69 6e5f 6f6e 655f 6669 6775  ist..in_one_figu
+00000af0: 7265 5a06 4d41 584c 454e da03 6578 7472  reZ.MAXLEN..extr
+00000b00: 3400 0000 da06 6c61 6265 6c73 da01 6472  4.....labels..dr
+00000b10: 3900 0000 da08 6669 6c65 7061 7468 7207  9.....filepathr.
+00000b20: 0000 0072 0700 0000 720a 0000 00da 1b5f  ...r....r......_
+00000b30: 6765 745f 6669 6c65 7061 7468 5f66 726f  get_filepath_fro
+00000b40: 6d5f 6469 6374 6c69 7374 5800 0000 731c  m_dictlistX...s.
+00000b50: 0000 0000 0104 0104 0104 0108 010e 0112  ................
+00000b60: 0118 010c 0208 0108 0112 0118 010c 017a  ...............z
+00000b70: 2654 6578 744c 6f67 6765 722e 5f67 6574  &TextLogger._get
+00000b80: 5f66 696c 6570 6174 685f 6672 6f6d 5f64  _filepath_from_d
+00000b90: 6963 746c 6973 7454 e92c 0100 0063 0400  ictlistT.,...c..
+00000ba0: 0000 0000 0000 0000 0000 0b00 0000 0600  ................
+00000bb0: 0000 0300 0000 7388 0000 0067 007d 047c  ......s....g.}.|
+00000bc0: 01a0 00a1 0044 005d 245c 027d 057d 067c  .....D.]$\.}.}.|
+00000bd0: 04a0 0187 0066 0164 0164 0284 087c 06a0  .....f.d.d...|..
+00000be0: 00a1 0044 0083 01a1 0101 0071 0c88 006a  ...D.......q...j
+00000bf0: 027c 047c 0264 038d 027d 0764 04a0 037c  .|.|.d...}.d...|
+00000c00: 07a1 017d 0874 04a0 04a1 007d 0974 0574  ...}.t.....}.t.t
+00000c10: 067c 0864 0583 037d 0a7c 097c 0a18 007c  .|.d...}.|.|...|
+00000c20: 036b 0472 8474 077c 047c 077c 0264 068d  .k.r.t.|.|.|.d..
+00000c30: 0301 0074 0874 067c 087c 0983 0301 0064  ...t.t.|.|.....d
+00000c40: 0053 0029 074e 6301 0000 0000 0000 0000  .S.).Nc.........
+00000c50: 0000 0003 0000 0006 0000 0013 0000 0073  ...............s
+00000c60: 2000 0000 6900 7c00 5d18 5c02 7d01 7d02   ...i.|.].\.}.}.
+00000c70: 7c01 6400 8800 6a00 7c01 6602 1600 9302  |.d...j.|.f.....
+00000c80: 7104 5300 2901 7223 0000 0029 0172 1600  q.S.).r#...).r..
+00000c90: 0000 2903 7208 0000 00da 0473 7562 6bda  ..).r......subk.
+00000ca0: 015f a901 721e 0000 0072 0700 0000 720a  ._..r....r....r.
+00000cb0: 0000 00da 0a3c 6469 6374 636f 6d70 3e6c  .....<dictcomp>l
+00000cc0: 0000 0073 0600 0000 0600 0600 0200 7a35  ...s..........z5
+00000cd0: 5465 7874 4c6f 6767 6572 2e6c 6f67 5f64  TextLogger.log_d
+00000ce0: 6566 6175 6c74 6469 6374 3266 6967 7572  efaultdict2figur
+00000cf0: 652e 3c6c 6f63 616c 733e 2e3c 6469 6374  e.<locals>.<dict
+00000d00: 636f 6d70 3e29 0272 4000 0000 7241 0000  comp>).r@...rA..
+00000d10: 0072 4900 0000 7201 0000 0029 03da 146c  .rI...r....)...l
+00000d20: 6162 656c 3266 696c 6570 6174 6873 5f6c  abel2filepaths_l
+00000d30: 6973 7472 3400 0000 7241 0000 0029 09da  istr4...rA...)..
+00000d40: 0569 7465 6d73 7237 0000 0072 4600 0000  .itemsr7...rF...
+00000d50: 720f 0000 00da 0474 696d 65da 0767 6574  r......time..get
+00000d60: 6174 7472 7213 0000 0072 0500 0000 da07  attrr....r......
+00000d70: 7365 7461 7474 7229 0b72 1e00 0000 da0c  setattr).r......
+00000d80: 6465 6661 756c 745f 6469 6374 7241 0000  default_dictrA..
+00000d90: 00da 0c73 6176 655f 6669 675f 7365 6372  ...save_fig_secr
+00000da0: 4c00 0000 7249 0000 00da 0176 7234 0000  L...rI.....vr4..
+00000db0: 00da 0874 696d 655f 7374 72da 036e 6f77  ...time_str..now
+00000dc0: 5a09 6c61 7374 5f74 696d 6572 0700 0000  Z.last_timer....
+00000dd0: 724a 0000 0072 0a00 0000 da16 6c6f 675f  rJ...r......log_
+00000de0: 6465 6661 756c 7464 6963 7432 6669 6775  defaultdict2figu
+00000df0: 7265 6900 0000 7318 0000 0000 0104 0110  rei...s.........
+00000e00: 011e 010e 020a 0108 010c 010c 0106 0102  ................
+00000e10: ff06 027a 2154 6578 744c 6f67 6765 722e  ...z!TextLogger.
+00000e20: 6c6f 675f 6465 6661 756c 7464 6963 7432  log_defaultdict2
+00000e30: 6669 6775 7265 6302 0000 0000 0000 0000  figurec.........
+00000e40: 0000 0005 0000 000a 0000 004b 0000 0073  ...........K...s
+00000e50: 6e00 0000 7c02 4400 5d64 7d03 7c03 7c00  n...|.D.]d}.|.|.
+00000e60: 6a00 6b07 7232 7c00 6a01 7222 7c00 a002  j.k.r2|.j.r"|...
+00000e70: 7c03 a101 0100 7c00 0400 6a00 7c03 6701  |.....|...j.|.g.
+00000e80: 3700 0200 5f00 7403 6401 7c00 6a04 7c03  7..._.t.d.|.j.|.
+00000e90: 6602 1600 6402 8302 8f1c 7d04 7c04 a005  f...d.....}.|...
+00000ea0: 6403 7c01 7c02 7c03 1900 6602 1600 a101  d.|.|.|...f.....
+00000eb0: 0100 5700 3500 5100 5200 5800 7104 6400  ..W.5.Q.R.X.q.d.
+00000ec0: 5300 2904 4e72 2300 0000 7228 0000 007a  S.).Nr#...r(...z
+00000ed0: 0825 3364 3a20 2573 0a29 0672 1c00 0000  .%3d: %s.).r....
+00000ee0: 721b 0000 0072 2600 0000 722a 0000 0072  r....r&...r*...r
+00000ef0: 1600 0000 722b 0000 0029 0572 1e00 0000  ....r+...).r....
+00000f00: 722c 0000 0072 2d00 0000 722e 0000 0072  r,...r-...r....r
+00000f10: 3000 0000 7207 0000 0072 0700 0000 720a  0...r....r....r.
+00000f20: 0000 00da 066c 6f67 7374 7278 0000 0073  .....logstrx...s
+00000f30: 0e00 0000 0003 0801 0a01 0601 0a01 1001  ................
+00000f40: 1601 7a11 5465 7874 4c6f 6767 6572 2e6c  ..z.TextLogger.l
+00000f50: 6f67 7374 724e 2902 4672 1400 0000 2902  ogstrN).Fr....).
+00000f60: 723c 0000 0072 3d00 0000 2902 5472 4700  r<...r=...).TrG.
+00000f70: 0000 290d da08 5f5f 6e61 6d65 5f5f da0a  ..)...__name__..
+00000f80: 5f5f 6d6f 6475 6c65 5f5f da0c 5f5f 7175  __module__..__qu
+00000f90: 616c 6e61 6d65 5f5f da07 5f5f 646f 635f  alname__..__doc_
+00000fa0: 5f72 2000 0000 7222 0000 0072 2600 0000  _r ...r"...r&...
+00000fb0: 7231 0000 0072 3a00 0000 723b 0000 0072  r1...r:...r;...r
+00000fc0: 4600 0000 7256 0000 0072 5700 0000 7207  F...rV...rW...r.
+00000fd0: 0000 0072 0700 0000 7207 0000 0072 0a00  ...r....r....r..
+00000fe0: 0000 7213 0000 0015 0000 0073 1400 0000  ..r........s....
+00000ff0: 0801 0405 0a0a 0806 0808 080f 080b 080b  ................
+00001000: 0a11 0a0f 7213 0000 0054 723c 0000 0063  ....r....Tr<...c
+00001010: 0900 0000 0000 0000 0000 0000 0b00 0000  ................
+00001020: 0500 0000 0300 0000 739a 0000 007c 0873  ........s....|.s
+00001030: 0864 0053 007c 0364 006b 0972 8e74 007c  .d.S.|.d.k.r.t.|
+00001040: 0164 018d 0189 0174 0174 0283 017d 097c  .d.....t.t...}.|
+00001050: 00a0 03a1 0044 005d 245c 0289 007d 0a87  .....D.]$\...}..
+00001060: 0087 0166 0264 0264 0384 087c 0aa0 03a1  ...f.d.d...|....
+00001070: 0044 0083 017c 0988 003c 0071 2a7c 097d  .D...|...<.q*|.}
+00001080: 007c 0572 787c 00a0 03a1 0044 005d 165c  .|.rx|.....D.].\
+00001090: 0289 007d 0a7c 036a 047c 0266 017c 0a8e  ...}.|.j.|.f.|..
+000010a0: 0101 0071 607c 0672 967c 036a 057c 007c  ...q`|.r.|.j.|.|
+000010b0: 047c 0764 048d 0301 006e 0874 0664 0583  .|.d.....n.t.d..
+000010c0: 0101 0064 0053 0029 064e 2901 7210 0000  ...d.S.).N).r...
+000010d0: 0063 0100 0000 0000 0000 0000 0000 0300  .c..............
+000010e0: 0000 0400 0000 1300 0000 7326 0000 0069  ..........s&...i
+000010f0: 007c 005d 1e5c 027d 017d 0288 0164 0017  .|.].\.}.}...d..
+00001100: 0088 0017 0064 0017 007c 0117 007c 0293  .....d...|...|..
+00001110: 0271 0453 0029 01da 012e 7207 0000 0029  .q.S.)....r....)
+00001120: 0372 0800 0000 7248 0000 005a 0473 7562  .r....rH...Z.sub
+00001130: 76a9 0272 0900 0000 7211 0000 0072 0700  v..r....r....r..
+00001140: 0000 720a 0000 0072 4b00 0000 8e00 0000  ..r....rK.......
+00001150: 7306 0000 0006 0006 0012 007a 2e73 756d  s..........z.sum
+00001160: 6d61 7279 5f64 6566 6175 6c74 6469 6374  mary_defaultdict
+00001170: 3274 7874 6669 672e 3c6c 6f63 616c 733e  2txtfig.<locals>
+00001180: 2e3c 6469 6374 636f 6d70 3e29 0272 4100  .<dictcomp>).rA.
+00001190: 0000 7252 0000 007a 1474 6578 746c 6f67  ..rR...z.textlog
+000011a0: 6765 7220 6172 6520 4e6f 6e65 2129 0772  ger are None!).r
+000011b0: 1200 0000 7202 0000 00da 0464 6963 7472  ....r......dictr
+000011c0: 4d00 0000 7231 0000 0072 5600 0000 da05  M...r1...rV.....
+000011d0: 7072 696e 7429 0b72 5100 0000 7210 0000  print).rQ...r...
+000011e0: 00da 0473 7465 7072 2100 0000 7241 0000  ...stepr!...rA..
+000011f0: 00da 076c 6f67 5f74 7874 da07 6c6f 675f  ...log_txt..log_
+00001200: 6669 6772 5200 0000 da0f 6973 5f6d 6169  figrR.....is_mai
+00001210: 6e5f 7072 6f63 6573 735a 1164 6566 6175  n_processZ.defau
+00001220: 6c74 5f64 6963 745f 636f 7079 7253 0000  lt_dict_copyrS..
+00001230: 0072 0700 0000 725d 0000 0072 0a00 0000  .r....r]...r....
+00001240: da1a 7375 6d6d 6172 795f 6465 6661 756c  ..summary_defaul
+00001250: 7464 6963 7432 7478 7466 6967 8400 0000  tdict2txtfig....
+00001260: 731c 0000 0000 0304 0104 0108 010a 0108  s...............
+00001270: 0210 011e 0104 0204 0110 0110 0104 0112  ................
+00001280: 0272 6400 0000 4663 0900 0000 0000 0000  .rd...Fc........
+00001290: 0000 0000 1000 0000 0a00 0000 4300 0000  ............C...
+000012a0: 73a2 0000 007c 0873 0864 0053 0069 007d  s....|.s.d.S.i.}
+000012b0: 097c 00a0 00a1 0044 005d 1c5c 027d 0a7d  .|.....D.].\.}.}
+000012c0: 0b7c 0aa0 0164 0164 02a1 027d 0c7c 0b7c  .|...d.d...}.|.|
+000012d0: 097c 0c3c 0071 147c 097d 007c 0472 5074  .|.<.q.|.}.|.rPt
+000012e0: 0274 0383 017d 0d64 037d 0e7c 007c 0d7c  .t...}.d.}.|.|.|
+000012f0: 0e3c 006e 3674 0274 0383 017d 0d64 047d  .<.n6t.t...}.d.}
+00001300: 0e74 047c 00a0 00a1 0083 0144 005d 1c5c  .t.|.......D.].\
+00001310: 027d 0f5c 027d 0a7d 0b7c 0a7c 0b69 017c  .}.\.}.}.|.|.i.|
+00001320: 0d7c 0e7c 0f16 003c 0071 6874 057c 0d7c  .|.|...<.qht.|.|
+00001330: 017c 027c 0364 057c 057c 067c 0764 068d  .|.|.d.|.|.|.d..
+00001340: 0801 0064 0053 0029 074e fa01 2f7a 022d  ...d.S.).N../z.-
+00001350: 2dda 0273 617a 046d 6125 6454 2908 7251  -..saz.ma%dT).rQ
+00001360: 0000 0072 1000 0000 7260 0000 0072 2100  ...r....r`...r!.
+00001370: 0000 7241 0000 0072 6100 0000 7262 0000  ..rA...ra...rb..
+00001380: 0072 5200 0000 2906 724d 0000 00da 0772  .rR...).rM.....r
+00001390: 6570 6c61 6365 7202 0000 0072 5e00 0000  eplacer....r^...
+000013a0: da09 656e 756d 6572 6174 6572 6400 0000  ..enumeraterd...
+000013b0: 2910 5a09 6469 6374 5f64 6174 6172 1000  ).Z.dict_datar..
+000013c0: 0000 7260 0000 0072 2100 0000 5a0a 696e  ..r`...r!...Z.in
+000013d0: 5f6f 6e65 5f61 7865 7261 0000 0072 6200  _one_axera...rb.
+000013e0: 0000 7252 0000 0072 6300 0000 5a11 6e65  ..rR...rc...Z.ne
+000013f0: 775f 6b65 795f 6469 6374 5f64 6174 6172  w_key_dict_datar
+00001400: 0900 0000 7253 0000 005a 056e 6577 5f6b  ....rS...Z.new_k
+00001410: 7251 0000 0072 3f00 0000 da01 6972 0700  rQ...r?.....ir..
+00001420: 0000 7207 0000 0072 0a00 0000 da13 7375  ..r....r......su
+00001430: 6d6d 6172 795f 6469 6374 3274 7874 6669  mmary_dict2txtfi
+00001440: 679a 0000 0073 2c00 0000 0003 0401 0401  g....s,.........
+00001450: 0401 1001 0c01 0a01 0402 0401 0801 0401  ................
+00001460: 0a02 0801 0401 1801 1201 0801 0200 0201  ................
+00001470: 0200 0200 02fe 726a 0000 0029 0172 1f00  ......rj...).r..
+00001480: 0000 6301 0000 0000 0000 0000 0000 0001  ..c.............
+00001490: 0000 0003 0000 0043 0000 0073 0e00 0000  .......C...s....
+000014a0: 7400 a001 7c00 a101 0100 6400 5300 7215  t...|.....d.S.r.
+000014b0: 0000 0029 02da 1167 6c6f 6261 6c5f 7465  ...)...global_te
+000014c0: 7874 6c6f 6767 6572 7222 0000 0029 0172  xtloggerr"...).r
+000014d0: 2100 0000 7207 0000 0072 0700 0000 720a  !...r....r....r.
+000014e0: 0000 00da 1573 6574 5f67 6c6f 6261 6c5f  .....set_global_
+000014f0: 7465 7874 6c6f 6767 6572 b500 0000 7304  textlogger....s.
+00001500: 0000 0000 020a 0172 6c00 0000 2906 4e54  .......rl...).NT
+00001510: 5454 723c 0000 0054 2906 4e46 5454 723c  TTr<...T).NFTTr<
+00001520: 0000 0054 2912 da0b 636f 6c6c 6563 7469  ...T)...collecti
+00001530: 6f6e 7372 0200 0000 724e 0000 0072 0c00  onsr....rN...r..
+00001540: 0000 da07 6c6f 6767 696e 6772 1700 0000  ....loggingr....
+00001550: da08 6461 7465 7469 6d65 da03 7379 735a  ..datetime..sysZ
+00001560: 0a70 6c6f 745f 7574 696c 7372 0400 0000  .plot_utilsr....
+00001570: 7205 0000 0072 1200 0000 da06 6f62 6a65  r....r......obje
+00001580: 6374 7213 0000 0072 6400 0000 726a 0000  ctr....rd...rj..
+00001590: 0072 6b00 0000 726c 0000 0072 0700 0000  .rk...rl...r....
+000015a0: 7207 0000 0072 0700 0000 720a 0000 00da  r....r....r.....
+000015b0: 083c 6d6f 6475 6c65 3e01 0000 0073 3000  .<module>....s0.
+000015c0: 0000 0c01 0801 0801 1001 0801 0802 1003  ................
+000015d0: 080a 1070 0000 0001 0000 0000 0000 00fe  ...p............
+000015e0: 0a17 0000 0001 0000 0000 0000 00fe 0a19  ................
+000015f0: 0a02                                     ..
```

### Comparing `tl2-0.1.0/tl2/proj/logger/logging_utils_v2.py` & `tl2-0.1.1/tl2/proj/logger/logging_utils_v2.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,88 +1,88 @@
-import logging
-import copy
-from termcolor import colored
-
-
-class _ColorfulFormatter(logging.Formatter):
-  def __init__(self, *args, **kwargs):
-    self._root_name = kwargs.pop("root_name") + "."
-    self._abbrev_name = kwargs.pop("abbrev_name", "")
-    if len(self._abbrev_name):
-      self._abbrev_name = self._abbrev_name + "."
-    super(_ColorfulFormatter, self).__init__(*args, **kwargs)
-
-  def formatMessage(self, record):
-    record.name = record.name.replace(self._root_name, self._abbrev_name)
-    log = super(_ColorfulFormatter, self).formatMessage(record)
-    if record.levelno == logging.WARNING:
-      prefix = colored("WARNING", "red", attrs=["blink"])
-    elif record.levelno == logging.ERROR or record.levelno == logging.CRITICAL:
-      prefix = colored("ERROR", "red", attrs=["blink", "underline"])
-    else:
-      return log
-    return prefix + " " + log
-
-
-def _set_hander(logger,
-                filename,
-                stream=True,
-                level=logging.INFO,
-                mode='a'):
-
-  formatter = logging.Formatter(
-    fmt="[%(asctime)s] %(name)s:%(lineno)s %(levelname)s: %(message)s \t[%(filename)s:%(funcName)s():%(lineno)s]",
-    datefmt="%m/%d %H:%M:%S")
-  # formatter = logging.Formatter(FORMAT, datefmt=DATEFMT)
-
-  file_hander = logging.FileHandler(filename=filename, mode=mode)
-  file_hander.setLevel(level=level)
-  file_hander.setFormatter(formatter)
-  logger.addHandler(file_hander)
-
-  def info_msg(*argv):
-    # remove formats
-    org_formatters = []
-    for handler in logger.handlers:
-      org_formatters.append(handler.formatter)
-      handler.setFormatter(logging.Formatter("%(message)s"))
-
-    logger.info(*argv)
-
-    # restore formats
-    for handler, formatter in zip(logger.handlers, org_formatters):
-      handler.setFormatter(formatter)
-
-  logger.info_msg = info_msg
-
-  if stream:
-    stream_handler = logging.StreamHandler()
-    stream_handler.setLevel(level)
-
-    formatter = _ColorfulFormatter(
-      # colored("[%(asctime)s %(name)s]: ", "green") + "%(message)s",
-      colored("[%(asctime)s] %(name)s %(levelname)s:", "blue") + \
-      "%(message)s \t" + \
-      colored("[%(filename)s:%(funcName)s():%(lineno)s]", "blue"),
-      datefmt="%m/%d %H:%M:%S",
-      root_name='tl2',
-      abbrev_name='tl2',
-    )
-    stream_handler.setFormatter(formatter)
-    logger.addHandler(stream_handler)
-
-  return logger
-
-
-def get_logger(filename,
-               logger_names=('tl2', ),
-               stream=True,
-               level=logging.INFO,
-               mode='a'):
-
-  logger_names = list(logger_names) + [filename, ]
-  for name in logger_names:
-    logger = logging.getLogger(name)
-    logger.setLevel(level)
-    logger.propagate = False
-    _set_hander(logger=logger, filename=filename, stream=stream, level=level, mode=mode)
+import logging
+import copy
+from termcolor import colored
+
+
+class _ColorfulFormatter(logging.Formatter):
+  def __init__(self, *args, **kwargs):
+    self._root_name = kwargs.pop("root_name") + "."
+    self._abbrev_name = kwargs.pop("abbrev_name", "")
+    if len(self._abbrev_name):
+      self._abbrev_name = self._abbrev_name + "."
+    super(_ColorfulFormatter, self).__init__(*args, **kwargs)
+
+  def formatMessage(self, record):
+    record.name = record.name.replace(self._root_name, self._abbrev_name)
+    log = super(_ColorfulFormatter, self).formatMessage(record)
+    if record.levelno == logging.WARNING:
+      prefix = colored("WARNING", "red", attrs=["blink"])
+    elif record.levelno == logging.ERROR or record.levelno == logging.CRITICAL:
+      prefix = colored("ERROR", "red", attrs=["blink", "underline"])
+    else:
+      return log
+    return prefix + " " + log
+
+
+def _set_hander(logger,
+                filename,
+                stream=True,
+                level=logging.INFO,
+                mode='a'):
+
+  formatter = logging.Formatter(
+    fmt="[%(asctime)s] %(name)s:%(lineno)s %(levelname)s: %(message)s \t[%(filename)s:%(funcName)s():%(lineno)s]",
+    datefmt="%m/%d %H:%M:%S")
+  # formatter = logging.Formatter(FORMAT, datefmt=DATEFMT)
+
+  file_hander = logging.FileHandler(filename=filename, mode=mode)
+  file_hander.setLevel(level=level)
+  file_hander.setFormatter(formatter)
+  logger.addHandler(file_hander)
+
+  def info_msg(*argv):
+    # remove formats
+    org_formatters = []
+    for handler in logger.handlers:
+      org_formatters.append(handler.formatter)
+      handler.setFormatter(logging.Formatter("%(message)s"))
+
+    logger.info(*argv)
+
+    # restore formats
+    for handler, formatter in zip(logger.handlers, org_formatters):
+      handler.setFormatter(formatter)
+
+  logger.info_msg = info_msg
+
+  if stream:
+    stream_handler = logging.StreamHandler()
+    stream_handler.setLevel(level)
+
+    formatter = _ColorfulFormatter(
+      # colored("[%(asctime)s %(name)s]: ", "green") + "%(message)s",
+      colored("[%(asctime)s] %(name)s %(levelname)s:", "blue") + \
+      "%(message)s \t" + \
+      colored("[%(filename)s:%(funcName)s():%(lineno)s]", "blue"),
+      datefmt="%m/%d %H:%M:%S",
+      root_name='tl2',
+      abbrev_name='tl2',
+    )
+    stream_handler.setFormatter(formatter)
+    logger.addHandler(stream_handler)
+
+  return logger
+
+
+def get_logger(filename,
+               logger_names=('tl2', ),
+               stream=True,
+               level=logging.INFO,
+               mode='a'):
+
+  logger_names = list(logger_names) + [filename, ]
+  for name in logger_names:
+    logger = logging.getLogger(name)
+    logger.setLevel(level)
+    logger.propagate = False
+    _set_hander(logger=logger, filename=filename, stream=stream, level=level, mode=mode)
   return logger
```

### Comparing `tl2-0.1.0/tl2/proj/logger/plot_utils.py` & `tl2-0.1.1/tl2/proj/logger/plot_utils.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,197 +1,197 @@
-import numpy as np
-import math
-import re, os
-import multiprocessing
-
-
-class MatPlot(object):
-  def __init__(self, style='ggplot'):
-    """
-      plt.rcParams['axes.prop_cycle'] = plt.cycler(color=['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'purple', 'pink', 'brown', 'orange', 'teal', 'coral', 'lightblue', 'lime', 'lavender', 'turquoise', 'darkgreen', 'tan', 'salmon', 'gold', 'lightpurple', 'darkred', 'darkblue'])
-    :param style: [classic, ggplot]
-    """
-    import matplotlib.pyplot as plt
-    # R style
-    plt.style.use(style)
-    plt.rcParams['axes.prop_cycle'] = plt.cycler(
-      color=['blue', 'green', 'red', 'cyan', 'magenta', 'black', 'orange', 'lime', 'tan', 'salmon', 'gold', 'darkred',
-             'darkblue'])
-
-    pass
-
-  def get_fig_and_ax(self, nrows=1, ncols=1, ravel=False, fig_w_h=(6.4, 4.8)):
-    """
-    ax.legend(loc='best')
-    """
-    import matplotlib.pyplot as plt
-    figsize = (fig_w_h[0] * ncols, fig_w_h[1] * nrows)
-    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)
-    if ravel:
-      if ncols == 1 and nrows == 1:
-        axes = [axes]
-      else:
-        axes = axes.ravel()
-    return fig, axes
-
-  def save_to_png(self, fig, filepath, dpi=1000, bbox_inches='tight',
-                  pad_inches=0.1):
-    assert filepath.endswith('.png')
-    fig.savefig(
-      filepath, dpi=dpi, bbox_inches=bbox_inches, pad_inches=pad_inches)
-
-  def save_to_pdf(self, fig, filepath):
-    fig.savefig(filepath, bbox_inches='tight', pad_inches=0)
-
-  def parse_logfile_using_re(self, logfile, re_str):
-    """
-    import re
-
-    """
-    with open(logfile) as f:
-      logstr = f.read()
-      val = [float(x) for x in re_str.findall(logstr)]
-      idx = range(len(val))
-    return (idx, val)
-
-
-def parse_logfile(args, myargs):
-  config = getattr(myargs.config, args.command)
-  matplot = MatPlot()
-  fig, ax = matplot.get_fig_and_ax()
-  if len(config.logfiles) == 1:
-    logfiles = config.logfiles * len(config.re_strs)
-  for logfile, re_str in zip(logfiles, config.re_strs):
-    RE_STR = re.compile(re_str)
-    (idx, val) = matplot.parse_logfile_using_re(logfile=logfile, re_str=RE_STR)
-    ax.plot(idx, val, label=re_str)
-  ax.legend()
-  matplot.save_to_png(
-    fig, filepath=os.path.join(args.outdir, config.title + '.png'))
-  pass
-
-
-def _plot_figure(names, datas, outdir, in_one_axes=False):
-  import matplotlib
-  matplotlib.use('Agg')
-  import matplotlib.pyplot as plt
-  assert len(datas) == len(names)
-  filename = os.path.join(outdir, 'plot_' + '__'.join(names) + '.png')
-  matplot = MatPlot()
-  if not in_one_axes:
-    ncols = math.ceil(math.sqrt(len(names)))
-    nrows = (len(names) + ncols - 1) // ncols
-    fig, axes = matplot.get_fig_and_ax(nrows=nrows, ncols=ncols)
-    if ncols == 1 and nrows == 1:
-      axes = [axes]
-    else:
-      axes = axes.ravel()
-  else:
-    ncols = 1
-    nrows = 1
-    fig, axes = matplot.get_fig_and_ax(nrows=nrows, ncols=ncols)
-    axes = [axes] * len(names)
-
-  for idx, (label, data) in enumerate(zip(names, datas)):
-    data = data.reshape(-1, 2)
-    axes[idx].plot(data[:, 0], data[:, 1], marker='.', label=label, alpha=0.7)
-    axes[idx].legend(loc='best')
-
-  matplot.save_to_png(fig=fig, filepath=filename, dpi=None, bbox_inches=None)
-  plt.close(fig)
-  pass
-
-
-class PlotFigureProcessing(multiprocessing.Process):
-  """
-    worker = PlotFigureProcessing(args=(s, d, copytree))
-    worker.start()
-    worker.join()
-  """
-  def run(self):
-    names, filepaths, outdir, in_one_axes = self._args
-    datas = []
-    for filepath in filepaths:
-      data = np.loadtxt(filepath, delimiter=':')
-      datas.append(data)
-    _plot_figure(
-      names=names, datas=datas, outdir=outdir, in_one_axes=in_one_axes)
-    pass
-
-def plot_figure(names, filepaths, outdir, in_one_axes, join=False):
-  worker = PlotFigureProcessing(args=(names, filepaths, outdir, in_one_axes))
-  worker.start()
-
-  if join:
-    worker.join()
-  pass
-
-
-class PlotDefaultdict2figure(multiprocessing.Process):
-  """
-    worker = PlotDefaultdict2figure(args=(s, d, copytree))
-    worker.start()
-    worker.join()
-  """
-  def run(self):
-    label2filepaths_list, filepaths, in_one_figure = self._args
-    # load data
-    label2datas_list = []
-    for label2filepaths in label2filepaths_list:
-      label2datas_list.append({k: np.loadtxt(filepath, delimiter=':') for k, filepath in label2filepaths.items()})
-    self._plot_figure(label2datas_list=label2datas_list, filepaths=filepaths, in_one_figure=in_one_figure)
-    pass
-
-  def _plot_figure(self, label2datas_list, filepaths, in_one_figure=False):
-    import matplotlib
-    # matplotlib.use(arg='Agg', warn=False)
-    import matplotlib.pyplot as plt
-    plt.switch_backend('agg')
-    if in_one_figure:
-      self._plot_in_one_figure(label2datas_list, filepaths)
-    else:
-      self._plot_in_multi_figures(label2datas_list=label2datas_list, filepaths=filepaths)
-
-  def _plot_in_one_figure(self, label2datas_list, filepaths):
-    import matplotlib.pyplot as plt
-    assert len(filepaths) == 1
-    matplot = MatPlot()
-    # ncols = math.ceil(math.sqrt(len(label2datas_list)))
-    ncols = 2
-    nrows = (len(label2datas_list) + ncols - 1) // ncols
-    fig, axes = matplot.get_fig_and_ax(nrows=nrows, ncols=ncols, ravel=True)
-
-
-    for idx, label2datas in enumerate(label2datas_list):
-      for label, data in label2datas.items():
-        data = data.reshape(-1, 2)
-        axes[idx].plot(data[:, 0], data[:, 1], marker='.', label=label, alpha=0.7)
-      axes[idx].legend(loc='best')
-
-    matplot.save_to_png(fig=fig, filepath=filepaths[0], dpi=None, bbox_inches='tight')
-    plt.close(fig)
-    pass
-
-  def _plot_in_multi_figures(self, label2datas_list, filepaths):
-    import matplotlib.pyplot as plt
-    assert len(filepaths) == len(label2datas_list)
-    matplot = MatPlot()
-    for idx, label2datas in enumerate(label2datas_list):
-      fig, axes = matplot.get_fig_and_ax(nrows=1, ncols=1)
-
-      for label, data in label2datas.items():
-        data = data.reshape(-1, 2)
-        axes.plot(data[:, 0], data[:, 1], marker='.', label=label, alpha=0.7)
-      axes.legend(loc='best')
-
-      matplot.save_to_png(fig=fig, filepath=filepaths[idx], dpi=None, bbox_inches='tight')
-      plt.close(fig)
-    pass
-
-
-def plot_defaultdict2figure(label2filepaths_list, filepaths, in_one_figure, join=False):
-  worker = PlotDefaultdict2figure(args=(label2filepaths_list, filepaths, in_one_figure))
-  worker.start()
-
-  if join:
-    worker.join()
+import numpy as np
+import math
+import re, os
+import multiprocessing
+
+
+class MatPlot(object):
+  def __init__(self, style='ggplot'):
+    """
+      plt.rcParams['axes.prop_cycle'] = plt.cycler(color=['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'purple', 'pink', 'brown', 'orange', 'teal', 'coral', 'lightblue', 'lime', 'lavender', 'turquoise', 'darkgreen', 'tan', 'salmon', 'gold', 'lightpurple', 'darkred', 'darkblue'])
+    :param style: [classic, ggplot]
+    """
+    import matplotlib.pyplot as plt
+    # R style
+    plt.style.use(style)
+    plt.rcParams['axes.prop_cycle'] = plt.cycler(
+      color=['blue', 'green', 'red', 'cyan', 'magenta', 'black', 'orange', 'lime', 'tan', 'salmon', 'gold', 'darkred',
+             'darkblue'])
+
+    pass
+
+  def get_fig_and_ax(self, nrows=1, ncols=1, ravel=False, fig_w_h=(6.4, 4.8)):
+    """
+    ax.legend(loc='best')
+    """
+    import matplotlib.pyplot as plt
+    figsize = (fig_w_h[0] * ncols, fig_w_h[1] * nrows)
+    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)
+    if ravel:
+      if ncols == 1 and nrows == 1:
+        axes = [axes]
+      else:
+        axes = axes.ravel()
+    return fig, axes
+
+  def save_to_png(self, fig, filepath, dpi=1000, bbox_inches='tight',
+                  pad_inches=0.1):
+    assert filepath.endswith('.png')
+    fig.savefig(
+      filepath, dpi=dpi, bbox_inches=bbox_inches, pad_inches=pad_inches)
+
+  def save_to_pdf(self, fig, filepath):
+    fig.savefig(filepath, bbox_inches='tight', pad_inches=0)
+
+  def parse_logfile_using_re(self, logfile, re_str):
+    """
+    import re
+
+    """
+    with open(logfile) as f:
+      logstr = f.read()
+      val = [float(x) for x in re_str.findall(logstr)]
+      idx = range(len(val))
+    return (idx, val)
+
+
+def parse_logfile(args, myargs):
+  config = getattr(myargs.config, args.command)
+  matplot = MatPlot()
+  fig, ax = matplot.get_fig_and_ax()
+  if len(config.logfiles) == 1:
+    logfiles = config.logfiles * len(config.re_strs)
+  for logfile, re_str in zip(logfiles, config.re_strs):
+    RE_STR = re.compile(re_str)
+    (idx, val) = matplot.parse_logfile_using_re(logfile=logfile, re_str=RE_STR)
+    ax.plot(idx, val, label=re_str)
+  ax.legend()
+  matplot.save_to_png(
+    fig, filepath=os.path.join(args.outdir, config.title + '.png'))
+  pass
+
+
+def _plot_figure(names, datas, outdir, in_one_axes=False):
+  import matplotlib
+  matplotlib.use('Agg')
+  import matplotlib.pyplot as plt
+  assert len(datas) == len(names)
+  filename = os.path.join(outdir, 'plot_' + '__'.join(names) + '.png')
+  matplot = MatPlot()
+  if not in_one_axes:
+    ncols = math.ceil(math.sqrt(len(names)))
+    nrows = (len(names) + ncols - 1) // ncols
+    fig, axes = matplot.get_fig_and_ax(nrows=nrows, ncols=ncols)
+    if ncols == 1 and nrows == 1:
+      axes = [axes]
+    else:
+      axes = axes.ravel()
+  else:
+    ncols = 1
+    nrows = 1
+    fig, axes = matplot.get_fig_and_ax(nrows=nrows, ncols=ncols)
+    axes = [axes] * len(names)
+
+  for idx, (label, data) in enumerate(zip(names, datas)):
+    data = data.reshape(-1, 2)
+    axes[idx].plot(data[:, 0], data[:, 1], marker='.', label=label, alpha=0.7)
+    axes[idx].legend(loc='best')
+
+  matplot.save_to_png(fig=fig, filepath=filename, dpi=None, bbox_inches=None)
+  plt.close(fig)
+  pass
+
+
+class PlotFigureProcessing(multiprocessing.Process):
+  """
+    worker = PlotFigureProcessing(args=(s, d, copytree))
+    worker.start()
+    worker.join()
+  """
+  def run(self):
+    names, filepaths, outdir, in_one_axes = self._args
+    datas = []
+    for filepath in filepaths:
+      data = np.loadtxt(filepath, delimiter=':')
+      datas.append(data)
+    _plot_figure(
+      names=names, datas=datas, outdir=outdir, in_one_axes=in_one_axes)
+    pass
+
+def plot_figure(names, filepaths, outdir, in_one_axes, join=False):
+  worker = PlotFigureProcessing(args=(names, filepaths, outdir, in_one_axes))
+  worker.start()
+
+  if join:
+    worker.join()
+  pass
+
+
+class PlotDefaultdict2figure(multiprocessing.Process):
+  """
+    worker = PlotDefaultdict2figure(args=(s, d, copytree))
+    worker.start()
+    worker.join()
+  """
+  def run(self):
+    label2filepaths_list, filepaths, in_one_figure = self._args
+    # load data
+    label2datas_list = []
+    for label2filepaths in label2filepaths_list:
+      label2datas_list.append({k: np.loadtxt(filepath, delimiter=':') for k, filepath in label2filepaths.items()})
+    self._plot_figure(label2datas_list=label2datas_list, filepaths=filepaths, in_one_figure=in_one_figure)
+    pass
+
+  def _plot_figure(self, label2datas_list, filepaths, in_one_figure=False):
+    import matplotlib
+    # matplotlib.use(arg='Agg', warn=False)
+    import matplotlib.pyplot as plt
+    plt.switch_backend('agg')
+    if in_one_figure:
+      self._plot_in_one_figure(label2datas_list, filepaths)
+    else:
+      self._plot_in_multi_figures(label2datas_list=label2datas_list, filepaths=filepaths)
+
+  def _plot_in_one_figure(self, label2datas_list, filepaths):
+    import matplotlib.pyplot as plt
+    assert len(filepaths) == 1
+    matplot = MatPlot()
+    # ncols = math.ceil(math.sqrt(len(label2datas_list)))
+    ncols = 2
+    nrows = (len(label2datas_list) + ncols - 1) // ncols
+    fig, axes = matplot.get_fig_and_ax(nrows=nrows, ncols=ncols, ravel=True)
+
+
+    for idx, label2datas in enumerate(label2datas_list):
+      for label, data in label2datas.items():
+        data = data.reshape(-1, 2)
+        axes[idx].plot(data[:, 0], data[:, 1], marker='.', label=label, alpha=0.7)
+      axes[idx].legend(loc='best')
+
+    matplot.save_to_png(fig=fig, filepath=filepaths[0], dpi=None, bbox_inches='tight')
+    plt.close(fig)
+    pass
+
+  def _plot_in_multi_figures(self, label2datas_list, filepaths):
+    import matplotlib.pyplot as plt
+    assert len(filepaths) == len(label2datas_list)
+    matplot = MatPlot()
+    for idx, label2datas in enumerate(label2datas_list):
+      fig, axes = matplot.get_fig_and_ax(nrows=1, ncols=1)
+
+      for label, data in label2datas.items():
+        data = data.reshape(-1, 2)
+        axes.plot(data[:, 0], data[:, 1], marker='.', label=label, alpha=0.7)
+      axes.legend(loc='best')
+
+      matplot.save_to_png(fig=fig, filepath=filepaths[idx], dpi=None, bbox_inches='tight')
+      plt.close(fig)
+    pass
+
+
+def plot_defaultdict2figure(label2filepaths_list, filepaths, in_one_figure, join=False):
+  worker = PlotDefaultdict2figure(args=(label2filepaths_list, filepaths, in_one_figure))
+  worker.start()
+
+  if join:
+    worker.join()
   pass
```

### Comparing `tl2-0.1.0/tl2/proj/logger/textlogger.py` & `tl2-0.1.1/tl2/proj/logger/textlogger.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,184 +1,184 @@
-from collections import defaultdict
-import time
-import re
-import logging, os
-import datetime
-import sys
-
-from .plot_utils import plot_figure, plot_defaultdict2figure
-
-
-def get_prefix_abb(prefix):
-  # prefix_split = prefix.split('_')
-  prefix_split = re.split('_|/', prefix)
-  if len(prefix_split) == 1:
-    prefix_abb = prefix
-  else:
-    prefix_abb = ''.join([k[0] for k in prefix_split])
-  return prefix_abb
-
-
-class TextLogger(object):
-  """
-  # Logstyle is either:
-  # '%#.#f' for floating point representation in text
-  # '%#.#e' for exponent representation in text
-  """
-  def __init__(self, log_root, reinitialize=False, logstyle='%10.6f'):
-    self.root = log_root
-    if log_root and not os.path.exists(self.root):
-      os.makedirs(self.root)
-    self.reinitialize = reinitialize
-    self.metrics = []
-    # One of '%3.3f' or like '%3.3e'
-    self.logstyle = logstyle
-    pass
-
-  def update(self, textlogger):
-    self.root = textlogger.root
-    self.reinitialize = textlogger.reinitialize
-    self.logstyle = textlogger.logstyle
-    pass
-
-  def reinit(self, item):
-    """
-      Delete log if re-starting and log already exists
-    """
-    if os.path.exists('%s/%s.log' % (self.root, item)):
-      os.remove('%s/%s.log' % (self.root, item))
-
-
-  def log(self, itr, **kwargs):
-    """
-    Log in plaintext;
-    """
-    os.makedirs(self.root, exist_ok=True)
-    for arg in kwargs:
-      file_path = '%s/%s.log' % (self.root, arg)
-      if arg not in self.metrics:
-        os.makedirs(os.path.dirname(file_path), exist_ok=True)
-        if self.reinitialize:
-          self.reinit(arg)
-        self.metrics += [arg]
-      with open(file_path, 'a') as f:
-        f.write('%3.4f: %s\n' % (itr, self.logstyle % kwargs[arg]))
-
-  def log_axes(self, **kwargs):
-    names = []
-    filepaths = []
-    for arg in kwargs:
-      filename = '%s/%s.log' % (self.root, arg)
-      names.append(arg)
-      filepaths.append(filename)
-
-    plot_figure(names=names, filepaths=filepaths,
-                outdir=self.root, in_one_axes=False)
-
-  def log_ax(self, **kwargs):
-    names = []
-    filepaths = []
-    for arg in kwargs:
-      filename = '%s/%s.log' % (self.root, arg)
-      names.append(arg)
-      filepaths.append(filename)
-
-    plot_figure(names=names, filepaths=filepaths,
-                outdir=self.root, in_one_axes=True)
-
-  def _get_filepath_from_dictlist(self, dict_list, in_one_figure, MAXLEN=100, ext='.png'):
-    filepaths = []
-    if in_one_figure:
-      labels = []
-      for d in dict_list:
-        labels += d.keys()
-      filename = '__'.join(labels)[:MAXLEN]
-      filepath = os.path.join(self.root, '0plot__' + filename + ext)
-      filepaths.append(filepath)
-    else:
-      for d in dict_list:
-        labels = d.keys()
-        filename = '__'.join(labels)[:MAXLEN]
-        filepath = os.path.join(self.root, '0plot__' + filename + ext)
-        filepaths.append(filepath)
-    return filepaths
-
-  def log_defaultdict2figure(self, default_dict, in_one_figure=True, save_fig_sec=300):
-    label2filepaths_list = []
-    for _, v in default_dict.items():
-      label2filepaths_list.append({subk : '%s/%s.log' % (self.root, subk) for subk, _ in v.items()})
-    filepaths = self._get_filepath_from_dictlist(dict_list=label2filepaths_list, in_one_figure=in_one_figure)
-    # save figure after a while
-    time_str = '_'.join(filepaths)
-    now = time.time()
-    last_time = getattr(TextLogger, time_str, 0)
-    if now - last_time > save_fig_sec:
-      plot_defaultdict2figure(label2filepaths_list=label2filepaths_list, filepaths=filepaths,
-                              in_one_figure=in_one_figure)
-      setattr(TextLogger, time_str, now)
-
-
-  def logstr(self, itr, **kwargs):
-    # if not comm.is_main_process():
-    #   return
-    for arg in kwargs:
-      if arg not in self.metrics:
-        if self.reinitialize:
-          self.reinit(arg)
-        self.metrics += [arg]
-      with open('%s/%s.log' % (self.root, arg), 'a') as f:
-        f.write('%3d: %s\n' % (itr, kwargs[arg]))
-
-
-def summary_defaultdict2txtfig(default_dict, prefix, step,
-                               textlogger=None, in_one_figure=True,
-                               log_txt=True, log_fig=True, save_fig_sec=100, is_main_process=True):
-  if not is_main_process:
-    return
-  if textlogger is not None:
-    prefix_abb = get_prefix_abb(prefix=prefix)
-    default_dict_copy = defaultdict(dict)
-    # add prefix_abb and key to subkey
-    for k, v in default_dict.items():
-      default_dict_copy[k] = {prefix_abb + '.' + k + '.' + subk: subv for subk, subv in v.items()}
-    default_dict = default_dict_copy
-
-    if log_txt:
-      for k, v in default_dict.items():
-        textlogger.log(step, **v)
-    if log_fig:
-      textlogger.log_defaultdict2figure(default_dict, in_one_figure=in_one_figure, save_fig_sec=save_fig_sec)
-  else:
-    print('textlogger are None!')
-
-
-def summary_dict2txtfig(dict_data, prefix, step,
-                        textlogger=None, in_one_axe=False,
-                        log_txt=True, log_fig=True, save_fig_sec=100, is_main_process=True):
-  if not is_main_process:
-    return
-  new_key_dict_data = {}
-  for k, v in dict_data.items():
-    new_k = k.replace('/', '--')
-    new_key_dict_data[new_k] = v
-  dict_data = new_key_dict_data
-
-  if in_one_axe:
-    default_dict = defaultdict(dict)
-    keys = 'sa'
-    default_dict[keys] = dict_data
-  else:
-    default_dict = defaultdict(dict)
-    keys = 'ma%d'
-    for i, (k, v) in enumerate(dict_data.items()):
-      default_dict[keys%i] = {k: v}
-  summary_defaultdict2txtfig(default_dict=default_dict, prefix=prefix, step=step,
-                             textlogger=textlogger, in_one_figure=True,
-                             log_txt=log_txt, log_fig=log_fig, save_fig_sec=save_fig_sec)
-
-
-global_textlogger = TextLogger(log_root=None)
-
-def set_global_textlogger(textlogger):
-  global global_textlogger
-  global_textlogger.update(textlogger)
+from collections import defaultdict
+import time
+import re
+import logging, os
+import datetime
+import sys
+
+from .plot_utils import plot_figure, plot_defaultdict2figure
+
+
+def get_prefix_abb(prefix):
+  # prefix_split = prefix.split('_')
+  prefix_split = re.split('_|/', prefix)
+  if len(prefix_split) == 1:
+    prefix_abb = prefix
+  else:
+    prefix_abb = ''.join([k[0] for k in prefix_split])
+  return prefix_abb
+
+
+class TextLogger(object):
+  """
+  # Logstyle is either:
+  # '%#.#f' for floating point representation in text
+  # '%#.#e' for exponent representation in text
+  """
+  def __init__(self, log_root, reinitialize=False, logstyle='%10.6f'):
+    self.root = log_root
+    if log_root and not os.path.exists(self.root):
+      os.makedirs(self.root)
+    self.reinitialize = reinitialize
+    self.metrics = []
+    # One of '%3.3f' or like '%3.3e'
+    self.logstyle = logstyle
+    pass
+
+  def update(self, textlogger):
+    self.root = textlogger.root
+    self.reinitialize = textlogger.reinitialize
+    self.logstyle = textlogger.logstyle
+    pass
+
+  def reinit(self, item):
+    """
+      Delete log if re-starting and log already exists
+    """
+    if os.path.exists('%s/%s.log' % (self.root, item)):
+      os.remove('%s/%s.log' % (self.root, item))
+
+
+  def log(self, itr, **kwargs):
+    """
+    Log in plaintext;
+    """
+    os.makedirs(self.root, exist_ok=True)
+    for arg in kwargs:
+      file_path = '%s/%s.log' % (self.root, arg)
+      if arg not in self.metrics:
+        os.makedirs(os.path.dirname(file_path), exist_ok=True)
+        if self.reinitialize:
+          self.reinit(arg)
+        self.metrics += [arg]
+      with open(file_path, 'a') as f:
+        f.write('%3.4f: %s\n' % (itr, self.logstyle % kwargs[arg]))
+
+  def log_axes(self, **kwargs):
+    names = []
+    filepaths = []
+    for arg in kwargs:
+      filename = '%s/%s.log' % (self.root, arg)
+      names.append(arg)
+      filepaths.append(filename)
+
+    plot_figure(names=names, filepaths=filepaths,
+                outdir=self.root, in_one_axes=False)
+
+  def log_ax(self, **kwargs):
+    names = []
+    filepaths = []
+    for arg in kwargs:
+      filename = '%s/%s.log' % (self.root, arg)
+      names.append(arg)
+      filepaths.append(filename)
+
+    plot_figure(names=names, filepaths=filepaths,
+                outdir=self.root, in_one_axes=True)
+
+  def _get_filepath_from_dictlist(self, dict_list, in_one_figure, MAXLEN=100, ext='.png'):
+    filepaths = []
+    if in_one_figure:
+      labels = []
+      for d in dict_list:
+        labels += d.keys()
+      filename = '__'.join(labels)[:MAXLEN]
+      filepath = os.path.join(self.root, '0plot__' + filename + ext)
+      filepaths.append(filepath)
+    else:
+      for d in dict_list:
+        labels = d.keys()
+        filename = '__'.join(labels)[:MAXLEN]
+        filepath = os.path.join(self.root, '0plot__' + filename + ext)
+        filepaths.append(filepath)
+    return filepaths
+
+  def log_defaultdict2figure(self, default_dict, in_one_figure=True, save_fig_sec=300):
+    label2filepaths_list = []
+    for _, v in default_dict.items():
+      label2filepaths_list.append({subk : '%s/%s.log' % (self.root, subk) for subk, _ in v.items()})
+    filepaths = self._get_filepath_from_dictlist(dict_list=label2filepaths_list, in_one_figure=in_one_figure)
+    # save figure after a while
+    time_str = '_'.join(filepaths)
+    now = time.time()
+    last_time = getattr(TextLogger, time_str, 0)
+    if now - last_time > save_fig_sec:
+      plot_defaultdict2figure(label2filepaths_list=label2filepaths_list, filepaths=filepaths,
+                              in_one_figure=in_one_figure)
+      setattr(TextLogger, time_str, now)
+
+
+  def logstr(self, itr, **kwargs):
+    # if not comm.is_main_process():
+    #   return
+    for arg in kwargs:
+      if arg not in self.metrics:
+        if self.reinitialize:
+          self.reinit(arg)
+        self.metrics += [arg]
+      with open('%s/%s.log' % (self.root, arg), 'a') as f:
+        f.write('%3d: %s\n' % (itr, kwargs[arg]))
+
+
+def summary_defaultdict2txtfig(default_dict, prefix, step,
+                               textlogger=None, in_one_figure=True,
+                               log_txt=True, log_fig=True, save_fig_sec=100, is_main_process=True):
+  if not is_main_process:
+    return
+  if textlogger is not None:
+    prefix_abb = get_prefix_abb(prefix=prefix)
+    default_dict_copy = defaultdict(dict)
+    # add prefix_abb and key to subkey
+    for k, v in default_dict.items():
+      default_dict_copy[k] = {prefix_abb + '.' + k + '.' + subk: subv for subk, subv in v.items()}
+    default_dict = default_dict_copy
+
+    if log_txt:
+      for k, v in default_dict.items():
+        textlogger.log(step, **v)
+    if log_fig:
+      textlogger.log_defaultdict2figure(default_dict, in_one_figure=in_one_figure, save_fig_sec=save_fig_sec)
+  else:
+    print('textlogger are None!')
+
+
+def summary_dict2txtfig(dict_data, prefix, step,
+                        textlogger=None, in_one_axe=False,
+                        log_txt=True, log_fig=True, save_fig_sec=100, is_main_process=True):
+  if not is_main_process:
+    return
+  new_key_dict_data = {}
+  for k, v in dict_data.items():
+    new_k = k.replace('/', '--')
+    new_key_dict_data[new_k] = v
+  dict_data = new_key_dict_data
+
+  if in_one_axe:
+    default_dict = defaultdict(dict)
+    keys = 'sa'
+    default_dict[keys] = dict_data
+  else:
+    default_dict = defaultdict(dict)
+    keys = 'ma%d'
+    for i, (k, v) in enumerate(dict_data.items()):
+      default_dict[keys%i] = {k: v}
+  summary_defaultdict2txtfig(default_dict=default_dict, prefix=prefix, step=step,
+                             textlogger=textlogger, in_one_figure=True,
+                             log_txt=log_txt, log_fig=log_fig, save_fig_sec=save_fig_sec)
+
+
+global_textlogger = TextLogger(log_root=None)
+
+def set_global_textlogger(textlogger):
+  global global_textlogger
+  global_textlogger.update(textlogger)
   pass
```

### Comparing `tl2-0.1.0/tl2/proj/matplot/configs/Plot.yaml` & `tl2-0.1.1/tl2/proj/matplot/configs/Plot.yaml`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,86 +1,86 @@
-plot_figure:
-  xlabel: "Iterations"
-  ylabel: "FID"
-  fontsize:
-    tick_fs: 10
-    xylabel_fs: 17
-    legend_size: 16
-  clip_x:
-    - 0
-    - 170000
-  properties:
-#    xlim:
-#      - -200000
-#      - 51000000
-    ylim:
-      - 10
-      - 45
-  get_min_value: true
-  add_auxi_label: true
-  lines:
-    pigan_r64:
-      pkl_file: "tl2_lib/tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl"
-      dict_index: "FID_r64"
-      data_index: "20210919_211014_667-pigan"
-      color: 'dark_red'
-      properties:
-        label: "pi-GAN"
-#        ls: "--"
-        lw: 2
-#        marker: "."
-    nerfgan_r64:
-      pkl_file: "tl2_lib/tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl"
-      dict_index: "FID_r64"
-      data_index: "20210920_121122_187-nerfgan-auxD-warmupDfirstlayer"
-      color: 'blue'
-      properties:
-        label: "NeRFGAN"
-#        ls: "--"
-        lw: 2
-#        marker: "."
-
-  saved_file: "nerfgan_pigan_r64r128.pdf"
-
-plot_figure_smooth:
-  xlabel: "Iterations"
-  ylabel: "FID"
-  fontsize:
-    tick_fs: 10
-    xylabel_fs: 17
-    legend_size: 16
-  clip_x:
-    - 0
-    - 170000
-#  properties:
-#    xlim:
-#      - -200000
-#      - 51000000
-#    ylim:
-#      - 10
-#      - 45
-  get_min_value: true
-  add_auxi_label: true
-  lines:
-    pigan_r64:
-      pkl_file: "tl2_lib/tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl"
-      dict_index: "FID_r64"
-      data_index: "20210919_211014_667-pigan"
-      color: 'dark_red'
-      properties:
-        label: "pi-GAN"
-#        ls: "--"
-        lw: 2
-#        marker: "."
-    nerfgan_r64:
-      pkl_file: "tl2_lib/tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl"
-      dict_index: "FID_r64"
-      data_index: "20210920_121122_187-nerfgan-auxD-warmupDfirstlayer"
-      color: 'blue'
-      properties:
-        label: "NeRFGAN"
-#        ls: "--"
-        lw: 2
-#        marker: "."
-
-  saved_file: "nerfgan_pigan_r64r128.pdf"
-
+plot_figure:
+  xlabel: "Iterations"
+  ylabel: "FID"
+  fontsize:
+    tick_fs: 10
+    xylabel_fs: 17
+    legend_size: 16
+  clip_x:
+    - 0
+    - 170000
+  properties:
+#    xlim:
+#      - -200000
+#      - 51000000
+    ylim:
+      - 10
+      - 45
+  get_min_value: true
+  add_auxi_label: true
+  lines:
+    pigan_r64:
+      pkl_file: "tl2_lib/tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl"
+      dict_index: "FID_r64"
+      data_index: "20210919_211014_667-pigan"
+      color: 'dark_red'
+      properties:
+        label: "pi-GAN"
+#        ls: "--"
+        lw: 2
+#        marker: "."
+    nerfgan_r64:
+      pkl_file: "tl2_lib/tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl"
+      dict_index: "FID_r64"
+      data_index: "20210920_121122_187-nerfgan-auxD-warmupDfirstlayer"
+      color: 'blue'
+      properties:
+        label: "NeRFGAN"
+#        ls: "--"
+        lw: 2
+#        marker: "."
+
+  saved_file: "nerfgan_pigan_r64r128.pdf"
+
+plot_figure_smooth:
+  xlabel: "Iterations"
+  ylabel: "FID"
+  fontsize:
+    tick_fs: 10
+    xylabel_fs: 17
+    legend_size: 16
+  clip_x:
+    - 0
+    - 170000
+#  properties:
+#    xlim:
+#      - -200000
+#      - 51000000
+#    ylim:
+#      - 10
+#      - 45
+  get_min_value: true
+  add_auxi_label: true
+  lines:
+    pigan_r64:
+      pkl_file: "tl2_lib/tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl"
+      dict_index: "FID_r64"
+      data_index: "20210919_211014_667-pigan"
+      color: 'dark_red'
+      properties:
+        label: "pi-GAN"
+#        ls: "--"
+        lw: 2
+#        marker: "."
+    nerfgan_r64:
+      pkl_file: "tl2_lib/tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl"
+      dict_index: "FID_r64"
+      data_index: "20210920_121122_187-nerfgan-auxD-warmupDfirstlayer"
+      color: 'blue'
+      properties:
+        label: "NeRFGAN"
+#        ls: "--"
+        lw: 2
+#        marker: "."
+
+  saved_file: "nerfgan_pigan_r64r128.pdf"
+
```

### Comparing `tl2-0.1.0/tl2/proj/matplot/data/OmniGAN_ImageNet128_results.pkl` & `tl2-0.1.1/tl2/proj/matplot/data/OmniGAN_ImageNet128_results.pkl`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl` & `tl2-0.1.1/tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/matplot/plot_results.py` & `tl2-0.1.1/tl2/proj/matplot/plot_results.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,628 +1,628 @@
-from pathlib import Path
-import sys
-import pickle
-# import matplotlib
-# matplotlib.use('agg')
-import matplotlib.pyplot as plt
-import numpy as np
-import os
-import unittest
-
-
-class PlotResults(object):
-    
-    def __init__(self, ):
-        # PlotResults.setup_env()
-        
-        pass
-    
-    @staticmethod
-    def setup_env():
-        import os
-        try:
-            import mpld3
-        except:
-            os.system('pip install mpld3')
-    
-    def get_last_md_inter_time(self, filepath):
-        from datetime import datetime, timedelta
-
-        modi_time = datetime.fromtimestamp(os.path.getmtime(filepath))
-        modi_inter = datetime.now() - modi_time
-        modi_minutes = modi_inter.total_seconds() // 60
-        return int(modi_minutes)
-    
-    def get_fig_axes(self,
-                     rows,
-                     cols,
-                     figsize_wh=(15, 7),
-                     style="seaborn-whitegrid"):
-        import matplotlib.pyplot as plt
-        # plt.style.use('ggplot')
-        plt.style.use(style)
-        plt.rcParams['axes.prop_cycle'] = plt.cycler(
-            color=['blue',
-                   'green',
-                   'red',
-                   'cyan',
-                   'magenta',
-                   'black',
-                   'orange',
-                   'lime',
-                   'tan',
-                   'salmon',
-                   'gold',
-                   'darkred',
-                   'darkblue'])
-        fig, axes = plt.subplots(rows, cols, figsize=(figsize_wh[0]*cols, figsize_wh[1]*rows))
-        if rows * cols > 1:
-            axes = axes.ravel()
-        else:
-            axes = [axes]
-        return fig, axes
-    
-    def get_itr_val_str(self, data, ismax):
-        if ismax:
-            itr = int(data[:, 0][data[:, 1].argmax()])
-            val = data[:, 1].max()
-            return f'itr.{itr:06d}_maxv.{val:.3f}'
-        else:
-            itr = int(data[:, 0][data[:, 1].argmin()])
-            val = data[:, 1].min()
-            return f'itr.{itr:06d}_minv.{val:.3f}'
-
-    def _data_load_func(self, filepath):
-        data = np.loadtxt(filepath, delimiter=':')
-        data = data.reshape(-1, 2)
-        return data
-
-    def plot_defaultdicts(self, outfigure, default_dicts, show_max=True, figsize_wh=(15, 8), legend_size=12,
-                          dpi=500, data_load_func=None):
-
-        import tempfile
-        if not isinstance(show_max, list):
-            show_max = [show_max]
-        assert len(show_max) == len(default_dicts)
-
-        fig, axes = self.get_fig_axes(rows=len(default_dicts), cols=1, figsize_wh=figsize_wh)
-
-        if data_load_func is None:
-            data_load_func_list = [self._data_load_func, ] * len(default_dicts)
-        elif not isinstance(data_load_func, (list, tuple)):
-            data_load_func_list = [data_load_func, ] * len(default_dicts)
-        else:
-            data_load_func_list = data_load_func
-
-        label2datas_list = {}
-        for idx, (dict_name, default_dict) in enumerate(default_dicts.items()):
-            data_xlim = None
-            axes_prop = default_dict.get('properties')
-            if axes_prop is not None:
-              if 'xlim' in axes_prop:
-                data_xlim = axes_prop['xlim'][-1]
-
-            label2datas = {}
-            # for each result dir
-            for (result_dir, label2file) in default_dict.items():
-                if result_dir == 'properties':
-                    continue
-                # for each texlog file
-                for label, file in label2file.items():
-                    filepath = os.path.join(result_dir, file)
-                    if not os.path.exists(filepath):
-                      print(f'Not exist {filepath}, skip.')
-                      continue
-                    # get modified time
-                    modi_minutes = self.get_last_md_inter_time(filepath)
-
-                    data = data_load_func_list[idx](filepath)
-                    # data = np.loadtxt(filepath, delimiter=':')
-                    # data = data.reshape(-1, 2)
-                    # limit x in a range
-                    if data_xlim:
-                      data = data[data[:, 0] <= data_xlim]
-                    
-                    itr_val_str = self.get_itr_val_str(data, show_max[idx])
-                    label_str = f'{itr_val_str}' + f'-{modi_minutes:03d}m---' + label
-                    
-                    axes[idx].plot(data[:, 0], data[:, 1], label=label_str, marker='.', linewidth='5', markersize='10', alpha=0.5)
-                    label2datas[label] = data
-            axes[idx].legend(prop={'size': legend_size})
-            axes[idx].set(**default_dict['properties'])
-            axes[idx].grid(visible=True, which='major', color='#666666', linestyle='--', alpha=0.2)
-                    
-            label2datas_list[dict_name] = label2datas
-        fig.show()
-        fig.savefig(outfigure, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
-        return label2datas_list
-
-    def plot_results_pkl(self,
-                         outfigure,
-                         results_pkl,
-                         show_max=[True],
-                         figsize_wh=(15, 8),
-                         legend_size=12,
-                         dpi=500,
-                         ):
-      results_pkl = Path(results_pkl)
-      title = results_pkl.stem
-
-      with open(results_pkl, 'rb') as f:
-        default_dicts = pickle.load(f)
-
-      if len(show_max) != len(default_dicts):
-        show_max = show_max * len(default_dicts)
-      assert len(show_max) == len(default_dicts)
-
-      fig, axes = self.get_fig_axes(rows=len(default_dicts), cols=1, figsize_wh=figsize_wh)
-
-      label2datas_list = {}
-      # for each subfigure
-      for idx, (dict_name, default_dict) in enumerate(default_dicts.items()):
-        label2datas = {}
-        # for each line
-        print(f"\n{dict_name}: ")
-        for (data_name, data) in default_dict.items():
-
-          itr_val_str = self.get_itr_val_str(data, show_max[idx])
-          label_str = f'{itr_val_str}---' + data_name
-
-          axes[idx].plot(data[:, 0],
-                         data[:, 1],
-                         label=label_str,
-                         marker='.',
-                         linewidth='5',
-                         markersize='10',
-                         alpha=0.5)
-          print(data_name)
-          label2datas[data_name] = data
-        axes[idx].legend(prop={'size': legend_size})
-        axes[idx].set_ylabel(dict_name, fontsize=20)
-        axes[idx].grid(b=True, which='major', color='#666666', linestyle='--', alpha=0.2)
-
-        label2datas_list[dict_name] = label2datas
-      axes[0].set(title=title)
-      fig.show()
-      print(f'Saved to {outfigure}')
-      fig.savefig(outfigure, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
-      return label2datas_list
-
-
-class TestingPlot(unittest.TestCase):
-
-    def test__plot_text(self):
-        """
-        python -c "from exp.tests.test_styleganv2 import Testing_stylegan2_style_position;\
-          Testing_stylegan2_style_position().test_plot_FID_cifar10_style_position()"
-        """
-        if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-            os.environ['CUDA_VISIBLE_DEVICES'] = '3'
-        if 'TIME_STR' not in os.environ:
-            os.environ['TIME_STR'] = '0'
-        from tl2.launch.launch_utils import \
-            (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-        command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-        argv_str = f"""
-                      --tl_config_file none
-                      --tl_command none
-                      --tl_outdir {outdir}
-                      """
-        args = setup_outdir_and_yaml(argv_str)
-        outdir = args.tl_outdir
-
-        from tl2.proj.matplot.plot_results import PlotResults
-        import collections
-        import pickle
-
-        outfigure = os.path.join(outdir, 'IS.jpg')
-        default_dicts = collections.OrderedDict()
-        show_max = []
-
-        FID_FFHQ = collections.defaultdict(dict)
-        title = 'FID_FFHQ'
-        log_file = 'textdir/eval.ma0.fid.log'
-        dd = eval(title)
-        dd['results/train_ffhq/train_ffhq-20210712_223319_587/'] = \
-            {'20210712_223319_587-pi_gan': f"{log_file}", }
-        dd['results/nerf_inr_ffhq/train_ffhq-20210716_205928_189/'] = \
-            {'20210716_205928_189-nerf_inr': f"{log_file}", }
-        dd['results/nerf_inr_ffhq_v1/train_ffhq-20210717_221038_606/'] = \
-            {'20210717_221038_606-nerf_inr_v1': f"{log_file}", }
-        dd['results/nerf_inr_ffhq_v2/train_ffhq-20210718_145212_513/'] = \
-            {'20210718_145212_513-nerf_inr_v2': f"{log_file}", }
-
-        dd['properties'] = {'title': title, }
-        default_dicts[title] = dd
-        show_max.append(False)
-
-        plotobs = PlotResults()
-        label2datas_list = plotobs.plot_defaultdicts(
-            outfigure=outfigure, default_dicts=default_dicts, show_max=show_max, figsize_wh=(16, 7.2))
-        print(f'Save to {outfigure}.')
-
-        saved_data = '__'.join(outdir.split('/')[-2:])
-        saved_data = f"{outdir}/{saved_data}.pkl"
-        with open(saved_data, 'wb') as f:
-            pickle.dump(label2datas_list, f)
-        print(f"Save data to {saved_data}")
-        pass
-
-    def test__plot_text_bucket(self):
-      """
-      python -c "from exp.tests.test_styleganv2 import Testing_stylegan2_style_position;\
-        Testing_stylegan2_style_position().test_plot_FID_cifar10_style_position()"
-      """
-      if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-      if 'TIME_STR' not in os.environ:
-        os.environ['TIME_STR'] = '0'
-      from tl2.launch.launch_utils import \
-        (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-      command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-      argv_str = f"""
-                    --tl_config_file none
-                    --tl_command none
-                    --tl_outdir {outdir}
-                    """
-      args = setup_outdir_and_yaml(argv_str)
-      outdir = args.tl_outdir
-
-      from tl2.proj.matplot.plot_results import PlotResults
-      import collections
-      import pickle
-
-      outfigure = os.path.join(outdir, 'FID.jpg')
-      default_dicts = collections.OrderedDict()
-      show_max = []
-
-      bucket_root = "/home/ma-user/work/ZhouPeng/bucket_3690/"
-
-      FID_FFHQ_r128 = collections.defaultdict(dict)
-      title = 'FID_FFHQ_r128'
-      log_file = 'textdir/eval.ma0.FID.log'
-      dd = eval(title)
-      dd[f'{bucket_root}/results/stylegan3-exp/encoder_inr_train_v2/train_ffhq_r256_softplus-20211222_120944_857'] = \
-        {'20211222_120944_857-3dmm_210': f"{log_file}", }
-
-      dd['properties'] = {'title': title,
-                          # 'xlim': [0, 3000000],
-                          # 'ylim': [0, 100]
-                          }
-      default_dicts[title] = dd
-      show_max.append(False)
-
-      plotobs = PlotResults()
-      label2datas_list = plotobs.plot_defaultdicts(
-        outfigure=outfigure, default_dicts=default_dicts, show_max=show_max, figsize_wh=(16, 7.2))
-      print(f'Save to {outfigure}.')
-
-      saved_data = '__'.join(outdir.split('/')[-2:])
-      saved_data = f"{outdir}/{saved_data}.pkl"
-      with open(saved_data, 'wb') as f:
-        pickle.dump(label2datas_list, f)
-      print(f"Save data to {saved_data}")
-      pass
-
-    def test_plot_results_pkl(self, debug=True):
-      """
-      Usage:
-          proj_root=pi-GAN-exp
-          python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
-            -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-          cd /cache/$proj_root
-          cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
-          cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
-          pip install -e tl2_lib
-
-          export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-          export TIME_STR=1
-          export PYTHONPATH=.
-          python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-            Testing_Launch_v1().test_launch_ddp(debug=False)" \
-            --tl_opts test0 10 test1 11 --test 1
-
-      :return:
-      """
-      if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-      if 'TIME_STR' not in os.environ:
-        os.environ['TIME_STR'] = '0'
-      from tl2 import tl2_utils
-      from tl2.launch.launch_utils import \
-        (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-      tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-      tl_opts = ' '.join(tl_opts_list)
-      print(f'tl_opts:\n {tl_opts}')
-      # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-      # print(f'tl_opts:\n {tl_opts}')
-
-      command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-      argv_str = f"""
-                  --tl_config_file none
-                  --tl_command none
-                  --tl_outdir {outdir}
-                  --tl_opts {tl_opts}
-                  """
-      args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-      n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-      PORT = os.environ.get('PORT', 8888)
-
-      import pickle
-      from tl2.proj.matplot.plot_results import PlotResults
-
-      results_pkl = "datasets/results/OmniGAN_ImageNet128_results.pkl"
-
-      outfigure = os.path.join(args.tl_outdir, 'FID_IS.jpg')
-      plot_obs = PlotResults()
-      show_max = [False, True]
-      label2datas_list = plot_obs.plot_results_pkl(
-        outfigure=outfigure,
-        results_pkl=results_pkl,
-        show_max=show_max,
-        figsize_wh=(16, 7.2))
-
-      pass
-
-
-    def test_plot_figure(self):
-      """
-      Usage:
-          export TIME_STR=1
-          export PYTHONPATH=./exp:./BigGAN_PyTorch_1_lib:./
-          python -c "from exp.tests.test_BigGAN_v1 import Testing_Figures;\
-            Testing_Figures().test_save_early_collapse_on_cifar100()"
-
-      :return:
-      """
-      if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-        os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-      if 'TIME_STR' not in os.environ:
-        os.environ['TIME_STR'] = '0'
-      from tl2 import tl2_utils
-      from tl2.launch.launch_utils import \
-        (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-      tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-      tl_opts = ' '.join(tl_opts_list)
-      print(f'tl_opts:\n {tl_opts}')
-      # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-      # print(f'tl_opts:\n {tl_opts}')
-
-      command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-      argv_str = f"""
-                    --tl_config_file tl2_lib/tl2/proj/matplot/configs/Plot.yaml
-                    --tl_command {command}
-                    --tl_outdir {outdir}
-                    --tl_opts {tl_opts}
-                    """
-      args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-      import matplotlib.pyplot as plt
-      import numpy as np
-      import pickle
-      import pathlib
-      from tl2.proj.matplot import plt_utils
-
-      fig, ax = plt_utils.get_fig_ax(style='seaborn-paper')
-      # fig, ax = plt.subplots()
-
-      # ax.set_xticks(range(0, 600, 100))
-      ax.tick_params(labelsize=cfg.fontsize.tick_fs)
-      ax.set_xlabel(cfg.xlabel, fontsize=cfg.fontsize.xylabel_fs)
-      ax.set_ylabel(cfg.ylabel, fontsize=cfg.fontsize.xylabel_fs)
-
-      properties = cfg.get('properties', {})
-      ax.set(**properties)
-      for idx, (_, data_dict) in enumerate(cfg.lines.items()):
-        with open(data_dict.pkl_file, 'rb') as f:
-          loaded_data = pickle.load(f)
-        data = loaded_data[data_dict.dict_index][data_dict.data_index]
-
-        if 'clip_x' in cfg:
-          data_xlim = cfg.clip_x[-1]
-          data = data[data[:, 0] <= data_xlim]
-
-        if 'clip_x' in data_dict.properties:
-          data_xlim = data_dict.properties.clip_x[-1]
-          data_dict.properties.pop('clip_x')
-          data = data[data[:, 0] <= data_xlim]
-
-        if cfg.get_min_value:
-          best_index = data[:, 1].argmin()
-        else:
-          best_index = data[:, 1].argmax()
-        best_x = int(data[:, 0][best_index])
-        best_y = data[:, 1][best_index]
-
-        if cfg.add_auxi_label:
-          data_dict.properties.label = f'x_{best_x}-y_{best_y:.3f}-' + getattr(data_dict.properties, 'label', '')
-
-        linestyle = data_dict.properties.pop('ls', 'solid')
-        if linestyle.startswith('('):
-          linestyle = eval(linestyle)
-
-        ax.plot(data[:, 0], data[:, 1], color=plt_utils.colors_dict[data_dict.color], ls=linestyle,
-                **data_dict.properties)
-        pass
-
-      plt_utils.ax_legend(ax, font_size=cfg.fontsize.legend_size, loc="upper right")
-
-      saved_file = os.path.join(args.tl_outdir, cfg.saved_file)
-      plt_utils.savefig(saved_file, fig=fig, debug=True)
-
-      pass
-
-    def test_plot_figure_smooth(self):
-      """
-      Usage:
-          export TIME_STR=1
-          export PYTHONPATH=./exp:./BigGAN_PyTorch_1_lib:./
-          python -c "from exp.tests.test_BigGAN_v1 import Testing_Figures;\
-            Testing_Figures().test_save_early_collapse_on_cifar100()"
-
-      :return:
-      """
-      if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-        os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-      if 'TIME_STR' not in os.environ:
-        os.environ['TIME_STR'] = '0'
-      from tl2 import tl2_utils
-      from tl2.launch.launch_utils import \
-        (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-      tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-      tl_opts = ' '.join(tl_opts_list)
-      print(f'tl_opts:\n {tl_opts}')
-      # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-      # print(f'tl_opts:\n {tl_opts}')
-
-      command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-      argv_str = f"""
-                  --tl_config_file tl2_lib/tl2/proj/matplot/configs/Plot.yaml
-                  --tl_command {command}
-                  --tl_outdir {outdir}
-                  --tl_opts {tl_opts}
-                  """
-      args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-      import numpy as np
-      import pickle
-      import pathlib
-      from tl2.proj.matplot import plt_utils
-      from tsmoothie.smoother import LowessSmoother, ExponentialSmoother
-
-      fig, ax = plt_utils.get_fig_ax(style='seaborn-paper')
-      # fig, ax = plt.subplots()
-
-      # ax.set_xticks(range(0, 600, 100))
-      ax.tick_params(labelsize=cfg.fontsize.tick_fs)
-      ax.set_xlabel(cfg.xlabel, fontsize=cfg.fontsize.xylabel_fs)
-      ax.set_ylabel(cfg.ylabel, fontsize=cfg.fontsize.xylabel_fs)
-
-      properties = cfg.get('properties', {})
-      ax.set(**properties)
-      for idx, (_, data_dict) in enumerate(cfg.lines.items()):
-        with open(data_dict.pkl_file, 'rb') as f:
-          loaded_data = pickle.load(f)
-        data = loaded_data[data_dict.dict_index][data_dict.data_index]
-
-        if 'clip_x' in cfg:
-          data_xlim = cfg.clip_x[-1]
-          data = data[data[:, 0] <= data_xlim]
-
-        if 'clip_x' in data_dict.properties:
-          data_xlim = data_dict.properties.clip_x[-1]
-          data_dict.properties.pop('clip_x')
-          data = data[data[:, 0] <= data_xlim]
-
-        if cfg.get_min_value:
-          best_index = data[:, 1].argmin()
-        else:
-          best_index = data[:, 1].argmax()
-        best_x = int(data[:, 0][best_index])
-        best_y = data[:, 1][best_index]
-
-        if cfg.add_auxi_label:
-          data_dict.properties.label = f'x_{best_x}-y_{best_y:.3f}-' + getattr(data_dict.properties, 'label', '')
-
-        smoother = ExponentialSmoother(window_len=40, alpha=0.3)
-        smoother.smooth(data[:, 1])
-        low, up = smoother.get_intervals('sigma_interval')
-
-        y = smoother.smooth_data[0]
-        x = data[-len(y):, 0]
-        linestyle = data_dict.properties.pop('ls', 'solid')
-        if linestyle.startswith('('):
-          linestyle = eval(linestyle)
-
-        ax.plot(x, y, color=plt_utils.colors_dict[data_dict.color], ls=linestyle, **data_dict.properties)
-        # ax.plot(x, y, '.k')
-        ax.fill_between(x, low[0], up[0], alpha=0.3, color=plt_utils.colors_dict[data_dict.color])
-        pass
-
-      plt_utils.ax_legend(ax, font_size=cfg.fontsize.legend_size, loc="upper right")
-
-      saved_file = os.path.join(args.tl_outdir, cfg.saved_file)
-      plt_utils.savefig(saved_file, fig=fig, debug=True)
-
-      pass
-
-    def test__plot_smooth_figure(self):
-      """
-      Usage:
-          export TIME_STR=1
-          export PYTHONPATH=./exp:./BigGAN_PyTorch_1_lib:./
-          python -c "from exp.tests.test_BigGAN_v1 import Testing_Figures;\
-            Testing_Figures().test_save_early_collapse_on_cifar100()"
-
-      :return:
-      """
-      if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-        os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-      if 'TIME_STR' not in os.environ:
-        os.environ['TIME_STR'] = '0'
-      from tl2 import tl2_utils
-      from tl2.launch.launch_utils import \
-        (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-      tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-      tl_opts = ' '.join(tl_opts_list)
-      print(f'tl_opts:\n {tl_opts}')
-      # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-      # print(f'tl_opts:\n {tl_opts}')
-
-      command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-      argv_str = f"""
-                    --tl_config_file none
-                    --tl_command none
-                    --tl_outdir {outdir}
-                    --tl_opts {tl_opts}
-                    """
-      args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-      import numpy as np
-      import matplotlib.pyplot as plt
-      from tsmoothie.utils_func import sim_randomwalk
-      from tsmoothie.smoother import LowessSmoother, ExponentialSmoother
-      from tl2.proj.matplot import plt_utils
-
-      fig, axs = plt_utils.get_fig_ax(nrows=3, ncols=1, style='seaborn-paper')
-
-      # generate 3 randomwalks of lenght 200
-      np.random.seed(123)
-      data = sim_randomwalk(n_series=3, timesteps=200,
-                            process_noise=10, measure_noise=30)
-
-      # operate smoothing
-      # smoother = LowessSmoother(smooth_fraction=0.1, iterations=1)
-      # smoother.smooth(data)
-      # low, up = smoother.get_intervals('prediction_interval')
-
-      smoother = ExponentialSmoother(window_len=40, alpha=0.3)
-      smoother.smooth(data)
-      low, up = smoother.get_intervals('sigma_interval')
-
-      smooth_data = smoother.smooth_data
-
-      # plot the smoothed timeseries with intervals
-      for i in range(3):
-        axs[i].plot(smooth_data[i], linewidth=3, color='blue')
-        axs[i].plot(smoother.data[i], '.k')
-
-        axs[i].set(title=f"timeseries {i + 1}")
-        axs[i].set_xlabel('time')
-
-        # x = data[-len(smoother.smooth_data[0]):, 0]
-        x = range(len(smoother.data[i]))
-        axs[i].fill_between(x, low[i], up[i], alpha=0.3)
-
-      saved_file = os.path.join(args.tl_outdir, "fig.png")
-      plt_utils.savefig(saved_file, fig=fig, debug=True)
-
-      pass
-
-
+from pathlib import Path
+import sys
+import pickle
+# import matplotlib
+# matplotlib.use('agg')
+import matplotlib.pyplot as plt
+import numpy as np
+import os
+import unittest
+
+
+class PlotResults(object):
+    
+    def __init__(self, ):
+        # PlotResults.setup_env()
+        
+        pass
+    
+    @staticmethod
+    def setup_env():
+        import os
+        try:
+            import mpld3
+        except:
+            os.system('pip install mpld3')
+    
+    def get_last_md_inter_time(self, filepath):
+        from datetime import datetime, timedelta
+
+        modi_time = datetime.fromtimestamp(os.path.getmtime(filepath))
+        modi_inter = datetime.now() - modi_time
+        modi_minutes = modi_inter.total_seconds() // 60
+        return int(modi_minutes)
+    
+    def get_fig_axes(self,
+                     rows,
+                     cols,
+                     figsize_wh=(15, 7),
+                     style="seaborn-whitegrid"):
+        import matplotlib.pyplot as plt
+        # plt.style.use('ggplot')
+        plt.style.use(style)
+        plt.rcParams['axes.prop_cycle'] = plt.cycler(
+            color=['blue',
+                   'green',
+                   'red',
+                   'cyan',
+                   'magenta',
+                   'black',
+                   'orange',
+                   'lime',
+                   'tan',
+                   'salmon',
+                   'gold',
+                   'darkred',
+                   'darkblue'])
+        fig, axes = plt.subplots(rows, cols, figsize=(figsize_wh[0]*cols, figsize_wh[1]*rows))
+        if rows * cols > 1:
+            axes = axes.ravel()
+        else:
+            axes = [axes]
+        return fig, axes
+    
+    def get_itr_val_str(self, data, ismax):
+        if ismax:
+            itr = int(data[:, 0][data[:, 1].argmax()])
+            val = data[:, 1].max()
+            return f'itr.{itr:06d}_maxv.{val:.3f}'
+        else:
+            itr = int(data[:, 0][data[:, 1].argmin()])
+            val = data[:, 1].min()
+            return f'itr.{itr:06d}_minv.{val:.3f}'
+
+    def _data_load_func(self, filepath):
+        data = np.loadtxt(filepath, delimiter=':')
+        data = data.reshape(-1, 2)
+        return data
+
+    def plot_defaultdicts(self, outfigure, default_dicts, show_max=True, figsize_wh=(15, 8), legend_size=12,
+                          dpi=500, data_load_func=None):
+
+        import tempfile
+        if not isinstance(show_max, list):
+            show_max = [show_max]
+        assert len(show_max) == len(default_dicts)
+
+        fig, axes = self.get_fig_axes(rows=len(default_dicts), cols=1, figsize_wh=figsize_wh)
+
+        if data_load_func is None:
+            data_load_func_list = [self._data_load_func, ] * len(default_dicts)
+        elif not isinstance(data_load_func, (list, tuple)):
+            data_load_func_list = [data_load_func, ] * len(default_dicts)
+        else:
+            data_load_func_list = data_load_func
+
+        label2datas_list = {}
+        for idx, (dict_name, default_dict) in enumerate(default_dicts.items()):
+            data_xlim = None
+            axes_prop = default_dict.get('properties')
+            if axes_prop is not None:
+              if 'xlim' in axes_prop:
+                data_xlim = axes_prop['xlim'][-1]
+
+            label2datas = {}
+            # for each result dir
+            for (result_dir, label2file) in default_dict.items():
+                if result_dir == 'properties':
+                    continue
+                # for each texlog file
+                for label, file in label2file.items():
+                    filepath = os.path.join(result_dir, file)
+                    if not os.path.exists(filepath):
+                      print(f'Not exist {filepath}, skip.')
+                      continue
+                    # get modified time
+                    modi_minutes = self.get_last_md_inter_time(filepath)
+
+                    data = data_load_func_list[idx](filepath)
+                    # data = np.loadtxt(filepath, delimiter=':')
+                    # data = data.reshape(-1, 2)
+                    # limit x in a range
+                    if data_xlim:
+                      data = data[data[:, 0] <= data_xlim]
+                    
+                    itr_val_str = self.get_itr_val_str(data, show_max[idx])
+                    label_str = f'{itr_val_str}' + f'-{modi_minutes:03d}m---' + label
+                    
+                    axes[idx].plot(data[:, 0], data[:, 1], label=label_str, marker='.', linewidth='5', markersize='10', alpha=0.5)
+                    label2datas[label] = data
+            axes[idx].legend(prop={'size': legend_size})
+            axes[idx].set(**default_dict['properties'])
+            axes[idx].grid(visible=True, which='major', color='#666666', linestyle='--', alpha=0.2)
+                    
+            label2datas_list[dict_name] = label2datas
+        fig.show()
+        fig.savefig(outfigure, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
+        return label2datas_list
+
+    def plot_results_pkl(self,
+                         outfigure,
+                         results_pkl,
+                         show_max=[True],
+                         figsize_wh=(15, 8),
+                         legend_size=12,
+                         dpi=500,
+                         ):
+      results_pkl = Path(results_pkl)
+      title = results_pkl.stem
+
+      with open(results_pkl, 'rb') as f:
+        default_dicts = pickle.load(f)
+
+      if len(show_max) != len(default_dicts):
+        show_max = show_max * len(default_dicts)
+      assert len(show_max) == len(default_dicts)
+
+      fig, axes = self.get_fig_axes(rows=len(default_dicts), cols=1, figsize_wh=figsize_wh)
+
+      label2datas_list = {}
+      # for each subfigure
+      for idx, (dict_name, default_dict) in enumerate(default_dicts.items()):
+        label2datas = {}
+        # for each line
+        print(f"\n{dict_name}: ")
+        for (data_name, data) in default_dict.items():
+
+          itr_val_str = self.get_itr_val_str(data, show_max[idx])
+          label_str = f'{itr_val_str}---' + data_name
+
+          axes[idx].plot(data[:, 0],
+                         data[:, 1],
+                         label=label_str,
+                         marker='.',
+                         linewidth='5',
+                         markersize='10',
+                         alpha=0.5)
+          print(data_name)
+          label2datas[data_name] = data
+        axes[idx].legend(prop={'size': legend_size})
+        axes[idx].set_ylabel(dict_name, fontsize=20)
+        axes[idx].grid(b=True, which='major', color='#666666', linestyle='--', alpha=0.2)
+
+        label2datas_list[dict_name] = label2datas
+      axes[0].set(title=title)
+      fig.show()
+      print(f'Saved to {outfigure}')
+      fig.savefig(outfigure, dpi=dpi, bbox_inches='tight', pad_inches=0.1)
+      return label2datas_list
+
+
+class TestingPlot(unittest.TestCase):
+
+    def test__plot_text(self):
+        """
+        python -c "from exp.tests.test_styleganv2 import Testing_stylegan2_style_position;\
+          Testing_stylegan2_style_position().test_plot_FID_cifar10_style_position()"
+        """
+        if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+            os.environ['CUDA_VISIBLE_DEVICES'] = '3'
+        if 'TIME_STR' not in os.environ:
+            os.environ['TIME_STR'] = '0'
+        from tl2.launch.launch_utils import \
+            (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+        command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+        argv_str = f"""
+                      --tl_config_file none
+                      --tl_command none
+                      --tl_outdir {outdir}
+                      """
+        args = setup_outdir_and_yaml(argv_str)
+        outdir = args.tl_outdir
+
+        from tl2.proj.matplot.plot_results import PlotResults
+        import collections
+        import pickle
+
+        outfigure = os.path.join(outdir, 'IS.jpg')
+        default_dicts = collections.OrderedDict()
+        show_max = []
+
+        FID_FFHQ = collections.defaultdict(dict)
+        title = 'FID_FFHQ'
+        log_file = 'textdir/eval.ma0.fid.log'
+        dd = eval(title)
+        dd['results/train_ffhq/train_ffhq-20210712_223319_587/'] = \
+            {'20210712_223319_587-pi_gan': f"{log_file}", }
+        dd['results/nerf_inr_ffhq/train_ffhq-20210716_205928_189/'] = \
+            {'20210716_205928_189-nerf_inr': f"{log_file}", }
+        dd['results/nerf_inr_ffhq_v1/train_ffhq-20210717_221038_606/'] = \
+            {'20210717_221038_606-nerf_inr_v1': f"{log_file}", }
+        dd['results/nerf_inr_ffhq_v2/train_ffhq-20210718_145212_513/'] = \
+            {'20210718_145212_513-nerf_inr_v2': f"{log_file}", }
+
+        dd['properties'] = {'title': title, }
+        default_dicts[title] = dd
+        show_max.append(False)
+
+        plotobs = PlotResults()
+        label2datas_list = plotobs.plot_defaultdicts(
+            outfigure=outfigure, default_dicts=default_dicts, show_max=show_max, figsize_wh=(16, 7.2))
+        print(f'Save to {outfigure}.')
+
+        saved_data = '__'.join(outdir.split('/')[-2:])
+        saved_data = f"{outdir}/{saved_data}.pkl"
+        with open(saved_data, 'wb') as f:
+            pickle.dump(label2datas_list, f)
+        print(f"Save data to {saved_data}")
+        pass
+
+    def test__plot_text_bucket(self):
+      """
+      python -c "from exp.tests.test_styleganv2 import Testing_stylegan2_style_position;\
+        Testing_stylegan2_style_position().test_plot_FID_cifar10_style_position()"
+      """
+      if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+      if 'TIME_STR' not in os.environ:
+        os.environ['TIME_STR'] = '0'
+      from tl2.launch.launch_utils import \
+        (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+      command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+      argv_str = f"""
+                    --tl_config_file none
+                    --tl_command none
+                    --tl_outdir {outdir}
+                    """
+      args = setup_outdir_and_yaml(argv_str)
+      outdir = args.tl_outdir
+
+      from tl2.proj.matplot.plot_results import PlotResults
+      import collections
+      import pickle
+
+      outfigure = os.path.join(outdir, 'FID.jpg')
+      default_dicts = collections.OrderedDict()
+      show_max = []
+
+      bucket_root = "/home/ma-user/work/ZhouPeng/bucket_3690/"
+
+      FID_FFHQ_r128 = collections.defaultdict(dict)
+      title = 'FID_FFHQ_r128'
+      log_file = 'textdir/eval.ma0.FID.log'
+      dd = eval(title)
+      dd[f'{bucket_root}/results/stylegan3-exp/encoder_inr_train_v2/train_ffhq_r256_softplus-20211222_120944_857'] = \
+        {'20211222_120944_857-3dmm_210': f"{log_file}", }
+
+      dd['properties'] = {'title': title,
+                          # 'xlim': [0, 3000000],
+                          # 'ylim': [0, 100]
+                          }
+      default_dicts[title] = dd
+      show_max.append(False)
+
+      plotobs = PlotResults()
+      label2datas_list = plotobs.plot_defaultdicts(
+        outfigure=outfigure, default_dicts=default_dicts, show_max=show_max, figsize_wh=(16, 7.2))
+      print(f'Save to {outfigure}.')
+
+      saved_data = '__'.join(outdir.split('/')[-2:])
+      saved_data = f"{outdir}/{saved_data}.pkl"
+      with open(saved_data, 'wb') as f:
+        pickle.dump(label2datas_list, f)
+      print(f"Save data to {saved_data}")
+      pass
+
+    def test_plot_results_pkl(self, debug=True):
+      """
+      Usage:
+          proj_root=pi-GAN-exp
+          python tl2_lib/tl2/modelarts/scripts/copy_tool.py \
+            -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+          cd /cache/$proj_root
+          cp tl2_lib/tl2/modelarts/sources/pip.conf.modelarts /root/.pip/pip.conf
+          cp tl2_lib/tl2/modelarts/sources/sources.list.modelarts /etc/apt/sources.list
+          pip install -e tl2_lib
+
+          export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+          export TIME_STR=1
+          export PYTHONPATH=.
+          python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+            Testing_Launch_v1().test_launch_ddp(debug=False)" \
+            --tl_opts test0 10 test1 11 --test 1
+
+      :return:
+      """
+      if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+      if 'TIME_STR' not in os.environ:
+        os.environ['TIME_STR'] = '0'
+      from tl2 import tl2_utils
+      from tl2.launch.launch_utils import \
+        (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+      tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+      tl_opts = ' '.join(tl_opts_list)
+      print(f'tl_opts:\n {tl_opts}')
+      # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+      # print(f'tl_opts:\n {tl_opts}')
+
+      command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+      argv_str = f"""
+                  --tl_config_file none
+                  --tl_command none
+                  --tl_outdir {outdir}
+                  --tl_opts {tl_opts}
+                  """
+      args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+      n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+      PORT = os.environ.get('PORT', 8888)
+
+      import pickle
+      from tl2.proj.matplot.plot_results import PlotResults
+
+      results_pkl = "datasets/results/OmniGAN_ImageNet128_results.pkl"
+
+      outfigure = os.path.join(args.tl_outdir, 'FID_IS.jpg')
+      plot_obs = PlotResults()
+      show_max = [False, True]
+      label2datas_list = plot_obs.plot_results_pkl(
+        outfigure=outfigure,
+        results_pkl=results_pkl,
+        show_max=show_max,
+        figsize_wh=(16, 7.2))
+
+      pass
+
+
+    def test_plot_figure(self):
+      """
+      Usage:
+          export TIME_STR=1
+          export PYTHONPATH=./exp:./BigGAN_PyTorch_1_lib:./
+          python -c "from exp.tests.test_BigGAN_v1 import Testing_Figures;\
+            Testing_Figures().test_save_early_collapse_on_cifar100()"
+
+      :return:
+      """
+      if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+        os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+      if 'TIME_STR' not in os.environ:
+        os.environ['TIME_STR'] = '0'
+      from tl2 import tl2_utils
+      from tl2.launch.launch_utils import \
+        (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+      tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+      tl_opts = ' '.join(tl_opts_list)
+      print(f'tl_opts:\n {tl_opts}')
+      # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+      # print(f'tl_opts:\n {tl_opts}')
+
+      command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+      argv_str = f"""
+                    --tl_config_file tl2_lib/tl2/proj/matplot/configs/Plot.yaml
+                    --tl_command {command}
+                    --tl_outdir {outdir}
+                    --tl_opts {tl_opts}
+                    """
+      args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+      import matplotlib.pyplot as plt
+      import numpy as np
+      import pickle
+      import pathlib
+      from tl2.proj.matplot import plt_utils
+
+      fig, ax = plt_utils.get_fig_ax(style='seaborn-paper')
+      # fig, ax = plt.subplots()
+
+      # ax.set_xticks(range(0, 600, 100))
+      ax.tick_params(labelsize=cfg.fontsize.tick_fs)
+      ax.set_xlabel(cfg.xlabel, fontsize=cfg.fontsize.xylabel_fs)
+      ax.set_ylabel(cfg.ylabel, fontsize=cfg.fontsize.xylabel_fs)
+
+      properties = cfg.get('properties', {})
+      ax.set(**properties)
+      for idx, (_, data_dict) in enumerate(cfg.lines.items()):
+        with open(data_dict.pkl_file, 'rb') as f:
+          loaded_data = pickle.load(f)
+        data = loaded_data[data_dict.dict_index][data_dict.data_index]
+
+        if 'clip_x' in cfg:
+          data_xlim = cfg.clip_x[-1]
+          data = data[data[:, 0] <= data_xlim]
+
+        if 'clip_x' in data_dict.properties:
+          data_xlim = data_dict.properties.clip_x[-1]
+          data_dict.properties.pop('clip_x')
+          data = data[data[:, 0] <= data_xlim]
+
+        if cfg.get_min_value:
+          best_index = data[:, 1].argmin()
+        else:
+          best_index = data[:, 1].argmax()
+        best_x = int(data[:, 0][best_index])
+        best_y = data[:, 1][best_index]
+
+        if cfg.add_auxi_label:
+          data_dict.properties.label = f'x_{best_x}-y_{best_y:.3f}-' + getattr(data_dict.properties, 'label', '')
+
+        linestyle = data_dict.properties.pop('ls', 'solid')
+        if linestyle.startswith('('):
+          linestyle = eval(linestyle)
+
+        ax.plot(data[:, 0], data[:, 1], color=plt_utils.colors_dict[data_dict.color], ls=linestyle,
+                **data_dict.properties)
+        pass
+
+      plt_utils.ax_legend(ax, font_size=cfg.fontsize.legend_size, loc="upper right")
+
+      saved_file = os.path.join(args.tl_outdir, cfg.saved_file)
+      plt_utils.savefig(saved_file, fig=fig, debug=True)
+
+      pass
+
+    def test_plot_figure_smooth(self):
+      """
+      Usage:
+          export TIME_STR=1
+          export PYTHONPATH=./exp:./BigGAN_PyTorch_1_lib:./
+          python -c "from exp.tests.test_BigGAN_v1 import Testing_Figures;\
+            Testing_Figures().test_save_early_collapse_on_cifar100()"
+
+      :return:
+      """
+      if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+        os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+      if 'TIME_STR' not in os.environ:
+        os.environ['TIME_STR'] = '0'
+      from tl2 import tl2_utils
+      from tl2.launch.launch_utils import \
+        (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+      tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+      tl_opts = ' '.join(tl_opts_list)
+      print(f'tl_opts:\n {tl_opts}')
+      # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+      # print(f'tl_opts:\n {tl_opts}')
+
+      command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+      argv_str = f"""
+                  --tl_config_file tl2_lib/tl2/proj/matplot/configs/Plot.yaml
+                  --tl_command {command}
+                  --tl_outdir {outdir}
+                  --tl_opts {tl_opts}
+                  """
+      args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+      import numpy as np
+      import pickle
+      import pathlib
+      from tl2.proj.matplot import plt_utils
+      from tsmoothie.smoother import LowessSmoother, ExponentialSmoother
+
+      fig, ax = plt_utils.get_fig_ax(style='seaborn-paper')
+      # fig, ax = plt.subplots()
+
+      # ax.set_xticks(range(0, 600, 100))
+      ax.tick_params(labelsize=cfg.fontsize.tick_fs)
+      ax.set_xlabel(cfg.xlabel, fontsize=cfg.fontsize.xylabel_fs)
+      ax.set_ylabel(cfg.ylabel, fontsize=cfg.fontsize.xylabel_fs)
+
+      properties = cfg.get('properties', {})
+      ax.set(**properties)
+      for idx, (_, data_dict) in enumerate(cfg.lines.items()):
+        with open(data_dict.pkl_file, 'rb') as f:
+          loaded_data = pickle.load(f)
+        data = loaded_data[data_dict.dict_index][data_dict.data_index]
+
+        if 'clip_x' in cfg:
+          data_xlim = cfg.clip_x[-1]
+          data = data[data[:, 0] <= data_xlim]
+
+        if 'clip_x' in data_dict.properties:
+          data_xlim = data_dict.properties.clip_x[-1]
+          data_dict.properties.pop('clip_x')
+          data = data[data[:, 0] <= data_xlim]
+
+        if cfg.get_min_value:
+          best_index = data[:, 1].argmin()
+        else:
+          best_index = data[:, 1].argmax()
+        best_x = int(data[:, 0][best_index])
+        best_y = data[:, 1][best_index]
+
+        if cfg.add_auxi_label:
+          data_dict.properties.label = f'x_{best_x}-y_{best_y:.3f}-' + getattr(data_dict.properties, 'label', '')
+
+        smoother = ExponentialSmoother(window_len=40, alpha=0.3)
+        smoother.smooth(data[:, 1])
+        low, up = smoother.get_intervals('sigma_interval')
+
+        y = smoother.smooth_data[0]
+        x = data[-len(y):, 0]
+        linestyle = data_dict.properties.pop('ls', 'solid')
+        if linestyle.startswith('('):
+          linestyle = eval(linestyle)
+
+        ax.plot(x, y, color=plt_utils.colors_dict[data_dict.color], ls=linestyle, **data_dict.properties)
+        # ax.plot(x, y, '.k')
+        ax.fill_between(x, low[0], up[0], alpha=0.3, color=plt_utils.colors_dict[data_dict.color])
+        pass
+
+      plt_utils.ax_legend(ax, font_size=cfg.fontsize.legend_size, loc="upper right")
+
+      saved_file = os.path.join(args.tl_outdir, cfg.saved_file)
+      plt_utils.savefig(saved_file, fig=fig, debug=True)
+
+      pass
+
+    def test__plot_smooth_figure(self):
+      """
+      Usage:
+          export TIME_STR=1
+          export PYTHONPATH=./exp:./BigGAN_PyTorch_1_lib:./
+          python -c "from exp.tests.test_BigGAN_v1 import Testing_Figures;\
+            Testing_Figures().test_save_early_collapse_on_cifar100()"
+
+      :return:
+      """
+      if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+        os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+      if 'TIME_STR' not in os.environ:
+        os.environ['TIME_STR'] = '0'
+      from tl2 import tl2_utils
+      from tl2.launch.launch_utils import \
+        (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+      tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+      tl_opts = ' '.join(tl_opts_list)
+      print(f'tl_opts:\n {tl_opts}')
+      # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+      # print(f'tl_opts:\n {tl_opts}')
+
+      command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+      argv_str = f"""
+                    --tl_config_file none
+                    --tl_command none
+                    --tl_outdir {outdir}
+                    --tl_opts {tl_opts}
+                    """
+      args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+      import numpy as np
+      import matplotlib.pyplot as plt
+      from tsmoothie.utils_func import sim_randomwalk
+      from tsmoothie.smoother import LowessSmoother, ExponentialSmoother
+      from tl2.proj.matplot import plt_utils
+
+      fig, axs = plt_utils.get_fig_ax(nrows=3, ncols=1, style='seaborn-paper')
+
+      # generate 3 randomwalks of lenght 200
+      np.random.seed(123)
+      data = sim_randomwalk(n_series=3, timesteps=200,
+                            process_noise=10, measure_noise=30)
+
+      # operate smoothing
+      # smoother = LowessSmoother(smooth_fraction=0.1, iterations=1)
+      # smoother.smooth(data)
+      # low, up = smoother.get_intervals('prediction_interval')
+
+      smoother = ExponentialSmoother(window_len=40, alpha=0.3)
+      smoother.smooth(data)
+      low, up = smoother.get_intervals('sigma_interval')
+
+      smooth_data = smoother.smooth_data
+
+      # plot the smoothed timeseries with intervals
+      for i in range(3):
+        axs[i].plot(smooth_data[i], linewidth=3, color='blue')
+        axs[i].plot(smoother.data[i], '.k')
+
+        axs[i].set(title=f"timeseries {i + 1}")
+        axs[i].set_xlabel('time')
+
+        # x = data[-len(smoother.smooth_data[0]):, 0]
+        x = range(len(smoother.data[i]))
+        axs[i].fill_between(x, low[i], up[i], alpha=0.3)
+
+      saved_file = os.path.join(args.tl_outdir, "fig.png")
+      plt_utils.savefig(saved_file, fig=fig, debug=True)
+
+      pass
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/matplot/plt_utils.py` & `tl2-0.1.1/tl2/proj/matplot/plt_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -176,16 +176,24 @@
     plt.switch_backend('TkAgg')
 
 
 def ax_legend(ax,
               font_size=20,
               loc='lower right',
               ncol=1,
-              framealpha=1):
-  ax.legend(prop={'size': font_size}, ncol=ncol, loc=loc, framealpha=framealpha)
+              framealpha=1,
+              legend_order=[]):
+  
+  if legend_order:
+    handles, labels = ax.get_legend_handles_labels()
+    plt.legend([handles[idx] for idx in legend_order], [labels[idx] for idx in legend_order],
+               prop={'size': font_size}, ncol=ncol, loc=loc, framealpha=framealpha)
+    
+  else:
+    ax.legend(prop={'size': font_size}, ncol=ncol, loc=loc, framealpha=framealpha)
   pass
 
 
 def savefig(saved_file,
             fig,
             pad_inches=0.0,
             debug=False,
```

### Comparing `tl2-0.1.0/tl2/proj/numpy/np_utils.py` & `tl2-0.1.1/tl2/proj/numpy/np_utils.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,54 +1,54 @@
-import numpy as np
-
-
-def np_choice(a, size, replace=False):
-  return np.random.choice(a, size=size, replace=replace)
-
-
-def np_deg2rad(degree):
-  rad = np.deg2rad(degree)
-  return rad
-
-
-def pd_read_csv(filepath,
-                sep=",",
-                header="infer"):
-  """
-              0         1     2   3
-    0   bbbbffdd    434343   228  D
-    1   bbbWWWff  43545343   289   E
-    2  ajkfbdafa   2345345  2312   F
-
-    print X[0]
-    0     bbbbffdd
-    1     bbbWWWff
-    2    ajkfbdafa
-
-  :param filepath:
-  :param sep:
-  :param header:
-  :return:
-  """
-  import pandas as pd
-
-  data = pd.read_csv(filepath, sep=sep, header=header)
-  return data
-
-
-def np_savez(saved_file, *args, **kwargs):
-  np.savez(saved_file, *args, **kwargs)
-  pass
-
-def np_load_dict(loaded_file, key):
-  loaded_data = np.load(loaded_file, allow_pickle=True)
-  data_dict = loaded_data[key][()]
-  return data_dict
-
-
-def get_random_state(seed):
-  # np.random.RandomState(seed).randn(w_avg_samples, G.z_dim)
-  rand_state = np.random.RandomState(seed)
-  return rand_state
-
-
-
+import numpy as np
+
+
+def np_choice(a, size, replace=False):
+  return np.random.choice(a, size=size, replace=replace)
+
+
+def np_deg2rad(degree):
+  rad = np.deg2rad(degree)
+  return rad
+
+
+def pd_read_csv(filepath,
+                sep=",",
+                header="infer"):
+  """
+              0         1     2   3
+    0   bbbbffdd    434343   228  D
+    1   bbbWWWff  43545343   289   E
+    2  ajkfbdafa   2345345  2312   F
+
+    print X[0]
+    0     bbbbffdd
+    1     bbbWWWff
+    2    ajkfbdafa
+
+  :param filepath:
+  :param sep:
+  :param header:
+  :return:
+  """
+  import pandas as pd
+
+  data = pd.read_csv(filepath, sep=sep, header=header)
+  return data
+
+
+def np_savez(saved_file, *args, **kwargs):
+  np.savez(saved_file, *args, **kwargs)
+  pass
+
+def np_load_dict(loaded_file, key):
+  loaded_data = np.load(loaded_file, allow_pickle=True)
+  data_dict = loaded_data[key][()]
+  return data_dict
+
+
+def get_random_state(seed):
+  # np.random.RandomState(seed).randn(w_avg_samples, G.z_dim)
+  rand_state = np.random.RandomState(seed)
+  return rand_state
+
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/pil/pil_utils.py` & `tl2-0.1.1/tl2/proj/pil/pil_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,15 +16,18 @@
 def pil_resize(img_pil, size=()):
   """
 
   :param img_pil:
   :param size: (w, h)
   :return:
   """
-  img_pil = img_pil.resize(size, Image.Resampling.LANCZOS)
+  try:
+    img_pil = img_pil.resize(size, Image.Resampling.LANCZOS)
+  except AttributeError:
+    img_pil = img_pil.resize(size, Image.LANCZOS)
   return img_pil
 
 
 def pil_to_bytes_jpeg(img_pil, quality=85):
   buf = BytesIO()
   img_pil.save(buf, format='jpeg', quality=quality)
   img_bytes = buf.getvalue()
@@ -157,15 +160,18 @@
   for idx, img in enumerate(image_list):
     row = idx // nrow
     col = idx % nrow
     merged_image.paste(img, (col * (max_w + pad), row * (max_h + pad)))
 
   if dst_size is not None:
     out_w, out_h = get_size(w=merged_image.size[0], h=merged_image.size[1], dst_size=dst_size, for_min_edge=False)
-    merged_image = merged_image.resize((out_w, out_h), Image.LANCZOS)
+    try:
+      merged_image = merged_image.resize((out_w, out_h), Image.Resampling.LANCZOS)
+    except AttributeError:
+      merged_image = merged_image.resize((out_w, out_h), Image.LANCZOS)
 
   if saved_file is not None:
     merged_image.save(saved_file)
   return merged_image
 
 
 def get_size(w, h, dst_size, for_min_edge=True):
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/datasets/dataset_celeba_align.py` & `tl2-0.1.1/tl2/proj/pytorch/datasets/dataset_celeba_align.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,131 +1,131 @@
-from pathlib import Path
-import PIL
-
-import torch
-from torch.utils.data import DataLoader, Dataset
-import torch.distributed as dist
-from torchvision.utils import make_grid
-import torchvision.transforms as transforms
-import torchvision.transforms.functional as trans_f
-
-from tl2.tl2_utils import read_image_list_from_files
-
-
-class CelebA_Align(Dataset):
-  """
-  python3 -m tl2.tools.get_data_list     \
-    --source_dir datasets/celeba/img_align_celeba  \
-    --outfile datasets/img_align_celeba.txt  \
-    --ext *.jpg
-  """
-
-  def __init__(self,
-               img_size,
-               image_list_file="datasets/img_align_celeba.txt",
-               verbose=False,
-               **kwargs):
-    super().__init__()
-
-    self.verbose = verbose
-    self.image_list = read_image_list_from_files(image_list_file)
-
-    assert len(self.image_list) > 0, "Can't find data; make sure you specify the path to your dataset"
-    self.transform = transforms.Compose(
-      [transforms.Resize(320),
-       transforms.CenterCrop(256),
-       transforms.ToTensor(),
-       transforms.Normalize([0.5], [0.5]),
-       transforms.RandomHorizontalFlip(p=0.5),
-       transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.NEAREST)])
-    pass
-
-  def __len__(self):
-    return len(self.image_list)
-
-  def __getitem__(self, index):
-    image_path = self.image_list[index][0]
-    X = PIL.Image.open(image_path)
-    X = self.transform(X)
-
-    if self.verbose:
-      return X, image_path
-    else:
-      return X
-
-
-def get_dataset_distributed(
-      batch_size,
-      img_size,
-      world_size,
-      rank,
-      num_workers=4,
-      shuffle=True,
-      drop_last=False,
-      pin_memory=False,
-      **kwargs):
-
-  dataset = CelebA_Align(img_size=img_size, verbose=True)
-
-  sampler = torch.utils.data.distributed.DistributedSampler(
-    dataset, num_replicas=world_size, rank=rank, shuffle=shuffle)
-  dataloader = torch.utils.data.DataLoader(
-    dataset,
-    sampler=sampler,
-    batch_size=batch_size,
-    shuffle=False,
-    num_workers=num_workers,
-    drop_last=drop_last,
-    pin_memory=pin_memory,
-  )
-
-  return dataloader
-
-
-def main(rank, world_size):
-  from tl2.proj.fvcore import global_cfg
-  from tl2.proj.pytorch.ddp import d2_comm, ddp_utils
-  from tl2.proj.pil import pil_utils
-
-  batch_size = 3
-  data_loader = get_dataset_distributed(
-    batch_size=batch_size, img_size=256, world_size=world_size, rank=rank,
-    num_workers=0, shuffle=False, )
-
-  data_iter = iter(data_loader)
-
-  data, label = next(data_iter)
-  data = data.cuda()
-
-  data_list = ddp_utils.all_gather_to_same_device(data)
-  data_list1 = ddp_utils.gather_to_same_device(data)
-  label_list = d2_comm.all_gather(label)
-  label_name = ""
-  for labels in label_list:
-    for label in labels:
-      label = Path(label)
-      label_name += f"{label.stem}_"
-    label_name += "\n"
-
-  if data_list1:
-    data = torch.cat(data_list1, dim=0)
-    merged_data = make_grid(data, nrow=batch_size, normalize=True, scale_each=True)
-    img_pil = trans_f.to_pil_image(merged_data)
-    caption = f"{data.shape}\n{label_name}"
-    caption = caption.strip('\n')
-    pil_utils.imshow_pil(img_pil, title=caption)
-
-  d2_comm.synchronize()
-  pass
-
-
-if __name__ == '__main__':
-  from tl2.proj.pytorch.ddp import ddp_utils
-  from tl2.launch.launch_utils import update_parser_defaults_from_yaml
-
-  rank = ddp_utils.parser_local_rank()
-  parser = update_parser_defaults_from_yaml(parser=None, is_main_process=(rank == 0))
-
-  rank, world_size = ddp_utils.ddp_init()
-
-  main(rank, world_size)
-
+from pathlib import Path
+import PIL
+
+import torch
+from torch.utils.data import DataLoader, Dataset
+import torch.distributed as dist
+from torchvision.utils import make_grid
+import torchvision.transforms as transforms
+import torchvision.transforms.functional as trans_f
+
+from tl2.tl2_utils import read_image_list_from_files
+
+
+class CelebA_Align(Dataset):
+  """
+  python3 -m tl2.tools.get_data_list     \
+    --source_dir datasets/celeba/img_align_celeba  \
+    --outfile datasets/img_align_celeba.txt  \
+    --ext *.jpg
+  """
+
+  def __init__(self,
+               img_size,
+               image_list_file="datasets/img_align_celeba.txt",
+               verbose=False,
+               **kwargs):
+    super().__init__()
+
+    self.verbose = verbose
+    self.image_list = read_image_list_from_files(image_list_file)
+
+    assert len(self.image_list) > 0, "Can't find data; make sure you specify the path to your dataset"
+    self.transform = transforms.Compose(
+      [transforms.Resize(320),
+       transforms.CenterCrop(256),
+       transforms.ToTensor(),
+       transforms.Normalize([0.5], [0.5]),
+       transforms.RandomHorizontalFlip(p=0.5),
+       transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.NEAREST)])
+    pass
+
+  def __len__(self):
+    return len(self.image_list)
+
+  def __getitem__(self, index):
+    image_path = self.image_list[index][0]
+    X = PIL.Image.open(image_path)
+    X = self.transform(X)
+
+    if self.verbose:
+      return X, image_path
+    else:
+      return X
+
+
+def get_dataset_distributed(
+      batch_size,
+      img_size,
+      world_size,
+      rank,
+      num_workers=4,
+      shuffle=True,
+      drop_last=False,
+      pin_memory=False,
+      **kwargs):
+
+  dataset = CelebA_Align(img_size=img_size, verbose=True)
+
+  sampler = torch.utils.data.distributed.DistributedSampler(
+    dataset, num_replicas=world_size, rank=rank, shuffle=shuffle)
+  dataloader = torch.utils.data.DataLoader(
+    dataset,
+    sampler=sampler,
+    batch_size=batch_size,
+    shuffle=False,
+    num_workers=num_workers,
+    drop_last=drop_last,
+    pin_memory=pin_memory,
+  )
+
+  return dataloader
+
+
+def main(rank, world_size):
+  from tl2.proj.fvcore import global_cfg
+  from tl2.proj.pytorch.ddp import d2_comm, ddp_utils
+  from tl2.proj.pil import pil_utils
+
+  batch_size = 3
+  data_loader = get_dataset_distributed(
+    batch_size=batch_size, img_size=256, world_size=world_size, rank=rank,
+    num_workers=0, shuffle=False, )
+
+  data_iter = iter(data_loader)
+
+  data, label = next(data_iter)
+  data = data.cuda()
+
+  data_list = ddp_utils.all_gather_to_same_device(data)
+  data_list1 = ddp_utils.gather_to_same_device(data)
+  label_list = d2_comm.all_gather(label)
+  label_name = ""
+  for labels in label_list:
+    for label in labels:
+      label = Path(label)
+      label_name += f"{label.stem}_"
+    label_name += "\n"
+
+  if data_list1:
+    data = torch.cat(data_list1, dim=0)
+    merged_data = make_grid(data, nrow=batch_size, normalize=True, scale_each=True)
+    img_pil = trans_f.to_pil_image(merged_data)
+    caption = f"{data.shape}\n{label_name}"
+    caption = caption.strip('\n')
+    pil_utils.imshow_pil(img_pil, title=caption)
+
+  d2_comm.synchronize()
+  pass
+
+
+if __name__ == '__main__':
+  from tl2.proj.pytorch.ddp import ddp_utils
+  from tl2.launch.launch_utils import update_parser_defaults_from_yaml
+
+  rank = ddp_utils.parser_local_rank()
+  parser = update_parser_defaults_from_yaml(parser=None, is_main_process=(rank == 0))
+
+  rank, world_size = ddp_utils.ddp_init()
+
+  main(rank, world_size)
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/datasets/dataset_danbooru2019_portraits.py` & `tl2-0.1.1/tl2/proj/pytorch/datasets/dataset_image_list.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,135 +1,139 @@
-import logging
-from pathlib import Path
-import PIL
-
-import torch
-from torch.utils.data import DataLoader, Dataset
-import torch.distributed as dist
-from torchvision.utils import make_grid
-import torchvision.transforms as transforms
-import torchvision.transforms.functional as trans_f
-
-from tl2.tl2_utils import read_image_list_from_files
-
-
-class Danbooru2019_Portraits(Dataset):
-  """
-  python3 -m tl2.tools.get_data_list     \
-    --source_dir datasets/Danbooru2019_Portraits/Danbooru2019_Portraits  \
-    --outfile datasets/Danbooru2019_Portraits.txt  \
-    --ext *.jpg
-  """
-
-  def __init__(self,
-               img_size,
-               image_list_file="datasets/Danbooru2019_Portraits/Danbooru2019_Portraits_rm_zero_width_gt_5.txt",
-               verbose=False,
-               **kwargs):
-    super().__init__()
-
-    self.verbose = verbose
-    self.image_list = read_image_list_from_files(image_list_file, compress=True)
-
-    assert len(self.image_list) > 0, "Can't find data; make sure you specify the path to your dataset"
-    self.transform = transforms.Compose(
-      [
-        transforms.ToTensor(),
-        transforms.Normalize([0.5], [0.5]),
-        transforms.RandomHorizontalFlip(p=0.5),
-        transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True)
-      ])
-
-    logger = logging.getLogger('tl')
-    logger.info(f"\nNum of images ({image_list_file}):\n {len(self)}")
-    pass
-
-  def __len__(self):
-    return len(self.image_list)
-
-  def __getitem__(self, index):
-    image_path = self.image_list[index]
-    X = PIL.Image.open(image_path)
-    X = self.transform(X)
-
-    if self.verbose:
-      return X, image_path
-    else:
-      return X
-
-
-def get_dataset_distributed(
-      batch_size,
-      img_size,
-      world_size,
-      rank,
-      num_workers=4,
-      shuffle=True,
-      drop_last=False,
-      pin_memory=False,
-      **kwargs):
-
-  dataset = Danbooru2019_Portraits(img_size=img_size, verbose=True)
-
-  sampler = torch.utils.data.distributed.DistributedSampler(
-    dataset, num_replicas=world_size, rank=rank, shuffle=shuffle)
-  dataloader = torch.utils.data.DataLoader(
-    dataset,
-    sampler=sampler,
-    batch_size=batch_size,
-    shuffle=False,
-    num_workers=num_workers,
-    drop_last=drop_last,
-    pin_memory=pin_memory,
-  )
-
-  return dataloader
-
-
-def main(rank, world_size):
-  from tl2.proj.fvcore import global_cfg
-  from tl2.proj.pytorch.ddp import d2_comm, ddp_utils
-  from tl2.proj.pil import pil_utils
-
-  batch_size = 3
-  data_loader = get_dataset_distributed(
-    batch_size=batch_size, img_size=128, world_size=world_size, rank=rank,
-    num_workers=0, shuffle=False, )
-
-  data_iter = iter(data_loader)
-
-  data, label = next(data_iter)
-  data = data.cuda()
-
-  data_list = ddp_utils.all_gather_to_same_device(data)
-  data_list1 = ddp_utils.gather_to_same_device(data)
-  label_list = d2_comm.all_gather(label)
-  label_name = ""
-  for labels in label_list:
-    for label in labels:
-      label = Path(label)
-      label_name += f"{label.stem}_"
-    label_name += "\n"
-
-  if data_list1:
-    data = torch.cat(data_list1, dim=0)
-    merged_data = make_grid(data, nrow=batch_size, normalize=True, scale_each=True)
-    img_pil = trans_f.to_pil_image(merged_data)
-    caption = f"{data.shape}\n{label_name}"
-    caption = caption.strip('\n')
-    pil_utils.imshow_pil(img_pil, title=caption)
-
-  d2_comm.synchronize()
-  pass
-
-
-if __name__ == '__main__':
-  from tl2.proj.pytorch.ddp import ddp_utils
-  from tl2.launch.launch_utils import update_parser_defaults_from_yaml
-
-  rank = ddp_utils.parser_local_rank()
-  parser = update_parser_defaults_from_yaml(parser=None, is_main_process=(rank == 0))
-
-  rank, world_size = ddp_utils.ddp_init()
-
-  main(rank, world_size)
-
+import logging
+from pathlib import Path
+import PIL
+
+import torch
+from torch.utils.data import DataLoader, Dataset
+import torch.distributed as dist
+from torchvision.utils import make_grid
+import torchvision.transforms as transforms
+import torchvision.transforms.functional as trans_f
+
+from tl2.tl2_utils import read_image_list_from_files
+
+
+class ImageList(Dataset):
+  """
+  python3 -m tl2.tools.get_data_list     \
+    --source_dir datasets/Danbooru2019_Portraits/Danbooru2019_Portraits  \
+    --outfile datasets/Danbooru2019_Portraits.txt  \
+    --ext *.jpg
+  """
+
+  def __init__(self,
+               img_size,
+               image_list_file="tl2_lib/tl2/proj/dlib/datasets/raw_face_list.txt",
+               transform=None,
+               verbose=False,
+               **kwargs):
+    super().__init__()
+
+    self.verbose = verbose
+    self.image_list = read_image_list_from_files(image_list_file, compress=True)
+
+    assert len(self.image_list) > 0, "Can't find data; make sure you specify the path to your dataset"
+
+    if transform is None:
+      self.transform = transforms.Compose(
+        [
+          transforms.ToTensor(),
+          transforms.Normalize([0.5], [0.5]),
+          transforms.RandomHorizontalFlip(p=0.5),
+          transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True)
+        ])
+    else:
+      self.transform = transform
+
+    logger = logging.getLogger('tl')
+    logger.info(f"\nNum of images ({image_list_file}): {len(self)}")
+    pass
+
+  def __len__(self):
+    return len(self.image_list)
+
+  def __getitem__(self, index):
+    image_path = self.image_list[index]
+    X = PIL.Image.open(image_path)
+    X = self.transform(X)
+
+    if self.verbose:
+      return X, index
+    else:
+      return X
+
+
+def get_dataset_distributed(
+      batch_size,
+      img_size,
+      world_size,
+      rank,
+      num_workers=4,
+      shuffle=True,
+      drop_last=False,
+      pin_memory=False,
+      **kwargs):
+
+  dataset = ImageList(img_size=img_size, verbose=True)
+
+  sampler = torch.utils.data.distributed.DistributedSampler(
+    dataset, num_replicas=world_size, rank=rank, shuffle=shuffle)
+  dataloader = torch.utils.data.DataLoader(
+    dataset,
+    sampler=sampler,
+    batch_size=batch_size,
+    shuffle=False,
+    num_workers=num_workers,
+    drop_last=drop_last,
+    pin_memory=pin_memory,
+  )
+
+  return dataloader
+
+
+def main(rank, world_size):
+  from tl2.proj.fvcore import global_cfg
+  from tl2.proj.pytorch.ddp import d2_comm, ddp_utils
+  from tl2.proj.pil import pil_utils
+
+  batch_size = 3
+  data_loader = get_dataset_distributed(
+    batch_size=batch_size, img_size=128, world_size=world_size, rank=rank,
+    num_workers=0, shuffle=False, )
+
+  data_iter = iter(data_loader)
+
+  data, label = next(data_iter)
+  data = data.cuda()
+
+  data_list = ddp_utils.all_gather_to_same_device(data)
+  data_list1 = ddp_utils.gather_to_same_device(data)
+  label_list = d2_comm.all_gather(label)
+  label_name = ""
+  for labels in label_list:
+    for label in labels:
+      label_name += f"{label.item()}_"
+    label_name += "\n"
+
+  if data_list1:
+    data = torch.cat(data_list1, dim=0)
+    merged_data = make_grid(data, nrow=batch_size, normalize=True, scale_each=True)
+    img_pil = trans_f.to_pil_image(merged_data)
+    caption = f"{data.shape}\n{label_name}"
+    caption = caption.strip('\n')
+    pil_utils.imshow_pil(img_pil, title=caption)
+
+  d2_comm.synchronize()
+  pass
+
+
+if __name__ == '__main__':
+  from tl2.proj.pytorch.ddp import ddp_utils
+  from tl2.launch.launch_utils import update_parser_defaults_from_yaml
+
+  rank = ddp_utils.parser_local_rank()
+  parser = update_parser_defaults_from_yaml(parser=None, is_main_process=(rank == 0))
+
+  rank, world_size = ddp_utils.ddp_init()
+
+  main(rank, world_size)
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/datasets/dataset_image_list.py` & `tl2-0.1.1/tl2/proj/pytorch/datasets/dataset_danbooru2019_portraits.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,139 +1,135 @@
-import logging
-from pathlib import Path
-import PIL
-
-import torch
-from torch.utils.data import DataLoader, Dataset
-import torch.distributed as dist
-from torchvision.utils import make_grid
-import torchvision.transforms as transforms
-import torchvision.transforms.functional as trans_f
-
-from tl2.tl2_utils import read_image_list_from_files
-
-
-class ImageList(Dataset):
-  """
-  python3 -m tl2.tools.get_data_list     \
-    --source_dir datasets/Danbooru2019_Portraits/Danbooru2019_Portraits  \
-    --outfile datasets/Danbooru2019_Portraits.txt  \
-    --ext *.jpg
-  """
-
-  def __init__(self,
-               img_size,
-               image_list_file="tl2_lib/tl2/proj/dlib/datasets/raw_face_list.txt",
-               transform=None,
-               verbose=False,
-               **kwargs):
-    super().__init__()
-
-    self.verbose = verbose
-    self.image_list = read_image_list_from_files(image_list_file, compress=True)
-
-    assert len(self.image_list) > 0, "Can't find data; make sure you specify the path to your dataset"
-
-    if transform is None:
-      self.transform = transforms.Compose(
-        [
-          transforms.ToTensor(),
-          transforms.Normalize([0.5], [0.5]),
-          transforms.RandomHorizontalFlip(p=0.5),
-          transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True)
-        ])
-    else:
-      self.transform = transform
-
-    logger = logging.getLogger('tl')
-    logger.info(f"\nNum of images ({image_list_file}): {len(self)}")
-    pass
-
-  def __len__(self):
-    return len(self.image_list)
-
-  def __getitem__(self, index):
-    image_path = self.image_list[index]
-    X = PIL.Image.open(image_path)
-    X = self.transform(X)
-
-    if self.verbose:
-      return X, index
-    else:
-      return X
-
-
-def get_dataset_distributed(
-      batch_size,
-      img_size,
-      world_size,
-      rank,
-      num_workers=4,
-      shuffle=True,
-      drop_last=False,
-      pin_memory=False,
-      **kwargs):
-
-  dataset = ImageList(img_size=img_size, verbose=True)
-
-  sampler = torch.utils.data.distributed.DistributedSampler(
-    dataset, num_replicas=world_size, rank=rank, shuffle=shuffle)
-  dataloader = torch.utils.data.DataLoader(
-    dataset,
-    sampler=sampler,
-    batch_size=batch_size,
-    shuffle=False,
-    num_workers=num_workers,
-    drop_last=drop_last,
-    pin_memory=pin_memory,
-  )
-
-  return dataloader
-
-
-def main(rank, world_size):
-  from tl2.proj.fvcore import global_cfg
-  from tl2.proj.pytorch.ddp import d2_comm, ddp_utils
-  from tl2.proj.pil import pil_utils
-
-  batch_size = 3
-  data_loader = get_dataset_distributed(
-    batch_size=batch_size, img_size=128, world_size=world_size, rank=rank,
-    num_workers=0, shuffle=False, )
-
-  data_iter = iter(data_loader)
-
-  data, label = next(data_iter)
-  data = data.cuda()
-
-  data_list = ddp_utils.all_gather_to_same_device(data)
-  data_list1 = ddp_utils.gather_to_same_device(data)
-  label_list = d2_comm.all_gather(label)
-  label_name = ""
-  for labels in label_list:
-    for label in labels:
-      label_name += f"{label.item()}_"
-    label_name += "\n"
-
-  if data_list1:
-    data = torch.cat(data_list1, dim=0)
-    merged_data = make_grid(data, nrow=batch_size, normalize=True, scale_each=True)
-    img_pil = trans_f.to_pil_image(merged_data)
-    caption = f"{data.shape}\n{label_name}"
-    caption = caption.strip('\n')
-    pil_utils.imshow_pil(img_pil, title=caption)
-
-  d2_comm.synchronize()
-  pass
-
-
-if __name__ == '__main__':
-  from tl2.proj.pytorch.ddp import ddp_utils
-  from tl2.launch.launch_utils import update_parser_defaults_from_yaml
-
-  rank = ddp_utils.parser_local_rank()
-  parser = update_parser_defaults_from_yaml(parser=None, is_main_process=(rank == 0))
-
-  rank, world_size = ddp_utils.ddp_init()
-
-  main(rank, world_size)
-
+import logging
+from pathlib import Path
+import PIL
+
+import torch
+from torch.utils.data import DataLoader, Dataset
+import torch.distributed as dist
+from torchvision.utils import make_grid
+import torchvision.transforms as transforms
+import torchvision.transforms.functional as trans_f
+
+from tl2.tl2_utils import read_image_list_from_files
+
+
+class Danbooru2019_Portraits(Dataset):
+  """
+  python3 -m tl2.tools.get_data_list     \
+    --source_dir datasets/Danbooru2019_Portraits/Danbooru2019_Portraits  \
+    --outfile datasets/Danbooru2019_Portraits.txt  \
+    --ext *.jpg
+  """
+
+  def __init__(self,
+               img_size,
+               image_list_file="datasets/Danbooru2019_Portraits/Danbooru2019_Portraits_rm_zero_width_gt_5.txt",
+               verbose=False,
+               **kwargs):
+    super().__init__()
+
+    self.verbose = verbose
+    self.image_list = read_image_list_from_files(image_list_file, compress=True)
+
+    assert len(self.image_list) > 0, "Can't find data; make sure you specify the path to your dataset"
+    self.transform = transforms.Compose(
+      [
+        transforms.ToTensor(),
+        transforms.Normalize([0.5], [0.5]),
+        transforms.RandomHorizontalFlip(p=0.5),
+        transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True)
+      ])
+
+    logger = logging.getLogger('tl')
+    logger.info(f"\nNum of images ({image_list_file}):\n {len(self)}")
+    pass
+
+  def __len__(self):
+    return len(self.image_list)
+
+  def __getitem__(self, index):
+    image_path = self.image_list[index]
+    X = PIL.Image.open(image_path)
+    X = self.transform(X)
+
+    if self.verbose:
+      return X, image_path
+    else:
+      return X
+
+
+def get_dataset_distributed(
+      batch_size,
+      img_size,
+      world_size,
+      rank,
+      num_workers=4,
+      shuffle=True,
+      drop_last=False,
+      pin_memory=False,
+      **kwargs):
+
+  dataset = Danbooru2019_Portraits(img_size=img_size, verbose=True)
+
+  sampler = torch.utils.data.distributed.DistributedSampler(
+    dataset, num_replicas=world_size, rank=rank, shuffle=shuffle)
+  dataloader = torch.utils.data.DataLoader(
+    dataset,
+    sampler=sampler,
+    batch_size=batch_size,
+    shuffle=False,
+    num_workers=num_workers,
+    drop_last=drop_last,
+    pin_memory=pin_memory,
+  )
+
+  return dataloader
+
+
+def main(rank, world_size):
+  from tl2.proj.fvcore import global_cfg
+  from tl2.proj.pytorch.ddp import d2_comm, ddp_utils
+  from tl2.proj.pil import pil_utils
+
+  batch_size = 3
+  data_loader = get_dataset_distributed(
+    batch_size=batch_size, img_size=128, world_size=world_size, rank=rank,
+    num_workers=0, shuffle=False, )
+
+  data_iter = iter(data_loader)
+
+  data, label = next(data_iter)
+  data = data.cuda()
+
+  data_list = ddp_utils.all_gather_to_same_device(data)
+  data_list1 = ddp_utils.gather_to_same_device(data)
+  label_list = d2_comm.all_gather(label)
+  label_name = ""
+  for labels in label_list:
+    for label in labels:
+      label = Path(label)
+      label_name += f"{label.stem}_"
+    label_name += "\n"
+
+  if data_list1:
+    data = torch.cat(data_list1, dim=0)
+    merged_data = make_grid(data, nrow=batch_size, normalize=True, scale_each=True)
+    img_pil = trans_f.to_pil_image(merged_data)
+    caption = f"{data.shape}\n{label_name}"
+    caption = caption.strip('\n')
+    pil_utils.imshow_pil(img_pil, title=caption)
+
+  d2_comm.synchronize()
+  pass
+
+
+if __name__ == '__main__':
+  from tl2.proj.pytorch.ddp import ddp_utils
+  from tl2.launch.launch_utils import update_parser_defaults_from_yaml
+
+  rank = ddp_utils.parser_local_rank()
+  parser = update_parser_defaults_from_yaml(parser=None, is_main_process=(rank == 0))
+
+  rank, world_size = ddp_utils.ddp_init()
+
+  main(rank, world_size)
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/datasets/tests/test_datasets.py` & `tl2-0.1.1/tl2/proj/pytorch/datasets/tests/test_datasets.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,233 +1,233 @@
-import os
-import sys
-import unittest
-
-
-class Testing_datasets(unittest.TestCase):
-
-  def test_dataset_celeba_align(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2_lib.tl2.proj.pytorch.datasets.tests.test_datasets import Testing_datasets;\
-          Testing_datasets().test_dataset_celeba_align(debug=False)"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-    PORT = os.environ.get('PORT', 8888)
-
-    cmd_str = f"""
-        python -m torch.distributed.launch --nproc_per_node={n_gpus} --master_port={PORT} 
-        tl2_lib/tl2/proj/pytorch/datasets/dataset_celeba_align.py
-        {get_append_cmd_str(args)}
-        """
-    if debug:
-      cmd_str += f"""
-                  --tl_debug
-                  --tl_opts 
-                  """
-    else:
-      cmd_str += f"""
-                  --tl_opts {tl_opts}
-                  """
-    start_cmd_run(cmd_str)
-    # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
-    # from template_lib.modelarts import modelarts_utils
-    # update_parser_defaults_from_yaml(parser)
-
-    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
-    #
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    pass
-
-  def test_dataset_danbooru2019_portraits(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2_lib.tl2.proj.pytorch.datasets.tests.test_datasets import Testing_datasets;\
-          Testing_datasets().test_dataset_celeba_align(debug=False)"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-    PORT = os.environ.get('PORT', 8888)
-
-    cmd_str = f"""
-        python -m torch.distributed.launch --nproc_per_node={n_gpus} --master_port={PORT} 
-        tl2_lib/tl2/proj/pytorch/datasets/dataset_danbooru2019_portraits.py
-        {get_append_cmd_str(args)}
-        """
-    if debug:
-      cmd_str += f"""
-                  --tl_debug
-                  --tl_opts 
-                  """
-    else:
-      cmd_str += f"""
-                  --tl_opts {tl_opts}
-                  """
-    start_cmd_run(cmd_str)
-    # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
-    # from template_lib.modelarts import modelarts_utils
-    # update_parser_defaults_from_yaml(parser)
-
-    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
-    #
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    pass
-
-  def test_dataset_image_list(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2_lib.tl2.proj.pytorch.datasets.tests.test_datasets import Testing_datasets;\
-          Testing_datasets().test_dataset_celeba_align(debug=False)"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-    PORT = os.environ.get('PORT', 8888)
-
-    cmd_str = f"""
-        python -m torch.distributed.launch --nproc_per_node={n_gpus} --master_port={PORT} 
-        tl2_lib/tl2/proj/pytorch/datasets/dataset_image_list.py
-        {get_append_cmd_str(args)}
-        """
-    if debug:
-      cmd_str += f"""
-                  --tl_debug
-                  --tl_opts 
-                  """
-    else:
-      cmd_str += f"""
-                  --tl_opts {tl_opts}
-                  """
-    start_cmd_run(cmd_str)
-    # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
-    # from template_lib.modelarts import modelarts_utils
-    # update_parser_defaults_from_yaml(parser)
-
-    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
-    #
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    pass
-
-
+import os
+import sys
+import unittest
+
+
+class Testing_datasets(unittest.TestCase):
+
+  def test_dataset_celeba_align(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2_lib.tl2.proj.pytorch.datasets.tests.test_datasets import Testing_datasets;\
+          Testing_datasets().test_dataset_celeba_align(debug=False)"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    PORT = os.environ.get('PORT', 8888)
+
+    cmd_str = f"""
+        python -m torch.distributed.launch --nproc_per_node={n_gpus} --master_port={PORT} 
+        tl2_lib/tl2/proj/pytorch/datasets/dataset_celeba_align.py
+        {get_append_cmd_str(args)}
+        """
+    if debug:
+      cmd_str += f"""
+                  --tl_debug
+                  --tl_opts 
+                  """
+    else:
+      cmd_str += f"""
+                  --tl_opts {tl_opts}
+                  """
+    start_cmd_run(cmd_str)
+    # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
+    # from template_lib.modelarts import modelarts_utils
+    # update_parser_defaults_from_yaml(parser)
+
+    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
+    #
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    pass
+
+  def test_dataset_danbooru2019_portraits(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2_lib.tl2.proj.pytorch.datasets.tests.test_datasets import Testing_datasets;\
+          Testing_datasets().test_dataset_celeba_align(debug=False)"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    PORT = os.environ.get('PORT', 8888)
+
+    cmd_str = f"""
+        python -m torch.distributed.launch --nproc_per_node={n_gpus} --master_port={PORT} 
+        tl2_lib/tl2/proj/pytorch/datasets/dataset_danbooru2019_portraits.py
+        {get_append_cmd_str(args)}
+        """
+    if debug:
+      cmd_str += f"""
+                  --tl_debug
+                  --tl_opts 
+                  """
+    else:
+      cmd_str += f"""
+                  --tl_opts {tl_opts}
+                  """
+    start_cmd_run(cmd_str)
+    # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
+    # from template_lib.modelarts import modelarts_utils
+    # update_parser_defaults_from_yaml(parser)
+
+    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
+    #
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    pass
+
+  def test_dataset_image_list(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2_lib.tl2.proj.pytorch.datasets.tests.test_datasets import Testing_datasets;\
+          Testing_datasets().test_dataset_celeba_align(debug=False)"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    PORT = os.environ.get('PORT', 8888)
+
+    cmd_str = f"""
+        python -m torch.distributed.launch --nproc_per_node={n_gpus} --master_port={PORT} 
+        tl2_lib/tl2/proj/pytorch/datasets/dataset_image_list.py
+        {get_append_cmd_str(args)}
+        """
+    if debug:
+      cmd_str += f"""
+                  --tl_debug
+                  --tl_opts 
+                  """
+    else:
+      cmd_str += f"""
+                  --tl_opts {tl_opts}
+                  """
+    start_cmd_run(cmd_str)
+    # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
+    # from template_lib.modelarts import modelarts_utils
+    # update_parser_defaults_from_yaml(parser)
+
+    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
+    #
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    pass
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/ddp/__pycache__/d2_comm.cpython-38.pyc` & `tl2-0.1.1/tl2/proj/pytorch/ddp/__pycache__/d2_comm.cpython-38.pyc`

 * *Format-specific differences are supported for Python .pyc files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: Byte-compiled Python module for CPython 3.8, timestamp-based, .py timestamp: Mon Oct  3 09:27:55 2022 UTC, .py size: 7730 bytes*

 * *Could not decompile bytecode: bad marshal data (unknown type code)*

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-00000000: 550d 0d0a 0000 0000 1bab 3a63 321e 0000  U.........:c2...
+00000000: 550d 0d0a 0000 0000 f13c 9362 391f 0000  U........<.b9...
 00000010: e300 0000 0000 0000 0000 0000 0000 0000  ................
 00000020: 0003 0000 0040 0000 0073 d400 0000 6400  .....@...s....d.
 00000030: 5a00 6401 6402 6c01 5a01 6401 6402 6c02  Z.d.d.l.Z.d.d.l.
 00000040: 5a02 6401 6402 6c03 5a04 6401 6402 6c05  Z.d.d.l.Z.d.d.l.
 00000050: 5a05 6401 6402 6c06 5a06 6401 6402 6c07  Z.d.d.l.Z.d.d.l.
 00000060: 6d08 5a09 0100 6402 5a0a 650b 6403 9c01  m.Z...d.Z.e.d...
 00000070: 6404 6405 8404 5a0c 650b 6403 9c01 6406  d.d...Z.e.d...d.
@@ -26,429 +26,427 @@
 00000190: 0000 0002 0000 0043 0000 0073 2000 0000  .......C...s ...
 000001a0: 7400 a001 a100 730c 6401 5300 7400 a002  t.....s.d.S.t...
 000001b0: a100 7318 6401 5300 7400 a003 a100 5300  ..s.d.S.t.....S.
 000001c0: 2902 4ee9 0100 0000 2904 da04 6469 7374  ).N.....)...dist
 000001d0: da0c 6973 5f61 7661 696c 6162 6c65 da0e  ..is_available..
 000001e0: 6973 5f69 6e69 7469 616c 697a 6564 da0e  is_initialized..
 000001f0: 6765 745f 776f 726c 645f 7369 7a65 a900  get_world_size..
-00000200: 7208 0000 0072 0800 0000 fa59 2f68 6f6d  r....r.....Y/hom
+00000200: 7208 0000 0072 0800 0000 fa3b 2f68 6f6d  r....r.....;/hom
 00000210: 652f 6d61 2d75 7365 722f 776f 726b 2f63  e/ma-user/work/c
-00000220: 6f64 652f 7374 796c 6567 616e 322d 6164  ode/stylegan2-ad
-00000230: 612d 7079 746f 7263 682d 6578 702f 746c  a-pytorch-exp/tl
-00000240: 325f 6c69 622f 746c 322f 7072 6f6a 2f70  2_lib/tl2/proj/p
-00000250: 7974 6f72 6368 2f64 6470 2f64 325f 636f  ytorch/ddp/d2_co
-00000260: 6d6d 2e70 7972 0700 0000 1500 0000 730a  mm.pyr........s.
-00000270: 0000 0000 0108 0104 0108 0104 0172 0700  .............r..
-00000280: 0000 6300 0000 0000 0000 0000 0000 0000  ..c.............
-00000290: 0000 0002 0000 0043 0000 0073 2000 0000  .......C...s ...
-000002a0: 7400 a001 a100 730c 6401 5300 7400 a002  t.....s.d.S.t...
-000002b0: a100 7318 6401 5300 7400 a003 a100 5300  ..s.d.S.t.....S.
-000002c0: a902 4e72 0100 0000 2904 7204 0000 0072  ..Nr....).r....r
-000002d0: 0500 0000 7206 0000 00da 0867 6574 5f72  ....r......get_r
-000002e0: 616e 6b72 0800 0000 7208 0000 0072 0800  ankr....r....r..
-000002f0: 0000 7209 0000 0072 0b00 0000 1d00 0000  ..r....r........
-00000300: 730a 0000 0000 0108 0104 0108 0104 0172  s..............r
-00000310: 0b00 0000 6300 0000 0000 0000 0000 0000  ....c...........
-00000320: 0000 0000 0003 0000 0043 0000 0073 3000  .........C...s0.
-00000330: 0000 7400 a001 a100 730c 6401 5300 7400  ..t.....s.d.S.t.
-00000340: a002 a100 7318 6401 5300 7403 6402 6b09  ....s.d.S.t.d.k.
-00000350: 7324 7404 8201 7400 6a05 7403 6403 8d01  s$t...t.j.t.d...
-00000360: 5300 2904 7a68 0a20 2020 2052 6574 7572  S.).zh.    Retur
-00000370: 6e73 3a0a 2020 2020 2020 2020 5468 6520  ns:.        The 
-00000380: 7261 6e6b 206f 6620 7468 6520 6375 7272  rank of the curr
-00000390: 656e 7420 7072 6f63 6573 7320 7769 7468  ent process with
-000003a0: 696e 2074 6865 206c 6f63 616c 2028 7065  in the local (pe
-000003b0: 722d 6d61 6368 696e 6529 2070 726f 6365  r-machine) proce
-000003c0: 7373 2067 726f 7570 2e0a 2020 2020 7201  ss group..    r.
-000003d0: 0000 004e a901 da05 6772 6f75 7029 0672  ...N....group).r
-000003e0: 0400 0000 7205 0000 0072 0600 0000 da14  ....r....r......
-000003f0: 5f4c 4f43 414c 5f50 524f 4345 5353 5f47  _LOCAL_PROCESS_G
-00000400: 524f 5550 da0e 4173 7365 7274 696f 6e45  ROUP..AssertionE
-00000410: 7272 6f72 720b 0000 0072 0800 0000 7208  rrorr....r....r.
-00000420: 0000 0072 0800 0000 7209 0000 00da 0e67  ...r....r......g
-00000430: 6574 5f6c 6f63 616c 5f72 616e 6b25 0000  et_local_rank%..
-00000440: 0073 0c00 0000 0005 0801 0401 0801 0401  .s..............
-00000450: 0c01 7210 0000 0063 0000 0000 0000 0000  ..r....c........
-00000460: 0000 0000 0000 0000 0300 0000 4300 0000  ............C...
-00000470: 7324 0000 0074 00a0 01a1 0073 0c64 0153  s$...t.....s.d.S
-00000480: 0074 00a0 02a1 0073 1864 0153 0074 006a  .t.....s.d.S.t.j
-00000490: 0374 0464 028d 0153 0029 037a 770a 2020  .t.d...S.).zw.  
-000004a0: 2020 5265 7475 726e 733a 0a20 2020 2020    Returns:.     
-000004b0: 2020 2054 6865 2073 697a 6520 6f66 2074     The size of t
-000004c0: 6865 2070 6572 2d6d 6163 6869 6e65 2070  he per-machine p
-000004d0: 726f 6365 7373 2067 726f 7570 2c0a 2020  rocess group,.  
-000004e0: 2020 2020 2020 692e 652e 2074 6865 206e        i.e. the n
-000004f0: 756d 6265 7220 6f66 2070 726f 6365 7373  umber of process
-00000500: 6573 2070 6572 206d 6163 6869 6e65 2e0a  es per machine..
-00000510: 2020 2020 7203 0000 0072 0c00 0000 2905      r....r....).
-00000520: 7204 0000 0072 0500 0000 7206 0000 0072  r....r....r....r
-00000530: 0700 0000 720e 0000 0072 0800 0000 7208  ....r....r....r.
-00000540: 0000 0072 0800 0000 7209 0000 00da 0e67  ...r....r......g
-00000550: 6574 5f6c 6f63 616c 5f73 697a 6532 0000  et_local_size2..
-00000560: 0073 0a00 0000 0006 0801 0401 0801 0401  .s..............
-00000570: 7211 0000 0063 0000 0000 0000 0000 0000  r....c..........
-00000580: 0000 0000 0000 0200 0000 4300 0000 730a  ..........C...s.
-00000590: 0000 0074 0083 0064 016b 0253 0072 0a00  ...t...d.k.S.r..
-000005a0: 0000 2901 720b 0000 0072 0800 0000 7208  ..).r....r....r.
-000005b0: 0000 0072 0800 0000 7209 0000 00da 0f69  ...r....r......i
-000005c0: 735f 6d61 696e 5f70 726f 6365 7373 3f00  s_main_process?.
-000005d0: 0000 7302 0000 0000 0172 1200 0000 6300  ..s......r....c.
-000005e0: 0000 0000 0000 0000 0000 0001 0000 0002  ................
-000005f0: 0000 0043 0000 0073 3800 0000 7400 a001  ...C...s8...t...
-00000600: a100 730c 6401 5300 7400 a002 a100 7318  ..s.d.S.t.....s.
-00000610: 6401 5300 7400 a003 a100 7d00 7c00 6402  d.S.t.....}.|.d.
-00000620: 6b02 722c 6401 5300 7400 a004 a100 0100  k.r,d.S.t.......
-00000630: 6401 5300 2903 7a6a 0a20 2020 2048 656c  d.S.).zj.    Hel
-00000640: 7065 7220 6675 6e63 7469 6f6e 2074 6f20  per function to 
-00000650: 7379 6e63 6872 6f6e 697a 6520 2862 6172  synchronize (bar
-00000660: 7269 6572 2920 616d 6f6e 6720 616c 6c20  rier) among all 
-00000670: 7072 6f63 6573 7365 7320 7768 656e 0a20  processes when. 
-00000680: 2020 2075 7369 6e67 2064 6973 7472 6962     using distrib
-00000690: 7574 6564 2074 7261 696e 696e 670a 2020  uted training.  
-000006a0: 2020 4e72 0300 0000 2905 7204 0000 0072    Nr....).r....r
-000006b0: 0500 0000 7206 0000 0072 0700 0000 5a07  ....r....r....Z.
-000006c0: 6261 7272 6965 7229 01da 0a77 6f72 6c64  barrier)...world
-000006d0: 5f73 697a 6572 0800 0000 7208 0000 0072  _sizer....r....r
-000006e0: 0900 0000 da0b 7379 6e63 6872 6f6e 697a  ......synchroniz
-000006f0: 6543 0000 0073 1000 0000 0005 0801 0401  eC...s..........
-00000700: 0801 0401 0801 0801 0401 7214 0000 0063  ..........r....c
-00000710: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00000720: 0300 0000 4300 0000 7324 0000 0074 00a0  ....C...s$...t..
-00000730: 01a1 0064 016b 0272 1874 006a 0264 0264  ...d.k.r.t.j.d.d
-00000740: 038d 0153 0074 006a 036a 0453 0064 0453  ...S.t.j.j.S.d.S
-00000750: 0029 057a 6a0a 2020 2020 5265 7475 726e  .).zj.    Return
-00000760: 2061 2070 726f 6365 7373 2067 726f 7570   a process group
-00000770: 2062 6173 6564 206f 6e20 676c 6f6f 2062   based on gloo b
-00000780: 6163 6b65 6e64 2c20 636f 6e74 6169 6e69  ackend, containi
-00000790: 6e67 2061 6c6c 2074 6865 2072 616e 6b73  ng all the ranks
-000007a0: 0a20 2020 2054 6865 2072 6573 756c 7420  .    The result 
-000007b0: 6973 2063 6163 6865 642e 0a20 2020 20da  is cached..    .
-000007c0: 046e 6363 6cda 0467 6c6f 6f29 01da 0762  .nccl..gloo)...b
-000007d0: 6163 6b65 6e64 4e29 0572 0400 0000 da0b  ackendN).r......
-000007e0: 6765 745f 6261 636b 656e 645a 096e 6577  get_backendZ.new
-000007f0: 5f67 726f 7570 720d 0000 005a 0557 4f52  _groupr....Z.WOR
-00000800: 4c44 7208 0000 0072 0800 0000 7208 0000  LDr....r....r...
-00000810: 0072 0900 0000 da16 5f67 6574 5f67 6c6f  .r......_get_glo
-00000820: 6261 6c5f 676c 6f6f 5f67 726f 7570 5200  bal_gloo_groupR.
-00000830: 0000 7306 0000 0000 060c 010c 0272 1900  ..s..........r..
-00000840: 0000 6302 0000 0000 0000 0000 0000 0008  ..c.............
-00000850: 0000 0007 0000 0043 0000 0073 8c00 0000  .......C...s....
-00000860: 7400 a001 7c01 a101 7d02 7c02 6401 6b06  t...|...}.|.d.k.
-00000870: 7316 7402 8201 7403 a004 7c02 6402 6b02  s.t...t...|.d.k.
-00000880: 7226 6403 6e02 6404 a101 7d03 7405 a006  r&d.n.d...}.t...
-00000890: 7c00 a101 7d04 7407 7c04 8301 6405 6b04  |...}.t.|...d.k.
-000008a0: 726a 7408 a009 740a a101 7d05 7c05 a00b  rjt...t...}.|...
-000008b0: 6406 a00c 740d 8300 7407 7c04 8301 6405  d...t...t.|...d.
-000008c0: 1b00 7c03 a103 a101 0100 7403 6a0e a00f  ..|.......t.j...
-000008d0: 7c04 a101 7d06 7403 a010 7c06 a101 6a11  |...}.t...|...j.
-000008e0: 7c03 6407 8d01 7d07 7c07 5300 2908 4e29  |.d...}.|.S.).N)
-000008f0: 0272 1600 0000 7215 0000 0072 1600 0000  .r....r....r....
-00000900: da03 6370 755a 0463 7564 6169 0000 0040  ..cpuZ.cudai...@
-00000910: 7a3b 5261 6e6b 207b 7d20 7472 7969 6e67  z;Rank {} trying
-00000920: 2074 6f20 616c 6c2d 6761 7468 6572 207b   to all-gather {
-00000930: 3a2e 3266 7d20 4742 206f 6620 6461 7461  :.2f} GB of data
-00000940: 206f 6e20 6465 7669 6365 207b 7d29 01da   on device {})..
-00000950: 0664 6576 6963 6529 1272 0400 0000 7218  .device).r....r.
-00000960: 0000 0072 0f00 0000 da05 746f 7263 6872  ...r......torchr
-00000970: 1b00 0000 da06 7069 636b 6c65 da05 6475  ......pickle..du
-00000980: 6d70 73da 036c 656e da07 6c6f 6767 696e  mps..len..loggin
-00000990: 67da 0967 6574 4c6f 6767 6572 da08 5f5f  g..getLogger..__
-000009a0: 6e61 6d65 5f5f da07 7761 726e 696e 67da  name__..warning.
-000009b0: 0666 6f72 6d61 7472 0b00 0000 5a0b 4279  .formatr....Z.By
-000009c0: 7465 5374 6f72 6167 65da 0b66 726f 6d5f  teStorage..from_
-000009d0: 6275 6666 6572 5a0a 4279 7465 5465 6e73  bufferZ.ByteTens
-000009e0: 6f72 da02 746f 2908 da04 6461 7461 720d  or..to)...datar.
-000009f0: 0000 0072 1700 0000 721b 0000 00da 0662  ...r....r......b
-00000a00: 7566 6665 72da 066c 6f67 6765 725a 0773  uffer..loggerZ.s
-00000a10: 746f 7261 6765 da06 7465 6e73 6f72 7208  torage..tensorr.
-00000a20: 0000 0072 0800 0000 7209 0000 00da 145f  ...r....r......_
-00000a30: 7365 7269 616c 697a 655f 746f 5f74 656e  serialize_to_ten
-00000a40: 736f 725e 0000 0073 2000 0000 0001 0a01  sor^...s .......
-00000a50: 0c01 1602 0a01 0c01 0a01 0401 0401 0400  ................
-00000a60: 0a00 02ff 02ff 0405 0c01 1201 722b 0000  ............r+..
-00000a70: 0063 0200 0000 0000 0000 0000 0000 0700  .c..............
-00000a80: 0000 0500 0000 0300 0000 73ae 0000 0074  ..........s....t
-00000a90: 006a 017c 0164 018d 017d 027c 0264 026b  .j.|.d...}.|.d.k
-00000aa0: 0573 1c74 0264 0383 0182 0174 036a 0488  .s.t.d.....t.j..
-00000ab0: 00a0 05a1 0067 0174 036a 0688 006a 0764  .....g.t.j...j.d
-00000ac0: 048d 037d 0387 0066 0164 0564 0684 0874  ...}...f.d.d...t
-00000ad0: 087c 0283 0144 0083 017d 0474 006a 097c  .|...D...}.t.j.|
-00000ae0: 047c 037c 0164 018d 0301 0064 0764 0684  .|.|.d.....d.d..
-00000af0: 007c 0444 0083 017d 0474 0a7c 0483 017d  .|.D...}.t.|...}
-00000b00: 057c 037c 056b 0372 a674 036a 0b7c 057c  .|.|.k.r.t.j.|.|
-00000b10: 0318 0066 0174 036a 0c88 006a 0764 048d  ...f.t.j...j.d..
-00000b20: 037d 0674 036a 0d88 007c 0666 0264 0864  .}.t.j...|.f.d.d
-00000b30: 098d 0289 007c 0488 0066 0253 0029 0a7a  .....|...f.S.).z
-00000b40: 7a0a 2020 2020 5265 7475 726e 733a 0a20  z.    Returns:. 
-00000b50: 2020 2020 2020 206c 6973 745b 696e 745d         list[int]
-00000b60: 3a20 7369 7a65 206f 6620 7468 6520 7465  : size of the te
-00000b70: 6e73 6f72 2c20 6f6e 2065 6163 6820 7261  nsor, on each ra
-00000b80: 6e6b 0a20 2020 2020 2020 2054 656e 736f  nk.        Tenso
-00000b90: 723a 2070 6164 6465 6420 7465 6e73 6f72  r: padded tensor
-00000ba0: 2074 6861 7420 6861 7320 7468 6520 6d61   that has the ma
-00000bb0: 7820 7369 7a65 0a20 2020 2072 0c00 0000  x size.    r....
-00000bc0: 7203 0000 007a 4863 6f6d 6d2e 6761 7468  r....zHcomm.gath
-00000bd0: 6572 2f61 6c6c 5f67 6174 6865 7220 6d75  er/all_gather mu
-00000be0: 7374 2062 6520 6361 6c6c 6564 2066 726f  st be called fro
-00000bf0: 6d20 7261 6e6b 7320 7769 7468 696e 2074  m ranks within t
-00000c00: 6865 2067 6976 656e 2067 726f 7570 21a9  he given group!.
-00000c10: 02da 0564 7479 7065 721b 0000 0063 0100  ...dtyper....c..
-00000c20: 0000 0000 0000 0000 0000 0200 0000 0700  ................
-00000c30: 0000 1300 0000 7322 0000 0067 007c 005d  ......s"...g.|.]
-00000c40: 1a7d 0174 006a 0164 0067 0174 006a 0288  .}.t.j.d.g.t.j..
-00000c50: 006a 0364 018d 0391 0271 0453 0029 0272  .j.d.....q.S.).r
-00000c60: 0300 0000 722c 0000 0029 0472 1c00 0000  ....r,...).r....
-00000c70: da05 7a65 726f 73da 0569 6e74 3634 721b  ..zeros..int64r.
-00000c80: 0000 00a9 02da 022e 30da 015f a901 722a  ........0.._..r*
-00000c90: 0000 0072 0800 0000 7209 0000 00da 0a3c  ...r....r......<
-00000ca0: 6c69 7374 636f 6d70 3e7b 0000 0073 0400  listcomp>{...s..
-00000cb0: 0000 0601 0200 7a2a 5f70 6164 5f74 6f5f  ......z*_pad_to_
-00000cc0: 6c61 7267 6573 745f 7465 6e73 6f72 2e3c  largest_tensor.<
-00000cd0: 6c6f 6361 6c73 3e2e 3c6c 6973 7463 6f6d  locals>.<listcom
-00000ce0: 703e 6301 0000 0000 0000 0000 0000 0002  p>c.............
-00000cf0: 0000 0005 0000 0053 0000 0073 1800 0000  .......S...s....
-00000d00: 6700 7c00 5d10 7d01 7400 7c01 a001 a100  g.|.].}.t.|.....
-00000d10: 8301 9102 7104 5300 7208 0000 0029 02da  ....q.S.r....)..
-00000d20: 0369 6e74 da04 6974 656d 2902 7231 0000  .int..item).r1..
-00000d30: 00da 0473 697a 6572 0800 0000 7208 0000  ...sizer....r...
-00000d40: 0072 0900 0000 7234 0000 007f 0000 0073  .r....r4.......s
-00000d50: 0400 0000 0600 0200 7201 0000 00a9 01da  ........r.......
-00000d60: 0364 696d 290e 7204 0000 0072 0700 0000  .dim).r....r....
-00000d70: 720f 0000 0072 1c00 0000 722a 0000 005a  r....r....r*...Z
-00000d80: 056e 756d 656c 722f 0000 0072 1b00 0000  .numelr/...r....
-00000d90: da05 7261 6e67 65da 0a61 6c6c 5f67 6174  ..range..all_gat
-00000da0: 6865 72da 036d 6178 722e 0000 00da 0575  her..maxr......u
-00000db0: 696e 7438 da03 6361 7429 0772 2a00 0000  int8..cat).r*...
-00000dc0: 720d 0000 0072 1300 0000 5a0a 6c6f 6361  r....r....Z.loca
-00000dd0: 6c5f 7369 7a65 da09 7369 7a65 5f6c 6973  l_size..size_lis
-00000de0: 74da 086d 6178 5f73 697a 65da 0770 6164  t..max_size..pad
-00000df0: 6469 6e67 7208 0000 0072 3300 0000 7209  dingr....r3...r.
-00000e00: 0000 00da 165f 7061 645f 746f 5f6c 6172  ....._pad_to_lar
-00000e10: 6765 7374 5f74 656e 736f 7270 0000 0073  gest_tensorp...s
-00000e20: 2000 0000 0006 0c02 06ff 0402 02fe 0403   ...............
-00000e30: 1a01 0a01 06ff 0603 1001 0e02 0804 0801  ................
-00000e40: 1a01 1201 7242 0000 0063 0200 0000 0000  ....rB...c......
-00000e50: 0000 0000 0000 0700 0000 0600 0000 0300  ................
-00000e60: 0000 73ba 0000 0074 0083 0064 016b 0272  ..s....t...d.k.r
-00000e70: 107c 0067 0153 007c 0164 026b 0872 1e74  .|.g.S.|.d.k.r.t
-00000e80: 0183 007d 0174 02a0 007c 01a1 0164 016b  ...}.t...|...d.k
-00000e90: 0272 327c 0067 0153 0074 037c 007c 0183  .r2|.g.S.t.|.|..
-00000ea0: 0289 0174 0488 017c 0183 025c 027d 0289  ...t...|...\.}..
-00000eb0: 0174 057c 0283 0189 0087 0087 0166 0264  .t.|.........f.d
-00000ec0: 0364 0484 087c 0244 0083 017d 0374 026a  .d...|.D...}.t.j
-00000ed0: 067c 0388 017c 0164 058d 0301 0067 007d  .|...|.d.....g.}
-00000ee0: 0474 077c 027c 0383 0244 005d 305c 027d  .t.|.|...D.]0\.}
-00000ef0: 0589 0188 01a0 08a1 00a0 09a1 00a0 0aa1  ................
-00000f00: 0064 027c 0585 0219 007d 067c 04a0 0b74  .d.|.....}.|...t
-00000f10: 0ca0 0d7c 06a1 01a1 0101 0071 847c 0453  ...|.......q.|.S
-00000f20: 0029 0661 3d01 0000 0a20 2020 2052 756e  .).a=....    Run
-00000f30: 2061 6c6c 5f67 6174 6865 7220 6f6e 2061   all_gather on a
-00000f40: 7262 6974 7261 7279 2070 6963 6b6c 6162  rbitrary picklab
-00000f50: 6c65 2064 6174 6120 286e 6f74 206e 6563  le data (not nec
-00000f60: 6573 7361 7269 6c79 2074 656e 736f 7273  essarily tensors
-00000f70: 292e 0a0a 2020 2020 4172 6773 3a0a 2020  )...    Args:.  
-00000f80: 2020 2020 2020 6461 7461 3a20 616e 7920        data: any 
-00000f90: 7069 636b 6c61 626c 6520 6f62 6a65 6374  picklable object
-00000fa0: 0a20 2020 2020 2020 2067 726f 7570 3a20  .        group: 
-00000fb0: 6120 746f 7263 6820 7072 6f63 6573 7320  a torch process 
-00000fc0: 6772 6f75 702e 2042 7920 6465 6661 756c  group. By defaul
-00000fd0: 742c 2077 696c 6c20 7573 6520 6120 6772  t, will use a gr
-00000fe0: 6f75 7020 7768 6963 680a 2020 2020 2020  oup which.      
-00000ff0: 2020 2020 2020 636f 6e74 6169 6e73 2061        contains a
-00001000: 6c6c 2072 616e 6b73 206f 6e20 676c 6f6f  ll ranks on gloo
-00001010: 2062 6163 6b65 6e64 2e0a 0a20 2020 2052   backend...    R
-00001020: 6574 7572 6e73 3a0a 2020 2020 2020 2020  eturns:.        
-00001030: 6c69 7374 5b64 6174 615d 3a20 6c69 7374  list[data]: list
-00001040: 206f 6620 6461 7461 2067 6174 6865 7265   of data gathere
-00001050: 6420 6672 6f6d 2065 6163 6820 7261 6e6b  d from each rank
-00001060: 0a20 2020 2072 0300 0000 4e63 0100 0000  .    r....Nc....
-00001070: 0000 0000 0000 0000 0200 0000 0700 0000  ................
-00001080: 1300 0000 7322 0000 0067 007c 005d 1a7d  ....s"...g.|.].}
-00001090: 0174 006a 0188 0066 0174 006a 0288 016a  .t.j...f.t.j...j
-000010a0: 0364 008d 0391 0271 0453 00a9 0172 2c00  .d.....q.S...r,.
-000010b0: 0000 a904 721c 0000 00da 0565 6d70 7479  ....r......empty
-000010c0: 723d 0000 0072 1b00 0000 7230 0000 00a9  r=...r....r0....
-000010d0: 0272 4000 0000 722a 0000 0072 0800 0000  .r@...r*...r....
-000010e0: 7209 0000 0072 3400 0000 a400 0000 7304  r....r4.......s.
-000010f0: 0000 0006 0102 007a 1e61 6c6c 5f67 6174  .......z.all_gat
-00001100: 6865 722e 3c6c 6f63 616c 733e 2e3c 6c69  her.<locals>.<li
-00001110: 7374 636f 6d70 3e72 0c00 0000 290e 7207  stcomp>r....).r.
-00001120: 0000 0072 1900 0000 7204 0000 0072 2b00  ...r....r....r+.
-00001130: 0000 7242 0000 0072 3c00 0000 723b 0000  ..rB...r<...r;..
-00001140: 00da 037a 6970 721a 0000 00da 056e 756d  ...zipr......num
-00001150: 7079 da07 746f 6279 7465 73da 0661 7070  py..tobytes..app
-00001160: 656e 6472 1d00 0000 da05 6c6f 6164 7329  endr......loads)
-00001170: 0772 2700 0000 720d 0000 0072 3f00 0000  .r'...r....r?...
-00001180: da0b 7465 6e73 6f72 5f6c 6973 74da 0964  ..tensor_list..d
-00001190: 6174 615f 6c69 7374 7237 0000 0072 2800  ata_listr7...r(.
-000011a0: 0000 7208 0000 0072 4600 0000 7209 0000  ..r....rF...r...
-000011b0: 0072 3b00 0000 8b00 0000 7324 0000 0000  .r;.......s$....
-000011c0: 0c0a 0106 0108 0106 010e 0106 020a 020e  ................
-000011d0: 0108 030c 0102 ff06 0310 0204 0112 0118  ................
-000011e0: 0112 0272 3b00 0000 6303 0000 0000 0000  ...r;...c.......
-000011f0: 0000 0000 0009 0000 0006 0000 0003 0000  ................
-00001200: 0073 ec00 0000 7400 8300 6401 6b02 7210  .s....t...d.k.r.
-00001210: 7c00 6701 5300 7c02 6402 6b08 721e 7401  |.g.S.|.d.k.r.t.
-00001220: 8300 7d02 7402 6a00 7c02 6403 8d01 6401  ..}.t.j.|.d...d.
-00001230: 6b02 7234 7c00 6701 5300 7402 6a03 7c02  k.r4|.g.S.t.j.|.
-00001240: 6403 8d01 7d03 7404 7c00 7c02 8302 8901  d...}.t.|.|.....
-00001250: 7405 8801 7c02 8302 5c02 7d04 8901 7c03  t...|...\.}...|.
-00001260: 7c01 6b02 72d2 7406 7c04 8301 8900 8700  |.k.r.t.|.......
-00001270: 8701 6602 6404 6405 8408 7c04 4400 8301  ..f.d.d...|.D...
-00001280: 7d05 7402 6a07 8801 7c05 7c01 7c02 6406  }.t.j...|.|.|.d.
-00001290: 8d04 0100 6700 7d06 7408 7c04 7c05 8302  ....g.}.t.|.|...
-000012a0: 4400 5d30 5c02 7d07 8901 8801 a009 a100  D.]0\.}.........
-000012b0: a00a a100 a00b a100 6402 7c07 8502 1900  ........d.|.....
-000012c0: 7d08 7c06 a00c 740d a00e 7c08 a101 a101  }.|...t...|.....
-000012d0: 0100 719c 7c06 5300 7402 6a07 8801 6700  ..q.|.S.t.j...g.
-000012e0: 7c01 7c02 6406 8d04 0100 6700 5300 6402  |.|.d.....g.S.d.
-000012f0: 5300 2907 618e 0100 000a 2020 2020 5275  S.).a.....    Ru
-00001300: 6e20 6761 7468 6572 206f 6e20 6172 6269  n gather on arbi
-00001310: 7472 6172 7920 7069 636b 6c61 626c 6520  trary picklable 
-00001320: 6461 7461 2028 6e6f 7420 6e65 6365 7373  data (not necess
-00001330: 6172 696c 7920 7465 6e73 6f72 7329 2e0a  arily tensors)..
-00001340: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     
-00001350: 2020 2064 6174 613a 2061 6e79 2070 6963     data: any pic
-00001360: 6b6c 6162 6c65 206f 626a 6563 740a 2020  klable object.  
-00001370: 2020 2020 2020 6473 7420 2869 6e74 293a        dst (int):
-00001380: 2064 6573 7469 6e61 7469 6f6e 2072 616e   destination ran
-00001390: 6b0a 2020 2020 2020 2020 6772 6f75 703a  k.        group:
-000013a0: 2061 2074 6f72 6368 2070 726f 6365 7373   a torch process
-000013b0: 2067 726f 7570 2e20 4279 2064 6566 6175   group. By defau
-000013c0: 6c74 2c20 7769 6c6c 2075 7365 2061 2067  lt, will use a g
-000013d0: 726f 7570 2077 6869 6368 0a20 2020 2020  roup which.     
-000013e0: 2020 2020 2020 2063 6f6e 7461 696e 7320         contains 
-000013f0: 616c 6c20 7261 6e6b 7320 6f6e 2067 6c6f  all ranks on glo
-00001400: 6f20 6261 636b 656e 642e 0a0a 2020 2020  o backend...    
-00001410: 5265 7475 726e 733a 0a20 2020 2020 2020  Returns:.       
-00001420: 206c 6973 745b 6461 7461 5d3a 206f 6e20   list[data]: on 
-00001430: 6473 742c 2061 206c 6973 7420 6f66 2064  dst, a list of d
-00001440: 6174 6120 6761 7468 6572 6564 2066 726f  ata gathered fro
-00001450: 6d20 6561 6368 2072 616e 6b2e 204f 7468  m each rank. Oth
-00001460: 6572 7769 7365 2c0a 2020 2020 2020 2020  erwise,.        
-00001470: 2020 2020 616e 2065 6d70 7479 206c 6973      an empty lis
-00001480: 742e 0a20 2020 2072 0300 0000 4e72 0c00  t..    r....Nr..
-00001490: 0000 6301 0000 0000 0000 0000 0000 0002  ..c.............
-000014a0: 0000 0007 0000 0013 0000 0073 2200 0000  ...........s"...
-000014b0: 6700 7c00 5d1a 7d01 7400 6a01 8800 6601  g.|.].}.t.j...f.
-000014c0: 7400 6a02 8801 6a03 6400 8d03 9102 7104  t.j...j.d.....q.
-000014d0: 5300 7243 0000 0072 4400 0000 7230 0000  S.rC...rD...r0..
-000014e0: 0072 4600 0000 7208 0000 0072 0900 0000  .rF...r....r....
-000014f0: 7234 0000 00cd 0000 0073 0400 0000 0601  r4.......s......
-00001500: 0200 7a1a 6761 7468 6572 2e3c 6c6f 6361  ..z.gather.<loca
-00001510: 6c73 3e2e 3c6c 6973 7463 6f6d 703e 2902  ls>.<listcomp>).
-00001520: da03 6473 7472 0d00 0000 290f 7207 0000  ..dstr....).r...
-00001530: 0072 1900 0000 7204 0000 0072 0b00 0000  .r....r....r....
-00001540: 722b 0000 0072 4200 0000 723c 0000 00da  r+...rB...r<....
-00001550: 0667 6174 6865 7272 4700 0000 721a 0000  .gatherrG...r...
-00001560: 0072 4800 0000 7249 0000 0072 4a00 0000  .rH...rI...rJ...
-00001570: 721d 0000 0072 4b00 0000 2909 7227 0000  r....rK...).r'..
-00001580: 0072 4e00 0000 720d 0000 00da 0472 616e  .rN...r......ran
-00001590: 6b72 3f00 0000 724c 0000 0072 4d00 0000  kr?...rL...rM...
-000015a0: 7237 0000 0072 2800 0000 7208 0000 0072  r7...r(...r....r
-000015b0: 4600 0000 7209 0000 0072 4f00 0000 b100  F...r....rO.....
-000015c0: 0000 732c 0000 0000 0e0a 0106 0108 0106  ..s,............
-000015d0: 0110 0106 010c 020a 010e 0308 0108 010c  ................
-000015e0: 0102 ff06 0312 0204 0112 0118 0112 0104  ................
-000015f0: 0212 0172 4f00 0000 6300 0000 0000 0000  ...rO...c.......
-00001600: 0000 0000 0002 0000 0003 0000 0043 0000  .............C..
-00001610: 0073 1c00 0000 7400 6a01 a002 6401 a101  .s....t.j...d...
-00001620: 7d00 7403 7c00 8301 7d01 7c01 6402 1900  }.t.|...}.|.d...
-00001630: 5300 2903 7afd 0a20 2020 2052 6574 7572  S.).z..    Retur
-00001640: 6e73 3a0a 2020 2020 2020 2020 696e 743a  ns:.        int:
-00001650: 2061 2072 616e 646f 6d20 6e75 6d62 6572   a random number
-00001660: 2074 6861 7420 6973 2074 6865 2073 616d   that is the sam
-00001670: 6520 6163 726f 7373 2061 6c6c 2077 6f72  e across all wor
-00001680: 6b65 7273 2e0a 2020 2020 2020 2020 2020  kers..          
-00001690: 2020 4966 2077 6f72 6b65 7273 206e 6565    If workers nee
-000016a0: 6420 6120 7368 6172 6564 2052 4e47 2c20  d a shared RNG, 
-000016b0: 7468 6579 2063 616e 2075 7365 2074 6869  they can use thi
-000016c0: 7320 7368 6172 6564 2073 6565 6420 746f  s shared seed to
-000016d0: 0a20 2020 2020 2020 2020 2020 2063 7265  .            cre
-000016e0: 6174 6520 6f6e 652e 0a0a 2020 2020 416c  ate one...    Al
-000016f0: 6c20 776f 726b 6572 7320 6d75 7374 2063  l workers must c
-00001700: 616c 6c20 7468 6973 2066 756e 6374 696f  all this functio
-00001710: 6e2c 206f 7468 6572 7769 7365 2069 7420  n, otherwise it 
-00001720: 7769 6c6c 2064 6561 646c 6f63 6b2e 0a20  will deadlock.. 
-00001730: 2020 206c 0300 0000 0000 0000 0200 7201     l..........r.
-00001740: 0000 0029 04da 026e 70da 0672 616e 646f  ...)...np..rando
-00001750: 6dda 0772 616e 6469 6e74 723b 0000 0029  m..randintr;...)
-00001760: 025a 0469 6e74 735a 0861 6c6c 5f69 6e74  .Z.intsZ.all_int
-00001770: 7372 0800 0000 7208 0000 0072 0900 0000  sr....r....r....
-00001780: da12 7368 6172 6564 5f72 616e 646f 6d5f  ..shared_random_
-00001790: 7365 6564 dc00 0000 7306 0000 0000 090c  seed....s.......
-000017a0: 0108 0172 5400 0000 5463 0200 0000 0000  ...rT...Tc......
-000017b0: 0000 0000 0000 0700 0000 0900 0000 4300  ..............C.
-000017c0: 0000 73a4 0000 0074 0083 007d 027c 0264  ..s....t...}.|.d
-000017d0: 016b 0072 127c 0053 0074 01a0 02a1 008f  .k.r.|.S.t......
-000017e0: 8001 0067 007d 0367 007d 0474 037c 00a0  ...g.}.g.}.t.|..
-000017f0: 04a1 0083 0144 005d 1c7d 057c 03a0 057c  .....D.].}.|...|
-00001800: 05a1 0101 007c 04a0 057c 007c 0519 00a1  .....|...|.|....
-00001810: 0101 0071 3074 016a 067c 0464 0264 038d  ...q0t.j.|.d.d..
-00001820: 027d 0474 076a 087c 0464 0264 048d 0201  .}.t.j.|.d.d....
-00001830: 0074 07a0 09a1 0064 026b 0272 827c 0172  .t.....d.k.r.|.r
-00001840: 827c 047c 021d 007d 0464 0564 0684 0074  .|.|...}.d.d...t
-00001850: 0a7c 037c 0483 0244 0083 017d 0657 0035  .|.|...D...}.W.5
-00001860: 0051 0052 0058 007c 0653 0029 0761 6501  .Q.R.X.|.S.).ae.
-00001870: 0000 0a20 2020 2052 6564 7563 6520 7468  ...    Reduce th
-00001880: 6520 7661 6c75 6573 2069 6e20 7468 6520  e values in the 
-00001890: 6469 6374 696f 6e61 7279 2066 726f 6d20  dictionary from 
-000018a0: 616c 6c20 7072 6f63 6573 7365 7320 736f  all processes so
-000018b0: 2074 6861 7420 7072 6f63 6573 7320 7769   that process wi
-000018c0: 7468 2072 616e 6b0a 2020 2020 3020 6861  th rank.    0 ha
-000018d0: 7320 7468 6520 7265 6475 6365 6420 7265  s the reduced re
-000018e0: 7375 6c74 732e 0a0a 2020 2020 4172 6773  sults...    Args
-000018f0: 3a0a 2020 2020 2020 2020 696e 7075 745f  :.        input_
-00001900: 6469 6374 2028 6469 6374 293a 2069 6e70  dict (dict): inp
-00001910: 7574 7320 746f 2062 6520 7265 6475 6365  uts to be reduce
-00001920: 642e 2041 6c6c 2074 6865 2076 616c 7565  d. All the value
-00001930: 7320 6d75 7374 2062 6520 7363 616c 6172  s must be scalar
-00001940: 2043 5544 4120 5465 6e73 6f72 2e0a 2020   CUDA Tensor..  
-00001950: 2020 2020 2020 6176 6572 6167 6520 2862        average (b
-00001960: 6f6f 6c29 3a20 7768 6574 6865 7220 746f  ool): whether to
-00001970: 2064 6f20 6176 6572 6167 6520 6f72 2073   do average or s
-00001980: 756d 0a0a 2020 2020 5265 7475 726e 733a  um..    Returns:
-00001990: 0a20 2020 2020 2020 2061 2064 6963 7420  .        a dict 
-000019a0: 7769 7468 2074 6865 2073 616d 6520 6b65  with the same ke
-000019b0: 7973 2061 7320 696e 7075 745f 6469 6374  ys as input_dict
-000019c0: 2c20 6166 7465 7220 7265 6475 6374 696f  , after reductio
-000019d0: 6e2e 0a20 2020 20e9 0200 0000 7201 0000  n..    .....r...
-000019e0: 0072 3800 0000 2901 724e 0000 0063 0100  .r8...).rN...c..
-000019f0: 0000 0000 0000 0000 0000 0300 0000 0400  ................
-00001a00: 0000 5300 0000 7316 0000 0069 007c 005d  ..S...s....i.|.]
-00001a10: 0e5c 027d 017d 027c 017c 0293 0271 0453  .\.}.}.|.|...q.S
-00001a20: 0072 0800 0000 7208 0000 0029 0372 3100  .r....r....).r1.
-00001a30: 0000 da01 6bda 0176 7208 0000 0072 0800  ....k..vr....r..
-00001a40: 0000 7209 0000 00da 0a3c 6469 6374 636f  ..r......<dictco
-00001a50: 6d70 3e06 0100 0073 0600 0000 0600 0600  mp>....s........
-00001a60: 0200 7a1f 7265 6475 6365 5f64 6963 742e  ..z.reduce_dict.
-00001a70: 3c6c 6f63 616c 733e 2e3c 6469 6374 636f  <locals>.<dictco
-00001a80: 6d70 3e29 0b72 0700 0000 721c 0000 005a  mp>).r....r....Z
-00001a90: 076e 6f5f 6772 6164 da06 736f 7274 6564  .no_grad..sorted
-00001aa0: da04 6b65 7973 724a 0000 00da 0573 7461  ..keysrJ.....sta
-00001ab0: 636b 7204 0000 00da 0672 6564 7563 6572  ckr......reducer
-00001ac0: 0b00 0000 7247 0000 0029 075a 0a69 6e70  ....rG...).Z.inp
-00001ad0: 7574 5f64 6963 74da 0761 7665 7261 6765  ut_dict..average
-00001ae0: 7213 0000 00da 056e 616d 6573 da06 7661  r......names..va
-00001af0: 6c75 6573 7256 0000 005a 0c72 6564 7563  luesrV...Z.reduc
-00001b00: 6564 5f64 6963 7472 0800 0000 7208 0000  ed_dictr....r...
-00001b10: 0072 0900 0000 da0b 7265 6475 6365 5f64  .r......reduce_d
-00001b20: 6963 74ea 0000 0073 1e00 0000 000c 0601  ict....s........
-00001b30: 0801 0401 0a01 0401 0402 1001 0a01 1001  ................
-00001b40: 0e01 0e01 1003 0801 1e01 7260 0000 0029  ..........r`...)
-00001b50: 014e 2902 7201 0000 004e 2901 5429 1bda  .N).r....N).T)..
-00001b60: 075f 5f64 6f63 5f5f da09 6675 6e63 746f  .__doc__..functo
-00001b70: 6f6c 7372 2000 0000 7248 0000 0072 5100  olsr ...rH...rQ.
-00001b80: 0000 721d 0000 0072 1c00 0000 5a11 746f  ..r....r....Z.to
-00001b90: 7263 682e 6469 7374 7269 6275 7465 645a  rch.distributedZ
-00001ba0: 0b64 6973 7472 6962 7574 6564 7204 0000  .distributedr...
-00001bb0: 0072 0e00 0000 7235 0000 0072 0700 0000  .r....r5...r....
-00001bc0: 720b 0000 0072 1000 0000 7211 0000 00da  r....r....r.....
-00001bd0: 0462 6f6f 6c72 1200 0000 7214 0000 00da  .boolr....r.....
-00001be0: 096c 7275 5f63 6163 6865 7219 0000 0072  .lru_cacher....r
-00001bf0: 2b00 0000 7242 0000 0072 3b00 0000 724f  +...rB...r;...rO
-00001c00: 0000 0072 5400 0000 7260 0000 0072 0800  ...rT...r`...r..
-00001c10: 0000 7208 0000 0072 0800 0000 7209 0000  ..r....r....r...
-00001c20: 00da 083c 6d6f 6475 6c65 3e02 0000 0073  ...<module>....s
-00001c30: 2a00 0000 0405 0801 0801 0801 0801 0801  *...............
-00001c40: 0c02 0407 0e08 0e08 0e0d 0e0d 0e04 080f  ................
-00001c50: 0601 0a0b 0812 081b 0a26 0a2b 080e       .........&.+..
+00000220: 6f64 652f 746c 322f 746c 322f 7072 6f6a  ode/tl2/tl2/proj
+00000230: 2f70 7974 6f72 6368 2f64 6470 2f64 325f  /pytorch/ddp/d2_
+00000240: 636f 6d6d 2e70 7972 0700 0000 1500 0000  comm.pyr........
+00000250: 730a 0000 0000 0108 0104 0108 0104 0172  s..............r
+00000260: 0700 0000 6300 0000 0000 0000 0000 0000  ....c...........
+00000270: 0000 0000 0002 0000 0043 0000 0073 2000  .........C...s .
+00000280: 0000 7400 a001 a100 730c 6401 5300 7400  ..t.....s.d.S.t.
+00000290: a002 a100 7318 6401 5300 7400 a003 a100  ....s.d.S.t.....
+000002a0: 5300 a902 4e72 0100 0000 2904 7204 0000  S...Nr....).r...
+000002b0: 0072 0500 0000 7206 0000 00da 0867 6574  .r....r......get
+000002c0: 5f72 616e 6b72 0800 0000 7208 0000 0072  _rankr....r....r
+000002d0: 0800 0000 7209 0000 0072 0b00 0000 1d00  ....r....r......
+000002e0: 0000 730a 0000 0000 0108 0104 0108 0104  ..s.............
+000002f0: 0172 0b00 0000 6300 0000 0000 0000 0000  .r....c.........
+00000300: 0000 0000 0000 0003 0000 0043 0000 0073  ...........C...s
+00000310: 3000 0000 7400 a001 a100 730c 6401 5300  0...t.....s.d.S.
+00000320: 7400 a002 a100 7318 6401 5300 7403 6402  t.....s.d.S.t.d.
+00000330: 6b09 7324 7404 8201 7400 6a05 7403 6403  k.s$t...t.j.t.d.
+00000340: 8d01 5300 2904 7a68 0a20 2020 2052 6574  ..S.).zh.    Ret
+00000350: 7572 6e73 3a0a 2020 2020 2020 2020 5468  urns:.        Th
+00000360: 6520 7261 6e6b 206f 6620 7468 6520 6375  e rank of the cu
+00000370: 7272 656e 7420 7072 6f63 6573 7320 7769  rrent process wi
+00000380: 7468 696e 2074 6865 206c 6f63 616c 2028  thin the local (
+00000390: 7065 722d 6d61 6368 696e 6529 2070 726f  per-machine) pro
+000003a0: 6365 7373 2067 726f 7570 2e0a 2020 2020  cess group..    
+000003b0: 7201 0000 004e a901 da05 6772 6f75 7029  r....N....group)
+000003c0: 0672 0400 0000 7205 0000 0072 0600 0000  .r....r....r....
+000003d0: da14 5f4c 4f43 414c 5f50 524f 4345 5353  .._LOCAL_PROCESS
+000003e0: 5f47 524f 5550 da0e 4173 7365 7274 696f  _GROUP..Assertio
+000003f0: 6e45 7272 6f72 720b 0000 0072 0800 0000  nErrorr....r....
+00000400: 7208 0000 0072 0800 0000 7209 0000 00da  r....r....r.....
+00000410: 0e67 6574 5f6c 6f63 616c 5f72 616e 6b25  .get_local_rank%
+00000420: 0000 0073 0c00 0000 0005 0801 0401 0801  ...s............
+00000430: 0401 0c01 7210 0000 0063 0000 0000 0000  ....r....c......
+00000440: 0000 0000 0000 0000 0000 0300 0000 4300  ..............C.
+00000450: 0000 7324 0000 0074 00a0 01a1 0073 0c64  ..s$...t.....s.d
+00000460: 0153 0074 00a0 02a1 0073 1864 0153 0074  .S.t.....s.d.S.t
+00000470: 006a 0374 0464 028d 0153 0029 037a 770a  .j.t.d...S.).zw.
+00000480: 2020 2020 5265 7475 726e 733a 0a20 2020      Returns:.   
+00000490: 2020 2020 2054 6865 2073 697a 6520 6f66       The size of
+000004a0: 2074 6865 2070 6572 2d6d 6163 6869 6e65   the per-machine
+000004b0: 2070 726f 6365 7373 2067 726f 7570 2c0a   process group,.
+000004c0: 2020 2020 2020 2020 692e 652e 2074 6865          i.e. the
+000004d0: 206e 756d 6265 7220 6f66 2070 726f 6365   number of proce
+000004e0: 7373 6573 2070 6572 206d 6163 6869 6e65  sses per machine
+000004f0: 2e0a 2020 2020 7203 0000 0072 0c00 0000  ..    r....r....
+00000500: 2905 7204 0000 0072 0500 0000 7206 0000  ).r....r....r...
+00000510: 0072 0700 0000 720e 0000 0072 0800 0000  .r....r....r....
+00000520: 7208 0000 0072 0800 0000 7209 0000 00da  r....r....r.....
+00000530: 0e67 6574 5f6c 6f63 616c 5f73 697a 6532  .get_local_size2
+00000540: 0000 0073 0a00 0000 0006 0801 0401 0801  ...s............
+00000550: 0401 7211 0000 0063 0000 0000 0000 0000  ..r....c........
+00000560: 0000 0000 0000 0000 0200 0000 4300 0000  ............C...
+00000570: 730a 0000 0074 0083 0064 016b 0253 0072  s....t...d.k.S.r
+00000580: 0a00 0000 2901 720b 0000 0072 0800 0000  ....).r....r....
+00000590: 7208 0000 0072 0800 0000 7209 0000 00da  r....r....r.....
+000005a0: 0f69 735f 6d61 696e 5f70 726f 6365 7373  .is_main_process
+000005b0: 3f00 0000 7302 0000 0000 0172 1200 0000  ?...s......r....
+000005c0: 6300 0000 0000 0000 0000 0000 0001 0000  c...............
+000005d0: 0002 0000 0043 0000 0073 3800 0000 7400  .....C...s8...t.
+000005e0: a001 a100 730c 6401 5300 7400 a002 a100  ....s.d.S.t.....
+000005f0: 7318 6401 5300 7400 a003 a100 7d00 7c00  s.d.S.t.....}.|.
+00000600: 6402 6b02 722c 6401 5300 7400 a004 a100  d.k.r,d.S.t.....
+00000610: 0100 6401 5300 2903 7a6a 0a20 2020 2048  ..d.S.).zj.    H
+00000620: 656c 7065 7220 6675 6e63 7469 6f6e 2074  elper function t
+00000630: 6f20 7379 6e63 6872 6f6e 697a 6520 2862  o synchronize (b
+00000640: 6172 7269 6572 2920 616d 6f6e 6720 616c  arrier) among al
+00000650: 6c20 7072 6f63 6573 7365 7320 7768 656e  l processes when
+00000660: 0a20 2020 2075 7369 6e67 2064 6973 7472  .    using distr
+00000670: 6962 7574 6564 2074 7261 696e 696e 670a  ibuted training.
+00000680: 2020 2020 4e72 0300 0000 2905 7204 0000      Nr....).r...
+00000690: 0072 0500 0000 7206 0000 0072 0700 0000  .r....r....r....
+000006a0: 5a07 6261 7272 6965 7229 01da 0a77 6f72  Z.barrier)...wor
+000006b0: 6c64 5f73 697a 6572 0800 0000 7208 0000  ld_sizer....r...
+000006c0: 0072 0900 0000 da0b 7379 6e63 6872 6f6e  .r......synchron
+000006d0: 697a 6543 0000 0073 1000 0000 0005 0801  izeC...s........
+000006e0: 0401 0801 0401 0801 0801 0401 7214 0000  ............r...
+000006f0: 0063 0000 0000 0000 0000 0000 0000 0000  .c..............
+00000700: 0000 0300 0000 4300 0000 7324 0000 0074  ......C...s$...t
+00000710: 00a0 01a1 0064 016b 0272 1874 006a 0264  .....d.k.r.t.j.d
+00000720: 0264 038d 0153 0074 006a 036a 0453 0064  .d...S.t.j.j.S.d
+00000730: 0453 0029 057a 6a0a 2020 2020 5265 7475  .S.).zj.    Retu
+00000740: 726e 2061 2070 726f 6365 7373 2067 726f  rn a process gro
+00000750: 7570 2062 6173 6564 206f 6e20 676c 6f6f  up based on gloo
+00000760: 2062 6163 6b65 6e64 2c20 636f 6e74 6169   backend, contai
+00000770: 6e69 6e67 2061 6c6c 2074 6865 2072 616e  ning all the ran
+00000780: 6b73 0a20 2020 2054 6865 2072 6573 756c  ks.    The resul
+00000790: 7420 6973 2063 6163 6865 642e 0a20 2020  t is cached..   
+000007a0: 20da 046e 6363 6cda 0467 6c6f 6f29 01da   ..nccl..gloo)..
+000007b0: 0762 6163 6b65 6e64 4e29 0572 0400 0000  .backendN).r....
+000007c0: da0b 6765 745f 6261 636b 656e 645a 096e  ..get_backendZ.n
+000007d0: 6577 5f67 726f 7570 720d 0000 005a 0557  ew_groupr....Z.W
+000007e0: 4f52 4c44 7208 0000 0072 0800 0000 7208  ORLDr....r....r.
+000007f0: 0000 0072 0900 0000 da16 5f67 6574 5f67  ...r......_get_g
+00000800: 6c6f 6261 6c5f 676c 6f6f 5f67 726f 7570  lobal_gloo_group
+00000810: 5200 0000 7306 0000 0000 060c 010c 0272  R...s..........r
+00000820: 1900 0000 6302 0000 0000 0000 0000 0000  ....c...........
+00000830: 0008 0000 0007 0000 0043 0000 0073 8c00  .........C...s..
+00000840: 0000 7400 a001 7c01 a101 7d02 7c02 6401  ..t...|...}.|.d.
+00000850: 6b06 7316 7402 8201 7403 a004 7c02 6402  k.s.t...t...|.d.
+00000860: 6b02 7226 6403 6e02 6404 a101 7d03 7405  k.r&d.n.d...}.t.
+00000870: a006 7c00 a101 7d04 7407 7c04 8301 6405  ..|...}.t.|...d.
+00000880: 6b04 726a 7408 a009 740a a101 7d05 7c05  k.rjt...t...}.|.
+00000890: a00b 6406 a00c 740d 8300 7407 7c04 8301  ..d...t...t.|...
+000008a0: 6405 1b00 7c03 a103 a101 0100 7403 6a0e  d...|.......t.j.
+000008b0: a00f 7c04 a101 7d06 7403 a010 7c06 a101  ..|...}.t...|...
+000008c0: 6a11 7c03 6407 8d01 7d07 7c07 5300 2908  j.|.d...}.|.S.).
+000008d0: 4e29 0272 1600 0000 7215 0000 0072 1600  N).r....r....r..
+000008e0: 0000 da03 6370 755a 0463 7564 6169 0000  ....cpuZ.cudai..
+000008f0: 0040 7a3b 5261 6e6b 207b 7d20 7472 7969  .@z;Rank {} tryi
+00000900: 6e67 2074 6f20 616c 6c2d 6761 7468 6572  ng to all-gather
+00000910: 207b 3a2e 3266 7d20 4742 206f 6620 6461   {:.2f} GB of da
+00000920: 7461 206f 6e20 6465 7669 6365 207b 7d29  ta on device {})
+00000930: 01da 0664 6576 6963 6529 1272 0400 0000  ...device).r....
+00000940: 7218 0000 0072 0f00 0000 da05 746f 7263  r....r......torc
+00000950: 6872 1b00 0000 da06 7069 636b 6c65 da05  hr......pickle..
+00000960: 6475 6d70 73da 036c 656e da07 6c6f 6767  dumps..len..logg
+00000970: 696e 67da 0967 6574 4c6f 6767 6572 da08  ing..getLogger..
+00000980: 5f5f 6e61 6d65 5f5f da07 7761 726e 696e  __name__..warnin
+00000990: 67da 0666 6f72 6d61 7472 0b00 0000 5a0b  g..formatr....Z.
+000009a0: 4279 7465 5374 6f72 6167 65da 0b66 726f  ByteStorage..fro
+000009b0: 6d5f 6275 6666 6572 5a0a 4279 7465 5465  m_bufferZ.ByteTe
+000009c0: 6e73 6f72 da02 746f 2908 da04 6461 7461  nsor..to)...data
+000009d0: 720d 0000 0072 1700 0000 721b 0000 00da  r....r....r.....
+000009e0: 0662 7566 6665 72da 066c 6f67 6765 725a  .buffer..loggerZ
+000009f0: 0773 746f 7261 6765 da06 7465 6e73 6f72  .storage..tensor
+00000a00: 7208 0000 0072 0800 0000 7209 0000 00da  r....r....r.....
+00000a10: 145f 7365 7269 616c 697a 655f 746f 5f74  ._serialize_to_t
+00000a20: 656e 736f 725e 0000 0073 2000 0000 0001  ensor^...s .....
+00000a30: 0a01 0c01 1602 0a01 0c01 0a01 0401 0401  ................
+00000a40: 0400 0a00 02ff 02ff 0405 0c01 1201 722b  ..............r+
+00000a50: 0000 0063 0200 0000 0000 0000 0000 0000  ...c............
+00000a60: 0700 0000 0500 0000 0300 0000 73ae 0000  ............s...
+00000a70: 0074 006a 017c 0164 018d 017d 027c 0264  .t.j.|.d...}.|.d
+00000a80: 026b 0573 1c74 0264 0383 0182 0174 036a  .k.s.t.d.....t.j
+00000a90: 0488 00a0 05a1 0067 0174 036a 0688 006a  .......g.t.j...j
+00000aa0: 0764 048d 037d 0387 0066 0164 0564 0684  .d...}...f.d.d..
+00000ab0: 0874 087c 0283 0144 0083 017d 0474 006a  .t.|...D...}.t.j
+00000ac0: 097c 047c 037c 0164 018d 0301 0064 0764  .|.|.|.d.....d.d
+00000ad0: 0684 007c 0444 0083 017d 0474 0a7c 0483  ...|.D...}.t.|..
+00000ae0: 017d 057c 037c 056b 0372 a674 036a 0b7c  .}.|.|.k.r.t.j.|
+00000af0: 057c 0318 0066 0174 036a 0c88 006a 0764  .|...f.t.j...j.d
+00000b00: 048d 037d 0674 036a 0d88 007c 0666 0264  ...}.t.j...|.f.d
+00000b10: 0864 098d 0289 007c 0488 0066 0253 0029  .d.....|...f.S.)
+00000b20: 0a7a 7a0a 2020 2020 5265 7475 726e 733a  .zz.    Returns:
+00000b30: 0a20 2020 2020 2020 206c 6973 745b 696e  .        list[in
+00000b40: 745d 3a20 7369 7a65 206f 6620 7468 6520  t]: size of the 
+00000b50: 7465 6e73 6f72 2c20 6f6e 2065 6163 6820  tensor, on each 
+00000b60: 7261 6e6b 0a20 2020 2020 2020 2054 656e  rank.        Ten
+00000b70: 736f 723a 2070 6164 6465 6420 7465 6e73  sor: padded tens
+00000b80: 6f72 2074 6861 7420 6861 7320 7468 6520  or that has the 
+00000b90: 6d61 7820 7369 7a65 0a20 2020 2072 0c00  max size.    r..
+00000ba0: 0000 7203 0000 007a 4863 6f6d 6d2e 6761  ..r....zHcomm.ga
+00000bb0: 7468 6572 2f61 6c6c 5f67 6174 6865 7220  ther/all_gather 
+00000bc0: 6d75 7374 2062 6520 6361 6c6c 6564 2066  must be called f
+00000bd0: 726f 6d20 7261 6e6b 7320 7769 7468 696e  rom ranks within
+00000be0: 2074 6865 2067 6976 656e 2067 726f 7570   the given group
+00000bf0: 21a9 02da 0564 7479 7065 721b 0000 0063  !....dtyper....c
+00000c00: 0100 0000 0000 0000 0000 0000 0200 0000  ................
+00000c10: 0700 0000 1300 0000 7322 0000 0067 007c  ........s"...g.|
+00000c20: 005d 1a7d 0174 006a 0164 0067 0174 006a  .].}.t.j.d.g.t.j
+00000c30: 0288 006a 0364 018d 0391 0271 0453 0029  ...j.d.....q.S.)
+00000c40: 0272 0300 0000 722c 0000 0029 0472 1c00  .r....r,...).r..
+00000c50: 0000 da05 7a65 726f 73da 0569 6e74 3634  ....zeros..int64
+00000c60: 721b 0000 00a9 02da 022e 30da 015f a901  r.........0.._..
+00000c70: 722a 0000 0072 0800 0000 7209 0000 00da  r*...r....r.....
+00000c80: 0a3c 6c69 7374 636f 6d70 3e7b 0000 0073  .<listcomp>{...s
+00000c90: 0400 0000 0601 0200 7a2a 5f70 6164 5f74  ........z*_pad_t
+00000ca0: 6f5f 6c61 7267 6573 745f 7465 6e73 6f72  o_largest_tensor
+00000cb0: 2e3c 6c6f 6361 6c73 3e2e 3c6c 6973 7463  .<locals>.<listc
+00000cc0: 6f6d 703e 6301 0000 0000 0000 0000 0000  omp>c...........
+00000cd0: 0002 0000 0005 0000 0053 0000 0073 1800  .........S...s..
+00000ce0: 0000 6700 7c00 5d10 7d01 7400 7c01 a001  ..g.|.].}.t.|...
+00000cf0: a100 8301 9102 7104 5300 7208 0000 0029  ......q.S.r....)
+00000d00: 02da 0369 6e74 da04 6974 656d 2902 7231  ...int..item).r1
+00000d10: 0000 00da 0473 697a 6572 0800 0000 7208  .....sizer....r.
+00000d20: 0000 0072 0900 0000 7234 0000 007f 0000  ...r....r4......
+00000d30: 0073 0400 0000 0600 0200 7201 0000 00a9  .s........r.....
+00000d40: 01da 0364 696d 290e 7204 0000 0072 0700  ...dim).r....r..
+00000d50: 0000 720f 0000 0072 1c00 0000 722a 0000  ..r....r....r*..
+00000d60: 005a 056e 756d 656c 722f 0000 0072 1b00  .Z.numelr/...r..
+00000d70: 0000 da05 7261 6e67 65da 0a61 6c6c 5f67  ....range..all_g
+00000d80: 6174 6865 72da 036d 6178 722e 0000 00da  ather..maxr.....
+00000d90: 0575 696e 7438 da03 6361 7429 0772 2a00  .uint8..cat).r*.
+00000da0: 0000 720d 0000 0072 1300 0000 5a0a 6c6f  ..r....r....Z.lo
+00000db0: 6361 6c5f 7369 7a65 da09 7369 7a65 5f6c  cal_size..size_l
+00000dc0: 6973 74da 086d 6178 5f73 697a 65da 0770  ist..max_size..p
+00000dd0: 6164 6469 6e67 7208 0000 0072 3300 0000  addingr....r3...
+00000de0: 7209 0000 00da 165f 7061 645f 746f 5f6c  r......_pad_to_l
+00000df0: 6172 6765 7374 5f74 656e 736f 7270 0000  argest_tensorp..
+00000e00: 0073 2000 0000 0006 0c02 06ff 0402 02fe  .s .............
+00000e10: 0403 1a01 0a01 06ff 0603 1001 0e02 0804  ................
+00000e20: 0801 1a01 1201 7242 0000 0063 0200 0000  ......rB...c....
+00000e30: 0000 0000 0000 0000 0700 0000 0600 0000  ................
+00000e40: 0300 0000 73ba 0000 0074 0083 0064 016b  ....s....t...d.k
+00000e50: 0272 107c 0067 0153 007c 0164 026b 0872  .r.|.g.S.|.d.k.r
+00000e60: 1e74 0183 007d 0174 02a0 007c 01a1 0164  .t...}.t...|...d
+00000e70: 016b 0272 327c 0067 0153 0074 037c 007c  .k.r2|.g.S.t.|.|
+00000e80: 0183 0289 0174 0488 017c 0183 025c 027d  .....t...|...\.}
+00000e90: 0289 0174 057c 0283 0189 0087 0087 0166  ...t.|.........f
+00000ea0: 0264 0364 0484 087c 0244 0083 017d 0374  .d.d...|.D...}.t
+00000eb0: 026a 067c 0388 017c 0164 058d 0301 0067  .j.|...|.d.....g
+00000ec0: 007d 0474 077c 027c 0383 0244 005d 305c  .}.t.|.|...D.]0\
+00000ed0: 027d 0589 0188 01a0 08a1 00a0 09a1 00a0  .}..............
+00000ee0: 0aa1 0064 027c 0585 0219 007d 067c 04a0  ...d.|.....}.|..
+00000ef0: 0b74 0ca0 0d7c 06a1 01a1 0101 0071 847c  .t...|.......q.|
+00000f00: 0453 0029 0661 3d01 0000 0a20 2020 2052  .S.).a=....    R
+00000f10: 756e 2061 6c6c 5f67 6174 6865 7220 6f6e  un all_gather on
+00000f20: 2061 7262 6974 7261 7279 2070 6963 6b6c   arbitrary pickl
+00000f30: 6162 6c65 2064 6174 6120 286e 6f74 206e  able data (not n
+00000f40: 6563 6573 7361 7269 6c79 2074 656e 736f  ecessarily tenso
+00000f50: 7273 292e 0a0a 2020 2020 4172 6773 3a0a  rs)...    Args:.
+00000f60: 2020 2020 2020 2020 6461 7461 3a20 616e          data: an
+00000f70: 7920 7069 636b 6c61 626c 6520 6f62 6a65  y picklable obje
+00000f80: 6374 0a20 2020 2020 2020 2067 726f 7570  ct.        group
+00000f90: 3a20 6120 746f 7263 6820 7072 6f63 6573  : a torch proces
+00000fa0: 7320 6772 6f75 702e 2042 7920 6465 6661  s group. By defa
+00000fb0: 756c 742c 2077 696c 6c20 7573 6520 6120  ult, will use a 
+00000fc0: 6772 6f75 7020 7768 6963 680a 2020 2020  group which.    
+00000fd0: 2020 2020 2020 2020 636f 6e74 6169 6e73          contains
+00000fe0: 2061 6c6c 2072 616e 6b73 206f 6e20 676c   all ranks on gl
+00000ff0: 6f6f 2062 6163 6b65 6e64 2e0a 0a20 2020  oo backend...   
+00001000: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
+00001010: 2020 6c69 7374 5b64 6174 615d 3a20 6c69    list[data]: li
+00001020: 7374 206f 6620 6461 7461 2067 6174 6865  st of data gathe
+00001030: 7265 6420 6672 6f6d 2065 6163 6820 7261  red from each ra
+00001040: 6e6b 0a20 2020 2072 0300 0000 4e63 0100  nk.    r....Nc..
+00001050: 0000 0000 0000 0000 0000 0200 0000 0700  ................
+00001060: 0000 1300 0000 7322 0000 0067 007c 005d  ......s"...g.|.]
+00001070: 1a7d 0174 006a 0188 0066 0174 006a 0288  .}.t.j...f.t.j..
+00001080: 016a 0364 008d 0391 0271 0453 00a9 0172  .j.d.....q.S...r
+00001090: 2c00 0000 a904 721c 0000 00da 0565 6d70  ,.....r......emp
+000010a0: 7479 723d 0000 0072 1b00 0000 7230 0000  tyr=...r....r0..
+000010b0: 00a9 0272 4000 0000 722a 0000 0072 0800  ...r@...r*...r..
+000010c0: 0000 7209 0000 0072 3400 0000 a400 0000  ..r....r4.......
+000010d0: 7304 0000 0006 0102 007a 1e61 6c6c 5f67  s........z.all_g
+000010e0: 6174 6865 722e 3c6c 6f63 616c 733e 2e3c  ather.<locals>.<
+000010f0: 6c69 7374 636f 6d70 3e72 0c00 0000 290e  listcomp>r....).
+00001100: 7207 0000 0072 1900 0000 7204 0000 0072  r....r....r....r
+00001110: 2b00 0000 7242 0000 0072 3c00 0000 723b  +...rB...r<...r;
+00001120: 0000 00da 037a 6970 721a 0000 00da 056e  .....zipr......n
+00001130: 756d 7079 da07 746f 6279 7465 73da 0661  umpy..tobytes..a
+00001140: 7070 656e 6472 1d00 0000 da05 6c6f 6164  ppendr......load
+00001150: 7329 0772 2700 0000 720d 0000 0072 3f00  s).r'...r....r?.
+00001160: 0000 da0b 7465 6e73 6f72 5f6c 6973 74da  ....tensor_list.
+00001170: 0964 6174 615f 6c69 7374 7237 0000 0072  .data_listr7...r
+00001180: 2800 0000 7208 0000 0072 4600 0000 7209  (...r....rF...r.
+00001190: 0000 0072 3b00 0000 8b00 0000 7324 0000  ...r;.......s$..
+000011a0: 0000 0c0a 0106 0108 0106 010e 0106 020a  ................
+000011b0: 020e 0108 030c 0102 ff06 0310 0204 0112  ................
+000011c0: 0118 0112 0272 3b00 0000 6303 0000 0000  .....r;...c.....
+000011d0: 0000 0000 0000 0009 0000 0006 0000 0003  ................
+000011e0: 0000 0073 ec00 0000 7400 8300 6401 6b02  ...s....t...d.k.
+000011f0: 7210 7c00 6701 5300 7c02 6402 6b08 721e  r.|.g.S.|.d.k.r.
+00001200: 7401 8300 7d02 7402 6a00 7c02 6403 8d01  t...}.t.j.|.d...
+00001210: 6401 6b02 7234 7c00 6701 5300 7402 6a03  d.k.r4|.g.S.t.j.
+00001220: 7c02 6403 8d01 7d03 7404 7c00 7c02 8302  |.d...}.t.|.|...
+00001230: 8901 7405 8801 7c02 8302 5c02 7d04 8901  ..t...|...\.}...
+00001240: 7c03 7c01 6b02 72d2 7406 7c04 8301 8900  |.|.k.r.t.|.....
+00001250: 8700 8701 6602 6404 6405 8408 7c04 4400  ....f.d.d...|.D.
+00001260: 8301 7d05 7402 6a07 8801 7c05 7c01 7c02  ..}.t.j...|.|.|.
+00001270: 6406 8d04 0100 6700 7d06 7408 7c04 7c05  d.....g.}.t.|.|.
+00001280: 8302 4400 5d30 5c02 7d07 8901 8801 a009  ..D.]0\.}.......
+00001290: a100 a00a a100 a00b a100 6402 7c07 8502  ..........d.|...
+000012a0: 1900 7d08 7c06 a00c 740d a00e 7c08 a101  ..}.|...t...|...
+000012b0: a101 0100 719c 7c06 5300 7402 6a07 8801  ....q.|.S.t.j...
+000012c0: 6700 7c01 7c02 6406 8d04 0100 6700 5300  g.|.|.d.....g.S.
+000012d0: 6402 5300 2907 618e 0100 000a 2020 2020  d.S.).a.....    
+000012e0: 5275 6e20 6761 7468 6572 206f 6e20 6172  Run gather on ar
+000012f0: 6269 7472 6172 7920 7069 636b 6c61 626c  bitrary picklabl
+00001300: 6520 6461 7461 2028 6e6f 7420 6e65 6365  e data (not nece
+00001310: 7373 6172 696c 7920 7465 6e73 6f72 7329  ssarily tensors)
+00001320: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   
+00001330: 2020 2020 2064 6174 613a 2061 6e79 2070       data: any p
+00001340: 6963 6b6c 6162 6c65 206f 626a 6563 740a  icklable object.
+00001350: 2020 2020 2020 2020 6473 7420 2869 6e74          dst (int
+00001360: 293a 2064 6573 7469 6e61 7469 6f6e 2072  ): destination r
+00001370: 616e 6b0a 2020 2020 2020 2020 6772 6f75  ank.        grou
+00001380: 703a 2061 2074 6f72 6368 2070 726f 6365  p: a torch proce
+00001390: 7373 2067 726f 7570 2e20 4279 2064 6566  ss group. By def
+000013a0: 6175 6c74 2c20 7769 6c6c 2075 7365 2061  ault, will use a
+000013b0: 2067 726f 7570 2077 6869 6368 0a20 2020   group which.   
+000013c0: 2020 2020 2020 2020 2063 6f6e 7461 696e           contain
+000013d0: 7320 616c 6c20 7261 6e6b 7320 6f6e 2067  s all ranks on g
+000013e0: 6c6f 6f20 6261 636b 656e 642e 0a0a 2020  loo backend...  
+000013f0: 2020 5265 7475 726e 733a 0a20 2020 2020    Returns:.     
+00001400: 2020 206c 6973 745b 6461 7461 5d3a 206f     list[data]: o
+00001410: 6e20 6473 742c 2061 206c 6973 7420 6f66  n dst, a list of
+00001420: 2064 6174 6120 6761 7468 6572 6564 2066   data gathered f
+00001430: 726f 6d20 6561 6368 2072 616e 6b2e 204f  rom each rank. O
+00001440: 7468 6572 7769 7365 2c0a 2020 2020 2020  therwise,.      
+00001450: 2020 2020 2020 616e 2065 6d70 7479 206c        an empty l
+00001460: 6973 742e 0a20 2020 2072 0300 0000 4e72  ist..    r....Nr
+00001470: 0c00 0000 6301 0000 0000 0000 0000 0000  ....c...........
+00001480: 0002 0000 0007 0000 0013 0000 0073 2200  .............s".
+00001490: 0000 6700 7c00 5d1a 7d01 7400 6a01 8800  ..g.|.].}.t.j...
+000014a0: 6601 7400 6a02 8801 6a03 6400 8d03 9102  f.t.j...j.d.....
+000014b0: 7104 5300 7243 0000 0072 4400 0000 7230  q.S.rC...rD...r0
+000014c0: 0000 0072 4600 0000 7208 0000 0072 0900  ...rF...r....r..
+000014d0: 0000 7234 0000 00cd 0000 0073 0400 0000  ..r4.......s....
+000014e0: 0601 0200 7a1a 6761 7468 6572 2e3c 6c6f  ....z.gather.<lo
+000014f0: 6361 6c73 3e2e 3c6c 6973 7463 6f6d 703e  cals>.<listcomp>
+00001500: 2902 da03 6473 7472 0d00 0000 290f 7207  )...dstr....).r.
+00001510: 0000 0072 1900 0000 7204 0000 0072 0b00  ...r....r....r..
+00001520: 0000 722b 0000 0072 4200 0000 723c 0000  ..r+...rB...r<..
+00001530: 00da 0667 6174 6865 7272 4700 0000 721a  ...gatherrG...r.
+00001540: 0000 0072 4800 0000 7249 0000 0072 4a00  ...rH...rI...rJ.
+00001550: 0000 721d 0000 0072 4b00 0000 2909 7227  ..r....rK...).r'
+00001560: 0000 0072 4e00 0000 720d 0000 00da 0472  ...rN...r......r
+00001570: 616e 6b72 3f00 0000 724c 0000 0072 4d00  ankr?...rL...rM.
+00001580: 0000 7237 0000 0072 2800 0000 7208 0000  ..r7...r(...r...
+00001590: 0072 4600 0000 7209 0000 0072 4f00 0000  .rF...r....rO...
+000015a0: b100 0000 732c 0000 0000 0e0a 0106 0108  ....s,..........
+000015b0: 0106 0110 0106 010c 020a 010e 0308 0108  ................
+000015c0: 010c 0102 ff06 0312 0204 0112 0118 0112  ................
+000015d0: 0104 0212 0172 4f00 0000 6300 0000 0000  .....rO...c.....
+000015e0: 0000 0000 0000 0002 0000 0003 0000 0043  ...............C
+000015f0: 0000 0073 1c00 0000 7400 6a01 a002 6401  ...s....t.j...d.
+00001600: a101 7d00 7403 7c00 8301 7d01 7c01 6402  ..}.t.|...}.|.d.
+00001610: 1900 5300 2903 7afd 0a20 2020 2052 6574  ..S.).z..    Ret
+00001620: 7572 6e73 3a0a 2020 2020 2020 2020 696e  urns:.        in
+00001630: 743a 2061 2072 616e 646f 6d20 6e75 6d62  t: a random numb
+00001640: 6572 2074 6861 7420 6973 2074 6865 2073  er that is the s
+00001650: 616d 6520 6163 726f 7373 2061 6c6c 2077  ame across all w
+00001660: 6f72 6b65 7273 2e0a 2020 2020 2020 2020  orkers..        
+00001670: 2020 2020 4966 2077 6f72 6b65 7273 206e      If workers n
+00001680: 6565 6420 6120 7368 6172 6564 2052 4e47  eed a shared RNG
+00001690: 2c20 7468 6579 2063 616e 2075 7365 2074  , they can use t
+000016a0: 6869 7320 7368 6172 6564 2073 6565 6420  his shared seed 
+000016b0: 746f 0a20 2020 2020 2020 2020 2020 2063  to.            c
+000016c0: 7265 6174 6520 6f6e 652e 0a0a 2020 2020  reate one...    
+000016d0: 416c 6c20 776f 726b 6572 7320 6d75 7374  All workers must
+000016e0: 2063 616c 6c20 7468 6973 2066 756e 6374   call this funct
+000016f0: 696f 6e2c 206f 7468 6572 7769 7365 2069  ion, otherwise i
+00001700: 7420 7769 6c6c 2064 6561 646c 6f63 6b2e  t will deadlock.
+00001710: 0a20 2020 206c 0300 0000 0000 0000 0200  .    l..........
+00001720: 7201 0000 0029 04da 026e 70da 0672 616e  r....)...np..ran
+00001730: 646f 6dda 0772 616e 6469 6e74 723b 0000  dom..randintr;..
+00001740: 0029 025a 0469 6e74 735a 0861 6c6c 5f69  .).Z.intsZ.all_i
+00001750: 6e74 7372 0800 0000 7208 0000 0072 0900  ntsr....r....r..
+00001760: 0000 da12 7368 6172 6564 5f72 616e 646f  ....shared_rando
+00001770: 6d5f 7365 6564 dc00 0000 7306 0000 0000  m_seed....s.....
+00001780: 090c 0108 0172 5400 0000 5463 0200 0000  .....rT...Tc....
+00001790: 0000 0000 0000 0000 0700 0000 0900 0000  ................
+000017a0: 4300 0000 73a4 0000 0074 0083 007d 027c  C...s....t...}.|
+000017b0: 0264 016b 0072 127c 0053 0074 01a0 02a1  .d.k.r.|.S.t....
+000017c0: 008f 8001 0067 007d 0367 007d 0474 037c  .....g.}.g.}.t.|
+000017d0: 00a0 04a1 0083 0144 005d 1c7d 057c 03a0  .......D.].}.|..
+000017e0: 057c 05a1 0101 007c 04a0 057c 007c 0519  .|.....|...|.|..
+000017f0: 00a1 0101 0071 3074 016a 067c 0464 0264  .....q0t.j.|.d.d
+00001800: 038d 027d 0474 076a 087c 0464 0264 048d  ...}.t.j.|.d.d..
+00001810: 0201 0074 07a0 09a1 0064 026b 0272 827c  ...t.....d.k.r.|
+00001820: 0172 827c 047c 021d 007d 0464 0564 0684  .r.|.|...}.d.d..
+00001830: 0074 0a7c 037c 0483 0244 0083 017d 0657  .t.|.|...D...}.W
+00001840: 0035 0051 0052 0058 007c 0653 0029 0761  .5.Q.R.X.|.S.).a
+00001850: 6501 0000 0a20 2020 2052 6564 7563 6520  e....    Reduce 
+00001860: 7468 6520 7661 6c75 6573 2069 6e20 7468  the values in th
+00001870: 6520 6469 6374 696f 6e61 7279 2066 726f  e dictionary fro
+00001880: 6d20 616c 6c20 7072 6f63 6573 7365 7320  m all processes 
+00001890: 736f 2074 6861 7420 7072 6f63 6573 7320  so that process 
+000018a0: 7769 7468 2072 616e 6b0a 2020 2020 3020  with rank.    0 
+000018b0: 6861 7320 7468 6520 7265 6475 6365 6420  has the reduced 
+000018c0: 7265 7375 6c74 732e 0a0a 2020 2020 4172  results...    Ar
+000018d0: 6773 3a0a 2020 2020 2020 2020 696e 7075  gs:.        inpu
+000018e0: 745f 6469 6374 2028 6469 6374 293a 2069  t_dict (dict): i
+000018f0: 6e70 7574 7320 746f 2062 6520 7265 6475  nputs to be redu
+00001900: 6365 642e 2041 6c6c 2074 6865 2076 616c  ced. All the val
+00001910: 7565 7320 6d75 7374 2062 6520 7363 616c  ues must be scal
+00001920: 6172 2043 5544 4120 5465 6e73 6f72 2e0a  ar CUDA Tensor..
+00001930: 2020 2020 2020 2020 6176 6572 6167 6520          average 
+00001940: 2862 6f6f 6c29 3a20 7768 6574 6865 7220  (bool): whether 
+00001950: 746f 2064 6f20 6176 6572 6167 6520 6f72  to do average or
+00001960: 2073 756d 0a0a 2020 2020 5265 7475 726e   sum..    Return
+00001970: 733a 0a20 2020 2020 2020 2061 2064 6963  s:.        a dic
+00001980: 7420 7769 7468 2074 6865 2073 616d 6520  t with the same 
+00001990: 6b65 7973 2061 7320 696e 7075 745f 6469  keys as input_di
+000019a0: 6374 2c20 6166 7465 7220 7265 6475 6374  ct, after reduct
+000019b0: 696f 6e2e 0a20 2020 20e9 0200 0000 7201  ion..    .....r.
+000019c0: 0000 0072 3800 0000 2901 724e 0000 0063  ...r8...).rN...c
+000019d0: 0100 0000 0000 0000 0000 0000 0300 0000  ................
+000019e0: 0400 0000 5300 0000 7316 0000 0069 007c  ....S...s....i.|
+000019f0: 005d 0e5c 027d 017d 027c 017c 0293 0271  .].\.}.}.|.|...q
+00001a00: 0453 0072 0800 0000 7208 0000 0029 0372  .S.r....r....).r
+00001a10: 3100 0000 da01 6bda 0176 7208 0000 0072  1.....k..vr....r
+00001a20: 0800 0000 7209 0000 00da 0a3c 6469 6374  ....r......<dict
+00001a30: 636f 6d70 3e06 0100 0073 0600 0000 0600  comp>....s......
+00001a40: 0600 0200 7a1f 7265 6475 6365 5f64 6963  ....z.reduce_dic
+00001a50: 742e 3c6c 6f63 616c 733e 2e3c 6469 6374  t.<locals>.<dict
+00001a60: 636f 6d70 3e29 0b72 0700 0000 721c 0000  comp>).r....r...
+00001a70: 005a 076e 6f5f 6772 6164 da06 736f 7274  .Z.no_grad..sort
+00001a80: 6564 da04 6b65 7973 724a 0000 00da 0573  ed..keysrJ.....s
+00001a90: 7461 636b 7204 0000 00da 0672 6564 7563  tackr......reduc
+00001aa0: 6572 0b00 0000 7247 0000 0029 075a 0a69  er....rG...).Z.i
+00001ab0: 6e70 7574 5f64 6963 74da 0761 7665 7261  nput_dict..avera
+00001ac0: 6765 7213 0000 00da 056e 616d 6573 da06  ger......names..
+00001ad0: 7661 6c75 6573 7256 0000 005a 0c72 6564  valuesrV...Z.red
+00001ae0: 7563 6564 5f64 6963 7472 0800 0000 7208  uced_dictr....r.
+00001af0: 0000 0072 0900 0000 da0b 7265 6475 6365  ...r......reduce
+00001b00: 5f64 6963 74ea 0000 0073 1e00 0000 000c  _dict....s......
+00001b10: 0601 0801 0401 0a01 0401 0402 1001 0a01  ................
+00001b20: 1001 0e01 0e01 1003 0801 1e01 7260 0000  ............r`..
+00001b30: 0029 014e 2902 7201 0000 004e 2901 5429  .).N).r....N).T)
+00001b40: 1bda 075f 5f64 6f63 5f5f da09 6675 6e63  ...__doc__..func
+00001b50: 746f 6f6c 7372 2000 0000 7248 0000 0072  toolsr ...rH...r
+00001b60: 5100 0000 721d 0000 0072 1c00 0000 5a11  Q...r....r....Z.
+00001b70: 746f 7263 682e 6469 7374 7269 6275 7465  torch.distribute
+00001b80: 645a 0b64 6973 7472 6962 7574 6564 7204  dZ.distributedr.
+00001b90: 0000 0072 0e00 0000 7235 0000 0072 0700  ...r....r5...r..
+00001ba0: 0000 720b 0000 0072 1000 0000 7211 0000  ..r....r....r...
+00001bb0: 00da 0462 6f6f 6c72 1200 0000 7214 0000  ...boolr....r...
+00001bc0: 00da 096c 7275 5f63 6163 6865 7219 0000  ...lru_cacher...
+00001bd0: 0072 2b00 0000 7242 0000 0072 3b00 0000  .r+...rB...r;...
+00001be0: 724f 0000 0072 5400 0000 7260 0000 0072  rO...rT...r`...r
+00001bf0: 0800 0000 7208 0000 0072 0800 0000 7209  ....r....r....r.
+00001c00: 0000 00da 083c 6d6f 6475 6c65 3e02 0000  .....<module>...
+00001c10: 0073 2a00 0000 0405 0801 0801 0801 0801  .s*.............
+00001c20: 0801 0c02 0407 0e08 0e08 0e0d 0e0d 0e04  ................
+00001c30: 080f 0601 0a0b 0812 081b 0a26 0a2b 080e  ...........&.+..
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/ddp/ddp_utils.py` & `tl2-0.1.1/tl2/proj/pytorch/ddp/ddp_utils.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/pytorch/downsampler.py` & `tl2-0.1.1/tl2/proj/pytorch/downsampler.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/cips3d/pigan_gen_celeba.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/cips3d/pigan_gen_celeba.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,1436 +1,1436 @@
-from itertools import chain
-import math
-import logging
-import collections
-from collections import OrderedDict
-import tqdm
-import random
-import time
-from einops import rearrange, repeat
-import numpy as np
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.cuda.amp import autocast
-
-from tl2.proj.fvcore import MODEL_REGISTRY, build_model
-# from tl2.proj.stylegan2_ada import persistence
-from tl2.launch.launch_utils import global_cfg
-from tl2.proj.pytorch.pytorch_hook import VerboseModel
-from tl2.proj.pytorch import torch_utils
-from tl2.proj.pytorch import torch_utils, init_func
-from tl2 import tl2_utils
-from tl2.proj.pytorch.examples.nerf import cam_params
-from tl2.proj.pytorch.examples.nerf import volume_rendering
-from tl2.proj.pytorch.examples.networks import nerf_net
-from tl2.proj.pytorch.examples.networks import multi_head_mapping
-from tl2.proj.pytorch.examples.networks import siren_net_pigan, cips_net
-
-from exp.pigan import pigan_utils
-from exp.dev.nerf_inr.models.generator_nerf_inr import INRNetwork
-from exp.dev.nerf_inr.models.generator_nerf_inr import GeneratorNerfINR as GeneratorNerfINR_base
-from exp.comm import comm_utils
-from exp.comm.models import nerf_network
-from exp.comm.models import inr_network
-from exp.comm.models import film_layer
-from exp.comm.models import mod_conv_fc
-# from exp.cips3d.models import multi_head_mapping
-
-
-class SkipLayer(nn.Module):
-  def __init__(self, ):
-    super(SkipLayer, self).__init__()
-
-  def forward(self, x0, x1):
-    # out = (x0 + x1) / math.pi
-    out = (x0 + x1)
-    return out
-
-
-
-class SinAct(nn.Module):
-  def __init__(self, ):
-    super(SinAct, self).__init__()
-
-  def forward(self, x):
-    return torch.sin(x)
-
-
-class LinearSinAct(nn.Module):
-  def __init__(self,
-               in_features,
-               out_features):
-    super(LinearSinAct, self).__init__()
-
-    self.linear = nn.Linear(in_features=in_features, out_features=out_features)
-    self.sin = SinAct()
-    pass
-
-  def forward(self, x, *args, **kwargs):
-    x = self.linear(x)
-    x = self.sin(x)
-    return x
-
-
-class FiLMLayer(nn.Module):
-  def __init__(self,
-               in_dim,
-               out_dim,
-               style_dim,
-               use_style_fc=True,
-               which_linear=nn.Linear,
-               **kwargs):
-    super(FiLMLayer, self).__init__()
-
-    self.in_dim = in_dim
-    self.out_dim = out_dim
-    self.style_dim = style_dim
-    self.use_style_fc = use_style_fc
-
-    self.linear = which_linear(in_dim, out_dim)
-    # self.linear.apply(film_layer.frequency_init(25))
-
-    # self.gain_scale = film_layer.LinearScale(scale=15, bias=30)
-    self.gain_scale = nn.Identity()
-    # Prepare gain and bias layers
-    if use_style_fc:
-      self.gain_fc = which_linear(style_dim, out_dim)
-      self.bias_fc = which_linear(style_dim, out_dim)
-      # self.gain_fc.weight.data.mul_(0.25)
-      # self.bias_fc.weight.data.mul_(0.25)
-    else:
-      self.style_dim = out_dim * 2
-
-    self.sin = SinAct()
-    self.lrelu = nn.LeakyReLU(0.2, inplace=True)
-    # self.register_buffer('stored_mean', torch.zeros(output_size))
-    # self.register_buffer('stored_var', torch.ones(output_size))
-    pass
-
-  def forward(self,
-              x,
-              style):
-    """
-
-    :param x: (b, c) or (b, n, c)
-    :param style: (b, c)
-    :return:
-    """
-
-    if self.use_style_fc:
-      gain = self.gain_fc(style)
-      gain = self.gain_scale(gain)
-      bias = self.bias_fc(style)
-    else:
-      style = rearrange(style, "b (n c) -> b n c", n=2)
-      gain, bias = style.unbind(dim=1)
-      gain = self.gain_scale(gain)
-
-    if x.dim() == 3:
-      gain = rearrange(gain, "b c -> b 1 c")
-      bias = rearrange(bias, "b c -> b 1 c")
-    elif x.dim() == 2:
-      pass
-    else:
-      assert 0
-
-    x = self.linear(x)
-
-    x = x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + 1e-8)
-    # out = self.sin(gain * x + bias)
-    out = self.lrelu((gain + 1.) * x + bias)
-
-    return out
-
-  def __repr__(self):
-    s = f'{self.__class__.__name__}(' \
-        f'in_dim={self.in_dim}, ' \
-        f'out_dim={self.out_dim}, ' \
-        f'style_dim={self.style_dim}, ' \
-        f'use_style_fc={self.use_style_fc}, ' \
-        f')'
-    return s
-
-
-class INRNetwork_Skip(nn.Module):
-  def __repr__(self): return f"{self.__class__.__name__}({self.repr})"
-
-  def __init__(self,
-               input_dim,
-               style_dim,
-               hidden_layers,
-               dim_scale=1,
-               rgb_dim=3,
-               device=None,
-               name_prefix='inr',
-               **kwargs):
-    """
-
-    :param z_dim:
-    :param hidden_dim:
-    :param rgb_dim:
-    :param device:
-    :param kwargs:
-    """
-    super().__init__()
-
-    self.repr = f"input_dim={input_dim}, " \
-                f"style_dim={style_dim}, " \
-                f"hidden_layers={hidden_layers}, " \
-                f"dim_scale={dim_scale}, "
-
-    self.device = device
-    self.rgb_dim = rgb_dim
-    self.hidden_layers = hidden_layers
-    self.name_prefix = name_prefix
-
-    self.channels = {
-      0: int(512 * dim_scale),  # 4
-      1: int(512 * dim_scale),  # 8
-      2: int(512 * dim_scale),  # 16
-      3: int(512 * dim_scale),  # 32
-      4: int(512 * dim_scale),  # 64
-      5: int(128 * dim_scale),  # 128
-      6: int(64 * dim_scale),  # 256
-      7: int(32 * dim_scale),  # 512
-      8: int(16 * dim_scale),  # 1024
-    }
-
-    self.style_dim_dict = {}
-
-    _out_dim = input_dim
-
-    self.network = nn.ModuleList()
-    self.to_rbgs = nn.ModuleList()
-    for i in range(hidden_layers):
-      _in_dim = _out_dim
-      _out_dim = self.channels[i]
-
-      _layer = film_layer.FiLMLayer(in_dim=_in_dim,
-                                    out_dim=_out_dim,
-                                    style_dim=style_dim)
-      self.network.append(_layer)
-      self.style_dim_dict[f'{name_prefix}_w{i}_0'] = _layer.style_dim
-
-      _layer = film_layer.FiLMLayer(in_dim=_out_dim,
-                                    out_dim=_out_dim,
-                                    style_dim=style_dim)
-      self.network.append(_layer)
-      self.style_dim_dict[f'{name_prefix}_w{i}_1'] = _layer.style_dim
-
-      to_rgb = inr_network.ToRGB(in_dim=_out_dim, dim_rgb=3)
-      self.to_rbgs.append(to_rgb)
-
-    self.tanh = nn.Sequential(
-      # nn.Linear(hidden_dim, rgb_dim),
-      nn.Tanh()
-    )
-    # self.to_rbg.apply(frequency_init(25))
-
-    torch_utils.print_number_params(
-      {
-        'network': self.network,
-        'to_rbgs': self.to_rbgs,
-        'inr_net': self
-      })
-    logging.getLogger('tl').info(self)
-    pass
-
-  def forward(self,
-              input,
-              style_dict,
-              **kwargs):
-    """
-
-    :param input: points xyz, (b, num_points, 3)
-    :param style_dict:
-    :param ray_directions: (b, num_points, 3)
-    :param kwargs:
-    :return:
-    - out: (b, num_points, 4), rgb(3) + sigma(1)
-    """
-
-    x = input
-    rgb = 0
-    for index in range(self.hidden_layers):
-
-      _layer = self.network[index * 2]
-      style = style_dict[f'{self.name_prefix}_w{index}_0']
-
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(_layer,
-                                     inputs_args=(x, style),
-                                     name_prefix=f"{self.name_prefix}.network.{index}.0.")
-      x = _layer(x, style)
-
-      _layer = self.network[index * 2 + 1]
-      style = style_dict[f'{self.name_prefix}_w{index}_1']
-
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(_layer,
-                                     inputs_args=(x, style),
-                                     name_prefix=f"{self.name_prefix}.network.{index}.1.")
-      x = _layer(x, style)
-
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.to_rbgs[index],
-                                     inputs_args=(x, rgb),
-                                     name_prefix=f'to_rgb.{index}')
-      rgb = self.to_rbgs[index](x, skip=rgb)
-
-    # if global_cfg.tl_debug:
-    #   VerboseModel.forward_verbose(self.to_rbg,
-    #                                inputs_args=(x, ),
-    #                                name_prefix='to_rgb.')
-    # out = self.to_rbg(x)
-    if global_cfg.tl_debug:
-      VerboseModel.forward_verbose(self.tanh,
-                                   inputs_args=(rgb, ),
-                                   name_prefix='tanh.')
-    out = self.tanh(rgb)
-    return out
-
-
-
-class ModSinLayer(nn.Module):
-  def __repr__(self): return f"{self.__class__.__name__}({self.repr})"
-
-  def __init__(self,
-               in_dim,
-               use_style_fc=False,
-               style_dim=None,
-               which_linear=nn.Linear,
-               spectral_norm=False,
-               eps=1e-5,
-               freq=1,
-               phase=0,
-               **kwargs):
-    super(ModSinLayer, self).__init__()
-
-    self.repr = f"in_dim={in_dim}, use_style_fc={use_style_fc}, style_dim={style_dim}, " \
-                f"freq={freq}, phase={phase}"
-
-    self.in_dim = in_dim
-    self.use_style_fc = use_style_fc
-    self.style_dim = style_dim
-    self.freq = freq
-    self.phase = phase
-
-    self.spectral_norm = spectral_norm
-    # Prepare gain and bias layers
-
-    if use_style_fc:
-      self.gain_fc = which_linear(style_dim, in_dim)
-      self.bias_fc = which_linear(style_dim, in_dim)
-      if spectral_norm:
-        self.gain_fc = nn.utils.spectral_norm(self.gain_fc)
-        self.bias_fc = nn.utils.spectral_norm(self.bias_fc)
-    else:
-      self.style_dim = in_dim * 2
-
-    self.eps = eps
-
-    self.lrelu = nn.LeakyReLU(0.2, inplace=True)
-    # self.register_buffer('stored_mean', torch.zeros(output_size))
-    # self.register_buffer('stored_var', torch.ones(output_size))
-    pass
-
-  def forward(self,
-              x,
-              style):
-    """
-    Calculate class-conditional gains and biases.
-
-    :param x: (b, c) or (b, n, c)
-    :param style: (b, c)
-    :return:
-    """
-    assert style.shape[-1] == self.style_dim
-
-    if self.use_style_fc:
-      gain = self.gain_fc(style) + 1.
-      bias = self.bias_fc(style)
-    else:
-      style = rearrange(style, "b (n c) -> b n c", n=2)
-      gain, bias = style.unbind(dim=1)
-      gain = gain + 1.
-
-    if x.dim() == 3:
-      gain = rearrange(gain, "b c -> b 1 c")
-      bias = rearrange(bias, "b c -> b 1 c")
-    elif x.dim() == 2:
-      pass
-    else:
-      assert 0
-
-    # x = torch.sin(self.freq * x + self.phase)
-    # out = x * gain + bias
-
-    x = x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + 1e-8)
-    x = x * gain + bias
-
-    out = self.lrelu(x)
-    return out
-
-
-class ModSinLayer_NoBias(nn.Module):
-  def __repr__(self): return f"{self.__class__.__name__}({self.repr})"
-
-  def __init__(self,
-               in_dim,
-               use_style_fc=False,
-               style_dim=None,
-               which_linear=nn.Linear,
-               spectral_norm=False,
-               eps=1e-5,
-               freq=1,
-               phase=0,
-               **kwargs):
-    super(ModSinLayer_NoBias, self).__init__()
-
-    self.repr = f"in_dim={in_dim}, use_style_fc={use_style_fc}, style_dim={style_dim}, " \
-                f"freq={freq}, phase={phase}"
-
-    self.in_dim = in_dim
-    self.use_style_fc = use_style_fc
-    self.style_dim = style_dim
-    self.freq = freq
-    self.phase = phase
-
-    self.spectral_norm = spectral_norm
-    # Prepare gain and bias layers
-
-    if use_style_fc:
-      self.gain_fc = which_linear(style_dim, in_dim)
-      # self.bias_fc = which_linear(style_dim, in_dim)
-      if spectral_norm:
-        self.gain_fc = nn.utils.spectral_norm(self.gain_fc)
-        # self.bias_fc = nn.utils.spectral_norm(self.bias_fc)
-    else:
-      self.style_dim = in_dim * 2
-
-    self.eps = eps
-    pass
-
-  def forward(self,
-              x,
-              style):
-    """
-    Calculate class-conditional gains and biases.
-
-    :param x: (b, c) or (b, n, c)
-    :param style: (b, c)
-    :return:
-    """
-    assert style.shape[-1] == self.style_dim
-
-    if self.use_style_fc:
-      gain = self.gain_fc(style) + 1.
-    else:
-      style = rearrange(style, "b (n c) -> b n c", n=2)
-      gain, bias = style.unbind(dim=1)
-      gain = gain + 1.
-
-    if x.dim() == 3:
-      gain = rearrange(gain, "b c -> b 1 c")
-    elif x.dim() == 2:
-      pass
-    else:
-      assert 0
-
-    x = torch.sin(self.freq * x + self.phase)
-    # out = x * gain + bias
-    out = x * gain
-    return out
-
-
-class SinBlock(nn.Module):
-  def __init__(self,
-               in_dim,
-               out_dim,
-               style_dim,
-               name_prefix,
-               ):
-    super().__init__()
-    self.in_dim = in_dim
-    self.out_dim = out_dim
-    self.style_dim = style_dim
-    self.name_prefix = name_prefix
-
-    self.style_dim_dict = {}
-
-    # self.mod1 = mod_conv_fc.Modulated_FC_Conv(in_channel=in_dim,
-    #                                           out_channel=out_dim,
-    #                                           style_dim=style_dim,
-    #                                           use_style_fc=True,
-    #                                           scale=1.,
-    #                                           # scale=None,
-    #                                           )
-    self.mod1 = mod_conv_fc.SinStyleMod(in_channel=in_dim,
-                                        out_channel=out_dim,
-                                        style_dim=style_dim,
-                                        use_style_fc=True,
-                                        )
-    self.style_dim_dict[f'{name_prefix}_0'] = self.mod1.style_dim
-    self.act1 = nn.LeakyReLU(0.2, inplace=True)
-
-    # self.mod2 = mod_conv_fc.Modulated_FC_Conv(in_channel=out_dim,
-    #                                           out_channel=out_dim,
-    #                                           style_dim=style_dim,
-    #                                           use_style_fc=True,
-    #                                           scale=1.,
-    #                                           # scale=None,
-    #                                           )
-    self.mod2 = mod_conv_fc.SinStyleMod(in_channel=out_dim,
-                                        out_channel=out_dim,
-                                        style_dim=style_dim,
-                                        use_style_fc=True,
-                                        )
-    self.style_dim_dict[f'{name_prefix}_1'] = self.mod2.style_dim
-    self.act2 = nn.LeakyReLU(0.2, inplace=True)
-
-    # self.linear1 = nn.Linear(in_dim, out_dim)
-    # self.mod1 = ModSinLayer(in_dim=out_dim, use_style_fc=True, style_dim=style_dim)
-    # self.style_dim_dict[f'{name_prefix}_0'] = self.mod1.style_dim
-
-    # self.linear2 = nn.Linear(out_dim, out_dim)
-    # self.mod2 = ModSinLayer(in_dim=out_dim, use_style_fc=True, style_dim=style_dim)
-    # self.style_dim_dict[f'{name_prefix}_1'] = self.mod2.style_dim
-
-    self.skip = SkipLayer()
-    pass
-
-  def forward(self,
-              x,
-              style_dict,
-              skip=False):
-    x_orig = x
-
-    style = style_dict[f'{self.name_prefix}_0']
-    x = self.mod1(x, style)
-    x = self.act1(x)
-
-    style = style_dict[f'{self.name_prefix}_1']
-    x = self.mod2(x, style)
-    out = self.act2(x)
-
-    # x = self.linear1(x)
-    # style = style_dict[f'{self.name_prefix}_0']
-    # x = self.mod1(x, style)
-
-    # x = self.linear2(x)
-    # style = style_dict[f'{self.name_prefix}_1']
-    # out = self.mod2(x, style)
-
-    if skip and out.shape[-1] == x_orig.shape[-1]:
-      # out = (out + x_orig) / 1.41421
-      out = self.skip(out, x_orig)
-    return out
-
-  def __repr__(self):
-    repr = f"{self.__class__.__name__}(in_dim={self.in_dim}, " \
-           f"out_dim={self.out_dim}, " \
-           f"style_dim={self.style_dim})"
-    return repr
-
-
-class ToRGB(nn.Module):
-  def __init__(self,
-               in_dim,
-               dim_rgb=3,
-               use_equal_fc=False):
-    super().__init__()
-    self.in_dim = in_dim
-    self.dim_rgb = dim_rgb
-
-    if use_equal_fc:
-      self.linear = mod_conv_fc.EqualLinear(in_dim, dim_rgb, scale=1.)
-    else:
-      self.linear = nn.Linear(in_dim, dim_rgb)
-    pass
-
-  def forward(self,
-              input,
-              skip=None):
-
-    out = self.linear(input)
-
-    if skip is not None:
-      out = out + skip
-    return out
-
-
-class SigmoidToTanh(nn.Module):
-  def __init__(self, ):
-    super(SigmoidToTanh, self).__init__()
-
-  def forward(self, x):
-    """
-
-    :param x: [0, 1] -> [-1, 1]
-    :return:
-    """
-    return x * 2. - 1
-
-
-@MODEL_REGISTRY.register(name_prefix=__name__)
-class Generator_Diffcam(nn.Module):
-
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               nerf_cfg,
-               mapping_shape_app_cfg,
-               inr_cfg,
-               mapping_inr_cfg,
-               shape_block_end_index=None,
-               app_block_end_index=None,
-               inr_block_end_index=None,
-               device='cuda',
-               inr_detach=False,
-               disable_inr=False,
-               **kwargs):
-    super(Generator_Diffcam, self).__init__()
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'nerf_cfg': nerf_cfg,
-      'mapping_shape_app_cfg': mapping_shape_app_cfg,
-      'inr_cfg': inr_cfg,
-      'mapping_inr_cfg': mapping_inr_cfg,
-      'shape_block_end_index': shape_block_end_index,
-      'app_block_end_index': app_block_end_index,
-      'inr_block_end_index': inr_block_end_index,
-      'inr_detach': inr_detach,
-      'disable_inr': disable_inr,
-    })
-
-    self.device = device
-    self.inr_block_end_index = inr_block_end_index
-    self.inr_detach = inr_detach
-    self.disable_inr = disable_inr
-
-    self.module_name_list = []
-
-    # nerf_net
-    self.nerf_net = siren_net_pigan.NeRF_Net(
-      shape_block_end_index=shape_block_end_index,
-      app_block_end_index=app_block_end_index,
-      **nerf_cfg)
-    self.module_name_list.append('nerf_net')
-
-    # mapping shape_app
-    self.mapping_shape_app = siren_net_pigan.StyleMappingShapeApp(
-      **mapping_shape_app_cfg,
-      style_dim_dict_shape=self.nerf_net.style_dim_dict_shape,
-      style_dim_dict_app=self.nerf_net.style_dim_dict_app)
-    self.module_name_list.append('mapping_shape_app')
-
-    # _in_dim = nerf_cfg.app_net_cfg.out_dim
-    _in_dim = self.nerf_net.out_dim
-
-    # inr_net
-    self.inr_net = cips_net.CIPSNet(**{
-      **inr_cfg,
-      "input_dim": _in_dim,
-      'add_out_layer': True,
-    })
-    self.module_name_list.append('inr_net')
-
-    self.mapping_inr = multi_head_mapping.MultiHeadMappingNetwork(**{
-      **mapping_inr_cfg,
-      'head_dim_dict': self.inr_net.style_dim_dict
-    })
-    self.module_name_list.append('mapping_inr')
-
-
-    self.aux_to_rbg = nn.Sequential(
-      SigmoidToTanh(),
-      # nn.Linear(_in_dim, 3),
-      # nn.Tanh()
-    )
-    self.aux_to_rbg.apply(nerf_network.frequency_init(25))
-    self.module_name_list.append('aux_to_rbg')
-
-
-    tl2_utils.print_repr(self)
-    pass
-
-  def get_subnet_grad_norm(self):
-    ret_dict = {}
-
-    for name in self.module_name_list:
-      subnet = getattr(self, name)
-      grad_norm = torch_utils.get_grad_norm_total(params=subnet.parameters())
-      ret_dict[f"G_GN.{name}"] = grad_norm
-    return ret_dict
-
-  def forward(self,
-              zs,
-              rays_o,
-              rays_d,
-              nerf_kwargs={},
-              psi=1,
-              return_aux_img=False,
-              grad_points=None,
-              forward_points=None,  # disable gradients
-              **kwargs):
-    """
-    Generates images from a noise vector, rendering parameters, and camera distribution.
-    Uses the hierarchical sampling scheme described in NeRF.
-
-    :param zs: {k: (b, z_dim), ...}
-    :param rays_o: (b, h, w, 3) in world space
-    :param rays_d: (b, h, w, 3) in world space
-
-    :return:
-    - pixels: (b, 3, h, w)
-    - pitch_yaw: (b, 2)
-    """
-
-    # mapping network
-    style_dict = self.mapping_network(**zs)
-
-    if psi < 1:
-      avg_styles = self.generate_avg_frequencies(device=self.device)
-      style_dict = self.get_truncated_freq_phase(
-        raw_style_dict=style_dict, avg_style_dict=avg_styles, raw_lambda=psi)
-
-    b, h, w, c = rays_o.shape
-    rays_o = rearrange(rays_o, "b h w c -> b (h w) c")
-    rays_d = rearrange(rays_d, "b h w c -> b (h w) c")
-
-    if grad_points is not None and grad_points < h * w:
-      imgs, ret_maps = self.part_grad_forward(
-        rays_o=rays_o,
-        rays_d=rays_d,
-        style_dict=style_dict,
-        nerf_kwargs=nerf_kwargs,
-        return_aux_img=return_aux_img,
-        grad_points=grad_points)
-
-    else:
-      imgs, ret_maps = self.whole_grad_forward(
-        rays_o=rays_o,
-        rays_d=rays_d,
-        style_dict=style_dict,
-        nerf_kwargs=nerf_kwargs,
-        return_aux_img=return_aux_img,
-        forward_points=forward_points)
-
-    imgs = rearrange(imgs, "b (h w) c -> b c h w", h=h, w=w)
-
-    ret_imgs = {}
-    for name, v_map in ret_maps.items():
-      if v_map.dim() == 3:
-        v_map = rearrange(v_map, "b (h w) c -> b c h w", h=h, w=w)
-      elif v_map.dim() == 2:
-        v_map = rearrange(v_map, "b (h w) -> b h w", h=h, w=w)
-      ret_imgs[name] = v_map
-
-    return imgs, ret_imgs
-
-  def get_rays_axis_angle(self,
-                          R,
-                          t,
-                          fx,
-                          fy,
-                          H: int,
-                          W: int,
-                          N_rays: int = -1):
-    """
-
-    :param R: (b, 3)
-    :param t: (b, 3)
-    :param fx:
-    :param fy:
-    :param H:
-    :param W:
-    :param N_rays:
-    :return
-
-    - rays_o: (b, H, W, 3)
-    - rays_d: (b, H, W, 3)
-    - select_inds: (b, H, W)
-    """
-
-    rays_o, rays_d, select_inds = cam_params.get_rays(
-      rot=R,
-      trans=t,
-      focal_x=fx,
-      focal_y=fy,
-      H=H,
-      W=W,
-      N_rays=N_rays,
-      flatten=False)
-
-    return rays_o, rays_d, select_inds
-
-  def get_batch_style_dict(self, b, style_dict):
-    ret_style_dict = {}
-    for name, style in style_dict.items():
-      ret_style_dict[name] = style[[b]]
-    return ret_style_dict
-
-  def whole_grad_forward(self,
-                         rays_o,
-                         rays_d,
-                         style_dict,
-                         nerf_kwargs,
-                         return_aux_img=True,
-                         forward_points=None,
-                         **kwargs):
-
-    if forward_points is not None and forward_points <= rays_o.shape[1]: # no gradients
-      # stage forward
-      with torch.no_grad():
-        batch_size = rays_o.shape[0]
-        num_points = rays_o.shape[1]
-
-        near = nerf_kwargs['near']
-        far = nerf_kwargs['far']
-        N_samples = nerf_kwargs['N_samples']
-        perturb = self.training
-        z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                            rays_d=rays_d,
-                                                            near=near,
-                                                            far=far,
-                                                            N_samples=N_samples,
-                                                            perturb=perturb)
-
-        batch_image_ddict = collections.defaultdict(list)
-        for b in range(batch_size):
-          image_ddict = collections.defaultdict(list)
-
-          head = 0
-          while head < num_points:
-            tail = head + forward_points
-            cur_style_dict = self.get_batch_style_dict(b=b, style_dict=style_dict)
-
-            cur_inr_img, cur_ret_maps = self.points_forward(
-              rays_o=rays_o[[b], head:tail], # (b, hxw, 3)
-              rays_d=rays_d[[b], head:tail], # (b, hxw, 3)
-              points=points[[b], head:tail], # (b, hxw, Nsamples, 3)
-              z_vals=z_vals[[b], head:tail], # (b, hxw, Nsamples)
-              style_dict=cur_style_dict,
-              nerf_kwargs=nerf_kwargs,
-              return_aux_img=return_aux_img)
-
-            image_ddict['inr_img'].append(cur_inr_img)
-            for k, v in cur_ret_maps.items():
-              image_ddict[k].append(v)
-            head += forward_points
-          for k, v in image_ddict.items():
-            one_image = torch.cat(v, dim=1)
-            batch_image_ddict[k].append(one_image)
-        ret_maps = {}
-        for k, v in batch_image_ddict.items():
-          ret_maps[k] = torch.cat(v, dim=0)
-        imgs = ret_maps.pop('inr_img')
-
-    else:
-      near = nerf_kwargs['near']
-      far = nerf_kwargs['far']
-      N_samples = nerf_kwargs['N_samples']
-      perturb = self.training
-      z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                          rays_d=rays_d,
-                                                          near=near,
-                                                          far=far,
-                                                          N_samples=N_samples,
-                                                          perturb=perturb)
-
-      # transformed_points = rearrange(transformed_points, "b (h w s) c -> b (h w) s c", h=img_size, s=num_steps)
-      # transformed_ray_directions_expanded = rearrange(transformed_ray_directions_expanded,
-      #                                                 "b (h w s) c -> b (h w) s c", h=img_size, s=num_steps)
-
-      imgs, ret_maps = self.points_forward(
-        rays_o=rays_o,
-        rays_d=rays_d,
-        points=points,
-        z_vals=z_vals,
-        style_dict=style_dict,
-        nerf_kwargs=nerf_kwargs,
-        return_aux_img=return_aux_img)
-
-    return imgs, ret_maps
-
-  def part_grad_forward(self,
-                        rays_o,
-                        rays_d,
-                        style_dict,
-                        nerf_kwargs,
-                        return_aux_img,
-                        grad_points):
-
-    near = nerf_kwargs['near']
-    far = nerf_kwargs['far']
-    N_samples = nerf_kwargs['N_samples']
-    perturb = self.training
-    # z_vals: (b, hxw, Nsamples), points: (b, hxw, Nsamples, 3)
-    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o, # (b, hxw, 3)
-                                                        rays_d=rays_d, # (b, hxw, 3)
-                                                        near=near,
-                                                        far=far,
-                                                        N_samples=N_samples,
-                                                        perturb=perturb)
-
-    # transformed_points = rearrange(transformed_points, "b (h w s) c -> b (h w) s c", h=img_size, s=num_steps)
-    # transformed_ray_directions_expanded = rearrange(transformed_ray_directions_expanded,
-    #                                                 "b (h w s) c -> b (h w) s c", h=img_size, s=num_steps)
-
-    batch_size = rays_o.shape[0]
-    num_points = rays_o.shape[1]
-    device = self.device
-    assert num_points > grad_points
-    idx_grad, idx_no_grad = torch_utils.batch_random_split_indices(bs=batch_size,
-                                                                   num_points=num_points,
-                                                                   grad_points=grad_points,
-                                                                   device=device)
-    # rand_idx = torch.randperm(num_points, device=device)
-    # idx_grad = rand_idx[:grad_points]
-    # idx_no_grad = rand_idx[grad_points:]
-
-    inr_img_grad, ret_maps_grad = self.points_forward(
-      rays_o=rays_o,
-      rays_d=rays_d,
-      points=points,
-      z_vals=z_vals,
-      style_dict=style_dict,
-      nerf_kwargs=nerf_kwargs,
-      return_aux_img=return_aux_img,
-      idx_grad=idx_grad)
-
-    with torch.no_grad():
-      inr_img_no_grad, ret_maps_no_grad = self.points_forward(
-        rays_o=rays_o,
-        rays_d=rays_d,
-        points=points,
-        z_vals=z_vals,
-        style_dict=style_dict,
-        nerf_kwargs=nerf_kwargs,
-        return_aux_img=return_aux_img,
-        idx_grad=idx_no_grad)
-
-    imgs = comm_utils.batch_scatter_points(idx_grad=idx_grad,
-                                           points_grad=inr_img_grad,
-                                           idx_no_grad=idx_no_grad,
-                                           points_no_grad=inr_img_no_grad,
-                                           num_points=num_points)
-    ret_maps = {}
-    for k in ret_maps_grad.keys():
-      comp_map = comm_utils.batch_scatter_points(idx_grad=idx_grad,
-                                                 points_grad=ret_maps_grad[k],
-                                                 idx_no_grad=idx_no_grad,
-                                                 points_no_grad=ret_maps_no_grad[k],
-                                                 num_points=num_points)
-      ret_maps[k] = comp_map
-    return imgs, ret_maps
-
-  def points_forward(self,
-                     rays_o,
-                     rays_d,
-                     points,
-                     z_vals,
-                     style_dict,
-                     nerf_kwargs,
-                     return_aux_img,
-                     idx_grad=None,
-                     disable_view_dir=True,
-                     **kwargs):
-    """
-
-    :param rays_o: (b, hxw, 3)
-    :param rays_d: (b, hxw, 3)
-    :param points: (b, hxw, Nsamples, 3)
-    :param z_vals: (b, hxw, Nsamples)
-    :param style_dict:
-    :param nerf_kwargs:
-    :param return_aux_img:
-    :param idx_grad: (b, N_grad, )
-    :param kwargs:
-    :return:
-    """
-
-    device = points.device
-
-    if disable_view_dir:
-      viewdirs = torch.zeros_like(rays_d)
-      viewdirs[..., -1] = -1
-    else:
-      viewdirs = F.normalize(rays_d, dim=-1)
-    # viewdirs = volume_rendering.get_viewdirs(rays_d=rays_d)
-    # viewdirs = viewdirs[..., None, :].expand_as(points)
-    N_samples = nerf_kwargs['N_samples']
-
-    if idx_grad is not None:
-      rays_o = comm_utils.batch_gather_points(points=rays_o, idx_grad=idx_grad)
-      rays_d = comm_utils.batch_gather_points(points=rays_d, idx_grad=idx_grad)
-      points = comm_utils.batch_gather_points(points=points, idx_grad=idx_grad)
-      z_vals = comm_utils.batch_gather_points(points=z_vals, idx_grad=idx_grad)
-
-
-    points = rearrange(points, "b Nrays Nsamples c -> b (Nrays Nsamples) c")
-    coarse_viewdirs = repeat(viewdirs, "b Nrays c -> b (Nrays Nsamples) c", Nsamples=N_samples)
-
-    # Model prediction on course points
-    coarse_output = self.nerf_net(
-      x=points,  # b (Nrays Nsamples) c
-      ray_directions=coarse_viewdirs, # b (Nrays Nsamples) c
-      style_dict=style_dict)
-
-    # Mask out values outside
-    # padd = 0.
-    # mask_box = torch.all(points <= 1. + padd, dim=-1) & torch.all(points >= -1. - padd, dim=-1)
-    # coarse_output[mask_box == 0] = 0.
-
-    coarse_output = rearrange(
-      coarse_output, "b (Nrays Nsamples) rgb_sigma -> b Nrays Nsamples rgb_sigma", Nsamples=N_samples)
-
-    # Re-sample fine points alont camera rays, as described in NeRF
-    if nerf_kwargs['N_importance'] > 0:
-
-      with torch.no_grad():
-        raw_sigma = coarse_output[..., -1]
-        perturb = self.training
-        fine_z_vals, fine_points = volume_rendering.get_fine_points(
-          z_vals=z_vals,
-          rays_o=rays_o,
-          rays_d=rays_d,
-          raw_sigma=raw_sigma,
-          N_importance=nerf_kwargs['N_importance'],
-          perturb=perturb,
-          raw_noise_std=nerf_kwargs['raw_noise_std'],
-          eps=nerf_kwargs['eps'])
-
-      # Model prediction on re-sampled find points
-      fine_points = rearrange(fine_points, "b Nrays Nsamples c -> b (Nrays Nsamples) c")
-      fine_viewdirs = repeat(viewdirs, "b Nrays c -> b (Nrays Nsamples) c", Nsamples=nerf_kwargs['N_importance'])
-
-      fine_output = self.nerf_net(
-        x=fine_points,  # b (Nrays Nsamples) c
-        ray_directions=fine_viewdirs,  # b (Nrays Nsamples) c
-        style_dict=style_dict)
-      fine_output = rearrange(
-        fine_output, "b (Nrays Nsamples) rgb_sigma -> b Nrays Nsamples rgb_sigma", Nsamples=nerf_kwargs['N_importance'])
-
-      # Combine course and fine points
-      DIM_SAMPLES = 2
-      all_z_vals = torch.cat([fine_z_vals, z_vals], dim=DIM_SAMPLES) # (b, N_rays, N_samples)
-      _, indices = torch.sort(all_z_vals, dim=DIM_SAMPLES) # (b, N_rays, N_samples)
-      # gather z_vals
-      all_z_vals = torch.gather(all_z_vals, DIM_SAMPLES, indices) # (b, N_rays, N_samples)
-
-      # (b, N_rays, N_samples, rgb_sigma)
-      all_outputs = torch.cat([fine_output, coarse_output], dim=DIM_SAMPLES)
-      view_shape = [*indices.shape, *(len(all_outputs.shape) - len(indices.shape)) * [1]]
-      all_outputs = torch.gather(all_outputs, DIM_SAMPLES, indices.view(view_shape).expand_as(all_outputs))
-
-    else:
-      all_outputs = coarse_output
-      all_z_vals = z_vals
-
-    # Create images with NeRF
-    all_raw_rgb = all_outputs[..., :-1]
-    all_raw_sigma = all_outputs[..., -1]
-
-    fea_rgb, ret_maps = volume_rendering.ray_integration(raw_rgb=all_raw_rgb,
-                                                         raw_sigma=all_raw_sigma,
-                                                         z_vals=all_z_vals,
-                                                         rays_d=rays_d,
-                                                         raw_noise_std=nerf_kwargs['raw_noise_std'],
-                                                         eps=nerf_kwargs['eps'])
-
-    pixels_fea = fea_rgb[..., :-3]
-    rgb = fea_rgb[..., -3:]
-
-    # inr_net
-    if self.disable_inr:
-      inr_img = self.aux_to_rbg(rgb)
-    else:
-      if self.inr_detach:
-        inr_img = self.inr_net(pixels_fea.detach(), style_dict, block_end_index=self.inr_block_end_index)
-      else:
-        inr_img = self.inr_net(pixels_fea, style_dict, block_end_index=self.inr_block_end_index)
-
-    if return_aux_img:
-      # aux rgb_branch
-      aux_img = self.aux_to_rbg(rgb)
-      ret_maps['aux_img'] = aux_img
-
-    return inr_img, ret_maps
-
-  def z_sampler(self,
-                shape,
-                device,
-                dist='gaussian'):
-    if dist == 'gaussian':
-      z = torch.randn(shape, device=device)
-    elif dist == 'uniform':
-      z = torch.rand(shape, device=device) * 2 - 1
-    return z
-
-  def get_zs(self,
-             b,
-             batch_split=1,
-             merge_z_shape_app=True):
-    z_shape = self.z_sampler(shape=(b, self.mapping_shape_app.z_dim), device=self.device)
-    if merge_z_shape_app:
-      z_app = None
-    else:
-      z_app = self.z_sampler(shape=(b, self.mapping_shape_app.z_dim), device=self.device)
-    z_inr = self.z_sampler(shape=(b, self.mapping_inr.z_dim), device=self.device)
-
-    if batch_split > 1:
-      zs_list = []
-      z_shape_list = z_shape.split(b // batch_split)
-      if z_app is None:
-        z_app_list = [None] * batch_split
-      else:
-        z_app_list = z_app.split(b // batch_split)
-      z_inr_list = z_inr.split(b // batch_split)
-      for z_shape_, z_app_, z_inr_ in zip(z_shape_list, z_app_list, z_inr_list):
-        zs_ = {
-          'z_shape': z_shape_,
-          'z_app': z_app_,
-          'z_inr': z_inr_,
-        }
-        zs_list.append(zs_)
-      return zs_list
-    else:
-      zs = {
-        'z_shape': z_shape,
-        'z_app': z_app,
-        'z_inr': z_inr,
-      }
-      return zs
-
-  def mapping_network(self,
-                      z_shape,
-                      z_app,
-                      z_inr):
-    if global_cfg.tl_debug:
-      VerboseModel.forward_verbose(self.mapping_shape_app,
-                                   inputs_args=(z_shape, z_app),
-                                   submodels=['base_net.network'],
-                                   name_prefix='mapping_shape_app.')
-      VerboseModel.forward_verbose(self.mapping_inr,
-                                   inputs_args=(z_inr,),
-                                   submodels=['base_net', ],
-                                   input_padding=50,
-                                   name_prefix='mapping_inr.')
-
-    style_dict = {}
-    style_dict.update(self.mapping_shape_app(z_shape, z_app))
-    style_dict.update(self.mapping_inr(z_inr))
-
-    return style_dict
-
-  def get_truncated_freq_phase(self,
-                               raw_style_dict,
-                               avg_style_dict,
-                               raw_lambda):
-
-    truncated_style_dict = {}
-    for name, avg_style in avg_style_dict.items():
-      raw_style = raw_style_dict[name]
-      truncated_style = avg_style + raw_lambda * (raw_style - avg_style)
-      truncated_style_dict[name] = truncated_style
-    return truncated_style_dict
-
-  def generate_avg_frequencies(self,
-                               num_samples=10000,
-                               device='cuda'):
-    """Calculates average frequencies and phase shifts"""
-
-    # z = torch.randn((num_samples, self.z_dim), device=device)
-    zs = self.get_zs(num_samples)
-    with torch.no_grad():
-      style_dict = self.mapping_network(**zs)
-
-    avg_styles = {}
-    for name, style in style_dict.items():
-      avg_styles[name] = style.mean(0, keepdim=True)
-
-    # self.avg_styles = avg_styles
-    return avg_styles
-
-  def staged_forward(self, *args, **kwargs):
-    raise NotImplementedError
-
-  def set_device(self, device):
-    pass
-
-  def forward_camera_pos_and_lookup(self,
-              zs,
-              img_size,
-              fov,
-              ray_start,
-              ray_end,
-              num_steps,
-              h_stddev,
-              v_stddev,
-              h_mean,
-              v_mean,
-              hierarchical_sample,
-              camera_pos,
-              camera_lookup,
-              psi=1,
-              sample_dist=None,
-              lock_view_dependence=False,
-              clamp_mode='relu',
-              nerf_noise=0.,
-              white_back=False,
-              last_back=False,
-              return_aux_img=False,
-              grad_points=None,
-              forward_points=None,
-              **kwargs):
-    """
-    Generates images from a noise vector, rendering parameters, and camera distribution.
-    Uses the hierarchical sampling scheme described in NeRF.
-
-    :param z: (b, z_dim)
-    :param img_size:
-    :param fov: face: 12
-    :param ray_start: face: 0.88
-    :param ray_end: face: 1.12
-    :param num_steps: face: 12
-    :param h_stddev: face: 0.3
-    :param v_stddev: face: 0.155
-    :param h_mean: face: pi/2
-    :param v_mean: face: pi/2
-    :param hierarchical_sample: face: true
-    :param camera_pos: (b, 3)
-    :param camera_lookup: (b, 3)
-    :param psi: [0, 1]
-    :param sample_dist: mode for sample_camera_positions, face: 'gaussian'
-    :param lock_view_dependence: face: false
-    :param clamp_mode: face: 'relu'
-    :param nerf_noise:
-    :param last_back: face: false
-    :param white_back: face: false
-    :param kwargs:
-    :return:
-    - pixels: (b, 3, h, w)
-    - pitch_yaw: (b, 2)
-    """
-
-    # mapping network
-    if global_cfg.tl_debug:
-      VerboseModel.forward_verbose(self.mapping_network_nerf,
-                                   inputs_args=(zs['z_nerf'],),
-                                   submodels=['base_net'],
-                                   name_prefix='mapping_nerf.')
-      VerboseModel.forward_verbose(self.mapping_network_inr,
-                                   inputs_args=(zs['z_inr'],),
-                                   submodels=['base_net', ],
-                                   input_padding=50,
-                                   name_prefix='mapping_inr.')
-    style_dict = self.mapping_network(**zs)
-
-    if psi < 1:
-      avg_styles = self.generate_avg_frequencies(device=self.device)
-      style_dict = self.get_truncated_freq_phase(
-        raw_style_dict=style_dict, avg_style_dict=avg_styles, raw_lambda=psi)
-
-    if grad_points is not None and grad_points < img_size ** 2:
-      imgs, pitch_yaw = self.part_grad_forward(
-        style_dict=style_dict,
-        img_size=img_size,
-        fov=fov,
-        ray_start=ray_start,
-        ray_end=ray_end,
-        num_steps=num_steps,
-        h_stddev=h_stddev,
-        v_stddev=v_stddev,
-        h_mean=h_mean,
-        v_mean=v_mean,
-        hierarchical_sample=hierarchical_sample,
-        sample_dist=sample_dist,
-        lock_view_dependence=lock_view_dependence,
-        clamp_mode=clamp_mode,
-        nerf_noise=nerf_noise,
-        white_back=white_back,
-        last_back=last_back,
-        return_aux_img=return_aux_img,
-        grad_points=grad_points,
-        camera_pos=camera_pos,
-        camera_lookup=camera_lookup,
-      )
-      return imgs, pitch_yaw
-    else:
-      imgs, pitch_yaw = self.whole_grad_forward(
-        style_dict=style_dict,
-        img_size=img_size,
-        fov=fov,
-        ray_start=ray_start,
-        ray_end=ray_end,
-        num_steps=num_steps,
-        h_stddev=h_stddev,
-        v_stddev=v_stddev,
-        h_mean=h_mean,
-        v_mean=v_mean,
-        hierarchical_sample=hierarchical_sample,
-        sample_dist=sample_dist,
-        lock_view_dependence=lock_view_dependence,
-        clamp_mode=clamp_mode,
-        nerf_noise=nerf_noise,
-        white_back=white_back,
-        last_back=last_back,
-        return_aux_img=return_aux_img,
-        forward_points=forward_points,
-        camera_pos=camera_pos,
-        camera_lookup=camera_lookup,
-      )
-      return imgs, pitch_yaw
-
-
-@MODEL_REGISTRY.register(name_prefix=__name__)
-class Generator_Diffcam_FreezeNeRF(Generator_Diffcam):
-
-  def load_nerf_ema(self, G_ema):
-    ret = self.nerf_net.load_state_dict(G_ema.nerf_net.state_dict())
-    ret = self.mapping_shape.load_state_dict(G_ema.mapping_shape.state_dict())
-    ret = self.mapping_app.load_state_dict(G_ema.mapping_app.state_dict())
-
-    ret = self.aux_to_rbg.load_state_dict(G_ema.aux_to_rbg.state_dict())
-
-    # ret = self.mapping_inr.load_state_dict(G_ema.mapping_inr.state_dict())
-    pass
-
-  def forward(self, **kwargs):
-
-    self.nerf_net.requires_grad_(False)
-    self.mapping_shape.requires_grad_(False)
-    self.mapping_app.requires_grad_(False)
-    self.aux_to_rbg.requires_grad_(False)
-
-    return super(Generator_Diffcam_FreezeNeRF, self).forward(**kwargs)
-
-  def mapping_network(self,
-                      z_shape,
-                      z_app,
-                      z_inr):
-    style_dict = {}
-    with torch.no_grad():
-      style_dict.update(self.mapping_shape(z_shape))
-      style_dict.update(self.mapping_app(z_app))
-    style_dict.update(self.mapping_inr(z_inr))
-
-
-    return style_dict
-
-  # def points_forward(self,
-  #                    style_dict,
-  #                    transformed_points,
-  #                    transformed_ray_directions_expanded,
-  #                    num_steps,
-  #                    hierarchical_sample,
-  #                    z_vals,
-  #                    clamp_mode,
-  #                    nerf_noise,
-  #                    transformed_ray_origins,
-  #                    transformed_ray_directions,
-  #                    white_back,
-  #                    last_back,
-  #                    return_aux_img,
-  #                    idx_grad=None,
-  #                    ):
-  #   """
-  #
-  #   :param style_dict:
-  #   :param transformed_points: (b, n, s, 3)
-  #   :param transformed_ray_directions_expanded: (b, n, s, 3)
-  #   :param num_steps: sampled points along a ray
-  #   :param hierarchical_sample:
-  #   :param z_vals: (b, n, s, 1)
-  #   :param clamp_mode: 'relu'
-  #   :param nerf_noise:
-  #   :param transformed_ray_origins: (b, n, 3)
-  #   :param transformed_ray_directions: (b, n, 3)
-  #   :param white_back:
-  #   :param last_back:
-  #   :return:
-  #   """
-  #   device = transformed_points.device
-  #   if idx_grad is not None:
-  #     transformed_points = comm_utils.gather_points(points=transformed_points, idx_grad=idx_grad)
-  #     transformed_ray_directions_expanded = comm_utils.gather_points(
-  #       points=transformed_ray_directions_expanded, idx_grad=idx_grad)
-  #     z_vals = comm_utils.gather_points(points=z_vals, idx_grad=idx_grad)
-  #     transformed_ray_origins = comm_utils.gather_points(points=transformed_ray_origins, idx_grad=idx_grad)
-  #     transformed_ray_directions = comm_utils.gather_points(points=transformed_ray_directions, idx_grad=idx_grad)
-  #
-  #   transformed_points = rearrange(transformed_points, "b n s c -> b (n s) c")
-  #   transformed_ray_directions_expanded = rearrange(transformed_ray_directions_expanded, "b n s c -> b (n s) c")
-  #
-  #   # Model prediction on course points
-  #   with torch.no_grad():
-  #     coarse_output = self.nerf_net(
-  #       x=transformed_points,  # (b, n x s, 3)
-  #       style_dict=style_dict,
-  #       ray_directions=transformed_ray_directions_expanded,
-  #     )
-  #   coarse_output = rearrange(coarse_output, "b (n s) rgb_sigma -> b n s rgb_sigma", s=num_steps)
-  #
-  #   # Re-sample fine points alont camera rays, as described in NeRF
-  #   if hierarchical_sample:
-  #     fine_points, fine_z_vals = self.get_fine_points_and_direction(
-  #       coarse_output=coarse_output,
-  #       z_vals=z_vals,
-  #       dim_rgb=self.nerf_net.rgb_dim,
-  #       clamp_mode=clamp_mode,
-  #       nerf_noise=nerf_noise,
-  #       num_steps=num_steps,
-  #       transformed_ray_origins=transformed_ray_origins,
-  #       transformed_ray_directions=transformed_ray_directions
-  #     )
-  #
-  #     # Model prediction on re-sampled find points
-  #     with torch.no_grad():
-  #       fine_output = self.nerf_net(
-  #         x=fine_points,  # (b, n x s, 3)
-  #         style_dict=style_dict,
-  #         ray_directions=transformed_ray_directions_expanded,  # (b, n x s, 3)
-  #       )
-  #     fine_output = rearrange(fine_output, "b (n s) rgb_sigma -> b n s rgb_sigma", s=num_steps)
-  #
-  #     # Combine course and fine points
-  #     all_outputs = torch.cat([fine_output, coarse_output], dim=-2)  # (b, n, s, dim_rgb_sigma)
-  #     all_z_vals = torch.cat([fine_z_vals, z_vals], dim=-2)  # (b, n, s, 1)
-  #     _, indices = torch.sort(all_z_vals, dim=-2)  # (b, n, s, 1)
-  #     all_z_vals = torch.gather(all_z_vals, -2, indices)  # (b, n, s, 1)
-  #     # (b, n, s, dim_rgb_sigma)
-  #     all_outputs = torch.gather(all_outputs, -2, indices.expand(-1, -1, -1, all_outputs.shape[-1]))
-  #   else:
-  #     all_outputs = coarse_output
-  #     all_z_vals = z_vals
-  #
-  #   # Create images with NeRF
-  #   pixels_fea, depth, weights = pigan_utils.fancy_integration(
-  #     rgb_sigma=all_outputs,
-  #     z_vals=all_z_vals,
-  #     device=device,
-  #     dim_rgb=self.nerf_net.rgb_dim,
-  #     white_back=white_back,
-  #     last_back=last_back,
-  #     clamp_mode=clamp_mode,
-  #     noise_std=nerf_noise)
-  #
-  #   inr_img = self.inr_net(pixels_fea, style_dict)
-  #
-  #   if return_aux_img:
-  #     # aux rgb_branch
-  #     with torch.no_grad():
-  #       aux_img = self.aux_to_rbg(pixels_fea)
-  #   else:
-  #     aux_img = None
-  #
-  #   return inr_img, aux_img
-
-
-
-
-
+from itertools import chain
+import math
+import logging
+import collections
+from collections import OrderedDict
+import tqdm
+import random
+import time
+from einops import rearrange, repeat
+import numpy as np
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.cuda.amp import autocast
+
+from tl2.proj.fvcore import MODEL_REGISTRY, build_model
+# from tl2.proj.stylegan2_ada import persistence
+from tl2.launch.launch_utils import global_cfg
+from tl2.proj.pytorch.pytorch_hook import VerboseModel
+from tl2.proj.pytorch import torch_utils
+from tl2.proj.pytorch import torch_utils, init_func
+from tl2 import tl2_utils
+from tl2.proj.pytorch.examples.nerf import cam_params
+from tl2.proj.pytorch.examples.nerf import volume_rendering
+from tl2.proj.pytorch.examples.networks import nerf_net
+from tl2.proj.pytorch.examples.networks import multi_head_mapping
+from tl2.proj.pytorch.examples.networks import siren_net_pigan, cips_net
+
+from exp.pigan import pigan_utils
+from exp.dev.nerf_inr.models.generator_nerf_inr import INRNetwork
+from exp.dev.nerf_inr.models.generator_nerf_inr import GeneratorNerfINR as GeneratorNerfINR_base
+from exp.comm import comm_utils
+from exp.comm.models import nerf_network
+from exp.comm.models import inr_network
+from exp.comm.models import film_layer
+from exp.comm.models import mod_conv_fc
+# from exp.cips3d.models import multi_head_mapping
+
+
+class SkipLayer(nn.Module):
+  def __init__(self, ):
+    super(SkipLayer, self).__init__()
+
+  def forward(self, x0, x1):
+    # out = (x0 + x1) / math.pi
+    out = (x0 + x1)
+    return out
+
+
+
+class SinAct(nn.Module):
+  def __init__(self, ):
+    super(SinAct, self).__init__()
+
+  def forward(self, x):
+    return torch.sin(x)
+
+
+class LinearSinAct(nn.Module):
+  def __init__(self,
+               in_features,
+               out_features):
+    super(LinearSinAct, self).__init__()
+
+    self.linear = nn.Linear(in_features=in_features, out_features=out_features)
+    self.sin = SinAct()
+    pass
+
+  def forward(self, x, *args, **kwargs):
+    x = self.linear(x)
+    x = self.sin(x)
+    return x
+
+
+class FiLMLayer(nn.Module):
+  def __init__(self,
+               in_dim,
+               out_dim,
+               style_dim,
+               use_style_fc=True,
+               which_linear=nn.Linear,
+               **kwargs):
+    super(FiLMLayer, self).__init__()
+
+    self.in_dim = in_dim
+    self.out_dim = out_dim
+    self.style_dim = style_dim
+    self.use_style_fc = use_style_fc
+
+    self.linear = which_linear(in_dim, out_dim)
+    # self.linear.apply(film_layer.frequency_init(25))
+
+    # self.gain_scale = film_layer.LinearScale(scale=15, bias=30)
+    self.gain_scale = nn.Identity()
+    # Prepare gain and bias layers
+    if use_style_fc:
+      self.gain_fc = which_linear(style_dim, out_dim)
+      self.bias_fc = which_linear(style_dim, out_dim)
+      # self.gain_fc.weight.data.mul_(0.25)
+      # self.bias_fc.weight.data.mul_(0.25)
+    else:
+      self.style_dim = out_dim * 2
+
+    self.sin = SinAct()
+    self.lrelu = nn.LeakyReLU(0.2, inplace=True)
+    # self.register_buffer('stored_mean', torch.zeros(output_size))
+    # self.register_buffer('stored_var', torch.ones(output_size))
+    pass
+
+  def forward(self,
+              x,
+              style):
+    """
+
+    :param x: (b, c) or (b, n, c)
+    :param style: (b, c)
+    :return:
+    """
+
+    if self.use_style_fc:
+      gain = self.gain_fc(style)
+      gain = self.gain_scale(gain)
+      bias = self.bias_fc(style)
+    else:
+      style = rearrange(style, "b (n c) -> b n c", n=2)
+      gain, bias = style.unbind(dim=1)
+      gain = self.gain_scale(gain)
+
+    if x.dim() == 3:
+      gain = rearrange(gain, "b c -> b 1 c")
+      bias = rearrange(bias, "b c -> b 1 c")
+    elif x.dim() == 2:
+      pass
+    else:
+      assert 0
+
+    x = self.linear(x)
+
+    x = x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + 1e-8)
+    # out = self.sin(gain * x + bias)
+    out = self.lrelu((gain + 1.) * x + bias)
+
+    return out
+
+  def __repr__(self):
+    s = f'{self.__class__.__name__}(' \
+        f'in_dim={self.in_dim}, ' \
+        f'out_dim={self.out_dim}, ' \
+        f'style_dim={self.style_dim}, ' \
+        f'use_style_fc={self.use_style_fc}, ' \
+        f')'
+    return s
+
+
+class INRNetwork_Skip(nn.Module):
+  def __repr__(self): return f"{self.__class__.__name__}({self.repr})"
+
+  def __init__(self,
+               input_dim,
+               style_dim,
+               hidden_layers,
+               dim_scale=1,
+               rgb_dim=3,
+               device=None,
+               name_prefix='inr',
+               **kwargs):
+    """
+
+    :param z_dim:
+    :param hidden_dim:
+    :param rgb_dim:
+    :param device:
+    :param kwargs:
+    """
+    super().__init__()
+
+    self.repr = f"input_dim={input_dim}, " \
+                f"style_dim={style_dim}, " \
+                f"hidden_layers={hidden_layers}, " \
+                f"dim_scale={dim_scale}, "
+
+    self.device = device
+    self.rgb_dim = rgb_dim
+    self.hidden_layers = hidden_layers
+    self.name_prefix = name_prefix
+
+    self.channels = {
+      0: int(512 * dim_scale),  # 4
+      1: int(512 * dim_scale),  # 8
+      2: int(512 * dim_scale),  # 16
+      3: int(512 * dim_scale),  # 32
+      4: int(512 * dim_scale),  # 64
+      5: int(128 * dim_scale),  # 128
+      6: int(64 * dim_scale),  # 256
+      7: int(32 * dim_scale),  # 512
+      8: int(16 * dim_scale),  # 1024
+    }
+
+    self.style_dim_dict = {}
+
+    _out_dim = input_dim
+
+    self.network = nn.ModuleList()
+    self.to_rbgs = nn.ModuleList()
+    for i in range(hidden_layers):
+      _in_dim = _out_dim
+      _out_dim = self.channels[i]
+
+      _layer = film_layer.FiLMLayer(in_dim=_in_dim,
+                                    out_dim=_out_dim,
+                                    style_dim=style_dim)
+      self.network.append(_layer)
+      self.style_dim_dict[f'{name_prefix}_w{i}_0'] = _layer.style_dim
+
+      _layer = film_layer.FiLMLayer(in_dim=_out_dim,
+                                    out_dim=_out_dim,
+                                    style_dim=style_dim)
+      self.network.append(_layer)
+      self.style_dim_dict[f'{name_prefix}_w{i}_1'] = _layer.style_dim
+
+      to_rgb = inr_network.ToRGB(in_dim=_out_dim, dim_rgb=3)
+      self.to_rbgs.append(to_rgb)
+
+    self.tanh = nn.Sequential(
+      # nn.Linear(hidden_dim, rgb_dim),
+      nn.Tanh()
+    )
+    # self.to_rbg.apply(frequency_init(25))
+
+    torch_utils.print_number_params(
+      {
+        'network': self.network,
+        'to_rbgs': self.to_rbgs,
+        'inr_net': self
+      })
+    logging.getLogger('tl').info(self)
+    pass
+
+  def forward(self,
+              input,
+              style_dict,
+              **kwargs):
+    """
+
+    :param input: points xyz, (b, num_points, 3)
+    :param style_dict:
+    :param ray_directions: (b, num_points, 3)
+    :param kwargs:
+    :return:
+    - out: (b, num_points, 4), rgb(3) + sigma(1)
+    """
+
+    x = input
+    rgb = 0
+    for index in range(self.hidden_layers):
+
+      _layer = self.network[index * 2]
+      style = style_dict[f'{self.name_prefix}_w{index}_0']
+
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(_layer,
+                                     inputs_args=(x, style),
+                                     name_prefix=f"{self.name_prefix}.network.{index}.0.")
+      x = _layer(x, style)
+
+      _layer = self.network[index * 2 + 1]
+      style = style_dict[f'{self.name_prefix}_w{index}_1']
+
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(_layer,
+                                     inputs_args=(x, style),
+                                     name_prefix=f"{self.name_prefix}.network.{index}.1.")
+      x = _layer(x, style)
+
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.to_rbgs[index],
+                                     inputs_args=(x, rgb),
+                                     name_prefix=f'to_rgb.{index}')
+      rgb = self.to_rbgs[index](x, skip=rgb)
+
+    # if global_cfg.tl_debug:
+    #   VerboseModel.forward_verbose(self.to_rbg,
+    #                                inputs_args=(x, ),
+    #                                name_prefix='to_rgb.')
+    # out = self.to_rbg(x)
+    if global_cfg.tl_debug:
+      VerboseModel.forward_verbose(self.tanh,
+                                   inputs_args=(rgb, ),
+                                   name_prefix='tanh.')
+    out = self.tanh(rgb)
+    return out
+
+
+
+class ModSinLayer(nn.Module):
+  def __repr__(self): return f"{self.__class__.__name__}({self.repr})"
+
+  def __init__(self,
+               in_dim,
+               use_style_fc=False,
+               style_dim=None,
+               which_linear=nn.Linear,
+               spectral_norm=False,
+               eps=1e-5,
+               freq=1,
+               phase=0,
+               **kwargs):
+    super(ModSinLayer, self).__init__()
+
+    self.repr = f"in_dim={in_dim}, use_style_fc={use_style_fc}, style_dim={style_dim}, " \
+                f"freq={freq}, phase={phase}"
+
+    self.in_dim = in_dim
+    self.use_style_fc = use_style_fc
+    self.style_dim = style_dim
+    self.freq = freq
+    self.phase = phase
+
+    self.spectral_norm = spectral_norm
+    # Prepare gain and bias layers
+
+    if use_style_fc:
+      self.gain_fc = which_linear(style_dim, in_dim)
+      self.bias_fc = which_linear(style_dim, in_dim)
+      if spectral_norm:
+        self.gain_fc = nn.utils.spectral_norm(self.gain_fc)
+        self.bias_fc = nn.utils.spectral_norm(self.bias_fc)
+    else:
+      self.style_dim = in_dim * 2
+
+    self.eps = eps
+
+    self.lrelu = nn.LeakyReLU(0.2, inplace=True)
+    # self.register_buffer('stored_mean', torch.zeros(output_size))
+    # self.register_buffer('stored_var', torch.ones(output_size))
+    pass
+
+  def forward(self,
+              x,
+              style):
+    """
+    Calculate class-conditional gains and biases.
+
+    :param x: (b, c) or (b, n, c)
+    :param style: (b, c)
+    :return:
+    """
+    assert style.shape[-1] == self.style_dim
+
+    if self.use_style_fc:
+      gain = self.gain_fc(style) + 1.
+      bias = self.bias_fc(style)
+    else:
+      style = rearrange(style, "b (n c) -> b n c", n=2)
+      gain, bias = style.unbind(dim=1)
+      gain = gain + 1.
+
+    if x.dim() == 3:
+      gain = rearrange(gain, "b c -> b 1 c")
+      bias = rearrange(bias, "b c -> b 1 c")
+    elif x.dim() == 2:
+      pass
+    else:
+      assert 0
+
+    # x = torch.sin(self.freq * x + self.phase)
+    # out = x * gain + bias
+
+    x = x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + 1e-8)
+    x = x * gain + bias
+
+    out = self.lrelu(x)
+    return out
+
+
+class ModSinLayer_NoBias(nn.Module):
+  def __repr__(self): return f"{self.__class__.__name__}({self.repr})"
+
+  def __init__(self,
+               in_dim,
+               use_style_fc=False,
+               style_dim=None,
+               which_linear=nn.Linear,
+               spectral_norm=False,
+               eps=1e-5,
+               freq=1,
+               phase=0,
+               **kwargs):
+    super(ModSinLayer_NoBias, self).__init__()
+
+    self.repr = f"in_dim={in_dim}, use_style_fc={use_style_fc}, style_dim={style_dim}, " \
+                f"freq={freq}, phase={phase}"
+
+    self.in_dim = in_dim
+    self.use_style_fc = use_style_fc
+    self.style_dim = style_dim
+    self.freq = freq
+    self.phase = phase
+
+    self.spectral_norm = spectral_norm
+    # Prepare gain and bias layers
+
+    if use_style_fc:
+      self.gain_fc = which_linear(style_dim, in_dim)
+      # self.bias_fc = which_linear(style_dim, in_dim)
+      if spectral_norm:
+        self.gain_fc = nn.utils.spectral_norm(self.gain_fc)
+        # self.bias_fc = nn.utils.spectral_norm(self.bias_fc)
+    else:
+      self.style_dim = in_dim * 2
+
+    self.eps = eps
+    pass
+
+  def forward(self,
+              x,
+              style):
+    """
+    Calculate class-conditional gains and biases.
+
+    :param x: (b, c) or (b, n, c)
+    :param style: (b, c)
+    :return:
+    """
+    assert style.shape[-1] == self.style_dim
+
+    if self.use_style_fc:
+      gain = self.gain_fc(style) + 1.
+    else:
+      style = rearrange(style, "b (n c) -> b n c", n=2)
+      gain, bias = style.unbind(dim=1)
+      gain = gain + 1.
+
+    if x.dim() == 3:
+      gain = rearrange(gain, "b c -> b 1 c")
+    elif x.dim() == 2:
+      pass
+    else:
+      assert 0
+
+    x = torch.sin(self.freq * x + self.phase)
+    # out = x * gain + bias
+    out = x * gain
+    return out
+
+
+class SinBlock(nn.Module):
+  def __init__(self,
+               in_dim,
+               out_dim,
+               style_dim,
+               name_prefix,
+               ):
+    super().__init__()
+    self.in_dim = in_dim
+    self.out_dim = out_dim
+    self.style_dim = style_dim
+    self.name_prefix = name_prefix
+
+    self.style_dim_dict = {}
+
+    # self.mod1 = mod_conv_fc.Modulated_FC_Conv(in_channel=in_dim,
+    #                                           out_channel=out_dim,
+    #                                           style_dim=style_dim,
+    #                                           use_style_fc=True,
+    #                                           scale=1.,
+    #                                           # scale=None,
+    #                                           )
+    self.mod1 = mod_conv_fc.SinStyleMod(in_channel=in_dim,
+                                        out_channel=out_dim,
+                                        style_dim=style_dim,
+                                        use_style_fc=True,
+                                        )
+    self.style_dim_dict[f'{name_prefix}_0'] = self.mod1.style_dim
+    self.act1 = nn.LeakyReLU(0.2, inplace=True)
+
+    # self.mod2 = mod_conv_fc.Modulated_FC_Conv(in_channel=out_dim,
+    #                                           out_channel=out_dim,
+    #                                           style_dim=style_dim,
+    #                                           use_style_fc=True,
+    #                                           scale=1.,
+    #                                           # scale=None,
+    #                                           )
+    self.mod2 = mod_conv_fc.SinStyleMod(in_channel=out_dim,
+                                        out_channel=out_dim,
+                                        style_dim=style_dim,
+                                        use_style_fc=True,
+                                        )
+    self.style_dim_dict[f'{name_prefix}_1'] = self.mod2.style_dim
+    self.act2 = nn.LeakyReLU(0.2, inplace=True)
+
+    # self.linear1 = nn.Linear(in_dim, out_dim)
+    # self.mod1 = ModSinLayer(in_dim=out_dim, use_style_fc=True, style_dim=style_dim)
+    # self.style_dim_dict[f'{name_prefix}_0'] = self.mod1.style_dim
+
+    # self.linear2 = nn.Linear(out_dim, out_dim)
+    # self.mod2 = ModSinLayer(in_dim=out_dim, use_style_fc=True, style_dim=style_dim)
+    # self.style_dim_dict[f'{name_prefix}_1'] = self.mod2.style_dim
+
+    self.skip = SkipLayer()
+    pass
+
+  def forward(self,
+              x,
+              style_dict,
+              skip=False):
+    x_orig = x
+
+    style = style_dict[f'{self.name_prefix}_0']
+    x = self.mod1(x, style)
+    x = self.act1(x)
+
+    style = style_dict[f'{self.name_prefix}_1']
+    x = self.mod2(x, style)
+    out = self.act2(x)
+
+    # x = self.linear1(x)
+    # style = style_dict[f'{self.name_prefix}_0']
+    # x = self.mod1(x, style)
+
+    # x = self.linear2(x)
+    # style = style_dict[f'{self.name_prefix}_1']
+    # out = self.mod2(x, style)
+
+    if skip and out.shape[-1] == x_orig.shape[-1]:
+      # out = (out + x_orig) / 1.41421
+      out = self.skip(out, x_orig)
+    return out
+
+  def __repr__(self):
+    repr = f"{self.__class__.__name__}(in_dim={self.in_dim}, " \
+           f"out_dim={self.out_dim}, " \
+           f"style_dim={self.style_dim})"
+    return repr
+
+
+class ToRGB(nn.Module):
+  def __init__(self,
+               in_dim,
+               dim_rgb=3,
+               use_equal_fc=False):
+    super().__init__()
+    self.in_dim = in_dim
+    self.dim_rgb = dim_rgb
+
+    if use_equal_fc:
+      self.linear = mod_conv_fc.EqualLinear(in_dim, dim_rgb, scale=1.)
+    else:
+      self.linear = nn.Linear(in_dim, dim_rgb)
+    pass
+
+  def forward(self,
+              input,
+              skip=None):
+
+    out = self.linear(input)
+
+    if skip is not None:
+      out = out + skip
+    return out
+
+
+class SigmoidToTanh(nn.Module):
+  def __init__(self, ):
+    super(SigmoidToTanh, self).__init__()
+
+  def forward(self, x):
+    """
+
+    :param x: [0, 1] -> [-1, 1]
+    :return:
+    """
+    return x * 2. - 1
+
+
+@MODEL_REGISTRY.register(name_prefix=__name__)
+class Generator_Diffcam(nn.Module):
+
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               nerf_cfg,
+               mapping_shape_app_cfg,
+               inr_cfg,
+               mapping_inr_cfg,
+               shape_block_end_index=None,
+               app_block_end_index=None,
+               inr_block_end_index=None,
+               device='cuda',
+               inr_detach=False,
+               disable_inr=False,
+               **kwargs):
+    super(Generator_Diffcam, self).__init__()
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'nerf_cfg': nerf_cfg,
+      'mapping_shape_app_cfg': mapping_shape_app_cfg,
+      'inr_cfg': inr_cfg,
+      'mapping_inr_cfg': mapping_inr_cfg,
+      'shape_block_end_index': shape_block_end_index,
+      'app_block_end_index': app_block_end_index,
+      'inr_block_end_index': inr_block_end_index,
+      'inr_detach': inr_detach,
+      'disable_inr': disable_inr,
+    })
+
+    self.device = device
+    self.inr_block_end_index = inr_block_end_index
+    self.inr_detach = inr_detach
+    self.disable_inr = disable_inr
+
+    self.module_name_list = []
+
+    # nerf_net
+    self.nerf_net = siren_net_pigan.NeRF_Net(
+      shape_block_end_index=shape_block_end_index,
+      app_block_end_index=app_block_end_index,
+      **nerf_cfg)
+    self.module_name_list.append('nerf_net')
+
+    # mapping shape_app
+    self.mapping_shape_app = siren_net_pigan.StyleMappingShapeApp(
+      **mapping_shape_app_cfg,
+      style_dim_dict_shape=self.nerf_net.style_dim_dict_shape,
+      style_dim_dict_app=self.nerf_net.style_dim_dict_app)
+    self.module_name_list.append('mapping_shape_app')
+
+    # _in_dim = nerf_cfg.app_net_cfg.out_dim
+    _in_dim = self.nerf_net.out_dim
+
+    # inr_net
+    self.inr_net = cips_net.CIPSNet(**{
+      **inr_cfg,
+      "input_dim": _in_dim,
+      'add_out_layer': True,
+    })
+    self.module_name_list.append('inr_net')
+
+    self.mapping_inr = multi_head_mapping.MultiHeadMappingNetwork(**{
+      **mapping_inr_cfg,
+      'head_dim_dict': self.inr_net.style_dim_dict
+    })
+    self.module_name_list.append('mapping_inr')
+
+
+    self.aux_to_rbg = nn.Sequential(
+      SigmoidToTanh(),
+      # nn.Linear(_in_dim, 3),
+      # nn.Tanh()
+    )
+    self.aux_to_rbg.apply(nerf_network.frequency_init(25))
+    self.module_name_list.append('aux_to_rbg')
+
+
+    tl2_utils.print_repr(self)
+    pass
+
+  def get_subnet_grad_norm(self):
+    ret_dict = {}
+
+    for name in self.module_name_list:
+      subnet = getattr(self, name)
+      grad_norm = torch_utils.get_grad_norm_total(params=subnet.parameters())
+      ret_dict[f"G_GN.{name}"] = grad_norm
+    return ret_dict
+
+  def forward(self,
+              zs,
+              rays_o,
+              rays_d,
+              nerf_kwargs={},
+              psi=1,
+              return_aux_img=False,
+              grad_points=None,
+              forward_points=None,  # disable gradients
+              **kwargs):
+    """
+    Generates images from a noise vector, rendering parameters, and camera distribution.
+    Uses the hierarchical sampling scheme described in NeRF.
+
+    :param zs: {k: (b, z_dim), ...}
+    :param rays_o: (b, h, w, 3) in world space
+    :param rays_d: (b, h, w, 3) in world space
+
+    :return:
+    - pixels: (b, 3, h, w)
+    - pitch_yaw: (b, 2)
+    """
+
+    # mapping network
+    style_dict = self.mapping_network(**zs)
+
+    if psi < 1:
+      avg_styles = self.generate_avg_frequencies(device=self.device)
+      style_dict = self.get_truncated_freq_phase(
+        raw_style_dict=style_dict, avg_style_dict=avg_styles, raw_lambda=psi)
+
+    b, h, w, c = rays_o.shape
+    rays_o = rearrange(rays_o, "b h w c -> b (h w) c")
+    rays_d = rearrange(rays_d, "b h w c -> b (h w) c")
+
+    if grad_points is not None and grad_points < h * w:
+      imgs, ret_maps = self.part_grad_forward(
+        rays_o=rays_o,
+        rays_d=rays_d,
+        style_dict=style_dict,
+        nerf_kwargs=nerf_kwargs,
+        return_aux_img=return_aux_img,
+        grad_points=grad_points)
+
+    else:
+      imgs, ret_maps = self.whole_grad_forward(
+        rays_o=rays_o,
+        rays_d=rays_d,
+        style_dict=style_dict,
+        nerf_kwargs=nerf_kwargs,
+        return_aux_img=return_aux_img,
+        forward_points=forward_points)
+
+    imgs = rearrange(imgs, "b (h w) c -> b c h w", h=h, w=w)
+
+    ret_imgs = {}
+    for name, v_map in ret_maps.items():
+      if v_map.dim() == 3:
+        v_map = rearrange(v_map, "b (h w) c -> b c h w", h=h, w=w)
+      elif v_map.dim() == 2:
+        v_map = rearrange(v_map, "b (h w) -> b h w", h=h, w=w)
+      ret_imgs[name] = v_map
+
+    return imgs, ret_imgs
+
+  def get_rays_axis_angle(self,
+                          R,
+                          t,
+                          fx,
+                          fy,
+                          H: int,
+                          W: int,
+                          N_rays: int = -1):
+    """
+
+    :param R: (b, 3)
+    :param t: (b, 3)
+    :param fx:
+    :param fy:
+    :param H:
+    :param W:
+    :param N_rays:
+    :return
+
+    - rays_o: (b, H, W, 3)
+    - rays_d: (b, H, W, 3)
+    - select_inds: (b, H, W)
+    """
+
+    rays_o, rays_d, select_inds = cam_params.get_rays(
+      rot=R,
+      trans=t,
+      focal_x=fx,
+      focal_y=fy,
+      H=H,
+      W=W,
+      N_rays=N_rays,
+      flatten=False)
+
+    return rays_o, rays_d, select_inds
+
+  def get_batch_style_dict(self, b, style_dict):
+    ret_style_dict = {}
+    for name, style in style_dict.items():
+      ret_style_dict[name] = style[[b]]
+    return ret_style_dict
+
+  def whole_grad_forward(self,
+                         rays_o,
+                         rays_d,
+                         style_dict,
+                         nerf_kwargs,
+                         return_aux_img=True,
+                         forward_points=None,
+                         **kwargs):
+
+    if forward_points is not None and forward_points <= rays_o.shape[1]: # no gradients
+      # stage forward
+      with torch.no_grad():
+        batch_size = rays_o.shape[0]
+        num_points = rays_o.shape[1]
+
+        near = nerf_kwargs['near']
+        far = nerf_kwargs['far']
+        N_samples = nerf_kwargs['N_samples']
+        perturb = self.training
+        z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                            rays_d=rays_d,
+                                                            near=near,
+                                                            far=far,
+                                                            N_samples=N_samples,
+                                                            perturb=perturb)
+
+        batch_image_ddict = collections.defaultdict(list)
+        for b in range(batch_size):
+          image_ddict = collections.defaultdict(list)
+
+          head = 0
+          while head < num_points:
+            tail = head + forward_points
+            cur_style_dict = self.get_batch_style_dict(b=b, style_dict=style_dict)
+
+            cur_inr_img, cur_ret_maps = self.points_forward(
+              rays_o=rays_o[[b], head:tail], # (b, hxw, 3)
+              rays_d=rays_d[[b], head:tail], # (b, hxw, 3)
+              points=points[[b], head:tail], # (b, hxw, Nsamples, 3)
+              z_vals=z_vals[[b], head:tail], # (b, hxw, Nsamples)
+              style_dict=cur_style_dict,
+              nerf_kwargs=nerf_kwargs,
+              return_aux_img=return_aux_img)
+
+            image_ddict['inr_img'].append(cur_inr_img)
+            for k, v in cur_ret_maps.items():
+              image_ddict[k].append(v)
+            head += forward_points
+          for k, v in image_ddict.items():
+            one_image = torch.cat(v, dim=1)
+            batch_image_ddict[k].append(one_image)
+        ret_maps = {}
+        for k, v in batch_image_ddict.items():
+          ret_maps[k] = torch.cat(v, dim=0)
+        imgs = ret_maps.pop('inr_img')
+
+    else:
+      near = nerf_kwargs['near']
+      far = nerf_kwargs['far']
+      N_samples = nerf_kwargs['N_samples']
+      perturb = self.training
+      z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                          rays_d=rays_d,
+                                                          near=near,
+                                                          far=far,
+                                                          N_samples=N_samples,
+                                                          perturb=perturb)
+
+      # transformed_points = rearrange(transformed_points, "b (h w s) c -> b (h w) s c", h=img_size, s=num_steps)
+      # transformed_ray_directions_expanded = rearrange(transformed_ray_directions_expanded,
+      #                                                 "b (h w s) c -> b (h w) s c", h=img_size, s=num_steps)
+
+      imgs, ret_maps = self.points_forward(
+        rays_o=rays_o,
+        rays_d=rays_d,
+        points=points,
+        z_vals=z_vals,
+        style_dict=style_dict,
+        nerf_kwargs=nerf_kwargs,
+        return_aux_img=return_aux_img)
+
+    return imgs, ret_maps
+
+  def part_grad_forward(self,
+                        rays_o,
+                        rays_d,
+                        style_dict,
+                        nerf_kwargs,
+                        return_aux_img,
+                        grad_points):
+
+    near = nerf_kwargs['near']
+    far = nerf_kwargs['far']
+    N_samples = nerf_kwargs['N_samples']
+    perturb = self.training
+    # z_vals: (b, hxw, Nsamples), points: (b, hxw, Nsamples, 3)
+    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o, # (b, hxw, 3)
+                                                        rays_d=rays_d, # (b, hxw, 3)
+                                                        near=near,
+                                                        far=far,
+                                                        N_samples=N_samples,
+                                                        perturb=perturb)
+
+    # transformed_points = rearrange(transformed_points, "b (h w s) c -> b (h w) s c", h=img_size, s=num_steps)
+    # transformed_ray_directions_expanded = rearrange(transformed_ray_directions_expanded,
+    #                                                 "b (h w s) c -> b (h w) s c", h=img_size, s=num_steps)
+
+    batch_size = rays_o.shape[0]
+    num_points = rays_o.shape[1]
+    device = self.device
+    assert num_points > grad_points
+    idx_grad, idx_no_grad = torch_utils.batch_random_split_indices(bs=batch_size,
+                                                                   num_points=num_points,
+                                                                   grad_points=grad_points,
+                                                                   device=device)
+    # rand_idx = torch.randperm(num_points, device=device)
+    # idx_grad = rand_idx[:grad_points]
+    # idx_no_grad = rand_idx[grad_points:]
+
+    inr_img_grad, ret_maps_grad = self.points_forward(
+      rays_o=rays_o,
+      rays_d=rays_d,
+      points=points,
+      z_vals=z_vals,
+      style_dict=style_dict,
+      nerf_kwargs=nerf_kwargs,
+      return_aux_img=return_aux_img,
+      idx_grad=idx_grad)
+
+    with torch.no_grad():
+      inr_img_no_grad, ret_maps_no_grad = self.points_forward(
+        rays_o=rays_o,
+        rays_d=rays_d,
+        points=points,
+        z_vals=z_vals,
+        style_dict=style_dict,
+        nerf_kwargs=nerf_kwargs,
+        return_aux_img=return_aux_img,
+        idx_grad=idx_no_grad)
+
+    imgs = comm_utils.batch_scatter_points(idx_grad=idx_grad,
+                                           points_grad=inr_img_grad,
+                                           idx_no_grad=idx_no_grad,
+                                           points_no_grad=inr_img_no_grad,
+                                           num_points=num_points)
+    ret_maps = {}
+    for k in ret_maps_grad.keys():
+      comp_map = comm_utils.batch_scatter_points(idx_grad=idx_grad,
+                                                 points_grad=ret_maps_grad[k],
+                                                 idx_no_grad=idx_no_grad,
+                                                 points_no_grad=ret_maps_no_grad[k],
+                                                 num_points=num_points)
+      ret_maps[k] = comp_map
+    return imgs, ret_maps
+
+  def points_forward(self,
+                     rays_o,
+                     rays_d,
+                     points,
+                     z_vals,
+                     style_dict,
+                     nerf_kwargs,
+                     return_aux_img,
+                     idx_grad=None,
+                     disable_view_dir=True,
+                     **kwargs):
+    """
+
+    :param rays_o: (b, hxw, 3)
+    :param rays_d: (b, hxw, 3)
+    :param points: (b, hxw, Nsamples, 3)
+    :param z_vals: (b, hxw, Nsamples)
+    :param style_dict:
+    :param nerf_kwargs:
+    :param return_aux_img:
+    :param idx_grad: (b, N_grad, )
+    :param kwargs:
+    :return:
+    """
+
+    device = points.device
+
+    if disable_view_dir:
+      viewdirs = torch.zeros_like(rays_d)
+      viewdirs[..., -1] = -1
+    else:
+      viewdirs = F.normalize(rays_d, dim=-1)
+    # viewdirs = volume_rendering.get_viewdirs(rays_d=rays_d)
+    # viewdirs = viewdirs[..., None, :].expand_as(points)
+    N_samples = nerf_kwargs['N_samples']
+
+    if idx_grad is not None:
+      rays_o = comm_utils.batch_gather_points(points=rays_o, idx_grad=idx_grad)
+      rays_d = comm_utils.batch_gather_points(points=rays_d, idx_grad=idx_grad)
+      points = comm_utils.batch_gather_points(points=points, idx_grad=idx_grad)
+      z_vals = comm_utils.batch_gather_points(points=z_vals, idx_grad=idx_grad)
+
+
+    points = rearrange(points, "b Nrays Nsamples c -> b (Nrays Nsamples) c")
+    coarse_viewdirs = repeat(viewdirs, "b Nrays c -> b (Nrays Nsamples) c", Nsamples=N_samples)
+
+    # Model prediction on course points
+    coarse_output = self.nerf_net(
+      x=points,  # b (Nrays Nsamples) c
+      ray_directions=coarse_viewdirs, # b (Nrays Nsamples) c
+      style_dict=style_dict)
+
+    # Mask out values outside
+    # padd = 0.
+    # mask_box = torch.all(points <= 1. + padd, dim=-1) & torch.all(points >= -1. - padd, dim=-1)
+    # coarse_output[mask_box == 0] = 0.
+
+    coarse_output = rearrange(
+      coarse_output, "b (Nrays Nsamples) rgb_sigma -> b Nrays Nsamples rgb_sigma", Nsamples=N_samples)
+
+    # Re-sample fine points alont camera rays, as described in NeRF
+    if nerf_kwargs['N_importance'] > 0:
+
+      with torch.no_grad():
+        raw_sigma = coarse_output[..., -1]
+        perturb = self.training
+        fine_z_vals, fine_points = volume_rendering.get_fine_points(
+          z_vals=z_vals,
+          rays_o=rays_o,
+          rays_d=rays_d,
+          raw_sigma=raw_sigma,
+          N_importance=nerf_kwargs['N_importance'],
+          perturb=perturb,
+          raw_noise_std=nerf_kwargs['raw_noise_std'],
+          eps=nerf_kwargs['eps'])
+
+      # Model prediction on re-sampled find points
+      fine_points = rearrange(fine_points, "b Nrays Nsamples c -> b (Nrays Nsamples) c")
+      fine_viewdirs = repeat(viewdirs, "b Nrays c -> b (Nrays Nsamples) c", Nsamples=nerf_kwargs['N_importance'])
+
+      fine_output = self.nerf_net(
+        x=fine_points,  # b (Nrays Nsamples) c
+        ray_directions=fine_viewdirs,  # b (Nrays Nsamples) c
+        style_dict=style_dict)
+      fine_output = rearrange(
+        fine_output, "b (Nrays Nsamples) rgb_sigma -> b Nrays Nsamples rgb_sigma", Nsamples=nerf_kwargs['N_importance'])
+
+      # Combine course and fine points
+      DIM_SAMPLES = 2
+      all_z_vals = torch.cat([fine_z_vals, z_vals], dim=DIM_SAMPLES) # (b, N_rays, N_samples)
+      _, indices = torch.sort(all_z_vals, dim=DIM_SAMPLES) # (b, N_rays, N_samples)
+      # gather z_vals
+      all_z_vals = torch.gather(all_z_vals, DIM_SAMPLES, indices) # (b, N_rays, N_samples)
+
+      # (b, N_rays, N_samples, rgb_sigma)
+      all_outputs = torch.cat([fine_output, coarse_output], dim=DIM_SAMPLES)
+      view_shape = [*indices.shape, *(len(all_outputs.shape) - len(indices.shape)) * [1]]
+      all_outputs = torch.gather(all_outputs, DIM_SAMPLES, indices.view(view_shape).expand_as(all_outputs))
+
+    else:
+      all_outputs = coarse_output
+      all_z_vals = z_vals
+
+    # Create images with NeRF
+    all_raw_rgb = all_outputs[..., :-1]
+    all_raw_sigma = all_outputs[..., -1]
+
+    fea_rgb, ret_maps = volume_rendering.ray_integration(raw_rgb=all_raw_rgb,
+                                                         raw_sigma=all_raw_sigma,
+                                                         z_vals=all_z_vals,
+                                                         rays_d=rays_d,
+                                                         raw_noise_std=nerf_kwargs['raw_noise_std'],
+                                                         eps=nerf_kwargs['eps'])
+
+    pixels_fea = fea_rgb[..., :-3]
+    rgb = fea_rgb[..., -3:]
+
+    # inr_net
+    if self.disable_inr:
+      inr_img = self.aux_to_rbg(rgb)
+    else:
+      if self.inr_detach:
+        inr_img = self.inr_net(pixels_fea.detach(), style_dict, block_end_index=self.inr_block_end_index)
+      else:
+        inr_img = self.inr_net(pixels_fea, style_dict, block_end_index=self.inr_block_end_index)
+
+    if return_aux_img:
+      # aux rgb_branch
+      aux_img = self.aux_to_rbg(rgb)
+      ret_maps['aux_img'] = aux_img
+
+    return inr_img, ret_maps
+
+  def z_sampler(self,
+                shape,
+                device,
+                dist='gaussian'):
+    if dist == 'gaussian':
+      z = torch.randn(shape, device=device)
+    elif dist == 'uniform':
+      z = torch.rand(shape, device=device) * 2 - 1
+    return z
+
+  def get_zs(self,
+             b,
+             batch_split=1,
+             merge_z_shape_app=True):
+    z_shape = self.z_sampler(shape=(b, self.mapping_shape_app.z_dim), device=self.device)
+    if merge_z_shape_app:
+      z_app = None
+    else:
+      z_app = self.z_sampler(shape=(b, self.mapping_shape_app.z_dim), device=self.device)
+    z_inr = self.z_sampler(shape=(b, self.mapping_inr.z_dim), device=self.device)
+
+    if batch_split > 1:
+      zs_list = []
+      z_shape_list = z_shape.split(b // batch_split)
+      if z_app is None:
+        z_app_list = [None] * batch_split
+      else:
+        z_app_list = z_app.split(b // batch_split)
+      z_inr_list = z_inr.split(b // batch_split)
+      for z_shape_, z_app_, z_inr_ in zip(z_shape_list, z_app_list, z_inr_list):
+        zs_ = {
+          'z_shape': z_shape_,
+          'z_app': z_app_,
+          'z_inr': z_inr_,
+        }
+        zs_list.append(zs_)
+      return zs_list
+    else:
+      zs = {
+        'z_shape': z_shape,
+        'z_app': z_app,
+        'z_inr': z_inr,
+      }
+      return zs
+
+  def mapping_network(self,
+                      z_shape,
+                      z_app,
+                      z_inr):
+    if global_cfg.tl_debug:
+      VerboseModel.forward_verbose(self.mapping_shape_app,
+                                   inputs_args=(z_shape, z_app),
+                                   submodels=['base_net.network'],
+                                   name_prefix='mapping_shape_app.')
+      VerboseModel.forward_verbose(self.mapping_inr,
+                                   inputs_args=(z_inr,),
+                                   submodels=['base_net', ],
+                                   input_padding=50,
+                                   name_prefix='mapping_inr.')
+
+    style_dict = {}
+    style_dict.update(self.mapping_shape_app(z_shape, z_app))
+    style_dict.update(self.mapping_inr(z_inr))
+
+    return style_dict
+
+  def get_truncated_freq_phase(self,
+                               raw_style_dict,
+                               avg_style_dict,
+                               raw_lambda):
+
+    truncated_style_dict = {}
+    for name, avg_style in avg_style_dict.items():
+      raw_style = raw_style_dict[name]
+      truncated_style = avg_style + raw_lambda * (raw_style - avg_style)
+      truncated_style_dict[name] = truncated_style
+    return truncated_style_dict
+
+  def generate_avg_frequencies(self,
+                               num_samples=10000,
+                               device='cuda'):
+    """Calculates average frequencies and phase shifts"""
+
+    # z = torch.randn((num_samples, self.z_dim), device=device)
+    zs = self.get_zs(num_samples)
+    with torch.no_grad():
+      style_dict = self.mapping_network(**zs)
+
+    avg_styles = {}
+    for name, style in style_dict.items():
+      avg_styles[name] = style.mean(0, keepdim=True)
+
+    # self.avg_styles = avg_styles
+    return avg_styles
+
+  def staged_forward(self, *args, **kwargs):
+    raise NotImplementedError
+
+  def set_device(self, device):
+    pass
+
+  def forward_camera_pos_and_lookup(self,
+              zs,
+              img_size,
+              fov,
+              ray_start,
+              ray_end,
+              num_steps,
+              h_stddev,
+              v_stddev,
+              h_mean,
+              v_mean,
+              hierarchical_sample,
+              camera_pos,
+              camera_lookup,
+              psi=1,
+              sample_dist=None,
+              lock_view_dependence=False,
+              clamp_mode='relu',
+              nerf_noise=0.,
+              white_back=False,
+              last_back=False,
+              return_aux_img=False,
+              grad_points=None,
+              forward_points=None,
+              **kwargs):
+    """
+    Generates images from a noise vector, rendering parameters, and camera distribution.
+    Uses the hierarchical sampling scheme described in NeRF.
+
+    :param z: (b, z_dim)
+    :param img_size:
+    :param fov: face: 12
+    :param ray_start: face: 0.88
+    :param ray_end: face: 1.12
+    :param num_steps: face: 12
+    :param h_stddev: face: 0.3
+    :param v_stddev: face: 0.155
+    :param h_mean: face: pi/2
+    :param v_mean: face: pi/2
+    :param hierarchical_sample: face: true
+    :param camera_pos: (b, 3)
+    :param camera_lookup: (b, 3)
+    :param psi: [0, 1]
+    :param sample_dist: mode for sample_camera_positions, face: 'gaussian'
+    :param lock_view_dependence: face: false
+    :param clamp_mode: face: 'relu'
+    :param nerf_noise:
+    :param last_back: face: false
+    :param white_back: face: false
+    :param kwargs:
+    :return:
+    - pixels: (b, 3, h, w)
+    - pitch_yaw: (b, 2)
+    """
+
+    # mapping network
+    if global_cfg.tl_debug:
+      VerboseModel.forward_verbose(self.mapping_network_nerf,
+                                   inputs_args=(zs['z_nerf'],),
+                                   submodels=['base_net'],
+                                   name_prefix='mapping_nerf.')
+      VerboseModel.forward_verbose(self.mapping_network_inr,
+                                   inputs_args=(zs['z_inr'],),
+                                   submodels=['base_net', ],
+                                   input_padding=50,
+                                   name_prefix='mapping_inr.')
+    style_dict = self.mapping_network(**zs)
+
+    if psi < 1:
+      avg_styles = self.generate_avg_frequencies(device=self.device)
+      style_dict = self.get_truncated_freq_phase(
+        raw_style_dict=style_dict, avg_style_dict=avg_styles, raw_lambda=psi)
+
+    if grad_points is not None and grad_points < img_size ** 2:
+      imgs, pitch_yaw = self.part_grad_forward(
+        style_dict=style_dict,
+        img_size=img_size,
+        fov=fov,
+        ray_start=ray_start,
+        ray_end=ray_end,
+        num_steps=num_steps,
+        h_stddev=h_stddev,
+        v_stddev=v_stddev,
+        h_mean=h_mean,
+        v_mean=v_mean,
+        hierarchical_sample=hierarchical_sample,
+        sample_dist=sample_dist,
+        lock_view_dependence=lock_view_dependence,
+        clamp_mode=clamp_mode,
+        nerf_noise=nerf_noise,
+        white_back=white_back,
+        last_back=last_back,
+        return_aux_img=return_aux_img,
+        grad_points=grad_points,
+        camera_pos=camera_pos,
+        camera_lookup=camera_lookup,
+      )
+      return imgs, pitch_yaw
+    else:
+      imgs, pitch_yaw = self.whole_grad_forward(
+        style_dict=style_dict,
+        img_size=img_size,
+        fov=fov,
+        ray_start=ray_start,
+        ray_end=ray_end,
+        num_steps=num_steps,
+        h_stddev=h_stddev,
+        v_stddev=v_stddev,
+        h_mean=h_mean,
+        v_mean=v_mean,
+        hierarchical_sample=hierarchical_sample,
+        sample_dist=sample_dist,
+        lock_view_dependence=lock_view_dependence,
+        clamp_mode=clamp_mode,
+        nerf_noise=nerf_noise,
+        white_back=white_back,
+        last_back=last_back,
+        return_aux_img=return_aux_img,
+        forward_points=forward_points,
+        camera_pos=camera_pos,
+        camera_lookup=camera_lookup,
+      )
+      return imgs, pitch_yaw
+
+
+@MODEL_REGISTRY.register(name_prefix=__name__)
+class Generator_Diffcam_FreezeNeRF(Generator_Diffcam):
+
+  def load_nerf_ema(self, G_ema):
+    ret = self.nerf_net.load_state_dict(G_ema.nerf_net.state_dict())
+    ret = self.mapping_shape.load_state_dict(G_ema.mapping_shape.state_dict())
+    ret = self.mapping_app.load_state_dict(G_ema.mapping_app.state_dict())
+
+    ret = self.aux_to_rbg.load_state_dict(G_ema.aux_to_rbg.state_dict())
+
+    # ret = self.mapping_inr.load_state_dict(G_ema.mapping_inr.state_dict())
+    pass
+
+  def forward(self, **kwargs):
+
+    self.nerf_net.requires_grad_(False)
+    self.mapping_shape.requires_grad_(False)
+    self.mapping_app.requires_grad_(False)
+    self.aux_to_rbg.requires_grad_(False)
+
+    return super(Generator_Diffcam_FreezeNeRF, self).forward(**kwargs)
+
+  def mapping_network(self,
+                      z_shape,
+                      z_app,
+                      z_inr):
+    style_dict = {}
+    with torch.no_grad():
+      style_dict.update(self.mapping_shape(z_shape))
+      style_dict.update(self.mapping_app(z_app))
+    style_dict.update(self.mapping_inr(z_inr))
+
+
+    return style_dict
+
+  # def points_forward(self,
+  #                    style_dict,
+  #                    transformed_points,
+  #                    transformed_ray_directions_expanded,
+  #                    num_steps,
+  #                    hierarchical_sample,
+  #                    z_vals,
+  #                    clamp_mode,
+  #                    nerf_noise,
+  #                    transformed_ray_origins,
+  #                    transformed_ray_directions,
+  #                    white_back,
+  #                    last_back,
+  #                    return_aux_img,
+  #                    idx_grad=None,
+  #                    ):
+  #   """
+  #
+  #   :param style_dict:
+  #   :param transformed_points: (b, n, s, 3)
+  #   :param transformed_ray_directions_expanded: (b, n, s, 3)
+  #   :param num_steps: sampled points along a ray
+  #   :param hierarchical_sample:
+  #   :param z_vals: (b, n, s, 1)
+  #   :param clamp_mode: 'relu'
+  #   :param nerf_noise:
+  #   :param transformed_ray_origins: (b, n, 3)
+  #   :param transformed_ray_directions: (b, n, 3)
+  #   :param white_back:
+  #   :param last_back:
+  #   :return:
+  #   """
+  #   device = transformed_points.device
+  #   if idx_grad is not None:
+  #     transformed_points = comm_utils.gather_points(points=transformed_points, idx_grad=idx_grad)
+  #     transformed_ray_directions_expanded = comm_utils.gather_points(
+  #       points=transformed_ray_directions_expanded, idx_grad=idx_grad)
+  #     z_vals = comm_utils.gather_points(points=z_vals, idx_grad=idx_grad)
+  #     transformed_ray_origins = comm_utils.gather_points(points=transformed_ray_origins, idx_grad=idx_grad)
+  #     transformed_ray_directions = comm_utils.gather_points(points=transformed_ray_directions, idx_grad=idx_grad)
+  #
+  #   transformed_points = rearrange(transformed_points, "b n s c -> b (n s) c")
+  #   transformed_ray_directions_expanded = rearrange(transformed_ray_directions_expanded, "b n s c -> b (n s) c")
+  #
+  #   # Model prediction on course points
+  #   with torch.no_grad():
+  #     coarse_output = self.nerf_net(
+  #       x=transformed_points,  # (b, n x s, 3)
+  #       style_dict=style_dict,
+  #       ray_directions=transformed_ray_directions_expanded,
+  #     )
+  #   coarse_output = rearrange(coarse_output, "b (n s) rgb_sigma -> b n s rgb_sigma", s=num_steps)
+  #
+  #   # Re-sample fine points alont camera rays, as described in NeRF
+  #   if hierarchical_sample:
+  #     fine_points, fine_z_vals = self.get_fine_points_and_direction(
+  #       coarse_output=coarse_output,
+  #       z_vals=z_vals,
+  #       dim_rgb=self.nerf_net.rgb_dim,
+  #       clamp_mode=clamp_mode,
+  #       nerf_noise=nerf_noise,
+  #       num_steps=num_steps,
+  #       transformed_ray_origins=transformed_ray_origins,
+  #       transformed_ray_directions=transformed_ray_directions
+  #     )
+  #
+  #     # Model prediction on re-sampled find points
+  #     with torch.no_grad():
+  #       fine_output = self.nerf_net(
+  #         x=fine_points,  # (b, n x s, 3)
+  #         style_dict=style_dict,
+  #         ray_directions=transformed_ray_directions_expanded,  # (b, n x s, 3)
+  #       )
+  #     fine_output = rearrange(fine_output, "b (n s) rgb_sigma -> b n s rgb_sigma", s=num_steps)
+  #
+  #     # Combine course and fine points
+  #     all_outputs = torch.cat([fine_output, coarse_output], dim=-2)  # (b, n, s, dim_rgb_sigma)
+  #     all_z_vals = torch.cat([fine_z_vals, z_vals], dim=-2)  # (b, n, s, 1)
+  #     _, indices = torch.sort(all_z_vals, dim=-2)  # (b, n, s, 1)
+  #     all_z_vals = torch.gather(all_z_vals, -2, indices)  # (b, n, s, 1)
+  #     # (b, n, s, dim_rgb_sigma)
+  #     all_outputs = torch.gather(all_outputs, -2, indices.expand(-1, -1, -1, all_outputs.shape[-1]))
+  #   else:
+  #     all_outputs = coarse_output
+  #     all_z_vals = z_vals
+  #
+  #   # Create images with NeRF
+  #   pixels_fea, depth, weights = pigan_utils.fancy_integration(
+  #     rgb_sigma=all_outputs,
+  #     z_vals=all_z_vals,
+  #     device=device,
+  #     dim_rgb=self.nerf_net.rgb_dim,
+  #     white_back=white_back,
+  #     last_back=last_back,
+  #     clamp_mode=clamp_mode,
+  #     noise_std=nerf_noise)
+  #
+  #   inr_img = self.inr_net(pixels_fea, style_dict)
+  #
+  #   if return_aux_img:
+  #     # aux rgb_branch
+  #     with torch.no_grad():
+  #       aux_img = self.aux_to_rbg(pixels_fea)
+  #   else:
+  #     aux_img = None
+  #
+  #   return inr_img, aux_img
+
+
+
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/cips3d/pigan_gen_celeba.yaml` & `tl2-0.1.1/tl2/proj/pytorch/examples/cips3d/pigan_gen_celeba.yaml`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,74 +1,74 @@
-
-
-nerf_cfg: &nerf_cfg
-  shape_net_cfg:
-    input_dim: 3
-    hidden_dim: 256
-    out_dim: 256
-    N_layers: 8
-  app_net_cfg:
-    input_dim: 256
-    hidden_dim: 256
-    out_dim: 256
-    N_layers: 1
-
-mapping_shape_app_cfg: &mapping_shape_app_cfg
-  base_cfg:
-    z_dim: 256
-    hidden_dim: 256
-    N_layers: 3
-
-inr_cfg: &inr_cfg
-#  input_dim: 128
-  hidden_dim: 512
-  out_dim: 3
-  style_dim: 512
-  num_blocks: 9
-
-mapping_inr_cfg: &mapping_inr_cfg
-  z_dim: 512
-  hidden_dim: 512
-  base_layers: 8
-  head_layers: 0
-  add_norm: true
-  norm_out: true
-
-
-G_cfg_3D2D: &G_cfg_3D2D
-  register_modules:
-  - tl2_lib.tl2.proj.pytorch.examples.cips3d.pigan_gen_celeba
-  name: tl2_lib.tl2.proj.pytorch.examples.cips3d.pigan_gen_celeba.Generator_Diffcam
-  nerf_cfg: *nerf_cfg
-  mapping_shape_app_cfg: *mapping_shape_app_cfg
-  inr_cfg: *inr_cfg
-  mapping_inr_cfg: *mapping_inr_cfg
-  shape_block_end_index: 8
-  app_block_end_index: 1
-  inr_block_end_index: 9
-  inr_detach: true
-
-
-G_kwargs: &G_kwargs
-  psi: 1.
-  nerf_kwargs:
-#    near: 0.3
-#    far: 1.7
-#    near: 0.5
-#    far: 1.5
-    near: 0.88
-    far: 1.12
-    N_samples: 12
-    N_importance: 12
-    raw_noise_std: 0. # 1.0
-    eps: 1.e-10
-    h_stddev: 0.3
-    v_stddev: 0.155
-    sample_dist: 'gaussian'
-
-
-_build_generator:
-  G_cfg: *G_cfg_3D2D
-  G_kwargs: *G_kwargs
-  cam_cfg:
-    initial_fov: 12
-    normalize_rays_d: true
+
+
+nerf_cfg: &nerf_cfg
+  shape_net_cfg:
+    input_dim: 3
+    hidden_dim: 256
+    out_dim: 256
+    N_layers: 8
+  app_net_cfg:
+    input_dim: 256
+    hidden_dim: 256
+    out_dim: 256
+    N_layers: 1
+
+mapping_shape_app_cfg: &mapping_shape_app_cfg
+  base_cfg:
+    z_dim: 256
+    hidden_dim: 256
+    N_layers: 3
+
+inr_cfg: &inr_cfg
+#  input_dim: 128
+  hidden_dim: 512
+  out_dim: 3
+  style_dim: 512
+  num_blocks: 9
+
+mapping_inr_cfg: &mapping_inr_cfg
+  z_dim: 512
+  hidden_dim: 512
+  base_layers: 8
+  head_layers: 0
+  add_norm: true
+  norm_out: true
+
+
+G_cfg_3D2D: &G_cfg_3D2D
+  register_modules:
+  - tl2_lib.tl2.proj.pytorch.examples.cips3d.pigan_gen_celeba
+  name: tl2_lib.tl2.proj.pytorch.examples.cips3d.pigan_gen_celeba.Generator_Diffcam
+  nerf_cfg: *nerf_cfg
+  mapping_shape_app_cfg: *mapping_shape_app_cfg
+  inr_cfg: *inr_cfg
+  mapping_inr_cfg: *mapping_inr_cfg
+  shape_block_end_index: 8
+  app_block_end_index: 1
+  inr_block_end_index: 9
+  inr_detach: true
+
+
+G_kwargs: &G_kwargs
+  psi: 1.
+  nerf_kwargs:
+#    near: 0.3
+#    far: 1.7
+#    near: 0.5
+#    far: 1.5
+    near: 0.88
+    far: 1.12
+    N_samples: 12
+    N_importance: 12
+    raw_noise_std: 0. # 1.0
+    eps: 1.e-10
+    h_stddev: 0.3
+    v_stddev: 0.155
+    sample_dist: 'gaussian'
+
+
+_build_generator:
+  G_cfg: *G_cfg_3D2D
+  G_kwargs: *G_kwargs
+  cam_cfg:
+    initial_fov: 12
+    normalize_rays_d: true
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/cips3d/train_v6.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/cips3d/train_v6.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,743 +1,743 @@
-from PIL import Image
-import collections
-import shutil
-import traceback
-from functools import partial
-import pprint
-import logging
-import argparse
-import os
-import numpy as np
-import math
-from tqdm import tqdm
-import copy
-from einops import repeat, rearrange
-
-import torch
-import torch.distributed as dist
-import torch.multiprocessing as mp
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.nn.parallel import DistributedDataParallel as DDP
-from torchvision.utils import save_image, make_grid
-
-from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
-from tl2.modelarts import modelarts_utils, moxing_utils
-from tl2.proj.fvcore import build_model
-from tl2.proj.logger.textlogger import summary_dict2txtfig, summary_defaultdict2txtfig, global_textlogger
-from tl2 import tl2_utils
-from tl2.proj.pytorch import torch_utils
-from tl2.proj.argparser import argparser_utils
-from tl2.proj.pytorch.examples.dataset_stylegan3.dataset import get_training_dataloader, to_norm_tensor
-from tl2.proj.pytorch.ddp import ddp_utils
-from tl2.proj.pytorch.examples.nerf import cam_params_pigan
-
-from exp.dev.nerf_inr import curriculums
-from exp.pigan import datasets
-from exp.comm import comm_model_utils
-from exp.cips3d.scripts.setup_evaluation import setup_evaluation
-from exp.cips3d_inversion.scripts.gen_images import gen_images
-from exp.cips3d.scripts.eval_fid import eval_fid
-
-def setup_ddp(rank, world_size, port):
-  os.environ['MASTER_ADDR'] = 'localhost'
-  os.environ['MASTER_PORT'] = port
-
-  # initialize the process group
-  # dist.init_process_group("gloo", rank=rank, world_size=world_size)
-  dist.init_process_group("nccl", rank=rank, world_size=world_size)
-  torch.cuda.set_device(rank)
-  pass
-
-
-def cleanup():
-  dist.destroy_process_group()
-
-
-def saved_models(model_dict,
-                 info_msg,
-                 G,
-                 G_ema,
-                 G_kwargs,
-                 cam_param,
-                 fixed_z,
-                 img_size,
-                 device,
-                 saved_dir=None):
-  if saved_dir is None:
-    ckpt_max2keep = tl2_utils.MaxToKeep.get_named_max_to_keep(name='ckpt', use_circle_number=True)
-    saved_dir = ckpt_max2keep.step_and_ret_circle_dir(global_cfg.tl_ckptdir)
-  os.makedirs(saved_dir, exist_ok=True)
-
-  global_cfg.dump_to_file_with_command(f"{saved_dir}/config_command.yaml", global_cfg.tl_command)
-
-  torch_utils.save_models(save_dir=saved_dir, model_dict=model_dict)
-  tl2_utils.write_info_msg(saved_dir, info_msg)
-
-  save_images(saved_dir=saved_dir,
-              G=G,
-              G_ema=G_ema,
-              cam_param=cam_param,
-              G_kwargs=G_kwargs,
-              fixed_z=fixed_z,
-              img_size=img_size,
-              device=device)
-
-  torch.cuda.empty_cache()
-
-  pass
-
-
-@torch.no_grad()
-def _save_images(G,
-                 fixed_z,
-                 rays_o,
-                 rays_d,
-                 G_kwargs,
-                 saved_path,
-                 bs):
-
-  Gz, ret_imgs = G(zs=fixed_z,
-                   rays_o=rays_o,
-                   rays_d=rays_d,
-                   forward_points=128 ** 2,
-                   return_aux_img=True,
-                   **G_kwargs)
-  Gz_aux = ret_imgs['aux_img']
-  Gz = torch.cat([Gz, Gz_aux], dim=0)
-  save_image(Gz, saved_path, nrow=int(math.sqrt(bs)), normalize=True, scale_each=True)
-  pass
-
-@torch.no_grad()
-def save_images(saved_dir,
-                G,
-                G_ema,
-                cam_param,
-                G_kwargs,
-                fixed_z,
-                img_size,
-                device,
-                use_amp_G=False,
-                ):
-  G.eval()
-  G_ema.eval()
-
-  bs = len(list(fixed_z.values())[0])
-  G_kwargs = copy.deepcopy(G_kwargs)
-  H = W = img_size
-
-  # rays_o, rays_d = cam_param.get_rays_of_pose_avg(H=H, W=W, bs=bs)
-
-  with torch.cuda.amp.autocast(use_amp_G):
-    copied_metadata = copy.deepcopy(G_kwargs)
-    copied_metadata['nerf_kwargs']['h_stddev'] = 0.
-    copied_metadata['nerf_kwargs']['v_stddev'] = 0.
-
-    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-      device=device, bs=bs, intr=None, **copied_metadata['nerf_kwargs'])
-
-    _save_images(G=G, fixed_z=fixed_z, rays_o=rays_o, rays_d=rays_d,
-                 G_kwargs=copied_metadata,
-                 saved_path=f"{saved_dir}/0Gz.jpg", bs=bs)
-
-    _save_images(G=G_ema, fixed_z=fixed_z, rays_o=rays_o, rays_d=rays_d,
-                 G_kwargs=copied_metadata,
-                 saved_path=f"{saved_dir}/0Gz_ema.jpg", bs=bs)
-
-    copied_metadata['psi'] = 0.7
-    _save_images(G=G_ema, fixed_z=fixed_z, rays_o=rays_o, rays_d=rays_d,
-                 G_kwargs=copied_metadata,
-                 saved_path=f"{saved_dir}/0G_trunc_ema.jpg", bs=bs)
-
-  with torch.cuda.amp.autocast(use_amp_G):
-    copied_metadata = copy.deepcopy(G_kwargs)
-    copied_metadata['nerf_kwargs']['h_stddev'] = 0.
-    copied_metadata['nerf_kwargs']['v_stddev'] = 0.
-    copied_metadata['nerf_kwargs']['h_mean'] = math.pi * 0.5 + 0.5
-
-    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-      device=device, bs=bs, intr=None, **copied_metadata['nerf_kwargs'])
-
-    _save_images(G=G, fixed_z=fixed_z, rays_o=rays_o, rays_d=rays_d,
-                 G_kwargs=copied_metadata,
-                 saved_path=f"{saved_dir}/0Gz_tilted.jpg", bs=bs)
-
-    _save_images(G=G_ema, fixed_z=fixed_z, rays_o=rays_o, rays_d=rays_d,
-                 G_kwargs=copied_metadata,
-                 saved_path=f"{saved_dir}/0Gz_tilted_ema.jpg", bs=bs)
-
-  # Monitor mirror symmetry
-  bs = min(20, bs)
-  sub_fixed_z = {}
-  for name, z_ in fixed_z.items():
-    if z_ is not None:
-      sub_fixed_z[name] = z_[:bs]
-    else:
-      sub_fixed_z[name] = z_
-  fixed_z = sub_fixed_z
-
-  with torch.cuda.amp.autocast(use_amp_G):
-    copied_metadata = copy.deepcopy(G_kwargs)
-    copied_metadata['nerf_kwargs']['h_stddev'] = 0.
-    copied_metadata['nerf_kwargs']['v_stddev'] = 0.
-
-    copied_metadata['nerf_kwargs']['h_mean'] = math.pi * 0.5 - 0.15
-    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-      device=device, bs=bs, intr=None, **copied_metadata['nerf_kwargs'])
-
-    Gema_flip1, ret_imgs = G_ema(zs=fixed_z,
-                                 rays_o=rays_o,
-                                 rays_d=rays_d,
-                                 forward_points=128 ** 2,
-                                 return_aux_img=True,
-                                 **copied_metadata)
-    Gema_flip1_aux = ret_imgs['aux_img']
-    Gema_flip1 = torch.cat([Gema_flip1, Gema_flip1_aux], dim=0)
-
-    # resampling
-    copied_metadata['nerf_kwargs']['h_mean'] = math.pi * 0.5 + 0.15
-    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-      device=device, bs=bs, intr=None, **copied_metadata['nerf_kwargs'])
-
-
-    Gema_flip2, ret_imgs = G_ema(zs=fixed_z,
-                                 rays_o=rays_o,
-                                 rays_d=rays_d,
-                                 forward_points=128 ** 2,
-                                 return_aux_img=True,
-                                 **copied_metadata)
-    Gema_flip2_aux = ret_imgs['aux_img']
-    Gema_flip2 = torch.cat([Gema_flip2, Gema_flip2_aux], dim=0)
-
-    Gema_flip = torch.cat([Gema_flip1, Gema_flip2])
-    save_image(Gema_flip, f"{saved_dir}/0G_flip_ema.jpg", nrow=bs//2, normalize=True, scale_each=True)
-
-  pass
-
-
-def get_curriculum(curriculum_name):
-  curriculum = getattr(curriculums, curriculum_name)
-
-  # update curriculum_step
-  for curriculum_step in curriculum.keys():
-    if type(curriculum_step) == int:
-      # stage
-      curriculum_step_str = str(curriculum_step)
-      if curriculum_step_str in global_cfg.curriculum:
-        curriculum[curriculum_step].update(global_cfg.curriculum.get(curriculum_step_str).to_dict())
-    else:
-      # update attrs
-      if curriculum_step in global_cfg.curriculum:
-        curriculum[curriculum_step] = global_cfg.curriculum[curriculum_step]
-  for new_attr, value in global_cfg.curriculum.get('new_attrs', {}).items():
-    assert new_attr not in curriculum
-    curriculum[new_attr] = value
-  return curriculum
-
-
-def build_optimizer(generator_ddp,
-                    discriminator_ddp,
-                    cam_param_ddp):
-
-  optimizer_G = torch.optim.Adam(
-    params=[{'params': generator_ddp.parameters(),
-             'initial_lr': global_cfg.gen_lr}],
-    lr=global_cfg.gen_lr,
-    betas=global_cfg.betas,
-    weight_decay=0)
-  optimizer_D = torch.optim.Adam(
-    params=[{'params': discriminator_ddp.parameters(),
-             'initial_lr': global_cfg.disc_lr}],
-    lr=global_cfg.disc_lr,
-    betas=global_cfg.betas,
-    weight_decay=0)
-
-  optimizer_cam = torch.optim.Adam(
-    params=[{'params': cam_param_ddp.parameters(),
-             'initial_lr': global_cfg.cam_lr}],
-    lr=global_cfg.cam_lr,
-    betas=global_cfg.betas,
-    weight_decay=0)
-
-  # if global_cfg.tl_resume and global_cfg.load_optimizers:
-  #   model_dict = {
-  #     'optimizer_G': optimizer_G,
-  #     'optimizer_D': optimizer_D,
-  #     'scaler_G': scaler_G,
-  #     'scaler_D': scaler_D,
-  #   }
-  #   torch_utils.load_models(save_dir=resume_dir, model_dict=model_dict, strict=False, rank=rank)
-
-  # After load optimizer.
-  # optimizer_G.param_groups[0]['initial_lr'] = global_cfg.gen_lr
-  # optimizer_G.param_groups[0]['lr'] = global_cfg.gen_lr
-  # optimizer_G.param_groups[0]['betas'] = metadata['betas']
-  # optimizer_D.param_groups[0]['initial_lr'] = global_cfg.disc_lr
-  # optimizer_D.param_groups[0]['lr'] = global_cfg.disc_lr
-  # optimizer_D.param_groups[0]['betas'] = metadata['betas']
-
-  return optimizer_G, optimizer_D, optimizer_cam
-
-
-def train(rank,
-          world_size,
-          opt):
-  setup_ddp(rank, world_size, opt.port)
-
-  update_parser_defaults_from_yaml(parser=None, is_main_process=(rank == 0))
-  if rank == 0:
-    moxing_utils.setup_tl_outdir_obs(global_cfg)
-    moxing_utils.modelarts_sync_results_dir(global_cfg, join=True)
-
-  logger = logging.getLogger('tl')
-  device = torch.device(rank)
-  torch_utils.init_seeds(seed=global_cfg.seed, rank=rank)
-
-  # curriculum = get_curriculum(curriculum_name=opt.curriculum)
-
-  scaler_G = torch.cuda.amp.GradScaler(enabled=global_cfg.use_amp_G)
-  scaler_D = torch.cuda.amp.GradScaler(enabled=global_cfg.use_amp_D)
-
-  """dataset """
-  moxing_utils.copy_data(rank=rank, global_cfg=global_cfg, **global_cfg.obs_training_dataset)
-  dataset = build_model(global_cfg.data_cfg, kwargs_priority=True, resize_resolution=global_cfg.img_size)
-  # imgs, label, idx = dataset[0] # [0, 255]
-  data_loader = get_training_dataloader(dataset=dataset, rank=rank, num_gpus=world_size,
-                                        batch_size=global_cfg.batch_size * world_size,
-                                        num_workers=global_cfg.num_workers,
-                                        shuffle=True, sampler_seed=0)
-  data_loader_iter = iter(data_loader)
-  # batch_data = next(data_loader_iter)
-
-  H = W = global_cfg.img_size
-  cam_cfg = global_cfg.get('cam_cfg', {'freeze_intr': True,
-                                      'normalize_rays_d': True})
-  cam_param = cam_params_pigan.CamParams.from_config(num_imgs=1, H0=H, W0=W, **cam_cfg).to(device)
-  cam_param_ddp = DDP(cam_param, device_ids=[rank], find_unused_parameters=True, broadcast_buffers=False)
-
-  generator = build_model(cfg=global_cfg.G_cfg).to(device)
-  discriminator = build_model(cfg=global_cfg.D_cfg, kwargs_priority=True, diffaug=global_cfg.diffaug).to(device)
-  G_ema = copy.deepcopy(generator)
-  ema_start_itr = global_cfg.get('ema_start_itr', 1000)
-  ema_model = comm_model_utils.EMA(source=generator, target=G_ema, decay=0.999, start_itr=ema_start_itr)
-
-  # ddp
-  generator_ddp = DDP(generator, device_ids=[rank], find_unused_parameters=True, broadcast_buffers=False)
-  discriminator_ddp = DDP(discriminator, device_ids=[rank], find_unused_parameters=True, broadcast_buffers=False)
-  generator = generator_ddp.module
-  generator.set_device(device)
-
-  optimizer_G, optimizer_D, optimizer_cam = build_optimizer(generator_ddp=generator_ddp,
-                                                            discriminator_ddp=discriminator_ddp,
-                                                            cam_param_ddp=cam_param_ddp)
-
-  state_dict = {
-    'cur_fid': np.inf,
-    'best_fid': np.inf,
-    'worst_fid': 0,
-    'step': 0,
-  }
-
-  model_dict = {
-    'cam_param': cam_param_ddp.module,
-    'generator': generator_ddp.module,
-    'G_ema': G_ema,
-    'discriminator': discriminator_ddp.module,
-    # 'optimizer_G': optimizer_G,
-    # 'optimizer_D': optimizer_D,
-    'state_dict': state_dict,
-  }
-
-  if global_cfg.tl_resume or global_cfg.load_finetune:
-    if global_cfg.tl_resume:
-      resume_dir = f"{global_cfg.tl_resumedir}/ckptdir/resume"
-      torch_utils.load_models(save_dir=resume_dir, model_dict=model_dict, strict=False, rank=rank)
-    elif global_cfg.load_finetune:
-      moxing_utils.copy_data(rank=rank, global_cfg=global_cfg,
-                             datapath_obs=global_cfg.finetune_dir, datapath=global_cfg.finetune_dir)
-      finetune_model_dict = {
-        'cam_param': cam_param_ddp.module,
-        'generator': generator_ddp.module,
-        'G_ema': G_ema,
-        'discriminator': discriminator_ddp.module,
-      }
-      torch_utils.load_models(save_dir=global_cfg.finetune_dir, model_dict=finetune_model_dict,
-                              strict=False, rank=rank)
-    else:
-      assert 0
-
-    if global_cfg.get('load_nerf_ema', False):
-      generator.load_nerf_ema(G_ema)
-
-    if global_cfg.load_G_ema:
-      ema_model.update_target_dict(G_ema.state_dict())
-    else:
-      ema_model.update_target_dict(generator.state_dict())
-
-    if global_cfg.reset_best_fid:
-      state_dict['best_fid'] = np.inf
-    logger.info(pprint.pformat(state_dict))
-
-  if global_cfg.tl_debug:
-    global_cfg.fixed_z_bs = 4
-  fixed_z = generator.get_zs(global_cfg.fixed_z_bs)
-  dummy_tensor = torch.tensor([0.], device=device)
-  # ----------
-  #  Training
-  # ----------
-
-  start_itr = state_dict['step']
-  pbar = tl2_utils.TL_tqdm(total=global_cfg.total_iters, start=start_itr)
-
-
-  summary_ddict = collections.defaultdict(dict)
-
-  nerf_noise_disable = global_cfg.get('nerf_noise_disable', False)
-
-  G_kwargs = global_cfg.G_kwargs.to_dict()
-
-  for step in range(start_itr, global_cfg.total_iters):
-    pbar.update()
-    summary_ddict.clear()
-
-    # for i, (imgs, _) in enumerate(dataloader):
-      # real_imgs = imgs.to(device, non_blocking=True)
-    imgs, _, imgs_idx = next(data_loader_iter)
-    real_imgs = to_norm_tensor(imgs, device=device)
-    imgs_idx = imgs_idx.to(device)
-
-    generator_ddp.train()
-    discriminator_ddp.train()
-
-    if nerf_noise_disable:
-      nerf_noise = 0.
-    else:
-      nerf_noise = max(0, 1. - state_dict['step'] / 5000.)
-    G_kwargs['nerf_kwargs']['raw_noise_std'] = nerf_noise
-
-    if global_cfg.get('warmup_D', False):
-      alpha = min(1, step / global_cfg.fade_steps)
-    else:
-      alpha = 1.
-
-    '''TRAIN DISCRIMINATOR'''
-    torch_utils.requires_grad(generator_ddp, False)
-    torch_utils.requires_grad(discriminator_ddp, True)
-
-    aux_reg = global_cfg.train_aux_img and (step % global_cfg.update_aux_every == 0)
-
-    with torch.cuda.amp.autocast(global_cfg.use_amp_D):
-      # Generate images for discriminator training
-      with torch.no_grad():
-        # z = z_sampler((real_imgs.shape[0], metadata['latent_dim']),
-        #               device=device,
-        #               dist=metadata['z_dist'])
-        zs_list = generator.get_zs(real_imgs.shape[0], batch_split=global_cfg.batch_split)
-        if global_cfg.batch_split == 1:
-          zs_list = [zs_list]
-        gen_imgs = []
-        gen_imgs_aux = []
-        if global_cfg.img_size >= 256 and global_cfg.forward_points is not None:
-          forward_points = global_cfg.forward_points ** 2
-        else:
-          forward_points = None
-
-        imgs_idx_list = imgs_idx.chunk(global_cfg.batch_split)
-        for subset_z, sub_imgs_idx in zip(zs_list, imgs_idx_list):
-          # R, t, fx, fy = cam_param(sub_imgs_idx)
-          # rays_o, rays_d, _ = generator.get_rays_axis_angle(R=R, t=t, fx=fx, fy=fy, H=H, W=W, N_rays=-1)
-          intr = cam_param_ddp(mode='get_intrinsic')
-          rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-            device=device, bs=len(sub_imgs_idx), intr=intr, **G_kwargs['nerf_kwargs'])
-          g_imgs, ret_imgs = generator_ddp(zs=subset_z,
-                                           rays_o=rays_o,
-                                           rays_d=rays_d,
-                                           return_aux_img=aux_reg,
-                                           forward_points=forward_points,
-                                           grad_points=None,
-                                           **G_kwargs)
-
-          # find a bug when batch_split > 1 :  g_imgs: [Gz, Gz_aux]
-          gen_imgs.append(g_imgs)
-          if aux_reg:
-            g_imgs_aux = ret_imgs['aux_img']
-            gen_imgs_aux.append(g_imgs_aux)
-
-        gen_imgs = torch.cat(gen_imgs + gen_imgs_aux, axis=0)
-      # end torch.no_grad
-      if aux_reg:
-        real_imgs = torch.cat([real_imgs, real_imgs], dim=0)
-      real_imgs.requires_grad_()
-      r_preds, _, _ = discriminator_ddp(real_imgs, alpha=alpha, use_aux_disc=aux_reg, summary_ddict=summary_ddict)
-
-    d_regularize = step % global_cfg.d_reg_every == 0
-
-    if global_cfg.r1_lambda > 0 and d_regularize:
-      # Gradient penalty
-      grad_real = torch.autograd.grad(
-        outputs=scaler_D.scale(r_preds.sum()),
-        inputs=real_imgs,
-        create_graph=True)
-      inv_scale = 1. / scaler_D.get_scale()
-      grad_real = [p * inv_scale for p in grad_real][0]
-
-    with torch.cuda.amp.autocast(global_cfg.use_amp_D):
-      if global_cfg.r1_lambda > 0 and d_regularize:
-        # grad_penalty = (grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2).mean()
-        # grad_penalty = 0.5 * global_cfg.r1_lambda * global_cfg.d_reg_every * grad_penalty + 0 * r_preds[0]
-        grad_penalty = grad_real.flatten(start_dim=1).square().sum(dim=1, keepdim=True)
-        grad_penalty = 0.5 * global_cfg.r1_lambda * grad_penalty * global_cfg.d_reg_every  + 0. * r_preds
-      else:
-        grad_penalty = dummy_tensor
-
-      g_preds, _, _ = discriminator_ddp(gen_imgs, alpha=alpha, use_aux_disc=aux_reg)
-
-      d_loss = (torch.nn.functional.softplus(g_preds) +
-                torch.nn.functional.softplus(-r_preds) +
-                grad_penalty).mean()
-      # d_loss = torch.nn.functional.softplus(g_preds).mean() + \
-      #          torch.nn.functional.softplus(-r_preds).mean() + \
-      #          grad_penalty
-
-      if rank == 0:
-        with torch.no_grad():
-          summary_ddict['D_logits']['D_logits_real'] = r_preds.mean().item()
-          summary_ddict['D_logits']['D_logits_fake'] = g_preds.mean().item()
-          summary_ddict['grad_penalty']['grad_penalty'] = grad_penalty.mean().item()
-
-    optimizer_D.zero_grad()
-    scaler_D.scale(d_loss).backward()
-    scaler_D.unscale_(optimizer_D)
-    try:
-      D_total_norm = torch.nn.utils.clip_grad_norm_(discriminator_ddp.parameters(),
-                                                    global_cfg.grad_clip,
-                                                    # error_if_nonfinite=True, # torch >= 1.9
-                                                    )
-      summary_ddict['D_total_norm']['D_total_norm'] = D_total_norm.item()
-    except:
-      summary_ddict['D_total_norm']['D_total_norm'] = np.nan
-      logger.info(traceback.format_exc())
-      saved_models(model_dict=model_dict,
-                   info_msg=f"step: {state_dict['step']}",
-                   G=generator, G_ema=G_ema, G_kwargs=G_kwargs,
-                   fixed_z=fixed_z, img_size=global_cfg.img_size,
-                   saved_dir=f"{global_cfg.tl_ckptdir}/D_crupted")
-      # exit(0)
-      optimizer_D.zero_grad()
-
-    scaler_D.step(optimizer_D)
-    scaler_D.update()
-
-    ''' TRAIN GENERATOR '''
-    torch_utils.requires_grad(generator_ddp, True)
-    torch_utils.requires_grad(discriminator_ddp, False)
-
-    # z = z_sampler((imgs.shape[0], metadata['latent_dim']), device=device, dist=metadata['z_dist'])
-    zs_list = generator.get_zs(imgs.shape[0], batch_split=global_cfg.batch_split)
-    if global_cfg.batch_split == 1:
-      zs_list = [zs_list]
-
-    if global_cfg.grad_points is not None:
-      grad_points = global_cfg.grad_points ** 2
-    else:
-      grad_points = None
-
-    imgs_idx_list = imgs_idx.chunk(global_cfg.batch_split)
-    for subset_z, sub_imgs_idx in zip(zs_list, imgs_idx_list):
-      with torch.cuda.amp.autocast(global_cfg.use_amp_G):
-        # R, t, fx, fy = cam_param(sub_imgs_idx)
-        # rays_o, rays_d, _ = generator.get_rays_axis_angle(R=R, t=t, fx=fx, fy=fy, H=H, W=W, N_rays=-1)
-        intr = cam_param_ddp(mode='get_intrinsic')
-        rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-          device=device, bs=len(sub_imgs_idx), intr=intr, **G_kwargs['nerf_kwargs'])
-        gen_imgs, ret_imgs = generator_ddp(zs=subset_z,
-                                           rays_o=rays_o,
-                                           rays_d=rays_d,
-                                           return_aux_img=aux_reg,
-                                           grad_points=grad_points,
-                                           forward_points=None,
-                                           **G_kwargs)
-        if aux_reg:
-          g_imgs_aux = ret_imgs['aux_img']
-          gen_imgs = torch.cat([gen_imgs, g_imgs_aux], dim=0)
-
-        with torch.cuda.amp.autocast(global_cfg.use_amp_D):
-          g_preds, _, _ = discriminator_ddp(gen_imgs.to(torch.float32), alpha=alpha, use_aux_disc=aux_reg)
-        g_loss = torch.nn.functional.softplus(-g_preds).mean()
-      scaler_G.scale(g_loss).backward()
-    # end accumulate gradients
-    scaler_G.unscale_(optimizer_G)
-    scaler_G.unscale_(optimizer_cam)
-    try:
-      if hasattr(generator, 'get_subnet_grad_norm'):
-        grad_dict = generator.get_subnet_grad_norm()
-        summary_ddict['G_total_norm'].update(grad_dict)
-        G_total_norm = torch.nn.utils.clip_grad_norm_(generator_ddp.parameters(),
-                                                      global_cfg.grad_clip)
-        # G_total_norm = torch.nn.utils.clip_grad_norm_(generator.nerf_net.parameters(),
-        #                                               global_cfg.grad_clip)
-        # G_total_norm = torch.nn.utils.clip_grad_norm_(generator.mapping_shape_app.parameters(),
-        #                                               global_cfg.grad_clip)
-
-      else:
-        G_total_norm = torch.nn.utils.clip_grad_norm_(generator_ddp.parameters(),
-                                                      global_cfg.grad_clip,
-                                                      # metadata.get('grad_clip', 0.3),
-                                                      # error_if_nonfinite=True, # torch >= 1.9
-                                                      )
-        summary_ddict['G_total_norm']['G_total_norm'] = G_total_norm.item()
-    except:
-      summary_ddict['G_total_norm']['G_total_norm'] = np.nan
-      logger.info(traceback.format_exc())
-      saved_models(model_dict=model_dict,
-                   info_msg=f"step: {state_dict['step']}",
-                   G=generator, G_ema=G_ema, G_kwargs=G_kwargs,
-                   fixed_z=fixed_z, img_size=global_cfg.img_size,
-                   saved_dir=f"{global_cfg.tl_ckptdir}/G_crupted")
-      # exit(0)
-      optimizer_G.zero_grad()
-      optimizer_cam.zero_grad()
-    scaler_G.step(optimizer_G)
-    scaler_G.step(optimizer_cam)
-    scaler_G.update()
-
-    optimizer_G.zero_grad()
-    optimizer_cam.zero_grad()
-
-    # update ema
-    ema_model.update(itr=state_dict['step'], source_dict=generator_ddp.module.state_dict())
-
-    if (rank == 0 and (step + 1) % global_cfg.log_every == 0) or global_cfg.tl_debug:
-      summary_ddict['lr']['G_lr'] = torch_utils.get_optimizer_lr(optimizer_G)
-      summary_ddict['lr']['D_lr'] = torch_utils.get_optimizer_lr(optimizer_D)
-      summary_ddict['lr']['cam_lr'] = torch_utils.get_optimizer_lr(optimizer_cam)
-      summary_ddict['img_size']['img_size'] = global_cfg.img_size
-      summary_ddict['batch_size']['batch_size'] = global_cfg.batch_size * world_size
-      summary_ddict['grad_points']['grad_points'] = grad_points if grad_points is not None else global_cfg.img_size ** 2
-      summary_ddict['scaler']['scaler_G'] = scaler_G.get_scale()
-      summary_ddict['scaler']['scaler_D'] = scaler_D.get_scale()
-      summary_ddict['r1_lambda']['r1_lambda'] = global_cfg.r1_lambda
-      summary_ddict['grad_clip']['grad_clip'] = global_cfg.grad_clip
-      summary_ddict['nerf_noise']['nerf_noise'] = nerf_noise
-      summary_ddict['train_aux_img']['train_aux_img'] = int(global_cfg.train_aux_img)
-      summary_ddict['alpha']['alpha'] = alpha
-      summary_ddict['diffaug']['diffaug'] = int(global_cfg.diffaug)
-      summary_ddict['block_idx']['shape'] = global_cfg.G_cfg.shape_block_end_index
-      summary_ddict['block_idx']['app'] = global_cfg.G_cfg.app_block_end_index
-      summary_ddict['block_idx']['inr'] = global_cfg.G_cfg.inr_block_end_index
-      with torch.no_grad():
-        fx, fy = cam_param.get_focal()
-        summary_ddict['intr']['fx'] = fx.item()
-        summary_ddict['intr']['fy'] = fy.item()
-
-      if step > 1000:
-        summary_defaultdict2txtfig(summary_ddict, prefix='train', step=state_dict['step'], textlogger=global_textlogger)
-      summary_str = tl2_utils.get_print_dict_str(summary_ddict, outdir=global_cfg.tl_outdir,
-                                                 suffix_str=pbar.get_string())
-      print(summary_str)
-
-
-    state_dict['step'] += 1
-    if step == 0 or (step + 1) % global_cfg.eval_every == 0 or global_cfg.tl_debug:
-      # output real images
-      setup_evaluation(rank=rank,
-                       world_size=world_size,
-                       data_cfg=global_cfg.data_cfg,
-                       real_dir=f"{global_cfg.tl_outdir}/exp/fid/real",
-                       img_size=global_cfg.img_size,
-                       num_imgs=global_cfg.num_images_real_eval,
-                       del_fid_real_images=global_cfg.del_fid_real_images,
-                       shuffle=True)
-      global_cfg.del_fid_real_images = False
-      ddp_utils.d2_synchronize()
-
-      # output fake images
-      gen_images(rank=rank,
-                 world_size=world_size,
-                 generator=G_ema,
-                 cam_param=cam_param,
-                 G_kwargs=global_cfg.G_kwargs.to_dict(),
-                 fake_dir=f"{global_cfg.tl_outdir}/exp/fid/fake",
-                 num_imgs=global_cfg.num_images_gen_eval,
-                 img_size=global_cfg.img_size,
-                 batch_size=global_cfg.eval_batch_size,
-                 device=device)
-      ddp_utils.d2_synchronize()
-
-      moxing_utils.copy_data(rank=rank, global_cfg=global_cfg, **global_cfg.obs_inception_v3)
-      global_cfg.obs_inception_v3.disable = True
-      if rank == 0:
-        metric_dict = eval_fid(real_dir=f"{global_cfg.tl_outdir}/exp/fid/real",
-                               fake_dir=f"{global_cfg.tl_outdir}/exp/fid/fake")
-        logger.info(f"\nstep: {state_dict['step']}, {pprint.pformat(metric_dict)}\n")
-        summary_dict2txtfig(metric_dict, prefix='eval', step=state_dict['step'], textlogger=global_textlogger)
-        state_dict['cur_fid'] = metric_dict['FID']
-
-        if state_dict['best_fid'] > metric_dict['FID']:
-          state_dict['best_fid'] = metric_dict['FID']
-          saved_models(model_dict=model_dict,
-                       info_msg=f"step: {state_dict['step']}\n"
-                                f"cur_fid: {state_dict['cur_fid']}\n"
-                                f"best_fid: {state_dict['best_fid']}",
-                       G=generator, G_ema=G_ema, G_kwargs=global_cfg.G_kwargs.to_dict(),
-                       cam_param=cam_param,
-                       fixed_z=fixed_z, img_size=global_cfg.img_size,
-                       saved_dir=f"{global_cfg.tl_ckptdir}/best_fid",
-                       device=device)
-
-        info_msg = f"step: {state_dict['step']}\n" \
-                   f"cur_fid: {state_dict['cur_fid']}\n" \
-                   f"best_fid: {state_dict['best_fid']}"
-        # backup
-        saved_models(model_dict=model_dict,
-                     info_msg=info_msg,
-                     G=generator, G_ema=G_ema, G_kwargs=global_cfg.G_kwargs.to_dict(),
-                     cam_param=cam_param,
-                     fixed_z=fixed_z, img_size=global_cfg.img_size,
-                     device=device)
-        # resume
-        saved_models(model_dict=model_dict,
-                     info_msg=info_msg,
-                     G=generator, G_ema=G_ema, G_kwargs=global_cfg.G_kwargs.to_dict(),
-                     cam_param=cam_param,
-                     fixed_z=fixed_z, img_size=global_cfg.img_size,
-                     saved_dir=f"{global_cfg.tl_ckptdir}/resume",
-                     device=device)
-
-        moxing_utils.modelarts_sync_results_dir(cfg=global_cfg, join=False)
-        # end rank == 0
-      ddp_utils.d2_synchronize()
-
-  cleanup()
-  pass
-
-
-if __name__ == '__main__':
-  parser = argparse.ArgumentParser()
-  parser.add_argument("--n_epochs", type=int, default=60000, help="number of epochs of training")
-  parser.add_argument("--sample_interval", type=int, default=200, help="interval between image sampling")
-  parser.add_argument('--output_dir', type=str, default='results/')
-  parser.add_argument('--load_dir', type=str, default='')
-  parser.add_argument('--curriculum', type=str, default='CelebA')
-  parser.add_argument('--eval_freq', type=int, default=5000)
-  parser.add_argument('--port', type=str, default='12355')
-  parser.add_argument('--set_step', type=int, default=None)
-  parser.add_argument('--model_save_interval', type=int, default=5000)
-
-  argparser_utils.add_argument_bool(parser, 'modelarts', default=False)
-
-  update_parser_defaults_from_yaml(parser)
-
-  opt, _ = parser.parse_known_args()
-  argparser_utils.print_args(opt)
-
-  moxing_utils.setup_tl_outdir_obs(global_cfg, unzip_code=False)
-  moxing_utils.modelarts_sync_results_dir(global_cfg, join=True)
-
-  num_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-  if num_gpus > 1:
-    mp.spawn(train, args=(num_gpus, opt), nprocs=num_gpus, join=True)
-  else:
-    train(rank=0, world_size=num_gpus, opt=opt)
-
-  moxing_utils.modelarts_sync_results_dir(global_cfg, join=True)
+from PIL import Image
+import collections
+import shutil
+import traceback
+from functools import partial
+import pprint
+import logging
+import argparse
+import os
+import numpy as np
+import math
+from tqdm import tqdm
+import copy
+from einops import repeat, rearrange
+
+import torch
+import torch.distributed as dist
+import torch.multiprocessing as mp
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torchvision.utils import save_image, make_grid
+
+from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
+from tl2.modelarts import modelarts_utils, moxing_utils
+from tl2.proj.fvcore import build_model
+from tl2.proj.logger.textlogger import summary_dict2txtfig, summary_defaultdict2txtfig, global_textlogger
+from tl2 import tl2_utils
+from tl2.proj.pytorch import torch_utils
+from tl2.proj.argparser import argparser_utils
+from tl2.proj.pytorch.examples.dataset_stylegan3.dataset import get_training_dataloader, to_norm_tensor
+from tl2.proj.pytorch.ddp import ddp_utils
+from tl2.proj.pytorch.examples.nerf import cam_params_pigan
+
+from exp.dev.nerf_inr import curriculums
+from exp.pigan import datasets
+from exp.comm import comm_model_utils
+from exp.cips3d.scripts.setup_evaluation import setup_evaluation
+from exp.cips3d_inversion.scripts.gen_images import gen_images
+from exp.cips3d.scripts.eval_fid import eval_fid
+
+def setup_ddp(rank, world_size, port):
+  os.environ['MASTER_ADDR'] = 'localhost'
+  os.environ['MASTER_PORT'] = port
+
+  # initialize the process group
+  # dist.init_process_group("gloo", rank=rank, world_size=world_size)
+  dist.init_process_group("nccl", rank=rank, world_size=world_size)
+  torch.cuda.set_device(rank)
+  pass
+
+
+def cleanup():
+  dist.destroy_process_group()
+
+
+def saved_models(model_dict,
+                 info_msg,
+                 G,
+                 G_ema,
+                 G_kwargs,
+                 cam_param,
+                 fixed_z,
+                 img_size,
+                 device,
+                 saved_dir=None):
+  if saved_dir is None:
+    ckpt_max2keep = tl2_utils.MaxToKeep.get_named_max_to_keep(name='ckpt', use_circle_number=True)
+    saved_dir = ckpt_max2keep.step_and_ret_circle_dir(global_cfg.tl_ckptdir)
+  os.makedirs(saved_dir, exist_ok=True)
+
+  global_cfg.dump_to_file_with_command(f"{saved_dir}/config_command.yaml", global_cfg.tl_command)
+
+  torch_utils.save_models(save_dir=saved_dir, model_dict=model_dict)
+  tl2_utils.write_info_msg(saved_dir, info_msg)
+
+  save_images(saved_dir=saved_dir,
+              G=G,
+              G_ema=G_ema,
+              cam_param=cam_param,
+              G_kwargs=G_kwargs,
+              fixed_z=fixed_z,
+              img_size=img_size,
+              device=device)
+
+  torch.cuda.empty_cache()
+
+  pass
+
+
+@torch.no_grad()
+def _save_images(G,
+                 fixed_z,
+                 rays_o,
+                 rays_d,
+                 G_kwargs,
+                 saved_path,
+                 bs):
+
+  Gz, ret_imgs = G(zs=fixed_z,
+                   rays_o=rays_o,
+                   rays_d=rays_d,
+                   forward_points=128 ** 2,
+                   return_aux_img=True,
+                   **G_kwargs)
+  Gz_aux = ret_imgs['aux_img']
+  Gz = torch.cat([Gz, Gz_aux], dim=0)
+  save_image(Gz, saved_path, nrow=int(math.sqrt(bs)), normalize=True, scale_each=True)
+  pass
+
+@torch.no_grad()
+def save_images(saved_dir,
+                G,
+                G_ema,
+                cam_param,
+                G_kwargs,
+                fixed_z,
+                img_size,
+                device,
+                use_amp_G=False,
+                ):
+  G.eval()
+  G_ema.eval()
+
+  bs = len(list(fixed_z.values())[0])
+  G_kwargs = copy.deepcopy(G_kwargs)
+  H = W = img_size
+
+  # rays_o, rays_d = cam_param.get_rays_of_pose_avg(H=H, W=W, bs=bs)
+
+  with torch.cuda.amp.autocast(use_amp_G):
+    copied_metadata = copy.deepcopy(G_kwargs)
+    copied_metadata['nerf_kwargs']['h_stddev'] = 0.
+    copied_metadata['nerf_kwargs']['v_stddev'] = 0.
+
+    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+      device=device, bs=bs, intr=None, **copied_metadata['nerf_kwargs'])
+
+    _save_images(G=G, fixed_z=fixed_z, rays_o=rays_o, rays_d=rays_d,
+                 G_kwargs=copied_metadata,
+                 saved_path=f"{saved_dir}/0Gz.jpg", bs=bs)
+
+    _save_images(G=G_ema, fixed_z=fixed_z, rays_o=rays_o, rays_d=rays_d,
+                 G_kwargs=copied_metadata,
+                 saved_path=f"{saved_dir}/0Gz_ema.jpg", bs=bs)
+
+    copied_metadata['psi'] = 0.7
+    _save_images(G=G_ema, fixed_z=fixed_z, rays_o=rays_o, rays_d=rays_d,
+                 G_kwargs=copied_metadata,
+                 saved_path=f"{saved_dir}/0G_trunc_ema.jpg", bs=bs)
+
+  with torch.cuda.amp.autocast(use_amp_G):
+    copied_metadata = copy.deepcopy(G_kwargs)
+    copied_metadata['nerf_kwargs']['h_stddev'] = 0.
+    copied_metadata['nerf_kwargs']['v_stddev'] = 0.
+    copied_metadata['nerf_kwargs']['h_mean'] = math.pi * 0.5 + 0.5
+
+    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+      device=device, bs=bs, intr=None, **copied_metadata['nerf_kwargs'])
+
+    _save_images(G=G, fixed_z=fixed_z, rays_o=rays_o, rays_d=rays_d,
+                 G_kwargs=copied_metadata,
+                 saved_path=f"{saved_dir}/0Gz_tilted.jpg", bs=bs)
+
+    _save_images(G=G_ema, fixed_z=fixed_z, rays_o=rays_o, rays_d=rays_d,
+                 G_kwargs=copied_metadata,
+                 saved_path=f"{saved_dir}/0Gz_tilted_ema.jpg", bs=bs)
+
+  # Monitor mirror symmetry
+  bs = min(20, bs)
+  sub_fixed_z = {}
+  for name, z_ in fixed_z.items():
+    if z_ is not None:
+      sub_fixed_z[name] = z_[:bs]
+    else:
+      sub_fixed_z[name] = z_
+  fixed_z = sub_fixed_z
+
+  with torch.cuda.amp.autocast(use_amp_G):
+    copied_metadata = copy.deepcopy(G_kwargs)
+    copied_metadata['nerf_kwargs']['h_stddev'] = 0.
+    copied_metadata['nerf_kwargs']['v_stddev'] = 0.
+
+    copied_metadata['nerf_kwargs']['h_mean'] = math.pi * 0.5 - 0.15
+    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+      device=device, bs=bs, intr=None, **copied_metadata['nerf_kwargs'])
+
+    Gema_flip1, ret_imgs = G_ema(zs=fixed_z,
+                                 rays_o=rays_o,
+                                 rays_d=rays_d,
+                                 forward_points=128 ** 2,
+                                 return_aux_img=True,
+                                 **copied_metadata)
+    Gema_flip1_aux = ret_imgs['aux_img']
+    Gema_flip1 = torch.cat([Gema_flip1, Gema_flip1_aux], dim=0)
+
+    # resampling
+    copied_metadata['nerf_kwargs']['h_mean'] = math.pi * 0.5 + 0.15
+    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+      device=device, bs=bs, intr=None, **copied_metadata['nerf_kwargs'])
+
+
+    Gema_flip2, ret_imgs = G_ema(zs=fixed_z,
+                                 rays_o=rays_o,
+                                 rays_d=rays_d,
+                                 forward_points=128 ** 2,
+                                 return_aux_img=True,
+                                 **copied_metadata)
+    Gema_flip2_aux = ret_imgs['aux_img']
+    Gema_flip2 = torch.cat([Gema_flip2, Gema_flip2_aux], dim=0)
+
+    Gema_flip = torch.cat([Gema_flip1, Gema_flip2])
+    save_image(Gema_flip, f"{saved_dir}/0G_flip_ema.jpg", nrow=bs//2, normalize=True, scale_each=True)
+
+  pass
+
+
+def get_curriculum(curriculum_name):
+  curriculum = getattr(curriculums, curriculum_name)
+
+  # update curriculum_step
+  for curriculum_step in curriculum.keys():
+    if type(curriculum_step) == int:
+      # stage
+      curriculum_step_str = str(curriculum_step)
+      if curriculum_step_str in global_cfg.curriculum:
+        curriculum[curriculum_step].update(global_cfg.curriculum.get(curriculum_step_str).to_dict())
+    else:
+      # update attrs
+      if curriculum_step in global_cfg.curriculum:
+        curriculum[curriculum_step] = global_cfg.curriculum[curriculum_step]
+  for new_attr, value in global_cfg.curriculum.get('new_attrs', {}).items():
+    assert new_attr not in curriculum
+    curriculum[new_attr] = value
+  return curriculum
+
+
+def build_optimizer(generator_ddp,
+                    discriminator_ddp,
+                    cam_param_ddp):
+
+  optimizer_G = torch.optim.Adam(
+    params=[{'params': generator_ddp.parameters(),
+             'initial_lr': global_cfg.gen_lr}],
+    lr=global_cfg.gen_lr,
+    betas=global_cfg.betas,
+    weight_decay=0)
+  optimizer_D = torch.optim.Adam(
+    params=[{'params': discriminator_ddp.parameters(),
+             'initial_lr': global_cfg.disc_lr}],
+    lr=global_cfg.disc_lr,
+    betas=global_cfg.betas,
+    weight_decay=0)
+
+  optimizer_cam = torch.optim.Adam(
+    params=[{'params': cam_param_ddp.parameters(),
+             'initial_lr': global_cfg.cam_lr}],
+    lr=global_cfg.cam_lr,
+    betas=global_cfg.betas,
+    weight_decay=0)
+
+  # if global_cfg.tl_resume and global_cfg.load_optimizers:
+  #   model_dict = {
+  #     'optimizer_G': optimizer_G,
+  #     'optimizer_D': optimizer_D,
+  #     'scaler_G': scaler_G,
+  #     'scaler_D': scaler_D,
+  #   }
+  #   torch_utils.load_models(save_dir=resume_dir, model_dict=model_dict, strict=False, rank=rank)
+
+  # After load optimizer.
+  # optimizer_G.param_groups[0]['initial_lr'] = global_cfg.gen_lr
+  # optimizer_G.param_groups[0]['lr'] = global_cfg.gen_lr
+  # optimizer_G.param_groups[0]['betas'] = metadata['betas']
+  # optimizer_D.param_groups[0]['initial_lr'] = global_cfg.disc_lr
+  # optimizer_D.param_groups[0]['lr'] = global_cfg.disc_lr
+  # optimizer_D.param_groups[0]['betas'] = metadata['betas']
+
+  return optimizer_G, optimizer_D, optimizer_cam
+
+
+def train(rank,
+          world_size,
+          opt):
+  setup_ddp(rank, world_size, opt.port)
+
+  update_parser_defaults_from_yaml(parser=None, is_main_process=(rank == 0))
+  if rank == 0:
+    moxing_utils.setup_tl_outdir_obs(global_cfg)
+    moxing_utils.modelarts_sync_results_dir(global_cfg, join=True)
+
+  logger = logging.getLogger('tl')
+  device = torch.device(rank)
+  torch_utils.init_seeds(seed=global_cfg.seed, rank=rank)
+
+  # curriculum = get_curriculum(curriculum_name=opt.curriculum)
+
+  scaler_G = torch.cuda.amp.GradScaler(enabled=global_cfg.use_amp_G)
+  scaler_D = torch.cuda.amp.GradScaler(enabled=global_cfg.use_amp_D)
+
+  """dataset """
+  moxing_utils.copy_data(rank=rank, global_cfg=global_cfg, **global_cfg.obs_training_dataset)
+  dataset = build_model(global_cfg.data_cfg, kwargs_priority=True, resize_resolution=global_cfg.img_size)
+  # imgs, label, idx = dataset[0] # [0, 255]
+  data_loader = get_training_dataloader(dataset=dataset, rank=rank, num_gpus=world_size,
+                                        batch_size=global_cfg.batch_size * world_size,
+                                        num_workers=global_cfg.num_workers,
+                                        shuffle=True, sampler_seed=0)
+  data_loader_iter = iter(data_loader)
+  # batch_data = next(data_loader_iter)
+
+  H = W = global_cfg.img_size
+  cam_cfg = global_cfg.get('cam_cfg', {'freeze_intr': True,
+                                      'normalize_rays_d': True})
+  cam_param = cam_params_pigan.CamParams.from_config(num_imgs=1, H0=H, W0=W, **cam_cfg).to(device)
+  cam_param_ddp = DDP(cam_param, device_ids=[rank], find_unused_parameters=True, broadcast_buffers=False)
+
+  generator = build_model(cfg=global_cfg.G_cfg).to(device)
+  discriminator = build_model(cfg=global_cfg.D_cfg, kwargs_priority=True, diffaug=global_cfg.diffaug).to(device)
+  G_ema = copy.deepcopy(generator)
+  ema_start_itr = global_cfg.get('ema_start_itr', 1000)
+  ema_model = comm_model_utils.EMA(source=generator, target=G_ema, decay=0.999, start_itr=ema_start_itr)
+
+  # ddp
+  generator_ddp = DDP(generator, device_ids=[rank], find_unused_parameters=True, broadcast_buffers=False)
+  discriminator_ddp = DDP(discriminator, device_ids=[rank], find_unused_parameters=True, broadcast_buffers=False)
+  generator = generator_ddp.module
+  generator.set_device(device)
+
+  optimizer_G, optimizer_D, optimizer_cam = build_optimizer(generator_ddp=generator_ddp,
+                                                            discriminator_ddp=discriminator_ddp,
+                                                            cam_param_ddp=cam_param_ddp)
+
+  state_dict = {
+    'cur_fid': np.inf,
+    'best_fid': np.inf,
+    'worst_fid': 0,
+    'step': 0,
+  }
+
+  model_dict = {
+    'cam_param': cam_param_ddp.module,
+    'generator': generator_ddp.module,
+    'G_ema': G_ema,
+    'discriminator': discriminator_ddp.module,
+    # 'optimizer_G': optimizer_G,
+    # 'optimizer_D': optimizer_D,
+    'state_dict': state_dict,
+  }
+
+  if global_cfg.tl_resume or global_cfg.load_finetune:
+    if global_cfg.tl_resume:
+      resume_dir = f"{global_cfg.tl_resumedir}/ckptdir/resume"
+      torch_utils.load_models(save_dir=resume_dir, model_dict=model_dict, strict=False, rank=rank)
+    elif global_cfg.load_finetune:
+      moxing_utils.copy_data(rank=rank, global_cfg=global_cfg,
+                             datapath_obs=global_cfg.finetune_dir, datapath=global_cfg.finetune_dir)
+      finetune_model_dict = {
+        'cam_param': cam_param_ddp.module,
+        'generator': generator_ddp.module,
+        'G_ema': G_ema,
+        'discriminator': discriminator_ddp.module,
+      }
+      torch_utils.load_models(save_dir=global_cfg.finetune_dir, model_dict=finetune_model_dict,
+                              strict=False, rank=rank)
+    else:
+      assert 0
+
+    if global_cfg.get('load_nerf_ema', False):
+      generator.load_nerf_ema(G_ema)
+
+    if global_cfg.load_G_ema:
+      ema_model.update_target_dict(G_ema.state_dict())
+    else:
+      ema_model.update_target_dict(generator.state_dict())
+
+    if global_cfg.reset_best_fid:
+      state_dict['best_fid'] = np.inf
+    logger.info(pprint.pformat(state_dict))
+
+  if global_cfg.tl_debug:
+    global_cfg.fixed_z_bs = 4
+  fixed_z = generator.get_zs(global_cfg.fixed_z_bs)
+  dummy_tensor = torch.tensor([0.], device=device)
+  # ----------
+  #  Training
+  # ----------
+
+  start_itr = state_dict['step']
+  pbar = tl2_utils.TL_tqdm(total=global_cfg.total_iters, start=start_itr)
+
+
+  summary_ddict = collections.defaultdict(dict)
+
+  nerf_noise_disable = global_cfg.get('nerf_noise_disable', False)
+
+  G_kwargs = global_cfg.G_kwargs.to_dict()
+
+  for step in range(start_itr, global_cfg.total_iters):
+    pbar.update()
+    summary_ddict.clear()
+
+    # for i, (imgs, _) in enumerate(dataloader):
+      # real_imgs = imgs.to(device, non_blocking=True)
+    imgs, _, imgs_idx = next(data_loader_iter)
+    real_imgs = to_norm_tensor(imgs, device=device)
+    imgs_idx = imgs_idx.to(device)
+
+    generator_ddp.train()
+    discriminator_ddp.train()
+
+    if nerf_noise_disable:
+      nerf_noise = 0.
+    else:
+      nerf_noise = max(0, 1. - state_dict['step'] / 5000.)
+    G_kwargs['nerf_kwargs']['raw_noise_std'] = nerf_noise
+
+    if global_cfg.get('warmup_D', False):
+      alpha = min(1, step / global_cfg.fade_steps)
+    else:
+      alpha = 1.
+
+    '''TRAIN DISCRIMINATOR'''
+    torch_utils.requires_grad(generator_ddp, False)
+    torch_utils.requires_grad(discriminator_ddp, True)
+
+    aux_reg = global_cfg.train_aux_img and (step % global_cfg.update_aux_every == 0)
+
+    with torch.cuda.amp.autocast(global_cfg.use_amp_D):
+      # Generate images for discriminator training
+      with torch.no_grad():
+        # z = z_sampler((real_imgs.shape[0], metadata['latent_dim']),
+        #               device=device,
+        #               dist=metadata['z_dist'])
+        zs_list = generator.get_zs(real_imgs.shape[0], batch_split=global_cfg.batch_split)
+        if global_cfg.batch_split == 1:
+          zs_list = [zs_list]
+        gen_imgs = []
+        gen_imgs_aux = []
+        if global_cfg.img_size >= 256 and global_cfg.forward_points is not None:
+          forward_points = global_cfg.forward_points ** 2
+        else:
+          forward_points = None
+
+        imgs_idx_list = imgs_idx.chunk(global_cfg.batch_split)
+        for subset_z, sub_imgs_idx in zip(zs_list, imgs_idx_list):
+          # R, t, fx, fy = cam_param(sub_imgs_idx)
+          # rays_o, rays_d, _ = generator.get_rays_axis_angle(R=R, t=t, fx=fx, fy=fy, H=H, W=W, N_rays=-1)
+          intr = cam_param_ddp(mode='get_intrinsic')
+          rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+            device=device, bs=len(sub_imgs_idx), intr=intr, **G_kwargs['nerf_kwargs'])
+          g_imgs, ret_imgs = generator_ddp(zs=subset_z,
+                                           rays_o=rays_o,
+                                           rays_d=rays_d,
+                                           return_aux_img=aux_reg,
+                                           forward_points=forward_points,
+                                           grad_points=None,
+                                           **G_kwargs)
+
+          # find a bug when batch_split > 1 :  g_imgs: [Gz, Gz_aux]
+          gen_imgs.append(g_imgs)
+          if aux_reg:
+            g_imgs_aux = ret_imgs['aux_img']
+            gen_imgs_aux.append(g_imgs_aux)
+
+        gen_imgs = torch.cat(gen_imgs + gen_imgs_aux, axis=0)
+      # end torch.no_grad
+      if aux_reg:
+        real_imgs = torch.cat([real_imgs, real_imgs], dim=0)
+      real_imgs.requires_grad_()
+      r_preds, _, _ = discriminator_ddp(real_imgs, alpha=alpha, use_aux_disc=aux_reg, summary_ddict=summary_ddict)
+
+    d_regularize = step % global_cfg.d_reg_every == 0
+
+    if global_cfg.r1_lambda > 0 and d_regularize:
+      # Gradient penalty
+      grad_real = torch.autograd.grad(
+        outputs=scaler_D.scale(r_preds.sum()),
+        inputs=real_imgs,
+        create_graph=True)
+      inv_scale = 1. / scaler_D.get_scale()
+      grad_real = [p * inv_scale for p in grad_real][0]
+
+    with torch.cuda.amp.autocast(global_cfg.use_amp_D):
+      if global_cfg.r1_lambda > 0 and d_regularize:
+        # grad_penalty = (grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2).mean()
+        # grad_penalty = 0.5 * global_cfg.r1_lambda * global_cfg.d_reg_every * grad_penalty + 0 * r_preds[0]
+        grad_penalty = grad_real.flatten(start_dim=1).square().sum(dim=1, keepdim=True)
+        grad_penalty = 0.5 * global_cfg.r1_lambda * grad_penalty * global_cfg.d_reg_every  + 0. * r_preds
+      else:
+        grad_penalty = dummy_tensor
+
+      g_preds, _, _ = discriminator_ddp(gen_imgs, alpha=alpha, use_aux_disc=aux_reg)
+
+      d_loss = (torch.nn.functional.softplus(g_preds) +
+                torch.nn.functional.softplus(-r_preds) +
+                grad_penalty).mean()
+      # d_loss = torch.nn.functional.softplus(g_preds).mean() + \
+      #          torch.nn.functional.softplus(-r_preds).mean() + \
+      #          grad_penalty
+
+      if rank == 0:
+        with torch.no_grad():
+          summary_ddict['D_logits']['D_logits_real'] = r_preds.mean().item()
+          summary_ddict['D_logits']['D_logits_fake'] = g_preds.mean().item()
+          summary_ddict['grad_penalty']['grad_penalty'] = grad_penalty.mean().item()
+
+    optimizer_D.zero_grad()
+    scaler_D.scale(d_loss).backward()
+    scaler_D.unscale_(optimizer_D)
+    try:
+      D_total_norm = torch.nn.utils.clip_grad_norm_(discriminator_ddp.parameters(),
+                                                    global_cfg.grad_clip,
+                                                    # error_if_nonfinite=True, # torch >= 1.9
+                                                    )
+      summary_ddict['D_total_norm']['D_total_norm'] = D_total_norm.item()
+    except:
+      summary_ddict['D_total_norm']['D_total_norm'] = np.nan
+      logger.info(traceback.format_exc())
+      saved_models(model_dict=model_dict,
+                   info_msg=f"step: {state_dict['step']}",
+                   G=generator, G_ema=G_ema, G_kwargs=G_kwargs,
+                   fixed_z=fixed_z, img_size=global_cfg.img_size,
+                   saved_dir=f"{global_cfg.tl_ckptdir}/D_crupted")
+      # exit(0)
+      optimizer_D.zero_grad()
+
+    scaler_D.step(optimizer_D)
+    scaler_D.update()
+
+    ''' TRAIN GENERATOR '''
+    torch_utils.requires_grad(generator_ddp, True)
+    torch_utils.requires_grad(discriminator_ddp, False)
+
+    # z = z_sampler((imgs.shape[0], metadata['latent_dim']), device=device, dist=metadata['z_dist'])
+    zs_list = generator.get_zs(imgs.shape[0], batch_split=global_cfg.batch_split)
+    if global_cfg.batch_split == 1:
+      zs_list = [zs_list]
+
+    if global_cfg.grad_points is not None:
+      grad_points = global_cfg.grad_points ** 2
+    else:
+      grad_points = None
+
+    imgs_idx_list = imgs_idx.chunk(global_cfg.batch_split)
+    for subset_z, sub_imgs_idx in zip(zs_list, imgs_idx_list):
+      with torch.cuda.amp.autocast(global_cfg.use_amp_G):
+        # R, t, fx, fy = cam_param(sub_imgs_idx)
+        # rays_o, rays_d, _ = generator.get_rays_axis_angle(R=R, t=t, fx=fx, fy=fy, H=H, W=W, N_rays=-1)
+        intr = cam_param_ddp(mode='get_intrinsic')
+        rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+          device=device, bs=len(sub_imgs_idx), intr=intr, **G_kwargs['nerf_kwargs'])
+        gen_imgs, ret_imgs = generator_ddp(zs=subset_z,
+                                           rays_o=rays_o,
+                                           rays_d=rays_d,
+                                           return_aux_img=aux_reg,
+                                           grad_points=grad_points,
+                                           forward_points=None,
+                                           **G_kwargs)
+        if aux_reg:
+          g_imgs_aux = ret_imgs['aux_img']
+          gen_imgs = torch.cat([gen_imgs, g_imgs_aux], dim=0)
+
+        with torch.cuda.amp.autocast(global_cfg.use_amp_D):
+          g_preds, _, _ = discriminator_ddp(gen_imgs.to(torch.float32), alpha=alpha, use_aux_disc=aux_reg)
+        g_loss = torch.nn.functional.softplus(-g_preds).mean()
+      scaler_G.scale(g_loss).backward()
+    # end accumulate gradients
+    scaler_G.unscale_(optimizer_G)
+    scaler_G.unscale_(optimizer_cam)
+    try:
+      if hasattr(generator, 'get_subnet_grad_norm'):
+        grad_dict = generator.get_subnet_grad_norm()
+        summary_ddict['G_total_norm'].update(grad_dict)
+        G_total_norm = torch.nn.utils.clip_grad_norm_(generator_ddp.parameters(),
+                                                      global_cfg.grad_clip)
+        # G_total_norm = torch.nn.utils.clip_grad_norm_(generator.nerf_net.parameters(),
+        #                                               global_cfg.grad_clip)
+        # G_total_norm = torch.nn.utils.clip_grad_norm_(generator.mapping_shape_app.parameters(),
+        #                                               global_cfg.grad_clip)
+
+      else:
+        G_total_norm = torch.nn.utils.clip_grad_norm_(generator_ddp.parameters(),
+                                                      global_cfg.grad_clip,
+                                                      # metadata.get('grad_clip', 0.3),
+                                                      # error_if_nonfinite=True, # torch >= 1.9
+                                                      )
+        summary_ddict['G_total_norm']['G_total_norm'] = G_total_norm.item()
+    except:
+      summary_ddict['G_total_norm']['G_total_norm'] = np.nan
+      logger.info(traceback.format_exc())
+      saved_models(model_dict=model_dict,
+                   info_msg=f"step: {state_dict['step']}",
+                   G=generator, G_ema=G_ema, G_kwargs=G_kwargs,
+                   fixed_z=fixed_z, img_size=global_cfg.img_size,
+                   saved_dir=f"{global_cfg.tl_ckptdir}/G_crupted")
+      # exit(0)
+      optimizer_G.zero_grad()
+      optimizer_cam.zero_grad()
+    scaler_G.step(optimizer_G)
+    scaler_G.step(optimizer_cam)
+    scaler_G.update()
+
+    optimizer_G.zero_grad()
+    optimizer_cam.zero_grad()
+
+    # update ema
+    ema_model.update(itr=state_dict['step'], source_dict=generator_ddp.module.state_dict())
+
+    if (rank == 0 and (step + 1) % global_cfg.log_every == 0) or global_cfg.tl_debug:
+      summary_ddict['lr']['G_lr'] = torch_utils.get_optimizer_lr(optimizer_G)
+      summary_ddict['lr']['D_lr'] = torch_utils.get_optimizer_lr(optimizer_D)
+      summary_ddict['lr']['cam_lr'] = torch_utils.get_optimizer_lr(optimizer_cam)
+      summary_ddict['img_size']['img_size'] = global_cfg.img_size
+      summary_ddict['batch_size']['batch_size'] = global_cfg.batch_size * world_size
+      summary_ddict['grad_points']['grad_points'] = grad_points if grad_points is not None else global_cfg.img_size ** 2
+      summary_ddict['scaler']['scaler_G'] = scaler_G.get_scale()
+      summary_ddict['scaler']['scaler_D'] = scaler_D.get_scale()
+      summary_ddict['r1_lambda']['r1_lambda'] = global_cfg.r1_lambda
+      summary_ddict['grad_clip']['grad_clip'] = global_cfg.grad_clip
+      summary_ddict['nerf_noise']['nerf_noise'] = nerf_noise
+      summary_ddict['train_aux_img']['train_aux_img'] = int(global_cfg.train_aux_img)
+      summary_ddict['alpha']['alpha'] = alpha
+      summary_ddict['diffaug']['diffaug'] = int(global_cfg.diffaug)
+      summary_ddict['block_idx']['shape'] = global_cfg.G_cfg.shape_block_end_index
+      summary_ddict['block_idx']['app'] = global_cfg.G_cfg.app_block_end_index
+      summary_ddict['block_idx']['inr'] = global_cfg.G_cfg.inr_block_end_index
+      with torch.no_grad():
+        fx, fy = cam_param.get_focal()
+        summary_ddict['intr']['fx'] = fx.item()
+        summary_ddict['intr']['fy'] = fy.item()
+
+      if step > 1000:
+        summary_defaultdict2txtfig(summary_ddict, prefix='train', step=state_dict['step'], textlogger=global_textlogger)
+      summary_str = tl2_utils.get_print_dict_str(summary_ddict, outdir=global_cfg.tl_outdir,
+                                                 suffix_str=pbar.get_string())
+      print(summary_str)
+
+
+    state_dict['step'] += 1
+    if step == 0 or (step + 1) % global_cfg.eval_every == 0 or global_cfg.tl_debug:
+      # output real images
+      setup_evaluation(rank=rank,
+                       world_size=world_size,
+                       data_cfg=global_cfg.data_cfg,
+                       real_dir=f"{global_cfg.tl_outdir}/exp/fid/real",
+                       img_size=global_cfg.img_size,
+                       num_imgs=global_cfg.num_images_real_eval,
+                       del_fid_real_images=global_cfg.del_fid_real_images,
+                       shuffle=True)
+      global_cfg.del_fid_real_images = False
+      ddp_utils.d2_synchronize()
+
+      # output fake images
+      gen_images(rank=rank,
+                 world_size=world_size,
+                 generator=G_ema,
+                 cam_param=cam_param,
+                 G_kwargs=global_cfg.G_kwargs.to_dict(),
+                 fake_dir=f"{global_cfg.tl_outdir}/exp/fid/fake",
+                 num_imgs=global_cfg.num_images_gen_eval,
+                 img_size=global_cfg.img_size,
+                 batch_size=global_cfg.eval_batch_size,
+                 device=device)
+      ddp_utils.d2_synchronize()
+
+      moxing_utils.copy_data(rank=rank, global_cfg=global_cfg, **global_cfg.obs_inception_v3)
+      global_cfg.obs_inception_v3.disable = True
+      if rank == 0:
+        metric_dict = eval_fid(real_dir=f"{global_cfg.tl_outdir}/exp/fid/real",
+                               fake_dir=f"{global_cfg.tl_outdir}/exp/fid/fake")
+        logger.info(f"\nstep: {state_dict['step']}, {pprint.pformat(metric_dict)}\n")
+        summary_dict2txtfig(metric_dict, prefix='eval', step=state_dict['step'], textlogger=global_textlogger)
+        state_dict['cur_fid'] = metric_dict['FID']
+
+        if state_dict['best_fid'] > metric_dict['FID']:
+          state_dict['best_fid'] = metric_dict['FID']
+          saved_models(model_dict=model_dict,
+                       info_msg=f"step: {state_dict['step']}\n"
+                                f"cur_fid: {state_dict['cur_fid']}\n"
+                                f"best_fid: {state_dict['best_fid']}",
+                       G=generator, G_ema=G_ema, G_kwargs=global_cfg.G_kwargs.to_dict(),
+                       cam_param=cam_param,
+                       fixed_z=fixed_z, img_size=global_cfg.img_size,
+                       saved_dir=f"{global_cfg.tl_ckptdir}/best_fid",
+                       device=device)
+
+        info_msg = f"step: {state_dict['step']}\n" \
+                   f"cur_fid: {state_dict['cur_fid']}\n" \
+                   f"best_fid: {state_dict['best_fid']}"
+        # backup
+        saved_models(model_dict=model_dict,
+                     info_msg=info_msg,
+                     G=generator, G_ema=G_ema, G_kwargs=global_cfg.G_kwargs.to_dict(),
+                     cam_param=cam_param,
+                     fixed_z=fixed_z, img_size=global_cfg.img_size,
+                     device=device)
+        # resume
+        saved_models(model_dict=model_dict,
+                     info_msg=info_msg,
+                     G=generator, G_ema=G_ema, G_kwargs=global_cfg.G_kwargs.to_dict(),
+                     cam_param=cam_param,
+                     fixed_z=fixed_z, img_size=global_cfg.img_size,
+                     saved_dir=f"{global_cfg.tl_ckptdir}/resume",
+                     device=device)
+
+        moxing_utils.modelarts_sync_results_dir(cfg=global_cfg, join=False)
+        # end rank == 0
+      ddp_utils.d2_synchronize()
+
+  cleanup()
+  pass
+
+
+if __name__ == '__main__':
+  parser = argparse.ArgumentParser()
+  parser.add_argument("--n_epochs", type=int, default=60000, help="number of epochs of training")
+  parser.add_argument("--sample_interval", type=int, default=200, help="interval between image sampling")
+  parser.add_argument('--output_dir', type=str, default='results/')
+  parser.add_argument('--load_dir', type=str, default='')
+  parser.add_argument('--curriculum', type=str, default='CelebA')
+  parser.add_argument('--eval_freq', type=int, default=5000)
+  parser.add_argument('--port', type=str, default='12355')
+  parser.add_argument('--set_step', type=int, default=None)
+  parser.add_argument('--model_save_interval', type=int, default=5000)
+
+  argparser_utils.add_argument_bool(parser, 'modelarts', default=False)
+
+  update_parser_defaults_from_yaml(parser)
+
+  opt, _ = parser.parse_known_args()
+  argparser_utils.print_args(opt)
+
+  moxing_utils.setup_tl_outdir_obs(global_cfg, unzip_code=False)
+  moxing_utils.modelarts_sync_results_dir(global_cfg, join=True)
+
+  num_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+  if num_gpus > 1:
+    mp.spawn(train, args=(num_gpus, opt), nprocs=num_gpus, join=True)
+  else:
+    train(rank=0, world_size=num_gpus, opt=opt)
+
+  moxing_utils.modelarts_sync_results_dir(global_cfg, join=True)
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/dataset_stylegan3/dataset.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/dataset_stylegan3/dataset.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,400 +1,400 @@
-# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.
-#
-# NVIDIA CORPORATION and its licensors retain all intellectual property
-# and proprietary rights in and to this software, related documentation
-# and any modifications thereto.  Any use, reproduction, disclosure or
-# distribution of this software and related documentation without an express
-# license agreement from NVIDIA CORPORATION is strictly prohibited.
-
-"""Streaming images and labels from datasets created with dataset_tool.py."""
-
-import os
-import numpy as np
-import zipfile
-import PIL.Image
-import json
-import logging
-from pathlib import Path
-
-import torch
-
-from tl2.proj.fvcore import MODEL_REGISTRY
-from tl2 import tl2_utils
-from tl2.proj.pil import pil_utils
-
-
-# try:
-#   import pyspng
-# except ImportError:
-#   pyspng = None
-
-
-# ----------------------------------------------------------------------------
-
-def get_training_dataloader(dataset,
-                            rank,
-                            num_gpus,
-                            batch_size,
-                            num_workers,
-                            shuffle=True,
-                            sampler_seed=0,
-                            pin_memory=True,
-                            prefetch_factor=2):
-
-  batch_gpu = batch_size // num_gpus
-
-  if rank == 0:
-    repr_str = tl2_utils.dict2string(prefix_str='Data loader', dict_obj={
-      'path': dataset._path,
-      'rank': rank,
-      'num_gpus': num_gpus,
-      'batch_size': f"batch_gpu * num_gpus = {batch_gpu} * {num_gpus} = {batch_gpu*num_gpus}",
-      'num_workers': num_workers,
-      'shuffle': shuffle,
-      'sampler_seed': sampler_seed,
-      'pin_memory': pin_memory,
-      'prefetch_factor': prefetch_factor
-    })
-    logging.getLogger('tl').info(repr_str)
-
-  training_set_sampler = InfiniteSampler(
-    dataset=dataset, rank=rank, num_replicas=num_gpus, seed=sampler_seed, shuffle=shuffle)
-
-  data_loader = torch.utils.data.DataLoader(
-    dataset=dataset,
-    sampler=training_set_sampler,
-    batch_size=batch_gpu,
-    num_workers=num_workers,
-    pin_memory=pin_memory,
-    prefetch_factor=prefetch_factor)
-
-  return data_loader
-
-# ----------------------------------------------------------------------------
-def to_norm_tensor(img_tensor, device):
-  """
-
-  :param img_tensor: [0, 255]
-  :param device:
-  :return: [-1, 1]
-  """
-  return img_tensor.to(device).to(torch.float32) / 127.5 - 1
-
-# ----------------------------------------------------------------------------
-
-
-class EasyDict(dict):
-  """Convenience class that behaves like a dict but allows access with the attribute syntax."""
-
-  def __getattr__(self, name: str):
-    try:
-      return self[name]
-    except KeyError:
-      raise AttributeError(name)
-
-  def __setattr__(self, name: str, value) -> None:
-    self[name] = value
-
-  def __delattr__(self, name: str) -> None:
-    del self[name]
-
-
-# ----------------------------------------------------------------------------
-
-# Sampler for torch.utils.data.DataLoader that loops over the dataset
-# indefinitely, shuffling items as it goes.
-
-class InfiniteSampler(torch.utils.data.Sampler):
-  def __init__(self,
-               dataset,
-               rank=0,
-               num_replicas=1,
-               shuffle=True,
-               seed=0,
-               window_size=0.5):
-
-    assert len(dataset) > 0
-    assert num_replicas > 0
-    assert 0 <= rank < num_replicas
-    assert 0 <= window_size <= 1
-    super().__init__(dataset)
-    self.dataset = dataset
-    self.rank = rank
-    self.num_replicas = num_replicas
-    self.shuffle = shuffle
-    self.seed = seed
-    self.window_size = window_size
-    pass
-
-  def __iter__(self):
-    order = np.arange(len(self.dataset))
-    rnd = None
-    window = 0
-    if self.shuffle:
-      rnd = np.random.RandomState(self.seed)
-      rnd.shuffle(order)
-      window = int(np.rint(order.size * self.window_size))
-
-    idx = 0
-    while True:
-      i = idx % order.size
-      if idx % self.num_replicas == self.rank:
-        yield order[i]
-      if window >= 2:
-        j = (i - rnd.randint(window)) % order.size
-        order[i], order[j] = order[j], order[i]
-      idx += 1
-    pass
-
-# ----------------------------------------------------------------------------
-
-class Dataset(torch.utils.data.Dataset):
-  def __init__(self,
-               name,  # Name of the dataset.
-               raw_shape,  # Shape of the raw image data (NCHW).
-               max_size=None,  # Artificially limit the size of the dataset. None = no limit. Applied before xflip.
-               use_labels=False,  # Enable conditioning labels? False = label dimension is zero.
-               xflip=False,  # Artificially double the size of the dataset via x-flips. Applied after max_size.
-               random_seed=0,  # Random seed to use when applying max_size.
-               **kwargs
-               ):
-    self._name = name
-    self._raw_shape = list(raw_shape)
-    self._use_labels = use_labels
-    self._raw_labels = None
-    self._label_shape = None
-
-    # Apply max_size.
-    self._raw_idx = np.arange(self._raw_shape[0], dtype=np.int64)
-    if (max_size is not None) and (self._raw_idx.size > max_size):
-      np.random.RandomState(random_seed).shuffle(self._raw_idx)
-      self._raw_idx = np.sort(self._raw_idx[:max_size])
-
-    # Apply xflip.
-    self._xflip = np.zeros(self._raw_idx.size, dtype=np.uint8)
-    if xflip:
-      self._raw_idx = np.tile(self._raw_idx, 2)
-      self._xflip = np.concatenate([self._xflip, np.ones_like(self._xflip)])
-
-  def _get_raw_labels(self):
-    if self._raw_labels is None:
-      self._raw_labels = self._load_raw_labels() if self._use_labels else None
-      if self._raw_labels is None:
-        self._raw_labels = np.zeros([self._raw_shape[0], 0], dtype=np.float32)
-      assert isinstance(self._raw_labels, np.ndarray)
-      assert self._raw_labels.shape[0] == self._raw_shape[0]
-      assert self._raw_labels.dtype in [np.float32, np.int64]
-      if self._raw_labels.dtype == np.int64:
-        assert self._raw_labels.ndim == 1
-        assert np.all(self._raw_labels >= 0)
-    return self._raw_labels
-
-  def close(self):  # to be overridden by subclass
-    pass
-
-  def _load_raw_image(self, raw_idx):  # to be overridden by subclass
-    raise NotImplementedError
-
-  def _load_raw_labels(self):  # to be overridden by subclass
-    raise NotImplementedError
-
-  def __getstate__(self):
-    return dict(self.__dict__, _raw_labels=None)
-
-  def __del__(self):
-    try:
-      self.close()
-    except:
-      pass
-
-  def __len__(self):
-    return self._raw_idx.size
-
-  def __getitem__(self, idx):
-    image = self._load_raw_image(self._raw_idx[idx])
-    assert isinstance(image, np.ndarray)
-    assert list(image.shape) == self.image_shape
-    assert image.dtype == np.uint8
-    if self._xflip[idx]:
-      assert image.ndim == 3  # CHW
-      image = image[:, :, ::-1]
-    return image.copy(), self.get_label(idx), idx
-
-  def get_label(self, idx):
-    label = self._get_raw_labels()[self._raw_idx[idx]]
-    if label.dtype == np.int64:
-      onehot = np.zeros(self.label_shape, dtype=np.float32)
-      onehot[label] = 1
-      label = onehot
-    return label.copy()
-
-  def get_details(self, idx):
-    d = EasyDict()
-    d.raw_idx = int(self._raw_idx[idx])
-    d.xflip = (int(self._xflip[idx]) != 0)
-    d.raw_label = self._get_raw_labels()[d.raw_idx].copy()
-    return d
-
-  @property
-  def name(self):
-    return self._name
-
-  @property
-  def image_shape(self):
-    return list(self._raw_shape[1:])
-
-  @property
-  def num_channels(self):
-    assert len(self.image_shape) == 3  # CHW
-    return self.image_shape[0]
-
-  @property
-  def resolution(self):
-    assert len(self.image_shape) == 3  # CHW
-    assert self.image_shape[1] == self.image_shape[2]
-    return self.image_shape[1]
-
-  @property
-  def label_shape(self):
-    if self._label_shape is None:
-      raw_labels = self._get_raw_labels()
-      if raw_labels.dtype == np.int64:
-        self._label_shape = [int(np.max(raw_labels)) + 1]
-      else:
-        self._label_shape = raw_labels.shape[1:]
-    return list(self._label_shape)
-
-  @property
-  def label_dim(self):
-    assert len(self.label_shape) == 1
-    return self.label_shape[0]
-
-  @property
-  def has_labels(self):
-    return any(x != 0 for x in self.label_shape)
-
-  @property
-  def has_onehot_labels(self):
-    return self._get_raw_labels().dtype == np.int64
-
-
-# ----------------------------------------------------------------------------
-
-@MODEL_REGISTRY.register(name="ImageFolderDataset_of_stylegan")
-class ImageFolderDataset(Dataset):
-
-  def __repr__(self):
-    repr_str = tl2_utils.get_class_repr(self)
-    return repr_str
-
-  def __init__(self,
-               path,  # Path to directory or zip.
-               resize_resolution=None,  # Ensure specific resolution, None = highest available.
-               verbose=True,
-               **super_kwargs,  # Additional arguments for the Dataset base class.
-               ):
-
-    self._path = path
-    self._zipfile = None
-    self.resize_resolution = resize_resolution
-
-    if os.path.isdir(self._path):
-      self._type = 'dir'
-      self._all_fnames = {os.path.relpath(os.path.join(root, fname), start=self._path) for root, _dirs, files in
-                          os.walk(self._path) for fname in files}
-    elif self._file_ext(self._path) == '.zip':
-      self._type = 'zip'
-      self._all_fnames = set(self._get_zipfile().namelist())
-    elif os.path.isfile(self._path):
-      self._type = 'file'
-      self._all_fnames = {self._path}
-    else:
-      # raise IOError('Path must point to a directory or zip')
-      assert 0, f"{self._path}"
-
-    PIL.Image.init()
-    self._image_fnames = sorted(fname for fname in self._all_fnames if self._file_ext(fname) in PIL.Image.EXTENSION)
-    if len(self._image_fnames) == 0:
-      raise IOError('No image files found in the specified path')
-
-    # name = os.path.splitext(os.path.basename(self._path))[0]
-    name = Path(self._path).name
-    raw_shape = [len(self._image_fnames)] + list(self._load_raw_image(0).shape)
-    # if resolution is not None and (raw_shape[2] != resolution or raw_shape[3] != resolution):
-    #   raise IOError('Image files do not match the specified resolution')
-
-    super().__init__(name=name, raw_shape=raw_shape, **super_kwargs)
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'path': path,
-      'resize_resolution': resize_resolution,
-      'super_kwargs': super_kwargs,
-      'length': len(self),
-      'resolution': self.resolution,
-    }, use_pprint=True)
-
-    if verbose: logging.getLogger('tl').info(self)
-    pass
-
-  @staticmethod
-  def _file_ext(fname):
-    return os.path.splitext(fname)[1].lower()
-
-  def _get_zipfile(self):
-    assert self._type == 'zip'
-    if self._zipfile is None:
-      self._zipfile = zipfile.ZipFile(self._path)
-    return self._zipfile
-
-  def _open_file(self, fname):
-    if self._type == 'dir':
-      return open(os.path.join(self._path, fname), 'rb')
-    if self._type == 'zip':
-      return self._get_zipfile().open(fname, 'r')
-    if self._type == 'file':
-      return open(fname, 'rb')
-    return None
-
-  def close(self):
-    try:
-      if self._zipfile is not None:
-        self._zipfile.close()
-    finally:
-      self._zipfile = None
-
-  def __getstate__(self):
-    return dict(super().__getstate__(), _zipfile=None)
-
-  def _load_raw_image(self, raw_idx):
-    fname = self._image_fnames[raw_idx]
-    with self._open_file(fname) as f:
-      # if pyspng is not None and self._file_ext(fname) == '.png':
-      #   image = pyspng.load(f.read())
-      # else:
-      #   image = np.array(PIL.Image.open(f))
-      img_pil = PIL.Image.open(f).convert('RGB')
-
-      if self.resize_resolution is not None and img_pil.size[0] != self.resize_resolution:
-        img_pil = pil_utils.pil_resize(img_pil, size=(self.resize_resolution, self.resize_resolution))
-      image = np.array(img_pil)
-
-    # if image.ndim == 2:
-    #   image = image[:, :, np.newaxis]  # HW => HWC
-    image = image.transpose(2, 0, 1)  # HWC => CHW
-    return image
-
-  def _load_raw_labels(self):
-    fname = 'dataset.json'
-    if fname not in self._all_fnames:
-      return None
-    with self._open_file(fname) as f:
-      labels = json.load(f)['labels']
-    if labels is None:
-      return None
-    labels = dict(labels)
-    labels = [labels[fname.replace('\\', '/')] for fname in self._image_fnames]
-    labels = np.array(labels)
-    labels = labels.astype({1: np.int64, 2: np.float32}[labels.ndim])
-    return labels
-
-# ----------------------------------------------------------------------------
+# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.
+#
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+"""Streaming images and labels from datasets created with dataset_tool.py."""
+
+import os
+import numpy as np
+import zipfile
+import PIL.Image
+import json
+import logging
+from pathlib import Path
+
+import torch
+
+from tl2.proj.fvcore import MODEL_REGISTRY
+from tl2 import tl2_utils
+from tl2.proj.pil import pil_utils
+
+
+# try:
+#   import pyspng
+# except ImportError:
+#   pyspng = None
+
+
+# ----------------------------------------------------------------------------
+
+def get_training_dataloader(dataset,
+                            rank,
+                            num_gpus,
+                            batch_size,
+                            num_workers,
+                            shuffle=True,
+                            sampler_seed=0,
+                            pin_memory=True,
+                            prefetch_factor=2):
+
+  batch_gpu = batch_size // num_gpus
+
+  if rank == 0:
+    repr_str = tl2_utils.dict2string(prefix_str='Data loader', dict_obj={
+      'path': dataset._path,
+      'rank': rank,
+      'num_gpus': num_gpus,
+      'batch_size': f"batch_gpu * num_gpus = {batch_gpu} * {num_gpus} = {batch_gpu*num_gpus}",
+      'num_workers': num_workers,
+      'shuffle': shuffle,
+      'sampler_seed': sampler_seed,
+      'pin_memory': pin_memory,
+      'prefetch_factor': prefetch_factor
+    })
+    logging.getLogger('tl').info(repr_str)
+
+  training_set_sampler = InfiniteSampler(
+    dataset=dataset, rank=rank, num_replicas=num_gpus, seed=sampler_seed, shuffle=shuffle)
+
+  data_loader = torch.utils.data.DataLoader(
+    dataset=dataset,
+    sampler=training_set_sampler,
+    batch_size=batch_gpu,
+    num_workers=num_workers,
+    pin_memory=pin_memory,
+    prefetch_factor=prefetch_factor)
+
+  return data_loader
+
+# ----------------------------------------------------------------------------
+def to_norm_tensor(img_tensor, device):
+  """
+
+  :param img_tensor: [0, 255]
+  :param device:
+  :return: [-1, 1]
+  """
+  return img_tensor.to(device).to(torch.float32) / 127.5 - 1
+
+# ----------------------------------------------------------------------------
+
+
+class EasyDict(dict):
+  """Convenience class that behaves like a dict but allows access with the attribute syntax."""
+
+  def __getattr__(self, name: str):
+    try:
+      return self[name]
+    except KeyError:
+      raise AttributeError(name)
+
+  def __setattr__(self, name: str, value) -> None:
+    self[name] = value
+
+  def __delattr__(self, name: str) -> None:
+    del self[name]
+
+
+# ----------------------------------------------------------------------------
+
+# Sampler for torch.utils.data.DataLoader that loops over the dataset
+# indefinitely, shuffling items as it goes.
+
+class InfiniteSampler(torch.utils.data.Sampler):
+  def __init__(self,
+               dataset,
+               rank=0,
+               num_replicas=1,
+               shuffle=True,
+               seed=0,
+               window_size=0.5):
+
+    assert len(dataset) > 0
+    assert num_replicas > 0
+    assert 0 <= rank < num_replicas
+    assert 0 <= window_size <= 1
+    super().__init__(dataset)
+    self.dataset = dataset
+    self.rank = rank
+    self.num_replicas = num_replicas
+    self.shuffle = shuffle
+    self.seed = seed
+    self.window_size = window_size
+    pass
+
+  def __iter__(self):
+    order = np.arange(len(self.dataset))
+    rnd = None
+    window = 0
+    if self.shuffle:
+      rnd = np.random.RandomState(self.seed)
+      rnd.shuffle(order)
+      window = int(np.rint(order.size * self.window_size))
+
+    idx = 0
+    while True:
+      i = idx % order.size
+      if idx % self.num_replicas == self.rank:
+        yield order[i]
+      if window >= 2:
+        j = (i - rnd.randint(window)) % order.size
+        order[i], order[j] = order[j], order[i]
+      idx += 1
+    pass
+
+# ----------------------------------------------------------------------------
+
+class Dataset(torch.utils.data.Dataset):
+  def __init__(self,
+               name,  # Name of the dataset.
+               raw_shape,  # Shape of the raw image data (NCHW).
+               max_size=None,  # Artificially limit the size of the dataset. None = no limit. Applied before xflip.
+               use_labels=False,  # Enable conditioning labels? False = label dimension is zero.
+               xflip=False,  # Artificially double the size of the dataset via x-flips. Applied after max_size.
+               random_seed=0,  # Random seed to use when applying max_size.
+               **kwargs
+               ):
+    self._name = name
+    self._raw_shape = list(raw_shape)
+    self._use_labels = use_labels
+    self._raw_labels = None
+    self._label_shape = None
+
+    # Apply max_size.
+    self._raw_idx = np.arange(self._raw_shape[0], dtype=np.int64)
+    if (max_size is not None) and (self._raw_idx.size > max_size):
+      np.random.RandomState(random_seed).shuffle(self._raw_idx)
+      self._raw_idx = np.sort(self._raw_idx[:max_size])
+
+    # Apply xflip.
+    self._xflip = np.zeros(self._raw_idx.size, dtype=np.uint8)
+    if xflip:
+      self._raw_idx = np.tile(self._raw_idx, 2)
+      self._xflip = np.concatenate([self._xflip, np.ones_like(self._xflip)])
+
+  def _get_raw_labels(self):
+    if self._raw_labels is None:
+      self._raw_labels = self._load_raw_labels() if self._use_labels else None
+      if self._raw_labels is None:
+        self._raw_labels = np.zeros([self._raw_shape[0], 0], dtype=np.float32)
+      assert isinstance(self._raw_labels, np.ndarray)
+      assert self._raw_labels.shape[0] == self._raw_shape[0]
+      assert self._raw_labels.dtype in [np.float32, np.int64]
+      if self._raw_labels.dtype == np.int64:
+        assert self._raw_labels.ndim == 1
+        assert np.all(self._raw_labels >= 0)
+    return self._raw_labels
+
+  def close(self):  # to be overridden by subclass
+    pass
+
+  def _load_raw_image(self, raw_idx):  # to be overridden by subclass
+    raise NotImplementedError
+
+  def _load_raw_labels(self):  # to be overridden by subclass
+    raise NotImplementedError
+
+  def __getstate__(self):
+    return dict(self.__dict__, _raw_labels=None)
+
+  def __del__(self):
+    try:
+      self.close()
+    except:
+      pass
+
+  def __len__(self):
+    return self._raw_idx.size
+
+  def __getitem__(self, idx):
+    image = self._load_raw_image(self._raw_idx[idx])
+    assert isinstance(image, np.ndarray)
+    assert list(image.shape) == self.image_shape
+    assert image.dtype == np.uint8
+    if self._xflip[idx]:
+      assert image.ndim == 3  # CHW
+      image = image[:, :, ::-1]
+    return image.copy(), self.get_label(idx), idx
+
+  def get_label(self, idx):
+    label = self._get_raw_labels()[self._raw_idx[idx]]
+    if label.dtype == np.int64:
+      onehot = np.zeros(self.label_shape, dtype=np.float32)
+      onehot[label] = 1
+      label = onehot
+    return label.copy()
+
+  def get_details(self, idx):
+    d = EasyDict()
+    d.raw_idx = int(self._raw_idx[idx])
+    d.xflip = (int(self._xflip[idx]) != 0)
+    d.raw_label = self._get_raw_labels()[d.raw_idx].copy()
+    return d
+
+  @property
+  def name(self):
+    return self._name
+
+  @property
+  def image_shape(self):
+    return list(self._raw_shape[1:])
+
+  @property
+  def num_channels(self):
+    assert len(self.image_shape) == 3  # CHW
+    return self.image_shape[0]
+
+  @property
+  def resolution(self):
+    assert len(self.image_shape) == 3  # CHW
+    assert self.image_shape[1] == self.image_shape[2]
+    return self.image_shape[1]
+
+  @property
+  def label_shape(self):
+    if self._label_shape is None:
+      raw_labels = self._get_raw_labels()
+      if raw_labels.dtype == np.int64:
+        self._label_shape = [int(np.max(raw_labels)) + 1]
+      else:
+        self._label_shape = raw_labels.shape[1:]
+    return list(self._label_shape)
+
+  @property
+  def label_dim(self):
+    assert len(self.label_shape) == 1
+    return self.label_shape[0]
+
+  @property
+  def has_labels(self):
+    return any(x != 0 for x in self.label_shape)
+
+  @property
+  def has_onehot_labels(self):
+    return self._get_raw_labels().dtype == np.int64
+
+
+# ----------------------------------------------------------------------------
+
+@MODEL_REGISTRY.register(name="ImageFolderDataset_of_stylegan")
+class ImageFolderDataset(Dataset):
+
+  def __repr__(self):
+    repr_str = tl2_utils.get_class_repr(self)
+    return repr_str
+
+  def __init__(self,
+               path,  # Path to directory or zip.
+               resize_resolution=None,  # Ensure specific resolution, None = highest available.
+               verbose=True,
+               **super_kwargs,  # Additional arguments for the Dataset base class.
+               ):
+
+    self._path = path
+    self._zipfile = None
+    self.resize_resolution = resize_resolution
+
+    if os.path.isdir(self._path):
+      self._type = 'dir'
+      self._all_fnames = {os.path.relpath(os.path.join(root, fname), start=self._path) for root, _dirs, files in
+                          os.walk(self._path) for fname in files}
+    elif self._file_ext(self._path) == '.zip':
+      self._type = 'zip'
+      self._all_fnames = set(self._get_zipfile().namelist())
+    elif os.path.isfile(self._path):
+      self._type = 'file'
+      self._all_fnames = {self._path}
+    else:
+      # raise IOError('Path must point to a directory or zip')
+      assert 0, f"{self._path}"
+
+    PIL.Image.init()
+    self._image_fnames = sorted(fname for fname in self._all_fnames if self._file_ext(fname) in PIL.Image.EXTENSION)
+    if len(self._image_fnames) == 0:
+      raise IOError('No image files found in the specified path')
+
+    # name = os.path.splitext(os.path.basename(self._path))[0]
+    name = Path(self._path).name
+    raw_shape = [len(self._image_fnames)] + list(self._load_raw_image(0).shape)
+    # if resolution is not None and (raw_shape[2] != resolution or raw_shape[3] != resolution):
+    #   raise IOError('Image files do not match the specified resolution')
+
+    super().__init__(name=name, raw_shape=raw_shape, **super_kwargs)
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'path': path,
+      'resize_resolution': resize_resolution,
+      'super_kwargs': super_kwargs,
+      'length': len(self),
+      'resolution': self.resolution,
+    }, use_pprint=True)
+
+    if verbose: logging.getLogger('tl').info(self)
+    pass
+
+  @staticmethod
+  def _file_ext(fname):
+    return os.path.splitext(fname)[1].lower()
+
+  def _get_zipfile(self):
+    assert self._type == 'zip'
+    if self._zipfile is None:
+      self._zipfile = zipfile.ZipFile(self._path)
+    return self._zipfile
+
+  def _open_file(self, fname):
+    if self._type == 'dir':
+      return open(os.path.join(self._path, fname), 'rb')
+    if self._type == 'zip':
+      return self._get_zipfile().open(fname, 'r')
+    if self._type == 'file':
+      return open(fname, 'rb')
+    return None
+
+  def close(self):
+    try:
+      if self._zipfile is not None:
+        self._zipfile.close()
+    finally:
+      self._zipfile = None
+
+  def __getstate__(self):
+    return dict(super().__getstate__(), _zipfile=None)
+
+  def _load_raw_image(self, raw_idx):
+    fname = self._image_fnames[raw_idx]
+    with self._open_file(fname) as f:
+      # if pyspng is not None and self._file_ext(fname) == '.png':
+      #   image = pyspng.load(f.read())
+      # else:
+      #   image = np.array(PIL.Image.open(f))
+      img_pil = PIL.Image.open(f).convert('RGB')
+
+      if self.resize_resolution is not None and img_pil.size[0] != self.resize_resolution:
+        img_pil = pil_utils.pil_resize(img_pil, size=(self.resize_resolution, self.resize_resolution))
+      image = np.array(img_pil)
+
+    # if image.ndim == 2:
+    #   image = image[:, :, np.newaxis]  # HW => HWC
+    image = image.transpose(2, 0, 1)  # HWC => CHW
+    return image
+
+  def _load_raw_labels(self):
+    fname = 'dataset.json'
+    if fname not in self._all_fnames:
+      return None
+    with self._open_file(fname) as f:
+      labels = json.load(f)['labels']
+    if labels is None:
+      return None
+    labels = dict(labels)
+    labels = [labels[fname.replace('\\', '/')] for fname in self._image_fnames]
+    labels = np.array(labels)
+    labels = labels.astype({1: np.int64, 2: np.float32}[labels.ndim])
+    return labels
+
+# ----------------------------------------------------------------------------
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/comm_utils.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/comm_utils.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,120 +1,120 @@
-import logging
-import random
-import numpy as np
-import math
-
-import torch
-import torch.nn as nn
-
-
-class PosEmbedding(nn.Module):
-  def __init__(self,
-               max_logscale,
-               N_freqs,
-               logscale=True,
-               multi_pi=False,):
-    """
-    Defines a function that embeds x to (x, sin(2^k x), cos(2^k x), ...)
-    """
-    super().__init__()
-
-    self.N_freqs = N_freqs
-    self.funcs = [torch.sin, torch.cos]
-
-    if logscale:
-      self.freqs = 2 ** torch.linspace(0, max_logscale, N_freqs)
-    else:
-      self.freqs = torch.linspace(1, 2 ** max_logscale, N_freqs)
-    if multi_pi:
-      self.freqs = self.freqs * math.pi
-    pass
-
-  def get_out_dim(self):
-    outdim = 3 + 3 * 2 * self.N_freqs
-    return outdim
-
-  def forward(self, x):
-    """
-    Inputs:
-        x: (B, 3)
-
-    Outputs:
-        out: (B, 6*N_freqs+3)
-    """
-    out = [x]
-    for freq in self.freqs:
-      for func in self.funcs:
-        out += [func(freq * x)]
-
-    return torch.cat(out, -1)
-
-
-
-class EMA(object):
-  def __init__(self,
-               source,
-               target,
-               decay=0.9999,
-               start_itr=0):
-    """
-    # Simple wrapper that applies EMA to a model. Could be better done in 1.0 using
-    # the parameters() and buffers() module functions, but for now this works
-    # with state_dicts using .copy_
-
-    :param source: model
-    :param target: ema model
-    :param decay:
-    :param start_itr:
-    """
-    self.source = source
-    self.target = target
-    self.decay = decay
-    # Optional parameter indicating what iteration to start the decay at
-    self.start_itr = start_itr
-
-    logger = logging.getLogger('tl')
-
-    # Initialize target's params to be source's
-    self.source_dict = self.source.state_dict()
-    self.target_dict = self.target.state_dict()
-    logger.info(f'Initializing EMA [{decay}] parameters to be source parameters...')
-    self.update_target_dict(source_state_dict=self.source_dict)
-    pass
-
-  def update_target_dict(self, source_state_dict):
-    """
-    Reset the ema model weights.
-
-    :param source_state_dict:
-    :return:
-    """
-    with torch.no_grad():
-      for key in source_state_dict:
-        self.target_dict[key].data.copy_(source_state_dict[key].data)
-        # target_dict[key].data = source_dict[key].data # Doesn't work!
-    pass
-
-  def update(self, itr=None, source_dict=None):
-    """
-    # If an iteration counter is provided and itr is less than the start itr,
-    # peg the ema weights to the underlying weights.
-
-    :param itr:
-    :return:
-    """
-
-    if itr and itr < self.start_itr:
-      decay = 0.0
-    else:
-      decay = self.decay
-    with torch.no_grad():
-      if source_dict is None:
-        source_dict = self.source_dict
-
-      for key in source_dict:
-        self.target_dict[key].data.copy_(
-          self.target_dict[key].data * decay + source_dict[key].data * (1 - decay))
-    pass
-
-
-
+import logging
+import random
+import numpy as np
+import math
+
+import torch
+import torch.nn as nn
+
+
+class PosEmbedding(nn.Module):
+  def __init__(self,
+               max_logscale,
+               N_freqs,
+               logscale=True,
+               multi_pi=False,):
+    """
+    Defines a function that embeds x to (x, sin(2^k x), cos(2^k x), ...)
+    """
+    super().__init__()
+
+    self.N_freqs = N_freqs
+    self.funcs = [torch.sin, torch.cos]
+
+    if logscale:
+      self.freqs = 2 ** torch.linspace(0, max_logscale, N_freqs)
+    else:
+      self.freqs = torch.linspace(1, 2 ** max_logscale, N_freqs)
+    if multi_pi:
+      self.freqs = self.freqs * math.pi
+    pass
+
+  def get_out_dim(self):
+    outdim = 3 + 3 * 2 * self.N_freqs
+    return outdim
+
+  def forward(self, x):
+    """
+    Inputs:
+        x: (B, 3)
+
+    Outputs:
+        out: (B, 6*N_freqs+3)
+    """
+    out = [x]
+    for freq in self.freqs:
+      for func in self.funcs:
+        out += [func(freq * x)]
+
+    return torch.cat(out, -1)
+
+
+
+class EMA(object):
+  def __init__(self,
+               source,
+               target,
+               decay=0.9999,
+               start_itr=0):
+    """
+    # Simple wrapper that applies EMA to a model. Could be better done in 1.0 using
+    # the parameters() and buffers() module functions, but for now this works
+    # with state_dicts using .copy_
+
+    :param source: model
+    :param target: ema model
+    :param decay:
+    :param start_itr:
+    """
+    self.source = source
+    self.target = target
+    self.decay = decay
+    # Optional parameter indicating what iteration to start the decay at
+    self.start_itr = start_itr
+
+    logger = logging.getLogger('tl')
+
+    # Initialize target's params to be source's
+    self.source_dict = self.source.state_dict()
+    self.target_dict = self.target.state_dict()
+    logger.info(f'Initializing EMA [{decay}] parameters to be source parameters...')
+    self.update_target_dict(source_state_dict=self.source_dict)
+    pass
+
+  def update_target_dict(self, source_state_dict):
+    """
+    Reset the ema model weights.
+
+    :param source_state_dict:
+    :return:
+    """
+    with torch.no_grad():
+      for key in source_state_dict:
+        self.target_dict[key].data.copy_(source_state_dict[key].data)
+        # target_dict[key].data = source_dict[key].data # Doesn't work!
+    pass
+
+  def update(self, itr=None, source_dict=None):
+    """
+    # If an iteration counter is provided and itr is less than the start itr,
+    # peg the ema weights to the underlying weights.
+
+    :param itr:
+    :return:
+    """
+
+    if itr and itr < self.start_itr:
+      decay = 0.0
+    else:
+      decay = self.decay
+    with torch.no_grad():
+      if source_dict is None:
+        source_dict = self.source_dict
+
+      for key in source_dict:
+        self.target_dict[key].data.copy_(
+          self.target_dict[key].data * decay + source_dict[key].data * (1 - decay))
+    pass
+
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/config.yaml` & `tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/config.yaml`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,83 +1,83 @@
-root_obs: &root_obs s3://bucket-3690/ZhouPeng
-modelarts_download: &modelarts_download
-  ffhq256:
-    datapath_obs: '{global_cfg.root_obs}/keras/ffhq/downsample_ffhq_256x256.zip'
-    datapath: "datasets/ffhq/downsample_ffhq_256x256.zip"
-    overwrite: false
-    eval: true
-    unzip: true
-  ffhq256_list:
-    datapath_obs: '{global_cfg.root_obs}/keras/ffhq/ffhq_256.txt'
-    datapath: "datasets/ffhq/ffhq_256.txt"
-    overwrite: false
-    eval: true
-    unzip: false
-  fid_inception:
-    datapath_obs: '{global_cfg.root_obs}/keras/cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth'
-    datapath: "/home/ma-user/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth"
-    overwrite: false
-    eval: true
-    unzip: false
-
-
-G_cfg: &G_cfg
-  register_modules:
-  - tl2_lib.tl2.proj.pytorch.examples.gan_ddp.networks
-  name: tl2_lib.tl2.proj.pytorch.examples.gan_ddp.networks.Generator
-
-
-D_cfg: &D_cfg
-  register_modules:
-    - tl2_lib.tl2.proj.pytorch.examples.gan_ddp.networks
-  name: tl2_lib.tl2.proj.pytorch.examples.gan_ddp.networks.Discriminator
-
-
-data_cfg: &data_cfg
-  register_modules:
-    - tl2_lib.tl2.proj.pytorch.examples.gan_ddp.datasets
-  name: tl2_lib.tl2.proj.pytorch.examples.gan_ddp.datasets.CIFAR10
-  root: "datasets/cifar10"
-  img_size: 64
-
-
-train:
-  # resume
-  load_optimizers: true
-  load_G_ema: true
-  reset_best_fid: false
-  # log
-  print_every: 50
-  log_every: 50
-  log_every_start: 2000
-  save_every: 500
-  del_fid_real_images: true
-  num_images_real_eval: 2048
-  num_images_gen_eval: 2048
-  forward_bs: 10
-  # dataset
-  fixed_z_bs: 25
-  num_workers: 8
-  batch_size: 32
-  batch_split: 1
-  data_cfg: *data_cfg
-  # train
-  seed: 1234
-  ema_decay: 0.999
-  ema_start_itr: 1000
-  use_amp_D: false
-  use_amp_G: false
-  use_diffaug: false
-  d_reg_every: 1
-  r1_lambda: 10.
-  grad_clip: 10.
-  topk_interval: 2000
-#  topk_v: 0.6
-  topk_v: 0.
-  # optimizer:
-  gen_lr: 0.0002
-  disc_lr: 0.0002
-  betas: [0, 0.999]
-  G_cfg: *G_cfg
-  D_cfg: *D_cfg
-  root_obs: *root_obs
-  modelarts_download: *modelarts_download
+root_obs: &root_obs s3://bucket-3690/ZhouPeng
+modelarts_download: &modelarts_download
+  ffhq256:
+    datapath_obs: '{global_cfg.root_obs}/keras/ffhq/downsample_ffhq_256x256.zip'
+    datapath: "datasets/ffhq/downsample_ffhq_256x256.zip"
+    overwrite: false
+    eval: true
+    unzip: true
+  ffhq256_list:
+    datapath_obs: '{global_cfg.root_obs}/keras/ffhq/ffhq_256.txt'
+    datapath: "datasets/ffhq/ffhq_256.txt"
+    overwrite: false
+    eval: true
+    unzip: false
+  fid_inception:
+    datapath_obs: '{global_cfg.root_obs}/keras/cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth'
+    datapath: "/home/ma-user/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth"
+    overwrite: false
+    eval: true
+    unzip: false
+
+
+G_cfg: &G_cfg
+  register_modules:
+  - tl2_lib.tl2.proj.pytorch.examples.gan_ddp.networks
+  name: tl2_lib.tl2.proj.pytorch.examples.gan_ddp.networks.Generator
+
+
+D_cfg: &D_cfg
+  register_modules:
+    - tl2_lib.tl2.proj.pytorch.examples.gan_ddp.networks
+  name: tl2_lib.tl2.proj.pytorch.examples.gan_ddp.networks.Discriminator
+
+
+data_cfg: &data_cfg
+  register_modules:
+    - tl2_lib.tl2.proj.pytorch.examples.gan_ddp.datasets
+  name: tl2_lib.tl2.proj.pytorch.examples.gan_ddp.datasets.CIFAR10
+  root: "datasets/cifar10"
+  img_size: 64
+
+
+train:
+  # resume
+  load_optimizers: true
+  load_G_ema: true
+  reset_best_fid: false
+  # log
+  print_every: 50
+  log_every: 50
+  log_every_start: 2000
+  save_every: 500
+  del_fid_real_images: true
+  num_images_real_eval: 2048
+  num_images_gen_eval: 2048
+  forward_bs: 10
+  # dataset
+  fixed_z_bs: 25
+  num_workers: 8
+  batch_size: 32
+  batch_split: 1
+  data_cfg: *data_cfg
+  # train
+  seed: 1234
+  ema_decay: 0.999
+  ema_start_itr: 1000
+  use_amp_D: false
+  use_amp_G: false
+  use_diffaug: false
+  d_reg_every: 1
+  r1_lambda: 10.
+  grad_clip: 10.
+  topk_interval: 2000
+#  topk_v: 0.6
+  topk_v: 0.
+  # optimizer:
+  gen_lr: 0.0002
+  disc_lr: 0.0002
+  betas: [0, 0.999]
+  G_cfg: *G_cfg
+  D_cfg: *D_cfg
+  root_obs: *root_obs
+  modelarts_download: *modelarts_download
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/datasets.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/datasets.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,155 +1,155 @@
-import logging
-import os
-import glob
-import PIL
-import random
-import math
-import pickle
-import numpy as np
-
-import torch
-from torch.utils.data import DataLoader, Dataset
-import torchvision
-from torchvision.datasets import CIFAR10 as CIFAR10_base
-import torchvision.transforms as transforms
-
-from tl2.tl2_utils import read_image_list_from_files
-from tl2.proj.fvcore import MODEL_REGISTRY
-
-
-@MODEL_REGISTRY.register(name_prefix=__name__)
-class CIFAR10(CIFAR10_base):
-
-  def __init__(
-        self,
-        root: str,
-        img_size,
-        horizontal_flip=True,
-        train: bool = True,
-        transform=None,
-        download=True,
-  ):
-    super(CIFAR10, self).__init__(root=root, train=train, transform=transform, download=download)
-
-    if horizontal_flip:
-      self.transform = transforms.Compose(
-        [
-          transforms.ToTensor(),
-          transforms.Normalize([0.5], [0.5]),
-          transforms.RandomHorizontalFlip(p=0.5),
-          # transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),
-          transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR)
-        ])
-    else:
-      self.transform = transforms.Compose(
-        [
-          transforms.ToTensor(),
-          transforms.Normalize([0.5], [0.5]),
-          # transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),
-          transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR)
-        ])
-
-    logger = logging.getLogger('tl')
-    logger.info(f"\nNum of images ({self.__class__.__name__}):\n {len(self)}")
-
-    pass
-
-
-
-@MODEL_REGISTRY.register(name_prefix=__name__)
-class ImageList(Dataset):
-  """
-  python3 -m tl2.tools.get_data_list     \
-    --source_dir datasets/ffhq/downsample_ffhq_256x256/  \
-    --outfile datasets/ffhq/ffhq_256.txt  \
-    --ext *.png
-  """
-
-  def __init__(self,
-               img_size,
-               image_list_file="datasets/ffhq/ffhq_256.txt",
-               verbose=False,
-               horizontal_flip=True,
-               **kwargs):
-    super().__init__()
-
-    self.verbose = verbose
-    self.image_list = read_image_list_from_files(image_list_file, compress=True)
-
-    assert len(self.image_list) > 0, "Can't find data; make sure you specify the path to your dataset"
-    if horizontal_flip:
-      self.transform = transforms.Compose(
-        [
-          transforms.ToTensor(),
-          transforms.Normalize([0.5], [0.5]),
-          transforms.RandomHorizontalFlip(p=0.5),
-          # transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),
-          transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR)
-        ])
-    else:
-      self.transform = transforms.Compose(
-        [
-          transforms.ToTensor(),
-          transforms.Normalize([0.5], [0.5]),
-          # transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),
-          transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR)
-        ])
-
-    logger = logging.getLogger('tl')
-    logger.info(f"\nNum of images ({image_list_file}):\n {len(self)}")
-    pass
-
-  def __len__(self):
-    return len(self.image_list)
-
-  def __getitem__(self, index):
-    image_path = self.image_list[index]
-    X = PIL.Image.open(image_path)
-    X = self.transform(X)
-
-    if self.verbose:
-      return X, image_path
-    else:
-      return X, 0
-
-
-def get_dataset(name, subsample=None, batch_size=1, shuffle=True, **kwargs):
-    dataset = globals()[name](**kwargs)
-
-    dataloader = torch.utils.data.DataLoader(
-        dataset,
-        batch_size=batch_size,
-        shuffle=shuffle,
-        drop_last=True,
-        pin_memory=False,
-        num_workers=8
-    )
-    return dataloader, 3
-
-def get_dataloader_distributed(
-      dataset,
-      batch_size,
-      rank=0,
-      world_size=1,
-      num_workers=0,
-      shuffle=True,
-      drop_last=True,
-      **kwargs):
-
-    sampler = torch.utils.data.distributed.DistributedSampler(
-      dataset,
-      num_replicas=world_size,
-      rank=rank,
-      shuffle=shuffle,
-    )
-
-    dataloader = torch.utils.data.DataLoader(
-        dataset,
-        sampler=sampler,
-        batch_size=batch_size,
-        drop_last=drop_last,
-        pin_memory=True,
-        num_workers=num_workers,
-    )
-
-    return dataloader, sampler
+import logging
+import os
+import glob
+import PIL
+import random
+import math
+import pickle
+import numpy as np
+
+import torch
+from torch.utils.data import DataLoader, Dataset
+import torchvision
+from torchvision.datasets import CIFAR10 as CIFAR10_base
+import torchvision.transforms as transforms
+
+from tl2.tl2_utils import read_image_list_from_files
+from tl2.proj.fvcore import MODEL_REGISTRY
+
+
+@MODEL_REGISTRY.register(name_prefix=__name__)
+class CIFAR10(CIFAR10_base):
+
+  def __init__(
+        self,
+        root: str,
+        img_size,
+        horizontal_flip=True,
+        train: bool = True,
+        transform=None,
+        download=True,
+  ):
+    super(CIFAR10, self).__init__(root=root, train=train, transform=transform, download=download)
+
+    if horizontal_flip:
+      self.transform = transforms.Compose(
+        [
+          transforms.ToTensor(),
+          transforms.Normalize([0.5], [0.5]),
+          transforms.RandomHorizontalFlip(p=0.5),
+          # transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),
+          transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR)
+        ])
+    else:
+      self.transform = transforms.Compose(
+        [
+          transforms.ToTensor(),
+          transforms.Normalize([0.5], [0.5]),
+          # transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),
+          transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR)
+        ])
+
+    logger = logging.getLogger('tl')
+    logger.info(f"\nNum of images ({self.__class__.__name__}):\n {len(self)}")
+
+    pass
+
+
+
+@MODEL_REGISTRY.register(name_prefix=__name__)
+class ImageList(Dataset):
+  """
+  python3 -m tl2.tools.get_data_list     \
+    --source_dir datasets/ffhq/downsample_ffhq_256x256/  \
+    --outfile datasets/ffhq/ffhq_256.txt  \
+    --ext *.png
+  """
+
+  def __init__(self,
+               img_size,
+               image_list_file="datasets/ffhq/ffhq_256.txt",
+               verbose=False,
+               horizontal_flip=True,
+               **kwargs):
+    super().__init__()
+
+    self.verbose = verbose
+    self.image_list = read_image_list_from_files(image_list_file, compress=True)
+
+    assert len(self.image_list) > 0, "Can't find data; make sure you specify the path to your dataset"
+    if horizontal_flip:
+      self.transform = transforms.Compose(
+        [
+          transforms.ToTensor(),
+          transforms.Normalize([0.5], [0.5]),
+          transforms.RandomHorizontalFlip(p=0.5),
+          # transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),
+          transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR)
+        ])
+    else:
+      self.transform = transforms.Compose(
+        [
+          transforms.ToTensor(),
+          transforms.Normalize([0.5], [0.5]),
+          # transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),
+          transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR)
+        ])
+
+    logger = logging.getLogger('tl')
+    logger.info(f"\nNum of images ({image_list_file}):\n {len(self)}")
+    pass
+
+  def __len__(self):
+    return len(self.image_list)
+
+  def __getitem__(self, index):
+    image_path = self.image_list[index]
+    X = PIL.Image.open(image_path)
+    X = self.transform(X)
+
+    if self.verbose:
+      return X, image_path
+    else:
+      return X, 0
+
+
+def get_dataset(name, subsample=None, batch_size=1, shuffle=True, **kwargs):
+    dataset = globals()[name](**kwargs)
+
+    dataloader = torch.utils.data.DataLoader(
+        dataset,
+        batch_size=batch_size,
+        shuffle=shuffle,
+        drop_last=True,
+        pin_memory=False,
+        num_workers=8
+    )
+    return dataloader, 3
+
+def get_dataloader_distributed(
+      dataset,
+      batch_size,
+      rank=0,
+      world_size=1,
+      num_workers=0,
+      shuffle=True,
+      drop_last=True,
+      **kwargs):
+
+    sampler = torch.utils.data.distributed.DistributedSampler(
+      dataset,
+      num_replicas=world_size,
+      rank=rank,
+      shuffle=shuffle,
+    )
+
+    dataloader = torch.utils.data.DataLoader(
+        dataset,
+        sampler=sampler,
+        batch_size=batch_size,
+        drop_last=drop_last,
+        pin_memory=True,
+        num_workers=num_workers,
+    )
+
+    return dataloader, sampler
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/diff_aug.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/diff_aug.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,107 +1,107 @@
-"""
-Copyright (c) 2020, Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are met:
-
-* Redistributions of source code must retain the above copyright notice, this
-  list of conditions and the following disclaimer.
-
-* Redistributions in binary form must reproduce the above copyright notice,
-  this list of conditions and the following disclaimer in the documentation
-  and/or other materials provided with the distribution.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
-AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-"""
-
-
-import logging
-
-import torch
-import torch.nn.functional as F
-
-
-
-### Differentiable Augmentation for Data-Efficient GAN Training (https://arxiv.org/abs/2006.10738)
-### Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han
-### https://github.com/mit-han-lab/data-efficient-gans
-
-
-def DiffAugment(x, policy='color,translation,cutout', channels_first=True):
-    if policy:
-        if not channels_first:
-            x = x.permute(0, 3, 1, 2)
-        for p in policy.split(','):
-            for f in AUGMENT_FNS[p]:
-                x = f(x)
-        if not channels_first:
-            x = x.permute(0, 2, 3, 1)
-        x = x.contiguous()
-    return x
-
-
-def rand_brightness(x):
-    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)
-    return x
-
-
-def rand_saturation(x):
-    x_mean = x.mean(dim=1, keepdim=True)
-    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean
-    return x
-
-
-def rand_contrast(x):
-    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)
-    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean
-    return x
-
-
-def rand_translation(x, ratio=0.125):
-    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)
-    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)
-    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)
-    grid_batch, grid_x, grid_y = torch.meshgrid(
-        torch.arange(x.size(0), dtype=torch.long, device=x.device),
-        torch.arange(x.size(2), dtype=torch.long, device=x.device),
-        torch.arange(x.size(3), dtype=torch.long, device=x.device),
-    )
-    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)
-    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)
-    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])
-    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)
-    return x
-
-
-def rand_cutout(x, ratio=0.5):
-    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)
-    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)
-    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)
-    grid_batch, grid_x, grid_y = torch.meshgrid(
-        torch.arange(x.size(0), dtype=torch.long, device=x.device),
-        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),
-        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),
-    )
-    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)
-    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)
-    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)
-    mask[grid_batch, grid_x, grid_y] = 0
-    x = x * mask.unsqueeze(1)
-    return x
-
-
-AUGMENT_FNS = {
-    'color': [rand_brightness, rand_saturation, rand_contrast],
-    'translation': [rand_translation],
-    'cutout': [rand_cutout],
-}
+"""
+Copyright (c) 2020, Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+* Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+* Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+"""
+
+
+import logging
+
+import torch
+import torch.nn.functional as F
+
+
+
+### Differentiable Augmentation for Data-Efficient GAN Training (https://arxiv.org/abs/2006.10738)
+### Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han
+### https://github.com/mit-han-lab/data-efficient-gans
+
+
+def DiffAugment(x, policy='color,translation,cutout', channels_first=True):
+    if policy:
+        if not channels_first:
+            x = x.permute(0, 3, 1, 2)
+        for p in policy.split(','):
+            for f in AUGMENT_FNS[p]:
+                x = f(x)
+        if not channels_first:
+            x = x.permute(0, 2, 3, 1)
+        x = x.contiguous()
+    return x
+
+
+def rand_brightness(x):
+    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)
+    return x
+
+
+def rand_saturation(x):
+    x_mean = x.mean(dim=1, keepdim=True)
+    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean
+    return x
+
+
+def rand_contrast(x):
+    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)
+    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean
+    return x
+
+
+def rand_translation(x, ratio=0.125):
+    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)
+    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)
+    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)
+    grid_batch, grid_x, grid_y = torch.meshgrid(
+        torch.arange(x.size(0), dtype=torch.long, device=x.device),
+        torch.arange(x.size(2), dtype=torch.long, device=x.device),
+        torch.arange(x.size(3), dtype=torch.long, device=x.device),
+    )
+    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)
+    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)
+    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])
+    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)
+    return x
+
+
+def rand_cutout(x, ratio=0.5):
+    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)
+    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)
+    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)
+    grid_batch, grid_x, grid_y = torch.meshgrid(
+        torch.arange(x.size(0), dtype=torch.long, device=x.device),
+        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),
+        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),
+    )
+    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)
+    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)
+    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)
+    mask[grid_batch, grid_x, grid_y] = 0
+    x = x * mask.unsqueeze(1)
+    return x
+
+
+AUGMENT_FNS = {
+    'color': [rand_brightness, rand_saturation, rand_contrast],
+    'translation': [rand_translation],
+    'cutout': [rand_cutout],
+}
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/networks.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/networks.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,111 +1,111 @@
-import logging
-
-import torch
-import torch.nn as nn
-
-from tl2.proj.fvcore import MODEL_REGISTRY, global_cfg
-from tl2.proj.pytorch import torch_utils
-from tl2.proj.pytorch.pytorch_hook import VerboseModel
-
-
-@MODEL_REGISTRY.register(name_prefix=__name__)
-class Generator(nn.Module):
-  def __init__(self,
-               nz=100,
-               ngf=64,
-               nc=3,
-               ):
-    super(Generator, self).__init__()
-
-    self.nz = nz
-
-    self.main = nn.Sequential(
-      # input is Z, going into a convolution
-      nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),
-      nn.BatchNorm2d(ngf * 8),
-      nn.ReLU(True),
-      # state size. (ngf*8) x 4 x 4
-      nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
-      nn.BatchNorm2d(ngf * 4),
-      nn.ReLU(True),
-      # state size. (ngf*4) x 8 x 8
-      nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
-      nn.BatchNorm2d(ngf * 2),
-      nn.ReLU(True),
-      # state size. (ngf*2) x 16 x 16
-      nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
-      nn.BatchNorm2d(ngf),
-      nn.ReLU(True),
-      # state size. (ngf) x 32 x 32
-      nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),
-      nn.Tanh()
-      # state size. (nc) x 64 x 64
-    )
-
-    logger = logging.getLogger('tl')
-    torch_utils.print_number_params(
-      models_dict={
-        'G': self
-      }, logger=logger)
-    pass
-
-  def forward(self, input):
-    if global_cfg.tl_debug:
-      VerboseModel.forward_verbose(model=self.main,
-                                   inputs_args=(input, ),
-                                   name_prefix='G.main.')
-
-    output = self.main(input)
-    return output
-
-  def get_zs(self, bs, device='cuda'):
-    zs = torch.randn(bs, self.nz, 1, 1, device=device)
-    return zs
-
-
-@MODEL_REGISTRY.register(name_prefix=__name__)
-class Discriminator(nn.Module):
-  def __init__(self,
-               nc=3,
-               ndf=64,
-               ):
-    super(Discriminator, self).__init__()
-
-    self.main = nn.Sequential(
-      # input is (nc) x 64 x 64
-      nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
-      nn.LeakyReLU(0.2, inplace=True),
-      # state size. (ndf) x 32 x 32
-      nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
-      nn.BatchNorm2d(ndf * 2),
-      nn.LeakyReLU(0.2, inplace=True),
-      # state size. (ndf*2) x 16 x 16
-      nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
-      nn.BatchNorm2d(ndf * 4),
-      nn.LeakyReLU(0.2, inplace=True),
-      # state size. (ndf*4) x 8 x 8
-      nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
-      nn.BatchNorm2d(ndf * 8),
-      nn.LeakyReLU(0.2, inplace=True),
-      # state size. (ndf*8) x 4 x 4
-      nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
-      # nn.Sigmoid()
-    )
-
-    logger = logging.getLogger('tl')
-    torch_utils.print_number_params(
-      models_dict={
-        'D': self
-      }, logger=logger)
-    pass
-
-  def forward(self, input):
-    if global_cfg.tl_debug:
-      VerboseModel.forward_verbose(model=self.main,
-                                   inputs_args=(input,),
-                                   name_prefix='D.main.')
-    output = self.main(input)
-    # out = output.view(-1, 1).squeeze(1)
-    out = output.squeeze()
-
-    return out
+import logging
+
+import torch
+import torch.nn as nn
+
+from tl2.proj.fvcore import MODEL_REGISTRY, global_cfg
+from tl2.proj.pytorch import torch_utils
+from tl2.proj.pytorch.pytorch_hook import VerboseModel
+
+
+@MODEL_REGISTRY.register(name_prefix=__name__)
+class Generator(nn.Module):
+  def __init__(self,
+               nz=100,
+               ngf=64,
+               nc=3,
+               ):
+    super(Generator, self).__init__()
+
+    self.nz = nz
+
+    self.main = nn.Sequential(
+      # input is Z, going into a convolution
+      nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),
+      nn.BatchNorm2d(ngf * 8),
+      nn.ReLU(True),
+      # state size. (ngf*8) x 4 x 4
+      nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
+      nn.BatchNorm2d(ngf * 4),
+      nn.ReLU(True),
+      # state size. (ngf*4) x 8 x 8
+      nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
+      nn.BatchNorm2d(ngf * 2),
+      nn.ReLU(True),
+      # state size. (ngf*2) x 16 x 16
+      nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
+      nn.BatchNorm2d(ngf),
+      nn.ReLU(True),
+      # state size. (ngf) x 32 x 32
+      nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),
+      nn.Tanh()
+      # state size. (nc) x 64 x 64
+    )
+
+    logger = logging.getLogger('tl')
+    torch_utils.print_number_params(
+      models_dict={
+        'G': self
+      }, logger=logger)
+    pass
+
+  def forward(self, input):
+    if global_cfg.tl_debug:
+      VerboseModel.forward_verbose(model=self.main,
+                                   inputs_args=(input, ),
+                                   name_prefix='G.main.')
+
+    output = self.main(input)
+    return output
+
+  def get_zs(self, bs, device='cuda'):
+    zs = torch.randn(bs, self.nz, 1, 1, device=device)
+    return zs
+
+
+@MODEL_REGISTRY.register(name_prefix=__name__)
+class Discriminator(nn.Module):
+  def __init__(self,
+               nc=3,
+               ndf=64,
+               ):
+    super(Discriminator, self).__init__()
+
+    self.main = nn.Sequential(
+      # input is (nc) x 64 x 64
+      nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
+      nn.LeakyReLU(0.2, inplace=True),
+      # state size. (ndf) x 32 x 32
+      nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
+      nn.BatchNorm2d(ndf * 2),
+      nn.LeakyReLU(0.2, inplace=True),
+      # state size. (ndf*2) x 16 x 16
+      nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
+      nn.BatchNorm2d(ndf * 4),
+      nn.LeakyReLU(0.2, inplace=True),
+      # state size. (ndf*4) x 8 x 8
+      nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
+      nn.BatchNorm2d(ndf * 8),
+      nn.LeakyReLU(0.2, inplace=True),
+      # state size. (ndf*8) x 4 x 4
+      nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
+      # nn.Sigmoid()
+    )
+
+    logger = logging.getLogger('tl')
+    torch_utils.print_number_params(
+      models_dict={
+        'D': self
+      }, logger=logger)
+    pass
+
+  def forward(self, input):
+    if global_cfg.tl_debug:
+      VerboseModel.forward_verbose(model=self.main,
+                                   inputs_args=(input,),
+                                   name_prefix='D.main.')
+    output = self.main(input)
+    # out = output.view(-1, 1).squeeze(1)
+    out = output.squeeze()
+
+    return out
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/test_gan_ddp.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/multi_process_main/test_multi_process_main.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,109 +1,115 @@
-import time
-from pathlib import Path
+
+
+
+
+import json
+import numpy as np
 import os
 import sys
 import unittest
 import argparse
 
 
-class Testing_gan_ddp(unittest.TestCase):
+class Testing_Multi_Process_Main(unittest.TestCase):
 
-  def test_train(self, debug=True):
+  def test_multi_process_main(self, debug=True):
     """
     Usage:
 
         # export CUDA_VISIBLE_DEVICES=$cuda_devices
         # export RUN_NUM=$run_num
 
-        export CUDA_VISIBLE_DEVICES=7
-        export TIME_STR=0
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
         export PORT=12345
-        export PYTHONPATH=.:./piGAN_lib
-        python -c "from tl2_lib.tl2.proj.pytorch.examples.gan_ddp.test_gan_ddp import Testing_gan_ddp;\
-          Testing_gan_ddp().test_train(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/       \
-            use_amp_D False use_amp_G False \
-            del_fid_real_images True num_images_real_eval 2048 num_images_gen_eval 2048 \
-            save_every 1000 \
-          --tl_outdir results/gan_ddp/train-20211129_164456_187
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
 
     :return:
     """
     if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'
     if 'TIME_STR' not in os.environ:
       os.environ['TIME_STR'] = '0'
     if 'RUN_NUM' not in os.environ:
       os.environ['RUN_NUM'] = '0'
     from tl2 import tl2_utils
     from tl2.launch.launch_utils import \
       (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
 
     tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
     tl_opts = ' '.join(tl_opts_list)
     print(f'tl_opts:\n {tl_opts}')
 
     if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/gan_ddp/train-20211129_164456_187'])
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
       pass
     command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
     resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
              tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
     argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/gan_ddp/config.yaml
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/multi_process_main/config.yaml
                 --tl_command {command}
                 --tl_outdir {outdir}
                 {"--tl_resume --tl_resumedir " + outdir if resume else ""}
                 --tl_opts {tl_opts}
                 """
     args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
 
     if int(os.environ['RUN_NUM']) > 0:
       run_command = f"""
-                      python -c "from tl2.modelarts.tests.test_run import TestingRun;\
-                            TestingRun().test_run_v2(number={os.environ['RUN_NUM']}, )" \
-                            --tl_opts root_obs {cfg.root_obs}
-                      """
+                  python -c "from tl2.modelarts.tests.test_run import TestingRun;\
+                        TestingRun().test_run_v2(number={os.environ['RUN_NUM']}, )" \
+                        --tl_opts root_obs {cfg.root_obs}
+                  """
       p = tl2_utils.Worker(name='Run', args=(run_command,))
       p.start()
 
-    os.environ['PATH'] = f"{os.path.dirname(sys.executable)}:{os.environ['PATH']}"
-    os.environ['TORCH_EXTENSIONS_DIR'] = "torch_extensions"
-
     n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
     PORT = os.environ.get('PORT', 8888)
 
+    # if 'CUDA_HOME' not in os.environ: os.environ['CUDA_HOME'] = "/usr/local/cuda-10.2/"
+    # os.environ['PATH'] = f"{os.path.dirname(sys.executable)}:{os.environ['PATH']}"
+    # os.environ['DNNLIB_CACHE_DIR'] = "datasets/pretrained"
+    # os.environ['TORCH_EXTENSIONS_DIR'] = "torch_extensions"
+
+    if n_gpus > 1:
+      python_str = f"python -m torch.distributed.launch --nproc_per_node={n_gpus} --master_port={PORT} "
+    else:
+      python_str = "python "
+      
     cmd_str = f"""
-        python
-        tl2_lib/tl2/proj/pytorch/examples/gan_ddp/train.py
-        --port {PORT}
-        --modelarts True
-        --outdir {outdir}
+        {python_str}
+        tl2_lib/tl2/proj/pytorch/examples/multi_process_main/main.py
         {get_append_cmd_str(args)}
         """
     if debug:
       cmd_str += f"""
                   --tl_debug
                   --tl_opts 
-                    num_workers 0                    
-                    num_images_real_eval 10
-                    num_images_gen_eval 2
                   """
     else:
       cmd_str += f"""
+                  --num_workers {n_gpus}
                   --tl_opts {tl_opts}
-                    num_workers {n_gpus}
                   """
     start_cmd_run(cmd_str)
     # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
     # from tl2.modelarts import modelarts_utils
     # update_parser_defaults_from_yaml(parser)
 
     # modelarts_utils.setup_tl_outdir_obs(global_cfg)
     # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
     # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
     #
     # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
     # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
     pass
 
+
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/gan_ddp/train.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/train.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,692 +1,692 @@
-import collections
-import shutil
-import traceback
-import pprint
-import logging
-import argparse
-import os
-import numpy as np
-import math
-from tqdm import tqdm
-import copy
-
-import torch
-import torch.distributed as dist
-import torch.multiprocessing as mp
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.nn.parallel import DistributedDataParallel as DDP
-from torchvision.utils import save_image, make_grid
-import torchvision.transforms.functional as trans_f
-
-from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
-from tl2.modelarts import modelarts_utils
-from tl2.proj.fvcore import build_model
-from tl2.proj.logger.textlogger import summary_dict2txtfig, summary_defaultdict2txtfig, global_textlogger
-from tl2 import tl2_utils
-from tl2.proj.pytorch import torch_utils
-from tl2.proj.pytorch.ddp import ddp_utils
-from tl2.proj.argparser import argparser_utils
-from tl2.proj.pytorch.examples.gan_ddp import comm_utils
-from tl2.proj.pytorch.examples.gan_ddp import datasets
-from tl2.proj.pytorch.examples.gan_ddp import diff_aug
-
-from torch_fidelity import calculate_metrics
-
-
-def setup_ddp(rank, world_size, port):
-  os.environ['MASTER_ADDR'] = 'localhost'
-  os.environ['MASTER_PORT'] = port
-
-  # initialize the process group
-  # dist.init_process_group("gloo", rank=rank, world_size=world_size)
-  dist.init_process_group("nccl", rank=rank, world_size=world_size)
-  dist.barrier()
-  pass
-
-def cleanup():
-  dist.destroy_process_group()
-
-
-def to_range01(img):
-  """
-
-  :param img: [-1, 1]
-  :return:
-  """
-  img = (img + 1) * 0.5
-  return img
-
-
-def build_model_and_resume(
-      resume_dir,
-      rank,
-      logger,
-      device):
-  state_dict = {
-    'best_fid': np.inf,
-    'iters': 0,
-    'epochs': 0,
-  }
-  generator = build_model(cfg=global_cfg.G_cfg).to(device)
-  discriminator = build_model(cfg=global_cfg.D_cfg).to(device)
-  G_ema = copy.deepcopy(generator)
-  ema_model = comm_utils.EMA(
-    source=generator, target=G_ema, decay=global_cfg.ema_decay, start_itr=global_cfg.ema_start_itr)
-
-  if global_cfg.tl_resume:
-    model_dict = {
-      'generator': generator,
-      'discriminator': discriminator,
-      'state_dict': state_dict,
-    }
-    if global_cfg.load_G_ema:
-      model_dict['G_ema'] = G_ema
-    torch_utils.load_models(save_dir=resume_dir, model_dict=model_dict, strict=False, rank=rank)
-    if global_cfg.load_G_ema:
-      ema_model.update_target_dict(G_ema.state_dict())
-    else:
-      ema_model.update_target_dict(generator.state_dict())
-
-    if global_cfg.reset_best_fid:
-      state_dict['best_fid'] = np.inf
-    logger.info(pprint.pformat(state_dict))
-
-  return generator, discriminator, G_ema, ema_model, state_dict
-
-
-def build_optimizer(G, D):
-
-  optimizer_G = torch.optim.Adam(
-    params=[{'params': G.parameters(),
-             'initial_lr': global_cfg.gen_lr}],
-    lr=global_cfg.gen_lr,
-    betas=global_cfg.betas,
-    weight_decay=0)
-  optimizer_D = torch.optim.Adam(
-    params=[{'params': D.parameters(),
-             'initial_lr': global_cfg.disc_lr}],
-    lr=global_cfg.disc_lr,
-    betas=global_cfg.betas,
-    weight_decay=0)
-
-  return optimizer_G, optimizer_D
-
-def build_optimizer_and_resume(generator_ddp,
-                               discriminator_ddp,
-                               resume_dir,
-                               rank):
-  optimizer_G, optimizer_D = build_optimizer(G=generator_ddp, D=discriminator_ddp)
-
-  scaler_G = torch.cuda.amp.GradScaler(enabled=global_cfg.use_amp_G)
-  scaler_D = torch.cuda.amp.GradScaler(enabled=global_cfg.use_amp_D)
-
-  if global_cfg.tl_resume and global_cfg.load_optimizers:
-    model_dict = {
-      'optimizer_G': optimizer_G,
-      'optimizer_D': optimizer_D,
-      'scaler_G': scaler_G,
-      'scaler_D': scaler_D,
-    }
-    torch_utils.load_models(save_dir=resume_dir, model_dict=model_dict, strict=False, rank=rank)
-
-    optimizer_G.param_groups[0]['lr'] = global_cfg.gen_lr
-    optimizer_D.param_groups[0]['lr'] = global_cfg.disc_lr
-
-  return optimizer_G, optimizer_D, scaler_G, scaler_D
-
-
-def setup_eval_dataset(dataset,
-                       saved_dir,
-                       num_imgs,
-                       del_fid_real_images,
-                       world_size,
-                       rank,
-                       batch_size=100):
-
-  if del_fid_real_images and rank == 0:
-    shutil.rmtree(saved_dir, ignore_errors=True)
-
-  # Only make real images if they haven't been made yet
-  logger = logging.getLogger('tl')
-
-  # important
-  ddp_utils.d2_synchronize()
-
-  if not os.path.exists(saved_dir):
-    if rank == 0:
-      os.makedirs(saved_dir, exist_ok=True)
-      pbar = tqdm(total=num_imgs, desc=f"Outputting real images for eval: {saved_dir}")
-
-    ddp_utils.d2_synchronize()
-
-    dataloader, sampler = datasets.get_dataloader_distributed(
-      dataset=dataset,
-      batch_size=batch_size,
-      world_size=world_size,
-      rank=rank,
-      num_workers=0,
-      shuffle=False,
-      drop_last=False
-    )
-    count = 0
-    for idx, (imgs, _) in enumerate(dataloader):
-      if count >= num_imgs:
-        break
-      if rank == 0:
-        pbar.update(world_size * batch_size)
-
-      # img = (img.squeeze() + 1) * 0.5
-      imgs = to_range01(img=imgs)
-
-      for sub_idx, img in enumerate(imgs):
-        img_pil = trans_f.to_pil_image(img)
-
-        number = idx * world_size * batch_size + rank * batch_size + sub_idx
-        img_pil.save(f"{saved_dir}/{number:06d}.png")
-      count += world_size * batch_size
-  else:
-    logger.info("Real images exist.")
-
-  ddp_utils.d2_synchronize()
-  pass
-
-
-def setup_gen_images(generator,
-                     rank,
-                     world_size,
-                     fake_dir,
-                     num_imgs=2048,
-                     img_size=128):
-  if rank == 0:
-    shutil.rmtree(fake_dir, ignore_errors=True)
-    os.makedirs(fake_dir, exist_ok=True)
-
-  ddp_utils.d2_synchronize()
-
-  generator.eval()
-
-  if rank == 0:
-    pbar = tqdm(desc=f"Generating images at {img_size}x{img_size} {fake_dir}", total=num_imgs)
-
-  with torch.no_grad():
-    img_counter = 0
-    count = 0
-    while img_counter < num_imgs:
-      if rank == 0:
-        pbar.update(world_size)
-
-      zs = generator.get_zs(1)
-      img = generator(zs)
-
-      # img = (img.squeeze() + 1.) * 0.5
-      img = to_range01(img=img.squeeze())
-      img_pil = trans_f.to_pil_image(img)
-
-      number = count * world_size + rank
-      img_pil.save(f"{fake_dir}/{number:06d}.png")
-
-      img_counter += world_size
-      count += 1
-
-  if rank == 0: pbar.close()
-
-  ddp_utils.d2_synchronize()
-  pass
-
-
-def calculate_fid(real_dir,
-                  fake_dir):
-
-  metrics_dict = calculate_metrics(input1=real_dir,
-                                   input2=fake_dir,
-                                   cuda=True,
-                                   isc=False,
-                                   fid=True,
-                                   kid=False,
-                                   verbose=False)
-  fid = metrics_dict['frechet_inception_distance']
-
-  # torch.cuda.empty_cache()
-
-  return fid
-
-
-def saved_models_and_images(G_ema,
-                            generator,
-                            discriminator,
-                            optimizer_G,
-                            optimizer_D,
-                            scaler_G,
-                            scaler_D,
-                            state_dict,
-                            fixed_z,
-                            saved_dir=None,
-                            metadata=None,
-                            forward_bs=4,
-                            info_msg=""):
-  model_dict = {
-    'G_ema': G_ema,
-    'generator': generator,
-    'discriminator': discriminator,
-    'optimizer_G': optimizer_G,
-    'optimizer_D': optimizer_D,
-    'scaler_G': scaler_G,
-    'scaler_D': scaler_D,
-    'state_dict': state_dict,
-  }
-
-  info_msg = f"epoch: {state_dict['epochs']}\n" \
-             f"step: {state_dict['iters']}\n" + info_msg
-  if saved_dir is None:
-    ckpt_max2keep = tl2_utils.MaxToKeep.get_named_max_to_keep(name='ckpt', max_to_keep=4, use_circle_number=True)
-    saved_dir = ckpt_max2keep.step_and_ret_circle_dir(global_cfg.tl_ckptdir, info_msg=info_msg)
-
-  os.makedirs(saved_dir, exist_ok=True)
-
-  # save meta and global_cfg
-  if metadata is not None:
-    tl2_utils.json_dump(metadata, f"{saved_dir}/metadata.json")
-  global_cfg.dump_to_file_with_command(f"{saved_dir}/config_command.yaml", global_cfg.tl_command)
-
-  torch_utils.save_models(save_dir=saved_dir, model_dict=model_dict, info_msg=info_msg)
-
-  save_images(G_ema=G_ema, fixed_z=fixed_z, saved_dir=f"{saved_dir}/imgs", forward_bs=forward_bs)
-  torch.cuda.empty_cache()
-
-  pass
-
-
-@torch.no_grad()
-def save_images(G_ema,
-                fixed_z,
-                saved_dir,
-                forward_bs=4):
-  os.makedirs(saved_dir, exist_ok=True)
-  G_ema.eval()
-  bs = len(fixed_z)
-
-  num_iters = (bs + forward_bs - 1) // forward_bs
-
-  with torch.cuda.amp.autocast(global_cfg.use_amp_G):
-    imgs = []
-    for idx in range(num_iters):
-      z = fixed_z[idx * forward_bs : (idx + 1) * forward_bs]
-      img = G_ema(z)
-      imgs.append(img)
-
-  imgs = torch.cat(imgs, dim=0)
-
-  imgs = to_range01(img=imgs)
-
-  save_image(imgs, f"{saved_dir}/G_ema_z.png", nrow=int(math.sqrt(bs)), normalize=False)
-  pass
-
-
-def main(rank,
-         world_size,
-         opt,
-         ):
-  if world_size > 1:
-    setup_ddp(rank, world_size, opt.port)
-
-  torch.cuda.set_device(rank)
-  device = torch.device(rank)
-
-  update_parser_defaults_from_yaml(parser=None, is_main_process=(rank == 0))
-  if rank == 0 and opt.modelarts:
-    modelarts_utils.setup_tl_outdir_obs(global_cfg, unzip_code=False)
-    modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
-
-  logger = logging.getLogger('tl')
-  torch_utils.init_seeds(seed=global_cfg.seed, rank=rank)
-
-  resume_dir = f"{global_cfg.tl_resumedir}/ckptdir/resume"
-  generator, discriminator, G_ema, ema_model, state_dict = build_model_and_resume(
-    resume_dir=resume_dir, logger=logger, device=device, rank=rank)
-
-  if world_size > 1:
-    generator_ddp = DDP(generator, device_ids=[rank], find_unused_parameters=True, broadcast_buffers=False)
-    discriminator_ddp = DDP(discriminator, device_ids=[rank], find_unused_parameters=True, broadcast_buffers=False)
-    generator = generator_ddp.module
-    discriminator = discriminator_ddp.module
-  else:
-    generator_ddp = generator
-    discriminator_ddp = discriminator
-
-  optimizer_G, optimizer_D, scaler_G, scaler_D = build_optimizer_and_resume(
-    generator_ddp=generator_ddp, discriminator_ddp=discriminator_ddp, resume_dir=resume_dir, rank=rank)
-
-  # ----------
-  #  Training
-  # ----------
-
-  # dataset
-  dataset = build_model(cfg=global_cfg.data_cfg)
-  img, _ = dataset[0]
-  image_size = img.shape[-1]
-  dataloader, sampler = datasets.get_dataloader_distributed(
-    dataset=dataset,
-    batch_size=global_cfg.batch_size,
-    world_size=world_size,
-    rank=rank,
-    num_workers=global_cfg.num_workers,
-    shuffle=True,
-  )
-  assert global_cfg.batch_size % global_cfg.batch_split == 0
-
-  fixed_z = generator.get_zs(global_cfg.fixed_z_bs)
-  use_diffaug = global_cfg.use_diffaug
-  dummy_tensor = torch.tensor([0], device=device)
-
-  if rank == 0:
-    num_iters = (len(dataset) // world_size + global_cfg.batch_size - 1) // global_cfg.batch_size
-    pbar_iters = tqdm(total=num_iters)
-
-  out_dict = collections.defaultdict(dict)
-  # loop epochs
-  while True:
-    sampler.set_epoch(epoch=state_dict['epochs'])
-    if rank == 0:
-      pbar_iters.reset()
-
-    for i, (imgs, _) in enumerate(dataloader):
-
-      ddp_utils.d2_synchronize()
-
-      out_dict.clear()
-      out_dict['epoch']['epoch'] = state_dict['epochs']
-      out_dict['iter']['iter'] = state_dict['iters']
-
-      generator_ddp.train()
-      discriminator_ddp.train()
-
-      real_imgs = imgs.to(device, non_blocking=True)
-      cur_bs = real_imgs.shape[0]
-
-      # TRAIN DISCRIMINATOR
-      torch_utils.requires_grad(generator_ddp, False)
-      torch_utils.requires_grad(discriminator_ddp, True)
-
-      with torch.cuda.amp.autocast(global_cfg.use_amp_D):
-        # Generate images for discriminator training
-        with torch.no_grad():
-          zs = generator.get_zs(cur_bs)
-          split_batch_size = cur_bs // global_cfg.batch_split
-          gen_imgs = []
-          for split in range(global_cfg.batch_split):
-            subset_z = zs[split * split_batch_size : (split+1) * split_batch_size]
-            g_imgs = generator_ddp(subset_z)
-            gen_imgs.append(g_imgs)
-
-          gen_imgs = torch.cat(gen_imgs, axis=0)
-          if use_diffaug:
-            gen_imgs = diff_aug.DiffAugment(gen_imgs)
-
-        # real_imgs.requires_grad = True
-        if use_diffaug:
-          real_imgs = diff_aug.DiffAugment(real_imgs)
-        real_imgs.requires_grad_()
-        d_real_logits = discriminator_ddp(real_imgs)
-
-      d_regularize = i % global_cfg.d_reg_every == 0
-
-      # Gradient penalty
-      if global_cfg.r1_lambda > 0 and d_regularize:
-        grad_real = torch.autograd.grad(
-          outputs=scaler_D.scale(d_real_logits.sum()), inputs=real_imgs, create_graph=True)
-        inv_scale = 1. / scaler_D.get_scale()
-        grad_real = [p * inv_scale for p in grad_real][0]
-      with torch.cuda.amp.autocast(global_cfg.use_amp_D):
-        if global_cfg.r1_lambda > 0 and d_regularize:
-          grad_penalty = (grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2).mean()
-          grad_penalty = 0.5 * global_cfg.r1_lambda * global_cfg.d_reg_every * grad_penalty + 0 * d_real_logits[0]
-        else:
-          grad_penalty = dummy_tensor
-
-        d_fake_logits = discriminator_ddp(gen_imgs)
-
-        with torch.no_grad():
-          D_real_logits = d_real_logits.mean().item()
-          D_fake_logits = d_fake_logits.mean().item()
-          out_dict['D_logits']['D_real_logits'] = D_real_logits
-          out_dict['D_logits']['D_fake_logits'] = D_fake_logits
-
-        D_real_logits_loss = torch.nn.functional.softplus(-d_real_logits).mean()
-        D_fake_logits_loss = torch.nn.functional.softplus(d_fake_logits).mean()
-
-        d_loss = D_real_logits_loss + D_fake_logits_loss + grad_penalty
-
-        out_dict['D_logits_loss']['D_real_logits_loss'] = D_real_logits_loss.item()
-        out_dict['D_logits_loss']['D_fake_logits_loss'] = D_fake_logits_loss.item()
-        out_dict['grad_penalty']['grad_penalty'] = grad_penalty.item()
-        out_dict['d_loss']['d_loss'] = d_loss.item()
-
-      optimizer_D.zero_grad()
-      scaler_D.scale(d_loss).backward()
-      scaler_D.unscale_(optimizer_D)
-      try:
-        D_total_norm = torch.nn.utils.clip_grad_norm_(discriminator_ddp.parameters(), global_cfg.grad_clip,
-                                                      # error_if_nonfinite=True, # torch >= 1.9
-                                                      )
-        out_dict['norm']['D_total_norm'] = D_total_norm.item()
-      except:
-        logger.info(traceback.format_exc())
-        saved_models_and_images(G_ema=G_ema,
-                                generator=generator,
-                                discriminator=discriminator,
-                                optimizer_G=optimizer_G,
-                                optimizer_D=optimizer_D,
-                                scaler_G=scaler_G,
-                                scaler_D=scaler_D,
-                                state_dict=state_dict,
-                                fixed_z=fixed_z,
-                                saved_dir=f"{global_cfg.tl_ckptdir}/D_crupted",
-                                forward_bs=global_cfg.forward_bs)
-        optimizer_D.zero_grad()
-        D_total_norm = -1
-        out_dict['norm']['D_total_norm'] = D_total_norm
-        # exit(0)
-      if D_total_norm > 0:
-        scaler_D.step(optimizer_D)
-      scaler_D.update()
-
-      # TRAIN GENERATOR
-      torch_utils.requires_grad(generator_ddp, True)
-      torch_utils.requires_grad(discriminator_ddp, False)
-
-      zs = generator.get_zs(cur_bs)
-      split_batch_size = cur_bs // global_cfg.batch_split
-
-      optimizer_G.zero_grad()
-      for split in range(global_cfg.batch_split):
-        with torch.cuda.amp.autocast(global_cfg.use_amp_G):
-          subset_z = zs[split * split_batch_size : (split + 1) * split_batch_size]
-          gen_imgs = generator_ddp(subset_z).to(torch.float32)
-          if use_diffaug:
-            gen_imgs = diff_aug.DiffAugment(gen_imgs)
-
-        with torch.cuda.amp.autocast(global_cfg.use_amp_D):
-          g_logits = discriminator_ddp(gen_imgs)
-
-        if global_cfg.topk_v > 0:
-          topk_percentage = 0.99 ** (state_dict['iters'] / global_cfg.topk_interval)
-          topk_percentage = max(topk_percentage, global_cfg.topk_v)
-          topk_num = math.ceil(topk_percentage * cur_bs)
-          g_logits = torch.topk(g_logits, topk_num, dim=0).values
-        else:
-          topk_num = cur_bs
-
-        G_fake_logits_loss = torch.nn.functional.softplus(-g_logits).mean()
-
-        scaler_G.scale(G_fake_logits_loss).backward()
-
-      scaler_G.unscale_(optimizer_G)
-
-      with torch.no_grad():
-        G_fake_logits = g_logits.mean().item()
-        out_dict['D_logits']['G_fake_logits'] = G_fake_logits
-
-      out_dict['D_logits_loss']['G_fake_logits_loss'] = G_fake_logits_loss.item()
-      out_dict['topk_num']['topk_num'] = topk_num
-
-      try:
-        G_total_norm = torch.nn.utils.clip_grad_norm_(generator_ddp.parameters(), global_cfg.grad_clip,
-                                                      # error_if_nonfinite=True, # torch >= 1.9
-                                                      )
-        out_dict['norm']['G_total_norm'] = G_total_norm.item()
-      except:
-        logger.info(traceback.format_exc())
-        saved_models_and_images(G_ema=G_ema,
-                                generator=generator,
-                                discriminator=discriminator,
-                                optimizer_G=optimizer_G,
-                                optimizer_D=optimizer_D,
-                                scaler_G=scaler_G,
-                                scaler_D=scaler_D,
-                                state_dict=state_dict,
-                                fixed_z=fixed_z,
-                                saved_dir=f"{global_cfg.tl_ckptdir}/G_crupted",
-                                forward_bs=global_cfg.forward_bs)
-        G_total_norm = -1
-        out_dict['norm']['G_total_norm'] = G_total_norm
-        optimizer_G.zero_grad()
-        # exit(0)
-
-      if G_total_norm > 0:
-        scaler_G.step(optimizer_G)
-      scaler_G.update()
-
-      # update ema
-      ema_model.update(itr=state_dict['iters'], source_dict=generator.state_dict())
-
-      # log txt
-      if rank == 0:
-        out_dict['scalar']['scaler_G'] = scaler_G.get_scale()
-        out_dict['scalar']['scaler_D'] = scaler_D.get_scale()
-        out_dict['r1_lambda']['r1_lambda'] = global_cfg.r1_lambda
-        out_dict['grad_clip']['grad_clip'] = global_cfg.grad_clip
-        out_dict['batch_size']['batch_size'] = global_cfg.batch_size
-        out_dict['lr']['gen_lr'] = optimizer_G.param_groups[0]['lr']
-        out_dict['lr']['disc_lr'] = optimizer_D.param_groups[0]['lr']
-
-        if i % global_cfg.print_every == 0:
-          print_str = tl2_utils.get_print_dict_str(
-            metric_dict=out_dict, float_format="+.6f", outdir=global_cfg.tl_outdir)
-          tqdm.write(print_str)
-
-        if (state_dict['iters'] % global_cfg.log_every == 0) and \
-              state_dict['iters'] >= global_cfg.log_every_start or global_cfg.tl_debug:
-          summary_defaultdict2txtfig(out_dict, prefix="train", step=state_dict['iters'], textlogger=global_textlogger)
-
-      # eval ddp
-      if state_dict['iters'] % global_cfg.save_every == 0 or global_cfg.tl_debug:
-        eval_real_dir = f"{global_cfg.tl_outdir}/fid/real"
-        eval_fake_dir = f"{global_cfg.tl_outdir}/fid/fake"
-        # save real images
-        setup_eval_dataset(dataset=dataset,
-                           saved_dir=eval_real_dir,
-                           num_imgs=global_cfg.num_images_real_eval,
-                           del_fid_real_images=global_cfg.del_fid_real_images,
-                           world_size=world_size,
-                           rank=rank)
-        global_cfg.del_fid_real_images = False
-        ddp_utils.d2_synchronize()
-        # save fake images
-        setup_gen_images(generator=G_ema,
-                         rank=rank,
-                         world_size=world_size,
-                         fake_dir=eval_fake_dir,
-                         num_imgs=global_cfg.num_images_gen_eval,
-                         img_size=image_size)
-        ddp_utils.d2_synchronize()
-
-        if rank == 0:
-          # evaluation
-          FID = calculate_fid(real_dir=eval_real_dir, fake_dir=eval_fake_dir)
-          logger.info(f"\nepoch: {state_dict['epochs']}, step: {state_dict['iters']}, fid: {FID}\n")
-          summary_dict = {
-            'FID': FID
-          }
-          summary_dict2txtfig(summary_dict, prefix='eval', step=state_dict['iters'], textlogger=global_textlogger)
-
-          # save best models
-          if state_dict['best_fid'] > FID:
-            state_dict['best_fid'] = FID
-            saved_models_and_images(G_ema=G_ema,
-                                    generator=generator,
-                                    discriminator=discriminator,
-                                    optimizer_G=optimizer_G,
-                                    optimizer_D=optimizer_D,
-                                    scaler_G=scaler_G,
-                                    scaler_D=scaler_D,
-                                    state_dict=state_dict,
-                                    fixed_z=fixed_z,
-                                    saved_dir=f"{global_cfg.tl_ckptdir}/best_fid",
-                                    forward_bs=global_cfg.forward_bs,
-                                    info_msg=f"FID: {FID}")
-
-          # save models
-          saved_models_and_images(G_ema=G_ema,
-                                  generator=generator,
-                                  discriminator=discriminator,
-                                  optimizer_G=optimizer_G,
-                                  optimizer_D=optimizer_D,
-                                  scaler_G=scaler_G,
-                                  scaler_D=scaler_D,
-                                  state_dict=state_dict,
-                                  fixed_z=fixed_z,
-                                  forward_bs=global_cfg.forward_bs,
-                                  info_msg=f"FID: {FID}")
-          # save resume models
-          saved_models_and_images(G_ema=G_ema,
-                                  generator=generator,
-                                  discriminator=discriminator,
-                                  optimizer_G=optimizer_G,
-                                  optimizer_D=optimizer_D,
-                                  scaler_G=scaler_G,
-                                  scaler_D=scaler_D,
-                                  state_dict=state_dict,
-                                  fixed_z=fixed_z,
-                                  saved_dir=f"{global_cfg.tl_ckptdir}/resume",
-                                  forward_bs=global_cfg.forward_bs,
-                                  info_msg=f"FID: {FID}")
-          if opt.modelarts:
-            modelarts_utils.modelarts_sync_results_dir(cfg=global_cfg, join=False)
-
-      # no blocking
-      ddp_utils.d2_synchronize()
-      state_dict['iters'] += 1
-      if rank == 0:
-        pbar_iters.update(1)
-    state_dict['epochs'] += 1
-
-  cleanup()
-  pass
-
-
-if __name__ == '__main__':
-  parser = argparse.ArgumentParser()
-  parser.add_argument('--port', type=str, default='12355')
-  argparser_utils.add_argument_bool(parser, 'modelarts', default=False)
-
-  update_parser_defaults_from_yaml(parser)
-
-  opt, _ = parser.parse_known_args()
-  argparser_utils.print_args(opt)
-
-  if opt.modelarts:
-    modelarts_utils.setup_tl_outdir_obs(global_cfg, unzip_code=False)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
-
-  num_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-  if num_gpus > 1:
-    mp.spawn(main, args=(num_gpus, opt), nprocs=num_gpus, join=True)
-  else:
-    main(rank=0, world_size=num_gpus, opt=opt)
-
-  if opt.modelarts:
-    modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
-    modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+import collections
+import shutil
+import traceback
+import pprint
+import logging
+import argparse
+import os
+import numpy as np
+import math
+from tqdm import tqdm
+import copy
+
+import torch
+import torch.distributed as dist
+import torch.multiprocessing as mp
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torchvision.utils import save_image, make_grid
+import torchvision.transforms.functional as trans_f
+
+from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
+from tl2.modelarts import modelarts_utils
+from tl2.proj.fvcore import build_model
+from tl2.proj.logger.textlogger import summary_dict2txtfig, summary_defaultdict2txtfig, global_textlogger
+from tl2 import tl2_utils
+from tl2.proj.pytorch import torch_utils
+from tl2.proj.pytorch.ddp import ddp_utils
+from tl2.proj.argparser import argparser_utils
+from tl2.proj.pytorch.examples.gan_ddp import comm_utils
+from tl2.proj.pytorch.examples.gan_ddp import datasets
+from tl2.proj.pytorch.examples.gan_ddp import diff_aug
+
+from torch_fidelity import calculate_metrics
+
+
+def setup_ddp(rank, world_size, port):
+  os.environ['MASTER_ADDR'] = 'localhost'
+  os.environ['MASTER_PORT'] = port
+
+  # initialize the process group
+  # dist.init_process_group("gloo", rank=rank, world_size=world_size)
+  dist.init_process_group("nccl", rank=rank, world_size=world_size)
+  dist.barrier()
+  pass
+
+def cleanup():
+  dist.destroy_process_group()
+
+
+def to_range01(img):
+  """
+
+  :param img: [-1, 1]
+  :return:
+  """
+  img = (img + 1) * 0.5
+  return img
+
+
+def build_model_and_resume(
+      resume_dir,
+      rank,
+      logger,
+      device):
+  state_dict = {
+    'best_fid': np.inf,
+    'iters': 0,
+    'epochs': 0,
+  }
+  generator = build_model(cfg=global_cfg.G_cfg).to(device)
+  discriminator = build_model(cfg=global_cfg.D_cfg).to(device)
+  G_ema = copy.deepcopy(generator)
+  ema_model = comm_utils.EMA(
+    source=generator, target=G_ema, decay=global_cfg.ema_decay, start_itr=global_cfg.ema_start_itr)
+
+  if global_cfg.tl_resume:
+    model_dict = {
+      'generator': generator,
+      'discriminator': discriminator,
+      'state_dict': state_dict,
+    }
+    if global_cfg.load_G_ema:
+      model_dict['G_ema'] = G_ema
+    torch_utils.load_models(save_dir=resume_dir, model_dict=model_dict, strict=False, rank=rank)
+    if global_cfg.load_G_ema:
+      ema_model.update_target_dict(G_ema.state_dict())
+    else:
+      ema_model.update_target_dict(generator.state_dict())
+
+    if global_cfg.reset_best_fid:
+      state_dict['best_fid'] = np.inf
+    logger.info(pprint.pformat(state_dict))
+
+  return generator, discriminator, G_ema, ema_model, state_dict
+
+
+def build_optimizer(G, D):
+
+  optimizer_G = torch.optim.Adam(
+    params=[{'params': G.parameters(),
+             'initial_lr': global_cfg.gen_lr}],
+    lr=global_cfg.gen_lr,
+    betas=global_cfg.betas,
+    weight_decay=0)
+  optimizer_D = torch.optim.Adam(
+    params=[{'params': D.parameters(),
+             'initial_lr': global_cfg.disc_lr}],
+    lr=global_cfg.disc_lr,
+    betas=global_cfg.betas,
+    weight_decay=0)
+
+  return optimizer_G, optimizer_D
+
+def build_optimizer_and_resume(generator_ddp,
+                               discriminator_ddp,
+                               resume_dir,
+                               rank):
+  optimizer_G, optimizer_D = build_optimizer(G=generator_ddp, D=discriminator_ddp)
+
+  scaler_G = torch.cuda.amp.GradScaler(enabled=global_cfg.use_amp_G)
+  scaler_D = torch.cuda.amp.GradScaler(enabled=global_cfg.use_amp_D)
+
+  if global_cfg.tl_resume and global_cfg.load_optimizers:
+    model_dict = {
+      'optimizer_G': optimizer_G,
+      'optimizer_D': optimizer_D,
+      'scaler_G': scaler_G,
+      'scaler_D': scaler_D,
+    }
+    torch_utils.load_models(save_dir=resume_dir, model_dict=model_dict, strict=False, rank=rank)
+
+    optimizer_G.param_groups[0]['lr'] = global_cfg.gen_lr
+    optimizer_D.param_groups[0]['lr'] = global_cfg.disc_lr
+
+  return optimizer_G, optimizer_D, scaler_G, scaler_D
+
+
+def setup_eval_dataset(dataset,
+                       saved_dir,
+                       num_imgs,
+                       del_fid_real_images,
+                       world_size,
+                       rank,
+                       batch_size=100):
+
+  if del_fid_real_images and rank == 0:
+    shutil.rmtree(saved_dir, ignore_errors=True)
+
+  # Only make real images if they haven't been made yet
+  logger = logging.getLogger('tl')
+
+  # important
+  ddp_utils.d2_synchronize()
+
+  if not os.path.exists(saved_dir):
+    if rank == 0:
+      os.makedirs(saved_dir, exist_ok=True)
+      pbar = tqdm(total=num_imgs, desc=f"Outputting real images for eval: {saved_dir}")
+
+    ddp_utils.d2_synchronize()
+
+    dataloader, sampler = datasets.get_dataloader_distributed(
+      dataset=dataset,
+      batch_size=batch_size,
+      world_size=world_size,
+      rank=rank,
+      num_workers=0,
+      shuffle=False,
+      drop_last=False
+    )
+    count = 0
+    for idx, (imgs, _) in enumerate(dataloader):
+      if count >= num_imgs:
+        break
+      if rank == 0:
+        pbar.update(world_size * batch_size)
+
+      # img = (img.squeeze() + 1) * 0.5
+      imgs = to_range01(img=imgs)
+
+      for sub_idx, img in enumerate(imgs):
+        img_pil = trans_f.to_pil_image(img)
+
+        number = idx * world_size * batch_size + rank * batch_size + sub_idx
+        img_pil.save(f"{saved_dir}/{number:06d}.png")
+      count += world_size * batch_size
+  else:
+    logger.info("Real images exist.")
+
+  ddp_utils.d2_synchronize()
+  pass
+
+
+def setup_gen_images(generator,
+                     rank,
+                     world_size,
+                     fake_dir,
+                     num_imgs=2048,
+                     img_size=128):
+  if rank == 0:
+    shutil.rmtree(fake_dir, ignore_errors=True)
+    os.makedirs(fake_dir, exist_ok=True)
+
+  ddp_utils.d2_synchronize()
+
+  generator.eval()
+
+  if rank == 0:
+    pbar = tqdm(desc=f"Generating images at {img_size}x{img_size} {fake_dir}", total=num_imgs)
+
+  with torch.no_grad():
+    img_counter = 0
+    count = 0
+    while img_counter < num_imgs:
+      if rank == 0:
+        pbar.update(world_size)
+
+      zs = generator.get_zs(1)
+      img = generator(zs)
+
+      # img = (img.squeeze() + 1.) * 0.5
+      img = to_range01(img=img.squeeze())
+      img_pil = trans_f.to_pil_image(img)
+
+      number = count * world_size + rank
+      img_pil.save(f"{fake_dir}/{number:06d}.png")
+
+      img_counter += world_size
+      count += 1
+
+  if rank == 0: pbar.close()
+
+  ddp_utils.d2_synchronize()
+  pass
+
+
+def calculate_fid(real_dir,
+                  fake_dir):
+
+  metrics_dict = calculate_metrics(input1=real_dir,
+                                   input2=fake_dir,
+                                   cuda=True,
+                                   isc=False,
+                                   fid=True,
+                                   kid=False,
+                                   verbose=False)
+  fid = metrics_dict['frechet_inception_distance']
+
+  # torch.cuda.empty_cache()
+
+  return fid
+
+
+def saved_models_and_images(G_ema,
+                            generator,
+                            discriminator,
+                            optimizer_G,
+                            optimizer_D,
+                            scaler_G,
+                            scaler_D,
+                            state_dict,
+                            fixed_z,
+                            saved_dir=None,
+                            metadata=None,
+                            forward_bs=4,
+                            info_msg=""):
+  model_dict = {
+    'G_ema': G_ema,
+    'generator': generator,
+    'discriminator': discriminator,
+    'optimizer_G': optimizer_G,
+    'optimizer_D': optimizer_D,
+    'scaler_G': scaler_G,
+    'scaler_D': scaler_D,
+    'state_dict': state_dict,
+  }
+
+  info_msg = f"epoch: {state_dict['epochs']}\n" \
+             f"step: {state_dict['iters']}\n" + info_msg
+  if saved_dir is None:
+    ckpt_max2keep = tl2_utils.MaxToKeep.get_named_max_to_keep(name='ckpt', max_to_keep=4, use_circle_number=True)
+    saved_dir = ckpt_max2keep.step_and_ret_circle_dir(global_cfg.tl_ckptdir, info_msg=info_msg)
+
+  os.makedirs(saved_dir, exist_ok=True)
+
+  # save meta and global_cfg
+  if metadata is not None:
+    tl2_utils.json_dump(metadata, f"{saved_dir}/metadata.json")
+  global_cfg.dump_to_file_with_command(f"{saved_dir}/config_command.yaml", global_cfg.tl_command)
+
+  torch_utils.save_models(save_dir=saved_dir, model_dict=model_dict, info_msg=info_msg)
+
+  save_images(G_ema=G_ema, fixed_z=fixed_z, saved_dir=f"{saved_dir}/imgs", forward_bs=forward_bs)
+  torch.cuda.empty_cache()
+
+  pass
+
+
+@torch.no_grad()
+def save_images(G_ema,
+                fixed_z,
+                saved_dir,
+                forward_bs=4):
+  os.makedirs(saved_dir, exist_ok=True)
+  G_ema.eval()
+  bs = len(fixed_z)
+
+  num_iters = (bs + forward_bs - 1) // forward_bs
+
+  with torch.cuda.amp.autocast(global_cfg.use_amp_G):
+    imgs = []
+    for idx in range(num_iters):
+      z = fixed_z[idx * forward_bs : (idx + 1) * forward_bs]
+      img = G_ema(z)
+      imgs.append(img)
+
+  imgs = torch.cat(imgs, dim=0)
+
+  imgs = to_range01(img=imgs)
+
+  save_image(imgs, f"{saved_dir}/G_ema_z.png", nrow=int(math.sqrt(bs)), normalize=False)
+  pass
+
+
+def main(rank,
+         world_size,
+         opt,
+         ):
+  if world_size > 1:
+    setup_ddp(rank, world_size, opt.port)
+
+  torch.cuda.set_device(rank)
+  device = torch.device(rank)
+
+  update_parser_defaults_from_yaml(parser=None, is_main_process=(rank == 0))
+  if rank == 0 and opt.modelarts:
+    modelarts_utils.setup_tl_outdir_obs(global_cfg, unzip_code=False)
+    modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
+
+  logger = logging.getLogger('tl')
+  torch_utils.init_seeds(seed=global_cfg.seed, rank=rank)
+
+  resume_dir = f"{global_cfg.tl_resumedir}/ckptdir/resume"
+  generator, discriminator, G_ema, ema_model, state_dict = build_model_and_resume(
+    resume_dir=resume_dir, logger=logger, device=device, rank=rank)
+
+  if world_size > 1:
+    generator_ddp = DDP(generator, device_ids=[rank], find_unused_parameters=True, broadcast_buffers=False)
+    discriminator_ddp = DDP(discriminator, device_ids=[rank], find_unused_parameters=True, broadcast_buffers=False)
+    generator = generator_ddp.module
+    discriminator = discriminator_ddp.module
+  else:
+    generator_ddp = generator
+    discriminator_ddp = discriminator
+
+  optimizer_G, optimizer_D, scaler_G, scaler_D = build_optimizer_and_resume(
+    generator_ddp=generator_ddp, discriminator_ddp=discriminator_ddp, resume_dir=resume_dir, rank=rank)
+
+  # ----------
+  #  Training
+  # ----------
+
+  # dataset
+  dataset = build_model(cfg=global_cfg.data_cfg)
+  img, _ = dataset[0]
+  image_size = img.shape[-1]
+  dataloader, sampler = datasets.get_dataloader_distributed(
+    dataset=dataset,
+    batch_size=global_cfg.batch_size,
+    world_size=world_size,
+    rank=rank,
+    num_workers=global_cfg.num_workers,
+    shuffle=True,
+  )
+  assert global_cfg.batch_size % global_cfg.batch_split == 0
+
+  fixed_z = generator.get_zs(global_cfg.fixed_z_bs)
+  use_diffaug = global_cfg.use_diffaug
+  dummy_tensor = torch.tensor([0], device=device)
+
+  if rank == 0:
+    num_iters = (len(dataset) // world_size + global_cfg.batch_size - 1) // global_cfg.batch_size
+    pbar_iters = tqdm(total=num_iters)
+
+  out_dict = collections.defaultdict(dict)
+  # loop epochs
+  while True:
+    sampler.set_epoch(epoch=state_dict['epochs'])
+    if rank == 0:
+      pbar_iters.reset()
+
+    for i, (imgs, _) in enumerate(dataloader):
+
+      ddp_utils.d2_synchronize()
+
+      out_dict.clear()
+      out_dict['epoch']['epoch'] = state_dict['epochs']
+      out_dict['iter']['iter'] = state_dict['iters']
+
+      generator_ddp.train()
+      discriminator_ddp.train()
+
+      real_imgs = imgs.to(device, non_blocking=True)
+      cur_bs = real_imgs.shape[0]
+
+      # TRAIN DISCRIMINATOR
+      torch_utils.requires_grad(generator_ddp, False)
+      torch_utils.requires_grad(discriminator_ddp, True)
+
+      with torch.cuda.amp.autocast(global_cfg.use_amp_D):
+        # Generate images for discriminator training
+        with torch.no_grad():
+          zs = generator.get_zs(cur_bs)
+          split_batch_size = cur_bs // global_cfg.batch_split
+          gen_imgs = []
+          for split in range(global_cfg.batch_split):
+            subset_z = zs[split * split_batch_size : (split+1) * split_batch_size]
+            g_imgs = generator_ddp(subset_z)
+            gen_imgs.append(g_imgs)
+
+          gen_imgs = torch.cat(gen_imgs, axis=0)
+          if use_diffaug:
+            gen_imgs = diff_aug.DiffAugment(gen_imgs)
+
+        # real_imgs.requires_grad = True
+        if use_diffaug:
+          real_imgs = diff_aug.DiffAugment(real_imgs)
+        real_imgs.requires_grad_()
+        d_real_logits = discriminator_ddp(real_imgs)
+
+      d_regularize = i % global_cfg.d_reg_every == 0
+
+      # Gradient penalty
+      if global_cfg.r1_lambda > 0 and d_regularize:
+        grad_real = torch.autograd.grad(
+          outputs=scaler_D.scale(d_real_logits.sum()), inputs=real_imgs, create_graph=True)
+        inv_scale = 1. / scaler_D.get_scale()
+        grad_real = [p * inv_scale for p in grad_real][0]
+      with torch.cuda.amp.autocast(global_cfg.use_amp_D):
+        if global_cfg.r1_lambda > 0 and d_regularize:
+          grad_penalty = (grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2).mean()
+          grad_penalty = 0.5 * global_cfg.r1_lambda * global_cfg.d_reg_every * grad_penalty + 0 * d_real_logits[0]
+        else:
+          grad_penalty = dummy_tensor
+
+        d_fake_logits = discriminator_ddp(gen_imgs)
+
+        with torch.no_grad():
+          D_real_logits = d_real_logits.mean().item()
+          D_fake_logits = d_fake_logits.mean().item()
+          out_dict['D_logits']['D_real_logits'] = D_real_logits
+          out_dict['D_logits']['D_fake_logits'] = D_fake_logits
+
+        D_real_logits_loss = torch.nn.functional.softplus(-d_real_logits).mean()
+        D_fake_logits_loss = torch.nn.functional.softplus(d_fake_logits).mean()
+
+        d_loss = D_real_logits_loss + D_fake_logits_loss + grad_penalty
+
+        out_dict['D_logits_loss']['D_real_logits_loss'] = D_real_logits_loss.item()
+        out_dict['D_logits_loss']['D_fake_logits_loss'] = D_fake_logits_loss.item()
+        out_dict['grad_penalty']['grad_penalty'] = grad_penalty.item()
+        out_dict['d_loss']['d_loss'] = d_loss.item()
+
+      optimizer_D.zero_grad()
+      scaler_D.scale(d_loss).backward()
+      scaler_D.unscale_(optimizer_D)
+      try:
+        D_total_norm = torch.nn.utils.clip_grad_norm_(discriminator_ddp.parameters(), global_cfg.grad_clip,
+                                                      # error_if_nonfinite=True, # torch >= 1.9
+                                                      )
+        out_dict['norm']['D_total_norm'] = D_total_norm.item()
+      except:
+        logger.info(traceback.format_exc())
+        saved_models_and_images(G_ema=G_ema,
+                                generator=generator,
+                                discriminator=discriminator,
+                                optimizer_G=optimizer_G,
+                                optimizer_D=optimizer_D,
+                                scaler_G=scaler_G,
+                                scaler_D=scaler_D,
+                                state_dict=state_dict,
+                                fixed_z=fixed_z,
+                                saved_dir=f"{global_cfg.tl_ckptdir}/D_crupted",
+                                forward_bs=global_cfg.forward_bs)
+        optimizer_D.zero_grad()
+        D_total_norm = -1
+        out_dict['norm']['D_total_norm'] = D_total_norm
+        # exit(0)
+      if D_total_norm > 0:
+        scaler_D.step(optimizer_D)
+      scaler_D.update()
+
+      # TRAIN GENERATOR
+      torch_utils.requires_grad(generator_ddp, True)
+      torch_utils.requires_grad(discriminator_ddp, False)
+
+      zs = generator.get_zs(cur_bs)
+      split_batch_size = cur_bs // global_cfg.batch_split
+
+      optimizer_G.zero_grad()
+      for split in range(global_cfg.batch_split):
+        with torch.cuda.amp.autocast(global_cfg.use_amp_G):
+          subset_z = zs[split * split_batch_size : (split + 1) * split_batch_size]
+          gen_imgs = generator_ddp(subset_z).to(torch.float32)
+          if use_diffaug:
+            gen_imgs = diff_aug.DiffAugment(gen_imgs)
+
+        with torch.cuda.amp.autocast(global_cfg.use_amp_D):
+          g_logits = discriminator_ddp(gen_imgs)
+
+        if global_cfg.topk_v > 0:
+          topk_percentage = 0.99 ** (state_dict['iters'] / global_cfg.topk_interval)
+          topk_percentage = max(topk_percentage, global_cfg.topk_v)
+          topk_num = math.ceil(topk_percentage * cur_bs)
+          g_logits = torch.topk(g_logits, topk_num, dim=0).values
+        else:
+          topk_num = cur_bs
+
+        G_fake_logits_loss = torch.nn.functional.softplus(-g_logits).mean()
+
+        scaler_G.scale(G_fake_logits_loss).backward()
+
+      scaler_G.unscale_(optimizer_G)
+
+      with torch.no_grad():
+        G_fake_logits = g_logits.mean().item()
+        out_dict['D_logits']['G_fake_logits'] = G_fake_logits
+
+      out_dict['D_logits_loss']['G_fake_logits_loss'] = G_fake_logits_loss.item()
+      out_dict['topk_num']['topk_num'] = topk_num
+
+      try:
+        G_total_norm = torch.nn.utils.clip_grad_norm_(generator_ddp.parameters(), global_cfg.grad_clip,
+                                                      # error_if_nonfinite=True, # torch >= 1.9
+                                                      )
+        out_dict['norm']['G_total_norm'] = G_total_norm.item()
+      except:
+        logger.info(traceback.format_exc())
+        saved_models_and_images(G_ema=G_ema,
+                                generator=generator,
+                                discriminator=discriminator,
+                                optimizer_G=optimizer_G,
+                                optimizer_D=optimizer_D,
+                                scaler_G=scaler_G,
+                                scaler_D=scaler_D,
+                                state_dict=state_dict,
+                                fixed_z=fixed_z,
+                                saved_dir=f"{global_cfg.tl_ckptdir}/G_crupted",
+                                forward_bs=global_cfg.forward_bs)
+        G_total_norm = -1
+        out_dict['norm']['G_total_norm'] = G_total_norm
+        optimizer_G.zero_grad()
+        # exit(0)
+
+      if G_total_norm > 0:
+        scaler_G.step(optimizer_G)
+      scaler_G.update()
+
+      # update ema
+      ema_model.update(itr=state_dict['iters'], source_dict=generator.state_dict())
+
+      # log txt
+      if rank == 0:
+        out_dict['scalar']['scaler_G'] = scaler_G.get_scale()
+        out_dict['scalar']['scaler_D'] = scaler_D.get_scale()
+        out_dict['r1_lambda']['r1_lambda'] = global_cfg.r1_lambda
+        out_dict['grad_clip']['grad_clip'] = global_cfg.grad_clip
+        out_dict['batch_size']['batch_size'] = global_cfg.batch_size
+        out_dict['lr']['gen_lr'] = optimizer_G.param_groups[0]['lr']
+        out_dict['lr']['disc_lr'] = optimizer_D.param_groups[0]['lr']
+
+        if i % global_cfg.print_every == 0:
+          print_str = tl2_utils.get_print_dict_str(
+            metric_dict=out_dict, float_format="+.6f", outdir=global_cfg.tl_outdir)
+          tqdm.write(print_str)
+
+        if (state_dict['iters'] % global_cfg.log_every == 0) and \
+              state_dict['iters'] >= global_cfg.log_every_start or global_cfg.tl_debug:
+          summary_defaultdict2txtfig(out_dict, prefix="train", step=state_dict['iters'], textlogger=global_textlogger)
+
+      # eval ddp
+      if state_dict['iters'] % global_cfg.save_every == 0 or global_cfg.tl_debug:
+        eval_real_dir = f"{global_cfg.tl_outdir}/fid/real"
+        eval_fake_dir = f"{global_cfg.tl_outdir}/fid/fake"
+        # save real images
+        setup_eval_dataset(dataset=dataset,
+                           saved_dir=eval_real_dir,
+                           num_imgs=global_cfg.num_images_real_eval,
+                           del_fid_real_images=global_cfg.del_fid_real_images,
+                           world_size=world_size,
+                           rank=rank)
+        global_cfg.del_fid_real_images = False
+        ddp_utils.d2_synchronize()
+        # save fake images
+        setup_gen_images(generator=G_ema,
+                         rank=rank,
+                         world_size=world_size,
+                         fake_dir=eval_fake_dir,
+                         num_imgs=global_cfg.num_images_gen_eval,
+                         img_size=image_size)
+        ddp_utils.d2_synchronize()
+
+        if rank == 0:
+          # evaluation
+          FID = calculate_fid(real_dir=eval_real_dir, fake_dir=eval_fake_dir)
+          logger.info(f"\nepoch: {state_dict['epochs']}, step: {state_dict['iters']}, fid: {FID}\n")
+          summary_dict = {
+            'FID': FID
+          }
+          summary_dict2txtfig(summary_dict, prefix='eval', step=state_dict['iters'], textlogger=global_textlogger)
+
+          # save best models
+          if state_dict['best_fid'] > FID:
+            state_dict['best_fid'] = FID
+            saved_models_and_images(G_ema=G_ema,
+                                    generator=generator,
+                                    discriminator=discriminator,
+                                    optimizer_G=optimizer_G,
+                                    optimizer_D=optimizer_D,
+                                    scaler_G=scaler_G,
+                                    scaler_D=scaler_D,
+                                    state_dict=state_dict,
+                                    fixed_z=fixed_z,
+                                    saved_dir=f"{global_cfg.tl_ckptdir}/best_fid",
+                                    forward_bs=global_cfg.forward_bs,
+                                    info_msg=f"FID: {FID}")
+
+          # save models
+          saved_models_and_images(G_ema=G_ema,
+                                  generator=generator,
+                                  discriminator=discriminator,
+                                  optimizer_G=optimizer_G,
+                                  optimizer_D=optimizer_D,
+                                  scaler_G=scaler_G,
+                                  scaler_D=scaler_D,
+                                  state_dict=state_dict,
+                                  fixed_z=fixed_z,
+                                  forward_bs=global_cfg.forward_bs,
+                                  info_msg=f"FID: {FID}")
+          # save resume models
+          saved_models_and_images(G_ema=G_ema,
+                                  generator=generator,
+                                  discriminator=discriminator,
+                                  optimizer_G=optimizer_G,
+                                  optimizer_D=optimizer_D,
+                                  scaler_G=scaler_G,
+                                  scaler_D=scaler_D,
+                                  state_dict=state_dict,
+                                  fixed_z=fixed_z,
+                                  saved_dir=f"{global_cfg.tl_ckptdir}/resume",
+                                  forward_bs=global_cfg.forward_bs,
+                                  info_msg=f"FID: {FID}")
+          if opt.modelarts:
+            modelarts_utils.modelarts_sync_results_dir(cfg=global_cfg, join=False)
+
+      # no blocking
+      ddp_utils.d2_synchronize()
+      state_dict['iters'] += 1
+      if rank == 0:
+        pbar_iters.update(1)
+    state_dict['epochs'] += 1
+
+  cleanup()
+  pass
+
+
+if __name__ == '__main__':
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--port', type=str, default='12355')
+  argparser_utils.add_argument_bool(parser, 'modelarts', default=False)
+
+  update_parser_defaults_from_yaml(parser)
+
+  opt, _ = parser.parse_known_args()
+  argparser_utils.print_args(opt)
+
+  if opt.modelarts:
+    modelarts_utils.setup_tl_outdir_obs(global_cfg, unzip_code=False)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
+
+  num_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+  if num_gpus > 1:
+    mp.spawn(main, args=(num_gpus, opt), nprocs=num_gpus, join=True)
+  else:
+    main(rank=0, world_size=num_gpus, opt=opt)
+
+  if opt.modelarts:
+    modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
+    modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/multi_process_main/config.yaml` & `tl2-0.1.1/tl2/proj/pytorch/examples/multi_process_main/config.yaml`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-root_obs: &root_obs s3://bucket-7001/ZhouPeng
-modelarts_download: &modelarts_download
-  data_root:
-    datapath_obs: '{global_cfg.root_obs}/keras/mixed_faces/mixed_faces1024_v3.zip'
-    datapath: datasets/mixed_faces/mixed_faces1024_v3.zip
-    overwrite: false
-    eval: true
-    unzip: false
-  inception:
-    datapath_obs: '{global_cfg.root_obs}/keras/stylegan2-ada-pytorch-exp/datasets/pretrained/metrics/inception-2015-12-05.pt'
-    datapath: datasets/pretrained/metrics/inception-2015-12-05.pt
-    overwrite: false
-    eval: true
-    unzip: false
-  ffhq1024:
-    datapath_obs: '{global_cfg.root_obs}/keras/stylegan2-ada-pytorch-exp/datasets/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl'
-    datapath: datasets/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl
-    overwrite: false
-    eval: true
-    unzip: false
-
-
-multi_process_main:
-  image_list_file: "tl2_lib/data/images_r512/image_list.txt"
-  model_cfg:
-    register_modules:
-      - exp.dev.hrinversion.models.projector_v4
-    name: exp.dev.hrinversion.models.projector_v4.StyleGAN2Projector
-  root_obs: *root_obs
-  modelarts_download: *modelarts_download
-
+root_obs: &root_obs s3://bucket-7001/ZhouPeng
+modelarts_download: &modelarts_download
+  data_root:
+    datapath_obs: '{global_cfg.root_obs}/keras/mixed_faces/mixed_faces1024_v3.zip'
+    datapath: datasets/mixed_faces/mixed_faces1024_v3.zip
+    overwrite: false
+    eval: true
+    unzip: false
+  inception:
+    datapath_obs: '{global_cfg.root_obs}/keras/stylegan2-ada-pytorch-exp/datasets/pretrained/metrics/inception-2015-12-05.pt'
+    datapath: datasets/pretrained/metrics/inception-2015-12-05.pt
+    overwrite: false
+    eval: true
+    unzip: false
+  ffhq1024:
+    datapath_obs: '{global_cfg.root_obs}/keras/stylegan2-ada-pytorch-exp/datasets/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl'
+    datapath: datasets/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl
+    overwrite: false
+    eval: true
+    unzip: false
+
+
+multi_process_main:
+  image_list_file: "tl2_lib/data/images_r512/image_list.txt"
+  model_cfg:
+    register_modules:
+      - exp.dev.hrinversion.models.projector_v4
+    name: exp.dev.hrinversion.models.projector_v4.StyleGAN2Projector
+  root_obs: *root_obs
+  modelarts_download: *modelarts_download
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/multi_process_main/dataset.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/multi_process_main/dataset.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/multi_process_main/main.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/multi_process_main/main.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,180 +1,180 @@
-import logging
-import json
-import tqdm
-from time import perf_counter
-from pathlib import Path
-import argparse
-import os
-import random
-import numpy as np
-
-import torch
-import torch.distributed as dist
-import torch.utils.data as data_utils
-import torchvision.transforms as tv_trans
-
-from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
-from tl2.modelarts import modelarts_utils
-from tl2.proj.pil import pil_utils
-from tl2.proj.skimage import skimage_utils
-from tl2.proj.pytorch.ddp import ddp_utils
-from tl2.tl2_utils import AverageMeter
-from tl2.proj.logger.textlogger import summary_dict2txtfig, global_textlogger
-from tl2.proj.fvcore import build_model
-from tl2.proj.pytorch.examples.multi_process_main.dataset import ImageListDataset
-from tl2.proj.argparser import argparser_utils
-from tl2.proj.pytorch import torch_utils
-from tl2 import tl2_utils
-
-
-def build_parser():
-  ## runtime arguments
-  parser = argparse.ArgumentParser(description='Training configurations.')
-
-  argparser_utils.add_argument_int(parser, name='local_rank', default=0)
-  argparser_utils.add_argument_int(parser, name='seed', default=0)
-  argparser_utils.add_argument_int(parser, name='num_workers', default=0)
-
-  return parser
-
-
-def get_dataset(image_list_file,
-                distributed,
-                num_workers=0,
-                debug=False):
-
-  dataset = ImageListDataset(meta_file=image_list_file, )
-
-  if distributed:
-    sampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=False)
-  else:
-    sampler = None
-
-  data_loader = data_utils.DataLoader(
-    dataset,
-    batch_size=1,
-    shuffle=False,
-    sampler=sampler,
-    num_workers=num_workers,
-    pin_memory=False)
-
-  if global_cfg.tl_debug:
-    data_iter = iter(data_loader)
-    data = next(data_iter)
-
-  num_images = len(dataset)
-  return data_loader, num_images
-
-
-def main():
-
-  parser = build_parser()
-  args, _ = parser.parse_known_args()
-
-  rank, world_size = ddp_utils.ddp_init(seed=args.seed)
-  torch_utils.init_seeds(seed=args.seed, rank=rank)
-  device = torch.device('cuda')
-
-  is_main_process = (rank == 0)
-
-  update_parser_defaults_from_yaml(parser, is_main_process=is_main_process)
-  logger = logging.getLogger('tl')
-  if is_main_process:
-    modelarts_utils.setup_tl_outdir_obs(global_cfg)
-    modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
-
-  # dataset
-  distributed = ddp_utils.is_distributed()
-  data_loader, num_images = get_dataset(image_list_file=global_cfg.image_list_file,
-                                        distributed=distributed,
-                                        num_workers=args.num_workers,
-                                        debug=global_cfg.tl_debug)
-
-  # projector = build_model(
-  #   cfg=global_cfg.model_cfg,
-  #   kwargs_priority=True,
-  #   cfg_to_kwargs=True,
-  #   network_pkl=global_cfg.network_pkl,
-  #   loss_cfg=global_cfg.loss_cfg,
-  # )
-
-  if rank == 0:
-    pbar = tqdm.tqdm(desc=global_cfg.tl_outdir, total=num_images)
-
-  # lpips_metric = skimage_utils.LPIPS(device=device)
-
-  avg_recorder_dict = {}
-  for idx, image_path in enumerate(data_loader):
-    if rank == 0:
-      pbar.update(world_size)
-
-    # ret = ddp_utils.all_gather_to_same_device(image_path)
-    image_path = Path(image_path[0])
-
-    outdir = f"{global_cfg.tl_outdir}/exp/{image_path.stem}"
-    os.makedirs(outdir, exist_ok=True)
-
-    metric_dict = {
-      'rank': rank + 1
-    }
-
-    start_time = perf_counter()
-    # projector.reset()
-    # metric_dict = projector.project_wplus(
-    #   outdir=outdir,
-    #   image_path=image_path,
-    #   distributed=distributed,
-    #   lpips_metric=lpips_metric,
-    #   **global_cfg.project_wplus_kwargs
-    # )
-    elapsed_time = (perf_counter() - start_time)
-    if rank == 0:
-      tqdm.tqdm.write(f'processing {image_path.stem} elapsed: {elapsed_time:.1f} s')
-
-    metric_dict['elapsed_time_second'] = elapsed_time
-    metric_dict['elapsed_time_minute'] = elapsed_time / 60
-    with open(f"{outdir}/metrics.json", 'w') as f:
-      json.dump(metric_dict, f, indent=2)
-
-    # average value if distributed
-    if distributed:
-      metric_dict = ddp_utils.d2_reduce_dict(input_dict=metric_dict, average=True)
-      ddp_utils.d2_synchronize()
-
-    if rank == 0:
-      if len(avg_recorder_dict) == 0:
-        for k in metric_dict.keys():
-          avg_recorder_dict[k] = AverageMeter()
-      for k in metric_dict.keys():
-        avg_recorder_dict[k].update(metric_dict[k])
-
-      summary_dict = {}
-      loss_str = ""
-      for k in avg_recorder_dict.keys():
-        v_avg = avg_recorder_dict[k].avg
-        summary_dict[k] = v_avg
-        loss_str += f'{k}: {v_avg:.4g}, '
-      tqdm.tqdm.write(loss_str)
-      summary_dict2txtfig(summary_dict, prefix='eval', step=idx, textlogger=global_textlogger, save_fig_sec=30)
-
-    if idx % 20 == 0 and rank == 0:
-      modelarts_utils.modelarts_sync_results_dir(cfg=global_cfg, join=False)
-
-    if distributed: ddp_utils.d2_synchronize()
-
-    if global_cfg.tl_debug:
-      break
-
-  if is_main_process:
-    modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
-    modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-  if distributed: ddp_utils.d2_synchronize()
-  pass
-
-
-
-
-
-if __name__ == '__main__':
-  main()
+import logging
+import json
+import tqdm
+from time import perf_counter
+from pathlib import Path
+import argparse
+import os
+import random
+import numpy as np
+
+import torch
+import torch.distributed as dist
+import torch.utils.data as data_utils
+import torchvision.transforms as tv_trans
+
+from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
+from tl2.modelarts import modelarts_utils
+from tl2.proj.pil import pil_utils
+from tl2.proj.skimage import skimage_utils
+from tl2.proj.pytorch.ddp import ddp_utils
+from tl2.tl2_utils import AverageMeter
+from tl2.proj.logger.textlogger import summary_dict2txtfig, global_textlogger
+from tl2.proj.fvcore import build_model
+from tl2.proj.pytorch.examples.multi_process_main.dataset import ImageListDataset
+from tl2.proj.argparser import argparser_utils
+from tl2.proj.pytorch import torch_utils
+from tl2 import tl2_utils
+
+
+def build_parser():
+  ## runtime arguments
+  parser = argparse.ArgumentParser(description='Training configurations.')
+
+  argparser_utils.add_argument_int(parser, name='local_rank', default=0)
+  argparser_utils.add_argument_int(parser, name='seed', default=0)
+  argparser_utils.add_argument_int(parser, name='num_workers', default=0)
+
+  return parser
+
+
+def get_dataset(image_list_file,
+                distributed,
+                num_workers=0,
+                debug=False):
+
+  dataset = ImageListDataset(meta_file=image_list_file, )
+
+  if distributed:
+    sampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=False)
+  else:
+    sampler = None
+
+  data_loader = data_utils.DataLoader(
+    dataset,
+    batch_size=1,
+    shuffle=False,
+    sampler=sampler,
+    num_workers=num_workers,
+    pin_memory=False)
+
+  if global_cfg.tl_debug:
+    data_iter = iter(data_loader)
+    data = next(data_iter)
+
+  num_images = len(dataset)
+  return data_loader, num_images
+
+
+def main():
+
+  parser = build_parser()
+  args, _ = parser.parse_known_args()
+
+  rank, world_size = ddp_utils.ddp_init(seed=args.seed)
+  torch_utils.init_seeds(seed=args.seed, rank=rank)
+  device = torch.device('cuda')
+
+  is_main_process = (rank == 0)
+
+  update_parser_defaults_from_yaml(parser, is_main_process=is_main_process)
+  logger = logging.getLogger('tl')
+  if is_main_process:
+    modelarts_utils.setup_tl_outdir_obs(global_cfg)
+    modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
+
+  # dataset
+  distributed = ddp_utils.is_distributed()
+  data_loader, num_images = get_dataset(image_list_file=global_cfg.image_list_file,
+                                        distributed=distributed,
+                                        num_workers=args.num_workers,
+                                        debug=global_cfg.tl_debug)
+
+  # projector = build_model(
+  #   cfg=global_cfg.model_cfg,
+  #   kwargs_priority=True,
+  #   cfg_to_kwargs=True,
+  #   network_pkl=global_cfg.network_pkl,
+  #   loss_cfg=global_cfg.loss_cfg,
+  # )
+
+  if rank == 0:
+    pbar = tqdm.tqdm(desc=global_cfg.tl_outdir, total=num_images)
+
+  # lpips_metric = skimage_utils.LPIPS(device=device)
+
+  avg_recorder_dict = {}
+  for idx, image_path in enumerate(data_loader):
+    if rank == 0:
+      pbar.update(world_size)
+
+    # ret = ddp_utils.all_gather_to_same_device(image_path)
+    image_path = Path(image_path[0])
+
+    outdir = f"{global_cfg.tl_outdir}/exp/{image_path.stem}"
+    os.makedirs(outdir, exist_ok=True)
+
+    metric_dict = {
+      'rank': rank + 1
+    }
+
+    start_time = perf_counter()
+    # projector.reset()
+    # metric_dict = projector.project_wplus(
+    #   outdir=outdir,
+    #   image_path=image_path,
+    #   distributed=distributed,
+    #   lpips_metric=lpips_metric,
+    #   **global_cfg.project_wplus_kwargs
+    # )
+    elapsed_time = (perf_counter() - start_time)
+    if rank == 0:
+      tqdm.tqdm.write(f'processing {image_path.stem} elapsed: {elapsed_time:.1f} s')
+
+    metric_dict['elapsed_time_second'] = elapsed_time
+    metric_dict['elapsed_time_minute'] = elapsed_time / 60
+    with open(f"{outdir}/metrics.json", 'w') as f:
+      json.dump(metric_dict, f, indent=2)
+
+    # average value if distributed
+    if distributed:
+      metric_dict = ddp_utils.d2_reduce_dict(input_dict=metric_dict, average=True)
+      ddp_utils.d2_synchronize()
+
+    if rank == 0:
+      if len(avg_recorder_dict) == 0:
+        for k in metric_dict.keys():
+          avg_recorder_dict[k] = AverageMeter()
+      for k in metric_dict.keys():
+        avg_recorder_dict[k].update(metric_dict[k])
+
+      summary_dict = {}
+      loss_str = ""
+      for k in avg_recorder_dict.keys():
+        v_avg = avg_recorder_dict[k].avg
+        summary_dict[k] = v_avg
+        loss_str += f'{k}: {v_avg:.4g}, '
+      tqdm.tqdm.write(loss_str)
+      summary_dict2txtfig(summary_dict, prefix='eval', step=idx, textlogger=global_textlogger, save_fig_sec=30)
+
+    if idx % 20 == 0 and rank == 0:
+      modelarts_utils.modelarts_sync_results_dir(cfg=global_cfg, join=False)
+
+    if distributed: ddp_utils.d2_synchronize()
+
+    if global_cfg.tl_debug:
+      break
+
+  if is_main_process:
+    modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
+    modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+  if distributed: ddp_utils.d2_synchronize()
+  pass
+
+
+
+
+
+if __name__ == '__main__':
+  main()
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/multi_process_main/test_multi_process_main.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/gan_ddp/test_gan_ddp.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,115 +1,109 @@
-
-
-
-
-import json
-import numpy as np
-import os
-import sys
-import unittest
-import argparse
-
-
-class Testing_Multi_Process_Main(unittest.TestCase):
-
-  def test_multi_process_main(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/multi_process_main/config.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    if int(os.environ['RUN_NUM']) > 0:
-      run_command = f"""
-                  python -c "from tl2.modelarts.tests.test_run import TestingRun;\
-                        TestingRun().test_run_v2(number={os.environ['RUN_NUM']}, )" \
-                        --tl_opts root_obs {cfg.root_obs}
-                  """
-      p = tl2_utils.Worker(name='Run', args=(run_command,))
-      p.start()
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-    PORT = os.environ.get('PORT', 8888)
-
-    # if 'CUDA_HOME' not in os.environ: os.environ['CUDA_HOME'] = "/usr/local/cuda-10.2/"
-    # os.environ['PATH'] = f"{os.path.dirname(sys.executable)}:{os.environ['PATH']}"
-    # os.environ['DNNLIB_CACHE_DIR'] = "datasets/pretrained"
-    # os.environ['TORCH_EXTENSIONS_DIR'] = "torch_extensions"
-
-    if n_gpus > 1:
-      python_str = f"python -m torch.distributed.launch --nproc_per_node={n_gpus} --master_port={PORT} "
-    else:
-      python_str = "python "
-      
-    cmd_str = f"""
-        {python_str}
-        tl2_lib/tl2/proj/pytorch/examples/multi_process_main/main.py
-        {get_append_cmd_str(args)}
-        """
-    if debug:
-      cmd_str += f"""
-                  --tl_debug
-                  --tl_opts 
-                  """
-    else:
-      cmd_str += f"""
-                  --num_workers {n_gpus}
-                  --tl_opts {tl_opts}
-                  """
-    start_cmd_run(cmd_str)
-    # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
-    # from tl2.modelarts import modelarts_utils
-    # update_parser_defaults_from_yaml(parser)
-
-    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
-    #
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    pass
-
-
-
-
+import time
+from pathlib import Path
+import os
+import sys
+import unittest
+import argparse
+
+
+class Testing_gan_ddp(unittest.TestCase):
+
+  def test_train(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=7
+        export TIME_STR=0
+        export PORT=12345
+        export PYTHONPATH=.:./piGAN_lib
+        python -c "from tl2_lib.tl2.proj.pytorch.examples.gan_ddp.test_gan_ddp import Testing_gan_ddp;\
+          Testing_gan_ddp().test_train(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/       \
+            use_amp_D False use_amp_G False \
+            del_fid_real_images True num_images_real_eval 2048 num_images_gen_eval 2048 \
+            save_every 1000 \
+          --tl_outdir results/gan_ddp/train-20211129_164456_187
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/gan_ddp/train-20211129_164456_187'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/gan_ddp/config.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    if int(os.environ['RUN_NUM']) > 0:
+      run_command = f"""
+                      python -c "from tl2.modelarts.tests.test_run import TestingRun;\
+                            TestingRun().test_run_v2(number={os.environ['RUN_NUM']}, )" \
+                            --tl_opts root_obs {cfg.root_obs}
+                      """
+      p = tl2_utils.Worker(name='Run', args=(run_command,))
+      p.start()
+
+    os.environ['PATH'] = f"{os.path.dirname(sys.executable)}:{os.environ['PATH']}"
+    os.environ['TORCH_EXTENSIONS_DIR'] = "torch_extensions"
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    PORT = os.environ.get('PORT', 8888)
+
+    cmd_str = f"""
+        python
+        tl2_lib/tl2/proj/pytorch/examples/gan_ddp/train.py
+        --port {PORT}
+        --modelarts True
+        --outdir {outdir}
+        {get_append_cmd_str(args)}
+        """
+    if debug:
+      cmd_str += f"""
+                  --tl_debug
+                  --tl_opts 
+                    num_workers 0                    
+                    num_images_real_eval 10
+                    num_images_gen_eval 2
+                  """
+    else:
+      cmd_str += f"""
+                  --tl_opts {tl_opts}
+                    num_workers {n_gpus}
+                  """
+    start_cmd_run(cmd_str)
+    # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
+    # from tl2.modelarts import modelarts_utils
+    # update_parser_defaults_from_yaml(parser)
+
+    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
+    #
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    pass
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/nerf/cam_params.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/nerf/cam_params.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,727 +1,727 @@
-import utils
-import numpy as np
-from typing import Tuple
-import matplotlib.pyplot as plt
-from einops import rearrange
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-from pytorch3d import transforms as tr3d
-from . import geometry_tensor
-
-
-def _get_mesh_xy(H,
-                 W,
-                 device,
-                 prefix_dims=[] # broadcast for batch
-                 ):
-  """
-
-  :param H:
-  :param W:
-  :param device:
-  :param prefix_dims:
-  :return:
-  - x: (b, HxW)
-  - y: (b, HxW)
-  """
-  y, x = torch.meshgrid(torch.linspace(0, H - 1, H, device=device),
-                        torch.linspace(0, W - 1, W, device=device))
-
-  x = x.reshape([*[1] * len(prefix_dims), H * W]).expand([*prefix_dims, H * W]) + 0.5
-  y = y.reshape([*[1] * len(prefix_dims), H * W]).expand([*prefix_dims, H * W]) + 0.5
-
-  return x, y
-
-def _get_mesh_xy_deprecated(H,
-                            W,
-                            device,
-                            prefix_dims=[] # broadcast for batch
-                            ):
-  i, j = torch.meshgrid(torch.linspace(0, W - 1, W, device=device), torch.linspace(0, H - 1, H, device=device))
-  i = i.t().reshape([*len(prefix_dims) * [1], H * W]).expand([*prefix_dims, H * W])
-  j = j.t().reshape([*len(prefix_dims) * [1], H * W]).expand([*prefix_dims, H * W])
-
-  return i, j
-
-
-def _select_pixels(x,
-                   y,
-                   H,
-                   W,
-                   device,
-                   N_rays=-1,
-                   prefix_dims=[]):
-
-  if N_rays > 0 and N_rays < H * W:
-    select_inds = torch.multinomial(torch.ones(*[*prefix_dims, H*W], device=device),
-                                    num_samples=N_rays, replacement=False)
-    # select_inds = torch.from_numpy(
-    #   np.random.choice(H * W, size=[*prefix_dims, N_rays], replace=False)
-    # ).to(device)
-
-    x = torch.gather(x, dim=-1, index=select_inds)
-    y = torch.gather(y, dim=-1, index=select_inds)
-  else:
-    select_inds = torch.arange(H * W).to(device)
-    if len(prefix_dims) > 0:
-      select_inds = select_inds[None,].expand([*prefix_dims, -1])
-
-  return x, y, select_inds
-
-
-def _get_direction(H,
-                   W,
-                   focal_x,
-                   focal_y,
-                   prefix_dims,
-                   device,
-                   N_rays=-1,
-                   center_x=None,
-                   center_y=None,
-                   normalize_rays_d=False):
-  """
-
-  :param H:
-  :param W:
-  :param focal_x:
-  :param focal_y:
-  :param prefix_dims: [b, ]
-  :param device:
-  :param N_rays:
-  :return:
-  """
-  # [b, HxW]
-  x, y = _get_mesh_xy(H=H, W=W, device=device, prefix_dims=prefix_dims)
-  # x, y = _get_mesh_xy_deprecated(H=H, W=W, device=device, prefix_dims=prefix)
-  # assert (i == x).all()
-  # assert (j == y).all()
-
-  # [b, N_rays], pixel coordinate
-  x, y, select_inds = _select_pixels(x=x, y=y, H=H, W=W, N_rays=N_rays, device=device, prefix_dims=prefix_dims)
-
-  if center_x is None:
-    center_x = W / 2.
-  if center_y is None:
-    center_y = H / 2.
-
-  # [..., N_rays, 3], axes orientations : x right, y downwards, z positive, pixel coordinate to camera coordinate
-  dirs = torch.stack([(x - center_x) / focal_x,
-                      (y - center_y) / focal_y,
-                      torch.ones_like(x, device=device)], dim=-1)
-
-  if normalize_rays_d:
-    dirs = torch.nn.functional.normalize(dirs, dim=-1)
-  return dirs, select_inds
-
-
-def get_rays(
-      rot: torch.Tensor,
-      trans: torch.Tensor,
-      focal_x: torch.Tensor,
-      focal_y: torch.Tensor,
-      H: int,
-      W: int,
-      N_rays: int = -1,
-      representation='axis-angle', # 'quaternion'
-      flatten=True,
-      **kwargs):
-  """
-  < opencv / colmap convention, standard pinhole camera >
-    the camera is facing [+z] direction, x right, y downwards
-                  z
-                ↗
-              o------> x
-              ↓
-              y
-  :param rot: (b, 3)
-  :param trans: (b, 3)
-  :param focal_x: ()
-  :param focal_y: ()
-  :param H:
-  :param W:
-  :param N_rays: -1: all
-  :param representation:
-
-  :return
-
-  - rays_o: (b, N_rays, 3)
-  - rays_d: (b, N_rays, 3)
-  - select_inds: (b, N_rays)
-  """
-
-  device = rot.device
-  assert rot.shape[:-1] == trans.shape[:-1]
-  prefix = rot.shape[:-1]  # [b, ]
-
-  dirs, select_inds = _get_direction(H=H, W=W, focal_x=focal_x, focal_y=focal_y,
-                                     prefix_dims=prefix, device=device, N_rays=N_rays)
-
-  # ---------
-  # Translate camera frame's origin to the world frame. It is the origin of all rays.
-  # ---------
-
-  if representation == 'quaternion':
-    # rot: [..., 4], trans: [..., 3]
-    assert rot.shape[-1] == 4
-    quat = tr3d.standardize_quaternion(F.normalize(rot, dim=-1))
-    rays_d = tr3d.quaternion_apply(quat[..., None, :], dirs)
-    rays_o = trans[..., None, :].expand_as(rays_d)
-
-  elif representation == 'axis-angle':
-    # original paper, rot: [..., 3], trans: [..., 3]
-    assert rot.shape[-1] == 3
-    ## pytorch 3d implementation: axis-angle --> quaternion -->matrix, [..., 3, 3]
-    rot_m = tr3d.axis_angle_to_matrix(rot)
-    # rotation: matrix multiplication, [..., N_rays, 1, 3] * [..., 1, 3, 3]
-    rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :3, :3], dim=-1)
-    rays_o = trans[..., None, :].expand_as(rays_d)
-
-  elif representation == 'rotation6D':
-    assert rot.shape[-1] == 6
-    rot_m = tr3d.rotation_6d_to_matrix(rot)
-    # rotation: matrix multiplication
-    # rays_d = rot_m.view(*prefix, 1, 3, 3)\
-    #     .expand([*prefix, N_rays, 3, 3]).flatten(0,-3).bmm(
-    #     dirs.flatten(0, -2).view([-1, 3, 1]))
-    # [..., N_rays, 1, 3] * [..., 1, 3, 3]
-    rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :3, :3], dim=-1)
-    rays_o = trans[..., None, :].expand_as(rays_d)
-
-  else:
-    raise RuntimeError("please choose representation")
-
-  if not flatten and N_rays < 0:
-    rays_o = rearrange(rays_o, "b (h w) c -> b h w c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b (h w) c -> b h w c", h=H, w=W)
-    select_inds = rearrange(select_inds, "b (h w) -> b h w", h=H, w=W)
-
-  # [..., N_rays, 3]
-  return rays_o, rays_d, select_inds
-
-
-def parse_intrinsic(intr):
-  """
-
-  :param intr: (3, 3)
-  :return:
-  """
-
-  fx = intr[0, 0]
-  fy = intr[1, 1]
-  cx = intr[0, 2]
-  cy = intr[1, 2]
-  return fx, fy, cx, cy
-
-
-def get_rays_by_intr_and_extr(
-        intrinsics,
-        c2w,
-        H: int,
-        W: int,
-        N_rays: int = -1,
-        flatten=False,
-        normalize_rays_d=False,
-      **kwargs):
-  """
-  Support backprop.
-
-  :param intrinsics: (3, 3)
-  :param c2w: (b, 4, 4)
-  :param H:
-  :param W:
-  :param N_rays:
-  :param kwargs:
-  :return:
-  """
-
-  device = c2w.device
-  prefix = c2w.shape[:-2]  # [b, ]
-
-  fx, fy, cx, cy = parse_intrinsic(intr=intrinsics)
-
-  dirs, select_inds = _get_direction(H=H, W=W, focal_x=fx, focal_y=fy, center_x=cx, center_y=cy,
-                                     prefix_dims=prefix, device=device, N_rays=N_rays,
-                                     normalize_rays_d=normalize_rays_d)
-
-  # (b, 3, 3)
-  rot_m = c2w[..., :3, :3]
-  # (b, 3)
-  trans = c2w[..., :3, 3]
-
-  # rotation: matrix multiplication, [..., N_rays, 1, 3] * [..., 1, 3, 3]
-  rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :, :], dim=-1)
-  rays_o = trans[..., None, :].expand_as(rays_d)
-
-  if not flatten and N_rays < 0:
-    rays_o = rearrange(rays_o, "b (h w) c -> b h w c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b (h w) c -> b h w c", h=H, w=W)
-    select_inds = rearrange(select_inds, "b (h w) -> b h w", h=H, w=W)
-
-  return rays_o, rays_d, select_inds
-
-def get_focal(f,
-              H,
-              W,
-              intr_repr='square') -> Tuple[torch.Tensor, torch.Tensor]:
-  """
-
-  :param f:
-  :param H:
-  :param W:
-  :param intr_repr:
-  :return: (fx, fy)
-  """
-  if intr_repr == 'square':
-    f = f ** 2
-  elif intr_repr == 'ratio':
-    f = f
-  elif intr_repr == 'exp':
-    f = torch.exp(f)
-  else:
-    raise RuntimeError("Please choose intr_repr")
-  fx, fy = f
-  fx = fx * W
-  fy = fy * H
-  return fx, fy
-
-
-def get_rotation_matrix(rot,
-                        representation='quaternion'):
-  """
-
-  :param rot: (b, 3)
-  :param representation: ['axis-angle', ]
-  :return: rot_m: (b, 3, 3)
-  """
-  if representation == 'axis-angle':
-    assert rot.shape[-1] == 3
-    # pytorch3d's implementation: axis-angle -> quaternion -> rotation matrix
-    rot_m = tr3d.axis_angle_to_matrix(rot)
-  elif representation == 'quaternion':
-    assert rot.shape[-1] == 4
-    quat = F.normalize(rot)
-    rot_m = tr3d.quaternion_to_matrix(quat)  # [...,3,3]
-  elif representation == 'rotation6D':
-    assert rot.shape[-1] == 6
-    rot_m = tr3d.rotation_6d_to_matrix(rot)
-  else:
-    raise RuntimeError("Please choose representation.")
-  return rot_m
-
-
-def get_camera2world(rot,
-                     trans,
-                     representation='quaternion'):
-  """
-
-  :param rot: (b, 3)
-  :param trans: (b, 3)
-  :param representation: ['axis-angle', ]
-  :return: homo_m: (b, 4, 4)
-  """
-  assert rot.shape[:-1] == trans.shape[:-1]
-  prefix = rot.shape[:-1]
-  rot_m = get_rotation_matrix(rot, representation)
-  tmp = torch.cat((rot_m.view(*prefix, 3, 3), trans.view(*prefix, 3, 1)), dim=-1)
-  extend = torch.zeros(*prefix, 1, 4).to(rot.device)
-  extend[..., 0, 3] = 1.
-  homo_m = torch.cat((tmp, extend), dim=-2)  # [...,4,4]
-
-  return homo_m  # [...,4,4]
-
-
-class CamParams(nn.Module):
-  def __init__(self,
-               phi,
-               t,
-               f,
-               H0=None,
-               W0=None,
-               so3_repr=None,
-               intr_repr=None,
-               freeze_intr=False,
-               normalize_rays_d=False):
-    super().__init__()
-    # self.extra_attr_keys = []
-    # self.register_extra_attr('so3_repr', so3_repr)
-    # self.register_extra_attr('intr_repr', intr_repr)
-    # self.register_extra_attr('H0', H0)  # used to calc focal length
-    # self.register_extra_attr('W0', W0)  # used to calc focal length
-
-    self.so3_repr = so3_repr
-    self.intr_repr = intr_repr
-    self.H0 = H0
-    self.W0 = W0
-    self.freeze_intr = freeze_intr
-    self.normalize_rays_d = normalize_rays_d
-
-    self.phi = nn.Parameter(phi)
-    self.t = nn.Parameter(t) # initial value: 0
-    self.f = nn.Parameter(f)
-    pass
-
-  @staticmethod
-  def from_config(num_imgs=1,
-                  H0: float = 1000,
-                  W0: float = 1000,
-                  so3_repr: str = 'axis-angle',
-                  intr_repr: str = 'square',
-                  initial_fov: float = 53.13,
-                  freeze_intr=True,
-                  normalize_rays_d=False,):
-    """
-    # Camera parameters to optimize: phi, t, f
-    # phi, t here is for camera2world
-
-    :param num_imgs:
-    :param H0:
-    :param W0:
-    :param so3_repr:
-    :param intr_repr:
-    :param initial_fov:
-    :return:
-    """
-
-
-    if so3_repr == 'quaternion':
-      phi = torch.tensor([1., 0., 0., 0.])
-
-    elif so3_repr == 'axis-angle':
-      phi = torch.tensor([0., 0., 0.])
-
-    elif so3_repr == 'rotation6D':
-      phi = torch.tensor([1., 0., 0., 0., 1., 0.])
-
-    else:
-      raise RuntimeError("Please choose representation")
-
-    phi = phi[None, :].expand(num_imgs, -1)
-
-    t = torch.zeros(num_imgs, 3)
-    sx = 0.5 / np.tan((.5 * initial_fov * np.pi / 180.))
-    sy = 0.5 / np.tan((.5 * initial_fov * np.pi / 180.))
-    f = torch.tensor([sx, sy])
-
-    if intr_repr == 'square':
-      f = torch.sqrt(f)
-    elif intr_repr == 'ratio':
-      pass
-    elif intr_repr == 'exp':
-      f = torch.log(f)
-    else:
-      raise RuntimeError("Please choose intr_repr")
-
-    m = CamParams(phi=phi.contiguous(),
-                  t=t.contiguous(),
-                  f=f.contiguous(),
-                  H0=H0,
-                  W0=W0,
-                  so3_repr=so3_repr,
-                  intr_repr=intr_repr,
-                  freeze_intr=freeze_intr,
-                  normalize_rays_d=normalize_rays_d)
-    return m
-
-  @staticmethod
-  def from_state_dict(state_dict):
-    m = CamParams(**state_dict)
-    return m
-
-  def forward(self,
-              indices=None,
-              mode='default'):
-    """
-
-    :param indices:
-    :param mode: get_intrinsic,
-    :return:
-
-    """
-    if mode == 'default':
-      fx, fy = self.get_focal()
-      return self.phi[indices], self.t[indices], fx, fy
-    elif mode == 'get_intrinsic':
-      if self.freeze_intr:
-        self.f.requires_grad_(False)
-      intr = self.get_intrinsic()
-      return intr
-    else:
-      raise NotImplementedError
-
-  def _get_random_pose(self,
-                       bs,
-                       r=1,
-                       h_stddev=0.3,
-                       v_stddev=0.155,
-                       h_mean=np.pi * 0.5,
-                       v_mean=np.pi * 0.5,
-                       sample_dist='gaussian',
-                       device='cuda'):
-    """
-    < opencv / colmap convention, standard pinhole camera >
-    the camera is facing [+z] direction, x right, y downwards
-          z
-        ↗
-      o------> x
-      ↓
-      y
-
-    :param bs:
-    :param r:
-    :param h_stddev:
-    :param v_stddev:
-    :param h_mean:
-    :param v_mean:
-    :param sample_dist:
-    :param device:
-    :return
-
-    - c2ws: (b, 4, 4)
-
-    """
-
-    camera_origin, pitch, yaw = geometry_tensor.sample_camera_positions(bs=bs,
-                                                                        r=r,
-                                                                        horizontal_stddev=h_stddev,
-                                                                        vertical_stddev=v_stddev,
-                                                                        horizontal_mean=h_mean,
-                                                                        vertical_mean=v_mean,
-                                                                        mode=sample_dist,
-                                                                        device=device)
-
-    # to opencv coordinate
-    camera_origin[:, 1] *= -1
-    camera_origin[:, 2] *= -1
-
-    up = torch.zeros(bs, 3, device=device)
-    up[:, 1] = 1
-    focus_in_world = torch.zeros(bs, 3, device=device)
-
-    c2ws = geometry_tensor.look_at(cam_location=camera_origin, point=focus_in_world, up=up)
-
-    return c2ws
-
-  def get_rays_random_pose(self,
-                           device,
-                           bs,
-                           # pixel coordinate to camera coordinate
-                           intr=None,
-                           H=None,
-                           W=None,
-                           # for random camera pose
-                           r=1,
-                           h_stddev=0.3,
-                           v_stddev=0.155,
-                           h_mean=np.pi * 0.5,
-                           v_mean=np.pi * 0.5,
-                           sample_dist='gaussian',
-                           # for rays
-                           N_rays: int = -1,
-                           **kwargs):
-    """
-    :param intr: (3, 3)
-
-    :return
-
-    - rays_o: (b, H, W, 3)
-    - rays_d: (b, H, W, 3)
-    - select_inds: (b, H, W)
-    """
-
-    if intr is None:
-      H, W = self.H0, self.W0
-      intr = self.get_intrinsic(H, W, device=device)
-    else:
-      if H is None and W is None:
-        H, W = self.H0, self.W0
-      elif H is None or W is None:
-        assert 0
-
-    c2ws = self._get_random_pose(bs=bs,
-                                 r=r,
-                                 h_stddev=h_stddev,
-                                 v_stddev=v_stddev,
-                                 h_mean=h_mean,
-                                 v_mean=v_mean,
-                                 sample_dist=sample_dist,
-                                 device=device)
-
-    rays_o, rays_d, select_inds = get_rays_by_intr_and_extr(
-      intrinsics=intr,
-      c2w=c2ws,
-      H=H,
-      W=W,
-      N_rays=N_rays,
-      flatten=False,
-      normalize_rays_d=self.normalize_rays_d)
-
-    return rays_o, rays_d, select_inds
-
-  def get_focal(self) -> Tuple[torch.Tensor, torch.Tensor]:
-    """
-
-    :return: (fx, fy)
-    """
-    return get_focal(f=self.f, H=self.H0, W=self.W0, intr_repr=self.intr_repr)
-
-  @torch.no_grad()
-  def get_rays_of_pose_avg(self,
-                           H,
-                           W,
-                           bs=1):
-
-    intr = self.get_intrinsic(H, W)
-    c2ws = self.poses_avg()
-    c2ws = c2ws.expand([bs, *c2ws.shape])
-
-    rays_o, rays_d, select_inds = get_rays_by_intr_and_extr(intrinsics=intr,
-                                                            c2w=c2ws,
-                                                            H=H,
-                                                            W=W,
-                                                            N_rays=-1,
-                                                            flatten=False)
-
-    return rays_o, rays_d
-
-  @torch.no_grad()
-  def poses_avg(self):
-
-    c2ws = self.get_camera2worlds()
-    c2w_center = geometry_tensor.poses_avg(c2ws)
-    return c2w_center
-
-  def get_camera2worlds(self):
-    """
-    Support backprop.
-
-    :return:
-    """
-    c2ws = get_camera2world(self.phi, self.t, self.so3_repr)
-    return c2ws
-
-  def get_intrinsic(self,
-                    new_H=None,
-                    new_W=None,
-                    **kwargs):
-    """
-    Support backprop.
-
-    :param new_H:
-    :param new_W:
-    :return: intr: (3, 3)
-            [[fx, 0, cx]
-             [0, fy, cy]
-             [0, 0, 1]]
-    """
-    scale_x = new_W / self.W0 if new_W is not None else 1.
-    scale_y = new_H / self.H0 if new_H is not None else 1.
-
-    fx, fy = self.get_focal()
-
-    intr = torch.eye(3, device=fx.device)
-    cx = self.W0 / 2.
-    cy = self.H0 / 2.
-    # OK with grad: with produce grad_fn=<CopySlices>
-    intr[0, 0] = fx * scale_x
-    intr[1, 1] = fy * scale_y
-    intr[0, 2] = cx * scale_x
-    intr[1, 2] = cy * scale_y
-    return intr
-
-  def get_approx_bounds(self, near: float, far: float):
-    fx, fy = get_focal(self.f.data.cpu(), self.H0, self.W0, self.intr_repr)
-    rays_o, rays_d, _ = get_rays(self.phi.data.cpu(), self.t.data.cpu(), fx, fy, self.H0, self.W0, -1, self.so3_repr)
-    rays_e = rays_o + rays_d * (far - near)
-    rays_o = rays_o.reshape(-1, 3)
-    rays_e = rays_e.reshape(-1, 3)
-    all_points = np.concatenate([rays_o, rays_e], axis=0)
-    min_points = np.min(all_points, axis=0)
-    max_points = np.max(all_points, axis=0)
-    return min_points, max_points
-
-  # def register_extra_attr(self, k, v):
-  #   self.__dict__[k] = v
-  #   self.extra_attr_keys.append(k)
-
-  # def load_state_dict(self,
-  #                     state_dict,
-  #                     strict: bool = True):
-  #   # Load extra non-tensor parameters
-  #   for k in self.extra_attr_keys:
-  #     assert k in state_dict, 'could not found key: [{}] in state_dict'.format(k)
-  #     self.__dict__[k] = state_dict[k]
-  #   # Notice: DO NOT deep copy. we do not want meaningless memory usage
-  #   nn_statedict = {}
-  #   for k, v in state_dict.items():
-  #     if k not in self.extra_attr_keys:
-  #       nn_statedict[k] = v
-  #   return super().load_state_dict(nn_statedict, strict=strict)
-  #
-  # def state_dict(self):
-  #   sdict = super().state_dict()
-  #   for k in self.extra_attr_keys:
-  #     sdict[k] = self.__dict__[k]
-  #   return sdict
-
-
-
-
-
-
-
-
-# -----------------
-# camera plotting utils
-# -----------------
-
-def about2index(about):
-  if len(about) != 2:
-    raise ValueError("Convention must have 2 letters.")
-  if about[0] == about[1]:
-    raise ValueError(f"Invalid convention {about}.")
-  for letter in about:
-    if letter not in ("x", "y", "z"):
-      raise ValueError(f"Invalid letter {letter} in convention string.")
-  letter2index = {'x': 0, 'y': 1, 'z': 2}
-  i0 = letter2index[about[0]]
-  i1 = letter2index[about[1]]
-  return i0, i1
-
-
-def plot_cam_trans(cam_param: CamParams, about='xy', return_img=False):
-  # --------
-  # get about index
-  i0, i1 = about2index(about)
-
-  fig = plt.figure()
-  ax = fig.add_subplot(111)
-  t = cam_param.t.data.cpu()
-  # x, y, z = t.unbind(-1)
-  t1, t2 = t[..., i0].numpy(), t[..., i1].numpy()
-  ax.plot(t1, t2, '^-')
-
-  if return_img:
-    return utils.figure_to_image(fig)
-  else:
-    return fig
-
-
-def plot_cam_rot(cam_param: CamParams, representation: str = 'quaternion', about='xy'):
-  # --------
-  # get about index
-  i0, i1 = about2index(about)
-
-  # ---------
-  # plot
-  fig = plt.figure()
-  ax = fig.add_subplot(111)
-  R = cam_param.phi.data.cpu()
-  rot_m = get_rotation_matrix(R, representation)
-  euler = tr3d.matrix_to_euler_angles(rot_m, 'XYZ')
-  # rx, ry, rz = euler.unbind(-1)
-  r1, r2 = euler[..., i0].numpy(), euler[..., i1].numpy()
-  ax.plot(r1, r2, '^-')
-  return fig
+import utils
+import numpy as np
+from typing import Tuple
+import matplotlib.pyplot as plt
+from einops import rearrange
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from pytorch3d import transforms as tr3d
+from . import geometry_tensor
+
+
+def _get_mesh_xy(H,
+                 W,
+                 device,
+                 prefix_dims=[] # broadcast for batch
+                 ):
+  """
+
+  :param H:
+  :param W:
+  :param device:
+  :param prefix_dims:
+  :return:
+  - x: (b, HxW)
+  - y: (b, HxW)
+  """
+  y, x = torch.meshgrid(torch.linspace(0, H - 1, H, device=device),
+                        torch.linspace(0, W - 1, W, device=device))
+
+  x = x.reshape([*[1] * len(prefix_dims), H * W]).expand([*prefix_dims, H * W]) + 0.5
+  y = y.reshape([*[1] * len(prefix_dims), H * W]).expand([*prefix_dims, H * W]) + 0.5
+
+  return x, y
+
+def _get_mesh_xy_deprecated(H,
+                            W,
+                            device,
+                            prefix_dims=[] # broadcast for batch
+                            ):
+  i, j = torch.meshgrid(torch.linspace(0, W - 1, W, device=device), torch.linspace(0, H - 1, H, device=device))
+  i = i.t().reshape([*len(prefix_dims) * [1], H * W]).expand([*prefix_dims, H * W])
+  j = j.t().reshape([*len(prefix_dims) * [1], H * W]).expand([*prefix_dims, H * W])
+
+  return i, j
+
+
+def _select_pixels(x,
+                   y,
+                   H,
+                   W,
+                   device,
+                   N_rays=-1,
+                   prefix_dims=[]):
+
+  if N_rays > 0 and N_rays < H * W:
+    select_inds = torch.multinomial(torch.ones(*[*prefix_dims, H*W], device=device),
+                                    num_samples=N_rays, replacement=False)
+    # select_inds = torch.from_numpy(
+    #   np.random.choice(H * W, size=[*prefix_dims, N_rays], replace=False)
+    # ).to(device)
+
+    x = torch.gather(x, dim=-1, index=select_inds)
+    y = torch.gather(y, dim=-1, index=select_inds)
+  else:
+    select_inds = torch.arange(H * W).to(device)
+    if len(prefix_dims) > 0:
+      select_inds = select_inds[None,].expand([*prefix_dims, -1])
+
+  return x, y, select_inds
+
+
+def _get_direction(H,
+                   W,
+                   focal_x,
+                   focal_y,
+                   prefix_dims,
+                   device,
+                   N_rays=-1,
+                   center_x=None,
+                   center_y=None,
+                   normalize_rays_d=False):
+  """
+
+  :param H:
+  :param W:
+  :param focal_x:
+  :param focal_y:
+  :param prefix_dims: [b, ]
+  :param device:
+  :param N_rays:
+  :return:
+  """
+  # [b, HxW]
+  x, y = _get_mesh_xy(H=H, W=W, device=device, prefix_dims=prefix_dims)
+  # x, y = _get_mesh_xy_deprecated(H=H, W=W, device=device, prefix_dims=prefix)
+  # assert (i == x).all()
+  # assert (j == y).all()
+
+  # [b, N_rays], pixel coordinate
+  x, y, select_inds = _select_pixels(x=x, y=y, H=H, W=W, N_rays=N_rays, device=device, prefix_dims=prefix_dims)
+
+  if center_x is None:
+    center_x = W / 2.
+  if center_y is None:
+    center_y = H / 2.
+
+  # [..., N_rays, 3], axes orientations : x right, y downwards, z positive, pixel coordinate to camera coordinate
+  dirs = torch.stack([(x - center_x) / focal_x,
+                      (y - center_y) / focal_y,
+                      torch.ones_like(x, device=device)], dim=-1)
+
+  if normalize_rays_d:
+    dirs = torch.nn.functional.normalize(dirs, dim=-1)
+  return dirs, select_inds
+
+
+def get_rays(
+      rot: torch.Tensor,
+      trans: torch.Tensor,
+      focal_x: torch.Tensor,
+      focal_y: torch.Tensor,
+      H: int,
+      W: int,
+      N_rays: int = -1,
+      representation='axis-angle', # 'quaternion'
+      flatten=True,
+      **kwargs):
+  """
+  < opencv / colmap convention, standard pinhole camera >
+    the camera is facing [+z] direction, x right, y downwards
+                  z
+                ↗
+              o------> x
+              ↓
+              y
+  :param rot: (b, 3)
+  :param trans: (b, 3)
+  :param focal_x: ()
+  :param focal_y: ()
+  :param H:
+  :param W:
+  :param N_rays: -1: all
+  :param representation:
+
+  :return
+
+  - rays_o: (b, N_rays, 3)
+  - rays_d: (b, N_rays, 3)
+  - select_inds: (b, N_rays)
+  """
+
+  device = rot.device
+  assert rot.shape[:-1] == trans.shape[:-1]
+  prefix = rot.shape[:-1]  # [b, ]
+
+  dirs, select_inds = _get_direction(H=H, W=W, focal_x=focal_x, focal_y=focal_y,
+                                     prefix_dims=prefix, device=device, N_rays=N_rays)
+
+  # ---------
+  # Translate camera frame's origin to the world frame. It is the origin of all rays.
+  # ---------
+
+  if representation == 'quaternion':
+    # rot: [..., 4], trans: [..., 3]
+    assert rot.shape[-1] == 4
+    quat = tr3d.standardize_quaternion(F.normalize(rot, dim=-1))
+    rays_d = tr3d.quaternion_apply(quat[..., None, :], dirs)
+    rays_o = trans[..., None, :].expand_as(rays_d)
+
+  elif representation == 'axis-angle':
+    # original paper, rot: [..., 3], trans: [..., 3]
+    assert rot.shape[-1] == 3
+    ## pytorch 3d implementation: axis-angle --> quaternion -->matrix, [..., 3, 3]
+    rot_m = tr3d.axis_angle_to_matrix(rot)
+    # rotation: matrix multiplication, [..., N_rays, 1, 3] * [..., 1, 3, 3]
+    rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :3, :3], dim=-1)
+    rays_o = trans[..., None, :].expand_as(rays_d)
+
+  elif representation == 'rotation6D':
+    assert rot.shape[-1] == 6
+    rot_m = tr3d.rotation_6d_to_matrix(rot)
+    # rotation: matrix multiplication
+    # rays_d = rot_m.view(*prefix, 1, 3, 3)\
+    #     .expand([*prefix, N_rays, 3, 3]).flatten(0,-3).bmm(
+    #     dirs.flatten(0, -2).view([-1, 3, 1]))
+    # [..., N_rays, 1, 3] * [..., 1, 3, 3]
+    rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :3, :3], dim=-1)
+    rays_o = trans[..., None, :].expand_as(rays_d)
+
+  else:
+    raise RuntimeError("please choose representation")
+
+  if not flatten and N_rays < 0:
+    rays_o = rearrange(rays_o, "b (h w) c -> b h w c", h=H, w=W)
+    rays_d = rearrange(rays_d, "b (h w) c -> b h w c", h=H, w=W)
+    select_inds = rearrange(select_inds, "b (h w) -> b h w", h=H, w=W)
+
+  # [..., N_rays, 3]
+  return rays_o, rays_d, select_inds
+
+
+def parse_intrinsic(intr):
+  """
+
+  :param intr: (3, 3)
+  :return:
+  """
+
+  fx = intr[0, 0]
+  fy = intr[1, 1]
+  cx = intr[0, 2]
+  cy = intr[1, 2]
+  return fx, fy, cx, cy
+
+
+def get_rays_by_intr_and_extr(
+        intrinsics,
+        c2w,
+        H: int,
+        W: int,
+        N_rays: int = -1,
+        flatten=False,
+        normalize_rays_d=False,
+      **kwargs):
+  """
+  Support backprop.
+
+  :param intrinsics: (3, 3)
+  :param c2w: (b, 4, 4)
+  :param H:
+  :param W:
+  :param N_rays:
+  :param kwargs:
+  :return:
+  """
+
+  device = c2w.device
+  prefix = c2w.shape[:-2]  # [b, ]
+
+  fx, fy, cx, cy = parse_intrinsic(intr=intrinsics)
+
+  dirs, select_inds = _get_direction(H=H, W=W, focal_x=fx, focal_y=fy, center_x=cx, center_y=cy,
+                                     prefix_dims=prefix, device=device, N_rays=N_rays,
+                                     normalize_rays_d=normalize_rays_d)
+
+  # (b, 3, 3)
+  rot_m = c2w[..., :3, :3]
+  # (b, 3)
+  trans = c2w[..., :3, 3]
+
+  # rotation: matrix multiplication, [..., N_rays, 1, 3] * [..., 1, 3, 3]
+  rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :, :], dim=-1)
+  rays_o = trans[..., None, :].expand_as(rays_d)
+
+  if not flatten and N_rays < 0:
+    rays_o = rearrange(rays_o, "b (h w) c -> b h w c", h=H, w=W)
+    rays_d = rearrange(rays_d, "b (h w) c -> b h w c", h=H, w=W)
+    select_inds = rearrange(select_inds, "b (h w) -> b h w", h=H, w=W)
+
+  return rays_o, rays_d, select_inds
+
+def get_focal(f,
+              H,
+              W,
+              intr_repr='square') -> Tuple[torch.Tensor, torch.Tensor]:
+  """
+
+  :param f:
+  :param H:
+  :param W:
+  :param intr_repr:
+  :return: (fx, fy)
+  """
+  if intr_repr == 'square':
+    f = f ** 2
+  elif intr_repr == 'ratio':
+    f = f
+  elif intr_repr == 'exp':
+    f = torch.exp(f)
+  else:
+    raise RuntimeError("Please choose intr_repr")
+  fx, fy = f
+  fx = fx * W
+  fy = fy * H
+  return fx, fy
+
+
+def get_rotation_matrix(rot,
+                        representation='quaternion'):
+  """
+
+  :param rot: (b, 3)
+  :param representation: ['axis-angle', ]
+  :return: rot_m: (b, 3, 3)
+  """
+  if representation == 'axis-angle':
+    assert rot.shape[-1] == 3
+    # pytorch3d's implementation: axis-angle -> quaternion -> rotation matrix
+    rot_m = tr3d.axis_angle_to_matrix(rot)
+  elif representation == 'quaternion':
+    assert rot.shape[-1] == 4
+    quat = F.normalize(rot)
+    rot_m = tr3d.quaternion_to_matrix(quat)  # [...,3,3]
+  elif representation == 'rotation6D':
+    assert rot.shape[-1] == 6
+    rot_m = tr3d.rotation_6d_to_matrix(rot)
+  else:
+    raise RuntimeError("Please choose representation.")
+  return rot_m
+
+
+def get_camera2world(rot,
+                     trans,
+                     representation='quaternion'):
+  """
+
+  :param rot: (b, 3)
+  :param trans: (b, 3)
+  :param representation: ['axis-angle', ]
+  :return: homo_m: (b, 4, 4)
+  """
+  assert rot.shape[:-1] == trans.shape[:-1]
+  prefix = rot.shape[:-1]
+  rot_m = get_rotation_matrix(rot, representation)
+  tmp = torch.cat((rot_m.view(*prefix, 3, 3), trans.view(*prefix, 3, 1)), dim=-1)
+  extend = torch.zeros(*prefix, 1, 4).to(rot.device)
+  extend[..., 0, 3] = 1.
+  homo_m = torch.cat((tmp, extend), dim=-2)  # [...,4,4]
+
+  return homo_m  # [...,4,4]
+
+
+class CamParams(nn.Module):
+  def __init__(self,
+               phi,
+               t,
+               f,
+               H0=None,
+               W0=None,
+               so3_repr=None,
+               intr_repr=None,
+               freeze_intr=False,
+               normalize_rays_d=False):
+    super().__init__()
+    # self.extra_attr_keys = []
+    # self.register_extra_attr('so3_repr', so3_repr)
+    # self.register_extra_attr('intr_repr', intr_repr)
+    # self.register_extra_attr('H0', H0)  # used to calc focal length
+    # self.register_extra_attr('W0', W0)  # used to calc focal length
+
+    self.so3_repr = so3_repr
+    self.intr_repr = intr_repr
+    self.H0 = H0
+    self.W0 = W0
+    self.freeze_intr = freeze_intr
+    self.normalize_rays_d = normalize_rays_d
+
+    self.phi = nn.Parameter(phi)
+    self.t = nn.Parameter(t) # initial value: 0
+    self.f = nn.Parameter(f)
+    pass
+
+  @staticmethod
+  def from_config(num_imgs=1,
+                  H0: float = 1000,
+                  W0: float = 1000,
+                  so3_repr: str = 'axis-angle',
+                  intr_repr: str = 'square',
+                  initial_fov: float = 53.13,
+                  freeze_intr=True,
+                  normalize_rays_d=False,):
+    """
+    # Camera parameters to optimize: phi, t, f
+    # phi, t here is for camera2world
+
+    :param num_imgs:
+    :param H0:
+    :param W0:
+    :param so3_repr:
+    :param intr_repr:
+    :param initial_fov:
+    :return:
+    """
+
+
+    if so3_repr == 'quaternion':
+      phi = torch.tensor([1., 0., 0., 0.])
+
+    elif so3_repr == 'axis-angle':
+      phi = torch.tensor([0., 0., 0.])
+
+    elif so3_repr == 'rotation6D':
+      phi = torch.tensor([1., 0., 0., 0., 1., 0.])
+
+    else:
+      raise RuntimeError("Please choose representation")
+
+    phi = phi[None, :].expand(num_imgs, -1)
+
+    t = torch.zeros(num_imgs, 3)
+    sx = 0.5 / np.tan((.5 * initial_fov * np.pi / 180.))
+    sy = 0.5 / np.tan((.5 * initial_fov * np.pi / 180.))
+    f = torch.tensor([sx, sy])
+
+    if intr_repr == 'square':
+      f = torch.sqrt(f)
+    elif intr_repr == 'ratio':
+      pass
+    elif intr_repr == 'exp':
+      f = torch.log(f)
+    else:
+      raise RuntimeError("Please choose intr_repr")
+
+    m = CamParams(phi=phi.contiguous(),
+                  t=t.contiguous(),
+                  f=f.contiguous(),
+                  H0=H0,
+                  W0=W0,
+                  so3_repr=so3_repr,
+                  intr_repr=intr_repr,
+                  freeze_intr=freeze_intr,
+                  normalize_rays_d=normalize_rays_d)
+    return m
+
+  @staticmethod
+  def from_state_dict(state_dict):
+    m = CamParams(**state_dict)
+    return m
+
+  def forward(self,
+              indices=None,
+              mode='default'):
+    """
+
+    :param indices:
+    :param mode: get_intrinsic,
+    :return:
+
+    """
+    if mode == 'default':
+      fx, fy = self.get_focal()
+      return self.phi[indices], self.t[indices], fx, fy
+    elif mode == 'get_intrinsic':
+      if self.freeze_intr:
+        self.f.requires_grad_(False)
+      intr = self.get_intrinsic()
+      return intr
+    else:
+      raise NotImplementedError
+
+  def _get_random_pose(self,
+                       bs,
+                       r=1,
+                       h_stddev=0.3,
+                       v_stddev=0.155,
+                       h_mean=np.pi * 0.5,
+                       v_mean=np.pi * 0.5,
+                       sample_dist='gaussian',
+                       device='cuda'):
+    """
+    < opencv / colmap convention, standard pinhole camera >
+    the camera is facing [+z] direction, x right, y downwards
+          z
+        ↗
+      o------> x
+      ↓
+      y
+
+    :param bs:
+    :param r:
+    :param h_stddev:
+    :param v_stddev:
+    :param h_mean:
+    :param v_mean:
+    :param sample_dist:
+    :param device:
+    :return
+
+    - c2ws: (b, 4, 4)
+
+    """
+
+    camera_origin, pitch, yaw = geometry_tensor.sample_camera_positions(bs=bs,
+                                                                        r=r,
+                                                                        horizontal_stddev=h_stddev,
+                                                                        vertical_stddev=v_stddev,
+                                                                        horizontal_mean=h_mean,
+                                                                        vertical_mean=v_mean,
+                                                                        mode=sample_dist,
+                                                                        device=device)
+
+    # to opencv coordinate
+    camera_origin[:, 1] *= -1
+    camera_origin[:, 2] *= -1
+
+    up = torch.zeros(bs, 3, device=device)
+    up[:, 1] = 1
+    focus_in_world = torch.zeros(bs, 3, device=device)
+
+    c2ws = geometry_tensor.look_at(cam_location=camera_origin, point=focus_in_world, up=up)
+
+    return c2ws
+
+  def get_rays_random_pose(self,
+                           device,
+                           bs,
+                           # pixel coordinate to camera coordinate
+                           intr=None,
+                           H=None,
+                           W=None,
+                           # for random camera pose
+                           r=1,
+                           h_stddev=0.3,
+                           v_stddev=0.155,
+                           h_mean=np.pi * 0.5,
+                           v_mean=np.pi * 0.5,
+                           sample_dist='gaussian',
+                           # for rays
+                           N_rays: int = -1,
+                           **kwargs):
+    """
+    :param intr: (3, 3)
+
+    :return
+
+    - rays_o: (b, H, W, 3)
+    - rays_d: (b, H, W, 3)
+    - select_inds: (b, H, W)
+    """
+
+    if intr is None:
+      H, W = self.H0, self.W0
+      intr = self.get_intrinsic(H, W, device=device)
+    else:
+      if H is None and W is None:
+        H, W = self.H0, self.W0
+      elif H is None or W is None:
+        assert 0
+
+    c2ws = self._get_random_pose(bs=bs,
+                                 r=r,
+                                 h_stddev=h_stddev,
+                                 v_stddev=v_stddev,
+                                 h_mean=h_mean,
+                                 v_mean=v_mean,
+                                 sample_dist=sample_dist,
+                                 device=device)
+
+    rays_o, rays_d, select_inds = get_rays_by_intr_and_extr(
+      intrinsics=intr,
+      c2w=c2ws,
+      H=H,
+      W=W,
+      N_rays=N_rays,
+      flatten=False,
+      normalize_rays_d=self.normalize_rays_d)
+
+    return rays_o, rays_d, select_inds
+
+  def get_focal(self) -> Tuple[torch.Tensor, torch.Tensor]:
+    """
+
+    :return: (fx, fy)
+    """
+    return get_focal(f=self.f, H=self.H0, W=self.W0, intr_repr=self.intr_repr)
+
+  @torch.no_grad()
+  def get_rays_of_pose_avg(self,
+                           H,
+                           W,
+                           bs=1):
+
+    intr = self.get_intrinsic(H, W)
+    c2ws = self.poses_avg()
+    c2ws = c2ws.expand([bs, *c2ws.shape])
+
+    rays_o, rays_d, select_inds = get_rays_by_intr_and_extr(intrinsics=intr,
+                                                            c2w=c2ws,
+                                                            H=H,
+                                                            W=W,
+                                                            N_rays=-1,
+                                                            flatten=False)
+
+    return rays_o, rays_d
+
+  @torch.no_grad()
+  def poses_avg(self):
+
+    c2ws = self.get_camera2worlds()
+    c2w_center = geometry_tensor.poses_avg(c2ws)
+    return c2w_center
+
+  def get_camera2worlds(self):
+    """
+    Support backprop.
+
+    :return:
+    """
+    c2ws = get_camera2world(self.phi, self.t, self.so3_repr)
+    return c2ws
+
+  def get_intrinsic(self,
+                    new_H=None,
+                    new_W=None,
+                    **kwargs):
+    """
+    Support backprop.
+
+    :param new_H:
+    :param new_W:
+    :return: intr: (3, 3)
+            [[fx, 0, cx]
+             [0, fy, cy]
+             [0, 0, 1]]
+    """
+    scale_x = new_W / self.W0 if new_W is not None else 1.
+    scale_y = new_H / self.H0 if new_H is not None else 1.
+
+    fx, fy = self.get_focal()
+
+    intr = torch.eye(3, device=fx.device)
+    cx = self.W0 / 2.
+    cy = self.H0 / 2.
+    # OK with grad: with produce grad_fn=<CopySlices>
+    intr[0, 0] = fx * scale_x
+    intr[1, 1] = fy * scale_y
+    intr[0, 2] = cx * scale_x
+    intr[1, 2] = cy * scale_y
+    return intr
+
+  def get_approx_bounds(self, near: float, far: float):
+    fx, fy = get_focal(self.f.data.cpu(), self.H0, self.W0, self.intr_repr)
+    rays_o, rays_d, _ = get_rays(self.phi.data.cpu(), self.t.data.cpu(), fx, fy, self.H0, self.W0, -1, self.so3_repr)
+    rays_e = rays_o + rays_d * (far - near)
+    rays_o = rays_o.reshape(-1, 3)
+    rays_e = rays_e.reshape(-1, 3)
+    all_points = np.concatenate([rays_o, rays_e], axis=0)
+    min_points = np.min(all_points, axis=0)
+    max_points = np.max(all_points, axis=0)
+    return min_points, max_points
+
+  # def register_extra_attr(self, k, v):
+  #   self.__dict__[k] = v
+  #   self.extra_attr_keys.append(k)
+
+  # def load_state_dict(self,
+  #                     state_dict,
+  #                     strict: bool = True):
+  #   # Load extra non-tensor parameters
+  #   for k in self.extra_attr_keys:
+  #     assert k in state_dict, 'could not found key: [{}] in state_dict'.format(k)
+  #     self.__dict__[k] = state_dict[k]
+  #   # Notice: DO NOT deep copy. we do not want meaningless memory usage
+  #   nn_statedict = {}
+  #   for k, v in state_dict.items():
+  #     if k not in self.extra_attr_keys:
+  #       nn_statedict[k] = v
+  #   return super().load_state_dict(nn_statedict, strict=strict)
+  #
+  # def state_dict(self):
+  #   sdict = super().state_dict()
+  #   for k in self.extra_attr_keys:
+  #     sdict[k] = self.__dict__[k]
+  #   return sdict
+
+
+
+
+
+
+
+
+# -----------------
+# camera plotting utils
+# -----------------
+
+def about2index(about):
+  if len(about) != 2:
+    raise ValueError("Convention must have 2 letters.")
+  if about[0] == about[1]:
+    raise ValueError(f"Invalid convention {about}.")
+  for letter in about:
+    if letter not in ("x", "y", "z"):
+      raise ValueError(f"Invalid letter {letter} in convention string.")
+  letter2index = {'x': 0, 'y': 1, 'z': 2}
+  i0 = letter2index[about[0]]
+  i1 = letter2index[about[1]]
+  return i0, i1
+
+
+def plot_cam_trans(cam_param: CamParams, about='xy', return_img=False):
+  # --------
+  # get about index
+  i0, i1 = about2index(about)
+
+  fig = plt.figure()
+  ax = fig.add_subplot(111)
+  t = cam_param.t.data.cpu()
+  # x, y, z = t.unbind(-1)
+  t1, t2 = t[..., i0].numpy(), t[..., i1].numpy()
+  ax.plot(t1, t2, '^-')
+
+  if return_img:
+    return utils.figure_to_image(fig)
+  else:
+    return fig
+
+
+def plot_cam_rot(cam_param: CamParams, representation: str = 'quaternion', about='xy'):
+  # --------
+  # get about index
+  i0, i1 = about2index(about)
+
+  # ---------
+  # plot
+  fig = plt.figure()
+  ax = fig.add_subplot(111)
+  R = cam_param.phi.data.cpu()
+  rot_m = get_rotation_matrix(R, representation)
+  euler = tr3d.matrix_to_euler_angles(rot_m, 'XYZ')
+  # rx, ry, rz = euler.unbind(-1)
+  r1, r2 = euler[..., i0].numpy(), euler[..., i1].numpy()
+  ax.plot(r1, r2, '^-')
+  return fig
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/nerf/cam_params_pigan.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/nerf/cam_params_pigan.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,730 +1,730 @@
-import utils
-import numpy as np
-from typing import Tuple
-import matplotlib.pyplot as plt
-from einops import rearrange
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-from pytorch3d import transforms as tr3d
-from . import geometry_tensor
-
-
-def _get_mesh_xy(H,
-                 W,
-                 device,
-                 prefix_dims=[] # broadcast for batch
-                 ):
-  """
-
-  :param H:
-  :param W:
-  :param device:
-  :param prefix_dims:
-  :return:
-  - x: (b, HxW)
-  - y: (b, HxW)
-  """
-  y, x = torch.meshgrid(torch.linspace(H * 3/2, H * (-1/2), H, device=device),
-                        torch.linspace(W * (-1/2), W * 3/2, W, device=device))
-
-  x = x.reshape([*[1] * len(prefix_dims), H * W]).expand([*prefix_dims, H * W])
-  y = y.reshape([*[1] * len(prefix_dims), H * W]).expand([*prefix_dims, H * W])
-
-  return x, y
-
-def _get_mesh_xy_deprecated(H,
-                            W,
-                            device,
-                            prefix_dims=[] # broadcast for batch
-                            ):
-  i, j = torch.meshgrid(torch.linspace(0, W - 1, W, device=device), torch.linspace(0, H - 1, H, device=device))
-  i = i.t().reshape([*len(prefix_dims) * [1], H * W]).expand([*prefix_dims, H * W])
-  j = j.t().reshape([*len(prefix_dims) * [1], H * W]).expand([*prefix_dims, H * W])
-
-  return i, j
-
-
-def _select_pixels(x,
-                   y,
-                   H,
-                   W,
-                   device,
-                   N_rays=-1,
-                   prefix_dims=[]):
-
-  if N_rays > 0 and N_rays < H * W:
-    select_inds = torch.multinomial(torch.ones(*[*prefix_dims, H*W], device=device),
-                                    num_samples=N_rays, replacement=False)
-    # select_inds = torch.from_numpy(
-    #   np.random.choice(H * W, size=[*prefix_dims, N_rays], replace=False)
-    # ).to(device)
-
-    x = torch.gather(x, dim=-1, index=select_inds)
-    y = torch.gather(y, dim=-1, index=select_inds)
-  else:
-    select_inds = torch.arange(H * W).to(device)
-    if len(prefix_dims) > 0:
-      select_inds = select_inds[None,].expand([*prefix_dims, -1])
-
-  return x, y, select_inds
-
-
-def _get_direction(H,
-                   W,
-                   focal_x,
-                   focal_y,
-                   prefix_dims,
-                   device,
-                   N_rays=-1,
-                   center_x=None,
-                   center_y=None,
-                   normalize_rays_d=False):
-  """
-
-  :param H:
-  :param W:
-  :param focal_x:
-  :param focal_y:
-  :param prefix_dims: [b, ]
-  :param device:
-  :param N_rays:
-  :return:
-  """
-  # [b, HxW]
-  x, y = _get_mesh_xy(H=H, W=W, device=device, prefix_dims=prefix_dims)
-  # x, y = _get_mesh_xy_deprecated(H=H, W=W, device=device, prefix_dims=prefix)
-  # assert (i == x).all()
-  # assert (j == y).all()
-
-  # [b, N_rays], pixel coordinate
-  x, y, select_inds = _select_pixels(x=x, y=y, H=H, W=W, N_rays=N_rays, device=device, prefix_dims=prefix_dims)
-
-  if center_x is None:
-    center_x = W / 2.
-  if center_y is None:
-    center_y = H / 2.
-
-  # [..., N_rays, 3], axes orientations : x right, y downwards, z positive, pixel coordinate to camera coordinate
-  dirs = torch.stack([(x - center_x) / focal_x,
-                      (y - center_y) / focal_y,
-                      - torch.ones_like(x, device=device)], dim=-1)
-
-  if normalize_rays_d:
-    dirs = torch.nn.functional.normalize(dirs, dim=-1)
-  return dirs, select_inds
-
-
-def get_rays(
-      rot: torch.Tensor,
-      trans: torch.Tensor,
-      focal_x: torch.Tensor,
-      focal_y: torch.Tensor,
-      H: int,
-      W: int,
-      N_rays: int = -1,
-      representation='axis-angle', # 'quaternion'
-      flatten=True,
-      **kwargs):
-  """
-  < opencv / colmap convention, standard pinhole camera >
-    the camera is facing [+z] direction, x right, y downwards
-                  z
-                ↗
-              o------> x
-              ↓
-              y
-  :param rot: (b, 3)
-  :param trans: (b, 3)
-  :param focal_x: ()
-  :param focal_y: ()
-  :param H:
-  :param W:
-  :param N_rays: -1: all
-  :param representation:
-
-  :return
-
-  - rays_o: (b, N_rays, 3)
-  - rays_d: (b, N_rays, 3)
-  - select_inds: (b, N_rays)
-  """
-
-  device = rot.device
-  assert rot.shape[:-1] == trans.shape[:-1]
-  prefix = rot.shape[:-1]  # [b, ]
-
-  dirs, select_inds = _get_direction(H=H, W=W, focal_x=focal_x, focal_y=focal_y,
-                                     prefix_dims=prefix, device=device, N_rays=N_rays)
-
-  # ---------
-  # Translate camera frame's origin to the world frame. It is the origin of all rays.
-  # ---------
-
-  if representation == 'quaternion':
-    # rot: [..., 4], trans: [..., 3]
-    assert rot.shape[-1] == 4
-    quat = tr3d.standardize_quaternion(F.normalize(rot, dim=-1))
-    rays_d = tr3d.quaternion_apply(quat[..., None, :], dirs)
-    rays_o = trans[..., None, :].expand_as(rays_d)
-
-  elif representation == 'axis-angle':
-    # original paper, rot: [..., 3], trans: [..., 3]
-    assert rot.shape[-1] == 3
-    ## pytorch 3d implementation: axis-angle --> quaternion -->matrix, [..., 3, 3]
-    rot_m = tr3d.axis_angle_to_matrix(rot)
-    # rotation: matrix multiplication, [..., N_rays, 1, 3] * [..., 1, 3, 3]
-    rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :3, :3], dim=-1)
-    rays_o = trans[..., None, :].expand_as(rays_d)
-
-  elif representation == 'rotation6D':
-    assert rot.shape[-1] == 6
-    rot_m = tr3d.rotation_6d_to_matrix(rot)
-    # rotation: matrix multiplication
-    # rays_d = rot_m.view(*prefix, 1, 3, 3)\
-    #     .expand([*prefix, N_rays, 3, 3]).flatten(0,-3).bmm(
-    #     dirs.flatten(0, -2).view([-1, 3, 1]))
-    # [..., N_rays, 1, 3] * [..., 1, 3, 3]
-    rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :3, :3], dim=-1)
-    rays_o = trans[..., None, :].expand_as(rays_d)
-
-  else:
-    raise RuntimeError("please choose representation")
-
-  if not flatten and N_rays < 0:
-    rays_o = rearrange(rays_o, "b (h w) c -> b h w c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b (h w) c -> b h w c", h=H, w=W)
-    select_inds = rearrange(select_inds, "b (h w) -> b h w", h=H, w=W)
-
-  # [..., N_rays, 3]
-  return rays_o, rays_d, select_inds
-
-
-def parse_intrinsic(intr):
-  """
-
-  :param intr: (3, 3)
-  :return:
-  """
-
-  fx = intr[0, 0]
-  fy = intr[1, 1]
-  cx = intr[0, 2]
-  cy = intr[1, 2]
-  return fx, fy, cx, cy
-
-
-def get_rays_by_intr_and_extr(
-        intrinsics,
-        c2w,
-        H: int,
-        W: int,
-        N_rays: int = -1,
-        flatten=False,
-        normalize_rays_d=False,
-      **kwargs):
-  """
-  Support backprop.
-
-  :param intrinsics: (3, 3)
-  :param c2w: (b, 4, 4)
-  :param H:
-  :param W:
-  :param N_rays:
-  :param kwargs:
-  :return:
-  """
-
-  device = c2w.device
-  prefix = c2w.shape[:-2]  # [b, ]
-
-  fx, fy, cx, cy = parse_intrinsic(intr=intrinsics)
-
-  dirs, select_inds = _get_direction(H=H, W=W, focal_x=fx, focal_y=fy, center_x=cx, center_y=cy,
-                                     prefix_dims=prefix, device=device, N_rays=N_rays,
-                                     normalize_rays_d=normalize_rays_d)
-
-  # (b, 3, 3)
-  rot_m = c2w[..., :3, :3]
-  # (b, 3)
-  trans = c2w[..., :3, 3]
-
-  # rotation: matrix multiplication, [..., N_rays, 1, 3] * [..., 1, 3, 3]
-  rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :, :], dim=-1)
-  rays_o = trans[..., None, :].expand_as(rays_d)
-
-  if not flatten and N_rays < 0:
-    rays_o = rearrange(rays_o, "b (h w) c -> b h w c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b (h w) c -> b h w c", h=H, w=W)
-    select_inds = rearrange(select_inds, "b (h w) -> b h w", h=H, w=W)
-
-  return rays_o, rays_d, select_inds
-
-def get_focal(f,
-              H,
-              W,
-              intr_repr='square') -> Tuple[torch.Tensor, torch.Tensor]:
-  """
-
-  :param f:
-  :param H:
-  :param W:
-  :param intr_repr:
-  :return: (fx, fy)
-  """
-  if intr_repr == 'square':
-    f = f ** 2
-  elif intr_repr == 'ratio':
-    f = f
-  elif intr_repr == 'exp':
-    f = torch.exp(f)
-  else:
-    raise RuntimeError("Please choose intr_repr")
-  fx, fy = f
-  fx = fx * W
-  fy = fy * H
-  return fx, fy
-
-
-def get_rotation_matrix(rot,
-                        representation='quaternion'):
-  """
-
-  :param rot: (b, 3)
-  :param representation: ['axis-angle', ]
-  :return: rot_m: (b, 3, 3)
-  """
-  if representation == 'axis-angle':
-    assert rot.shape[-1] == 3
-    # pytorch3d's implementation: axis-angle -> quaternion -> rotation matrix
-    rot_m = tr3d.axis_angle_to_matrix(rot)
-  elif representation == 'quaternion':
-    assert rot.shape[-1] == 4
-    quat = F.normalize(rot)
-    rot_m = tr3d.quaternion_to_matrix(quat)  # [...,3,3]
-  elif representation == 'rotation6D':
-    assert rot.shape[-1] == 6
-    rot_m = tr3d.rotation_6d_to_matrix(rot)
-  else:
-    raise RuntimeError("Please choose representation.")
-  return rot_m
-
-
-def get_camera2world(rot,
-                     trans,
-                     representation='quaternion'):
-  """
-
-  :param rot: (b, 3)
-  :param trans: (b, 3)
-  :param representation: ['axis-angle', ]
-  :return: homo_m: (b, 4, 4)
-  """
-  assert rot.shape[:-1] == trans.shape[:-1]
-  prefix = rot.shape[:-1]
-  rot_m = get_rotation_matrix(rot, representation)
-  tmp = torch.cat((rot_m.view(*prefix, 3, 3), trans.view(*prefix, 3, 1)), dim=-1)
-  extend = torch.zeros(*prefix, 1, 4).to(rot.device)
-  extend[..., 0, 3] = 1.
-  homo_m = torch.cat((tmp, extend), dim=-2)  # [...,4,4]
-
-  return homo_m  # [...,4,4]
-
-
-class CamParams(nn.Module):
-  def __init__(self,
-               phi,
-               t,
-               f,
-               H0=None,
-               W0=None,
-               so3_repr=None,
-               intr_repr=None,
-               freeze_intr=False,
-               normalize_rays_d=False):
-    super().__init__()
-    # self.extra_attr_keys = []
-    # self.register_extra_attr('so3_repr', so3_repr)
-    # self.register_extra_attr('intr_repr', intr_repr)
-    # self.register_extra_attr('H0', H0)  # used to calc focal length
-    # self.register_extra_attr('W0', W0)  # used to calc focal length
-
-    self.so3_repr = so3_repr
-    self.intr_repr = intr_repr
-    self.H0 = H0
-    self.W0 = W0
-    self.freeze_intr = freeze_intr
-    self.normalize_rays_d = normalize_rays_d
-
-    self.phi = nn.Parameter(phi)
-    self.t = nn.Parameter(t) # initial value: 0
-    self.f = nn.Parameter(f)
-    pass
-
-  @staticmethod
-  def from_config(num_imgs=1,
-                  H0: float = 1000,
-                  W0: float = 1000,
-                  so3_repr: str = 'axis-angle',
-                  intr_repr: str = 'square',
-                  initial_fov: float = 12,
-                  freeze_intr=True,
-                  normalize_rays_d=True):
-    """
-    # Camera parameters to optimize: phi, t, f
-    # phi, t here is for camera2world
-
-    :param num_imgs:
-    :param H0:
-    :param W0:
-    :param so3_repr:
-    :param intr_repr:
-    :param initial_fov:
-    :return:
-    """
-
-
-    if so3_repr == 'quaternion':
-      phi = torch.tensor([1., 0., 0., 0.])
-
-    elif so3_repr == 'axis-angle':
-      phi = torch.tensor([0., 0., 0.])
-
-    elif so3_repr == 'rotation6D':
-      phi = torch.tensor([1., 0., 0., 0., 1., 0.])
-
-    else:
-      raise RuntimeError("Please choose representation")
-
-    phi = phi[None, :].expand(num_imgs, -1)
-
-    t = torch.zeros(num_imgs, 3)
-    sx = 1. / np.tan((.5 * initial_fov * np.pi / 180.))
-    sy = 1. / np.tan((.5 * initial_fov * np.pi / 180.))
-    f = torch.tensor([sx, sy])
-
-    if intr_repr == 'square':
-      f = torch.sqrt(f)
-    elif intr_repr == 'ratio':
-      pass
-    elif intr_repr == 'exp':
-      f = torch.log(f)
-    else:
-      raise RuntimeError("Please choose intr_repr")
-
-    m = CamParams(phi=phi.contiguous(),
-                  t=t.contiguous(),
-                  f=f.contiguous(),
-                  H0=H0,
-                  W0=W0,
-                  so3_repr=so3_repr,
-                  intr_repr=intr_repr,
-                  freeze_intr=freeze_intr,
-                  normalize_rays_d=normalize_rays_d)
-    return m
-
-  @staticmethod
-  def from_state_dict(state_dict):
-    m = CamParams(**state_dict)
-    return m
-
-  def forward(self,
-              indices=None,
-              mode='default'):
-    """
-
-    :param indices:
-    :param mode: get_intrinsic,
-    :return:
-
-    """
-    if mode == 'default':
-      fx, fy = self.get_focal()
-      return self.phi[indices], self.t[indices], fx, fy
-    elif mode == 'get_intrinsic':
-      if self.freeze_intr:
-        self.f.requires_grad_(False)
-      intr = self.get_intrinsic()
-      return intr
-    else:
-      raise NotImplementedError
-
-  def _get_random_pose(self,
-                       bs,
-                       r=1,
-                       h_stddev=0.3,
-                       v_stddev=0.155,
-                       h_mean=np.pi * 0.5,
-                       v_mean=np.pi * 0.5,
-                       sample_dist='gaussian',
-                       device='cuda'):
-    """
-    < opencv / colmap convention, standard pinhole camera >
-    the camera is facing [+z] direction, x right, y downwards
-          z
-        ↗
-      o------> x
-      ↓
-      y
-
-    :param bs:
-    :param r:
-    :param h_stddev:
-    :param v_stddev:
-    :param h_mean:
-    :param v_mean:
-    :param sample_dist:
-    :param device:
-    :return
-
-    - c2ws: (b, 4, 4)
-
-    """
-
-    camera_origin, pitch, yaw = geometry_tensor.sample_camera_positions(bs=bs,
-                                                                        r=r,
-                                                                        horizontal_stddev=h_stddev,
-                                                                        vertical_stddev=v_stddev,
-                                                                        horizontal_mean=h_mean,
-                                                                        vertical_mean=v_mean,
-                                                                        mode=sample_dist,
-                                                                        device=device)
-
-    # to opencv coordinate
-    # camera_origin[:, 1] *= -1
-    # camera_origin[:, 2] *= -1
-
-    up = torch.zeros(bs, 3, device=device)
-    up[:, 1] = 1
-    focus_in_world = torch.zeros(bs, 3, device=device)
-
-    # c2ws = geometry_tensor.look_at(cam_location=camera_origin, point=focus_in_world, up=up)
-    # Cam points in positive -z direction
-    forward_z = geometry_tensor.normalize(camera_origin - focus_in_world)
-    c2ws = geometry_tensor.view_matrix(forward_z, up, camera_origin)
-
-    return c2ws
-
-  def get_rays_random_pose(self,
-                           device,
-                           bs,
-                           # pixel coordinate to camera coordinate
-                           intr=None,
-                           H=None,
-                           W=None,
-                           # for random camera pose
-                           r=1,
-                           h_stddev=0.3,
-                           v_stddev=0.155,
-                           h_mean=np.pi * 0.5,
-                           v_mean=np.pi * 0.5,
-                           sample_dist='gaussian',
-                           # for rays
-                           N_rays: int = -1,
-                           **kwargs):
-    """
-    :param intr: (3, 3)
-
-    :return
-
-    - rays_o: (b, H, W, 3)
-    - rays_d: (b, H, W, 3)
-    - select_inds: (b, H, W)
-    """
-
-    if intr is None:
-      H, W = self.H0, self.W0
-      intr = self.get_intrinsic(H, W, device=device)
-    else:
-      if H is None and W is None:
-        H, W = self.H0, self.W0
-      elif H is None or W is None:
-        assert 0
-
-    c2ws = self._get_random_pose(bs=bs,
-                                 r=r,
-                                 h_stddev=h_stddev,
-                                 v_stddev=v_stddev,
-                                 h_mean=h_mean,
-                                 v_mean=v_mean,
-                                 sample_dist=sample_dist,
-                                 device=device)
-
-    rays_o, rays_d, select_inds = get_rays_by_intr_and_extr(
-      intrinsics=intr,
-      c2w=c2ws,
-      H=H,
-      W=W,
-      N_rays=N_rays,
-      flatten=False,
-      normalize_rays_d=self.normalize_rays_d)
-
-    return rays_o, rays_d, select_inds
-
-  def get_focal(self) -> Tuple[torch.Tensor, torch.Tensor]:
-    """
-
-    :return: (fx, fy)
-    """
-    return get_focal(f=self.f, H=self.H0, W=self.W0, intr_repr=self.intr_repr)
-
-  @torch.no_grad()
-  def get_rays_of_pose_avg(self,
-                           H,
-                           W,
-                           bs=1):
-
-    intr = self.get_intrinsic(H, W)
-    c2ws = self.poses_avg()
-    c2ws = c2ws.expand([bs, *c2ws.shape])
-
-    rays_o, rays_d, select_inds = get_rays_by_intr_and_extr(intrinsics=intr,
-                                                            c2w=c2ws,
-                                                            H=H,
-                                                            W=W,
-                                                            N_rays=-1,
-                                                            flatten=False)
-
-    return rays_o, rays_d
-
-  @torch.no_grad()
-  def poses_avg(self):
-
-    c2ws = self.get_camera2worlds()
-    c2w_center = geometry_tensor.poses_avg(c2ws)
-    return c2w_center
-
-  def get_camera2worlds(self):
-    """
-    Support backprop.
-
-    :return:
-    """
-    c2ws = get_camera2world(self.phi, self.t, self.so3_repr)
-    return c2ws
-
-  def get_intrinsic(self,
-                    new_H=None,
-                    new_W=None,
-                    **kwargs):
-    """
-    Support backprop.
-
-    :param new_H:
-    :param new_W:
-    :return: intr: (3, 3)
-            [[fx, 0, cx]
-             [0, fy, cy]
-             [0, 0, 1]]
-    """
-    scale_x = new_W / self.W0 if new_W is not None else 1.
-    scale_y = new_H / self.H0 if new_H is not None else 1.
-
-    fx, fy = self.get_focal()
-
-    intr = torch.eye(3, device=fx.device)
-    cx = self.W0 / 2.
-    cy = self.H0 / 2.
-    # OK with grad: with produce grad_fn=<CopySlices>
-    intr[0, 0] = fx * scale_x
-    intr[1, 1] = fy * scale_y
-    intr[0, 2] = cx * scale_x
-    intr[1, 2] = cy * scale_y
-    return intr
-
-  def get_approx_bounds(self, near: float, far: float):
-    fx, fy = get_focal(self.f.data.cpu(), self.H0, self.W0, self.intr_repr)
-    rays_o, rays_d, _ = get_rays(self.phi.data.cpu(), self.t.data.cpu(), fx, fy, self.H0, self.W0, -1, self.so3_repr)
-    rays_e = rays_o + rays_d * (far - near)
-    rays_o = rays_o.reshape(-1, 3)
-    rays_e = rays_e.reshape(-1, 3)
-    all_points = np.concatenate([rays_o, rays_e], axis=0)
-    min_points = np.min(all_points, axis=0)
-    max_points = np.max(all_points, axis=0)
-    return min_points, max_points
-
-  # def register_extra_attr(self, k, v):
-  #   self.__dict__[k] = v
-  #   self.extra_attr_keys.append(k)
-
-  # def load_state_dict(self,
-  #                     state_dict,
-  #                     strict: bool = True):
-  #   # Load extra non-tensor parameters
-  #   for k in self.extra_attr_keys:
-  #     assert k in state_dict, 'could not found key: [{}] in state_dict'.format(k)
-  #     self.__dict__[k] = state_dict[k]
-  #   # Notice: DO NOT deep copy. we do not want meaningless memory usage
-  #   nn_statedict = {}
-  #   for k, v in state_dict.items():
-  #     if k not in self.extra_attr_keys:
-  #       nn_statedict[k] = v
-  #   return super().load_state_dict(nn_statedict, strict=strict)
-  #
-  # def state_dict(self):
-  #   sdict = super().state_dict()
-  #   for k in self.extra_attr_keys:
-  #     sdict[k] = self.__dict__[k]
-  #   return sdict
-
-
-
-
-
-
-
-
-# -----------------
-# camera plotting utils
-# -----------------
-
-def about2index(about):
-  if len(about) != 2:
-    raise ValueError("Convention must have 2 letters.")
-  if about[0] == about[1]:
-    raise ValueError(f"Invalid convention {about}.")
-  for letter in about:
-    if letter not in ("x", "y", "z"):
-      raise ValueError(f"Invalid letter {letter} in convention string.")
-  letter2index = {'x': 0, 'y': 1, 'z': 2}
-  i0 = letter2index[about[0]]
-  i1 = letter2index[about[1]]
-  return i0, i1
-
-
-def plot_cam_trans(cam_param: CamParams, about='xy', return_img=False):
-  # --------
-  # get about index
-  i0, i1 = about2index(about)
-
-  fig = plt.figure()
-  ax = fig.add_subplot(111)
-  t = cam_param.t.data.cpu()
-  # x, y, z = t.unbind(-1)
-  t1, t2 = t[..., i0].numpy(), t[..., i1].numpy()
-  ax.plot(t1, t2, '^-')
-
-  if return_img:
-    return utils.figure_to_image(fig)
-  else:
-    return fig
-
-
-def plot_cam_rot(cam_param: CamParams, representation: str = 'quaternion', about='xy'):
-  # --------
-  # get about index
-  i0, i1 = about2index(about)
-
-  # ---------
-  # plot
-  fig = plt.figure()
-  ax = fig.add_subplot(111)
-  R = cam_param.phi.data.cpu()
-  rot_m = get_rotation_matrix(R, representation)
-  euler = tr3d.matrix_to_euler_angles(rot_m, 'XYZ')
-  # rx, ry, rz = euler.unbind(-1)
-  r1, r2 = euler[..., i0].numpy(), euler[..., i1].numpy()
-  ax.plot(r1, r2, '^-')
-  return fig
+import utils
+import numpy as np
+from typing import Tuple
+import matplotlib.pyplot as plt
+from einops import rearrange
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from pytorch3d import transforms as tr3d
+from . import geometry_tensor
+
+
+def _get_mesh_xy(H,
+                 W,
+                 device,
+                 prefix_dims=[] # broadcast for batch
+                 ):
+  """
+
+  :param H:
+  :param W:
+  :param device:
+  :param prefix_dims:
+  :return:
+  - x: (b, HxW)
+  - y: (b, HxW)
+  """
+  y, x = torch.meshgrid(torch.linspace(H * 3/2, H * (-1/2), H, device=device),
+                        torch.linspace(W * (-1/2), W * 3/2, W, device=device))
+
+  x = x.reshape([*[1] * len(prefix_dims), H * W]).expand([*prefix_dims, H * W])
+  y = y.reshape([*[1] * len(prefix_dims), H * W]).expand([*prefix_dims, H * W])
+
+  return x, y
+
+def _get_mesh_xy_deprecated(H,
+                            W,
+                            device,
+                            prefix_dims=[] # broadcast for batch
+                            ):
+  i, j = torch.meshgrid(torch.linspace(0, W - 1, W, device=device), torch.linspace(0, H - 1, H, device=device))
+  i = i.t().reshape([*len(prefix_dims) * [1], H * W]).expand([*prefix_dims, H * W])
+  j = j.t().reshape([*len(prefix_dims) * [1], H * W]).expand([*prefix_dims, H * W])
+
+  return i, j
+
+
+def _select_pixels(x,
+                   y,
+                   H,
+                   W,
+                   device,
+                   N_rays=-1,
+                   prefix_dims=[]):
+
+  if N_rays > 0 and N_rays < H * W:
+    select_inds = torch.multinomial(torch.ones(*[*prefix_dims, H*W], device=device),
+                                    num_samples=N_rays, replacement=False)
+    # select_inds = torch.from_numpy(
+    #   np.random.choice(H * W, size=[*prefix_dims, N_rays], replace=False)
+    # ).to(device)
+
+    x = torch.gather(x, dim=-1, index=select_inds)
+    y = torch.gather(y, dim=-1, index=select_inds)
+  else:
+    select_inds = torch.arange(H * W).to(device)
+    if len(prefix_dims) > 0:
+      select_inds = select_inds[None,].expand([*prefix_dims, -1])
+
+  return x, y, select_inds
+
+
+def _get_direction(H,
+                   W,
+                   focal_x,
+                   focal_y,
+                   prefix_dims,
+                   device,
+                   N_rays=-1,
+                   center_x=None,
+                   center_y=None,
+                   normalize_rays_d=False):
+  """
+
+  :param H:
+  :param W:
+  :param focal_x:
+  :param focal_y:
+  :param prefix_dims: [b, ]
+  :param device:
+  :param N_rays:
+  :return:
+  """
+  # [b, HxW]
+  x, y = _get_mesh_xy(H=H, W=W, device=device, prefix_dims=prefix_dims)
+  # x, y = _get_mesh_xy_deprecated(H=H, W=W, device=device, prefix_dims=prefix)
+  # assert (i == x).all()
+  # assert (j == y).all()
+
+  # [b, N_rays], pixel coordinate
+  x, y, select_inds = _select_pixels(x=x, y=y, H=H, W=W, N_rays=N_rays, device=device, prefix_dims=prefix_dims)
+
+  if center_x is None:
+    center_x = W / 2.
+  if center_y is None:
+    center_y = H / 2.
+
+  # [..., N_rays, 3], axes orientations : x right, y downwards, z positive, pixel coordinate to camera coordinate
+  dirs = torch.stack([(x - center_x) / focal_x,
+                      (y - center_y) / focal_y,
+                      - torch.ones_like(x, device=device)], dim=-1)
+
+  if normalize_rays_d:
+    dirs = torch.nn.functional.normalize(dirs, dim=-1)
+  return dirs, select_inds
+
+
+def get_rays(
+      rot: torch.Tensor,
+      trans: torch.Tensor,
+      focal_x: torch.Tensor,
+      focal_y: torch.Tensor,
+      H: int,
+      W: int,
+      N_rays: int = -1,
+      representation='axis-angle', # 'quaternion'
+      flatten=True,
+      **kwargs):
+  """
+  < opencv / colmap convention, standard pinhole camera >
+    the camera is facing [+z] direction, x right, y downwards
+                  z
+                ↗
+              o------> x
+              ↓
+              y
+  :param rot: (b, 3)
+  :param trans: (b, 3)
+  :param focal_x: ()
+  :param focal_y: ()
+  :param H:
+  :param W:
+  :param N_rays: -1: all
+  :param representation:
+
+  :return
+
+  - rays_o: (b, N_rays, 3)
+  - rays_d: (b, N_rays, 3)
+  - select_inds: (b, N_rays)
+  """
+
+  device = rot.device
+  assert rot.shape[:-1] == trans.shape[:-1]
+  prefix = rot.shape[:-1]  # [b, ]
+
+  dirs, select_inds = _get_direction(H=H, W=W, focal_x=focal_x, focal_y=focal_y,
+                                     prefix_dims=prefix, device=device, N_rays=N_rays)
+
+  # ---------
+  # Translate camera frame's origin to the world frame. It is the origin of all rays.
+  # ---------
+
+  if representation == 'quaternion':
+    # rot: [..., 4], trans: [..., 3]
+    assert rot.shape[-1] == 4
+    quat = tr3d.standardize_quaternion(F.normalize(rot, dim=-1))
+    rays_d = tr3d.quaternion_apply(quat[..., None, :], dirs)
+    rays_o = trans[..., None, :].expand_as(rays_d)
+
+  elif representation == 'axis-angle':
+    # original paper, rot: [..., 3], trans: [..., 3]
+    assert rot.shape[-1] == 3
+    ## pytorch 3d implementation: axis-angle --> quaternion -->matrix, [..., 3, 3]
+    rot_m = tr3d.axis_angle_to_matrix(rot)
+    # rotation: matrix multiplication, [..., N_rays, 1, 3] * [..., 1, 3, 3]
+    rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :3, :3], dim=-1)
+    rays_o = trans[..., None, :].expand_as(rays_d)
+
+  elif representation == 'rotation6D':
+    assert rot.shape[-1] == 6
+    rot_m = tr3d.rotation_6d_to_matrix(rot)
+    # rotation: matrix multiplication
+    # rays_d = rot_m.view(*prefix, 1, 3, 3)\
+    #     .expand([*prefix, N_rays, 3, 3]).flatten(0,-3).bmm(
+    #     dirs.flatten(0, -2).view([-1, 3, 1]))
+    # [..., N_rays, 1, 3] * [..., 1, 3, 3]
+    rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :3, :3], dim=-1)
+    rays_o = trans[..., None, :].expand_as(rays_d)
+
+  else:
+    raise RuntimeError("please choose representation")
+
+  if not flatten and N_rays < 0:
+    rays_o = rearrange(rays_o, "b (h w) c -> b h w c", h=H, w=W)
+    rays_d = rearrange(rays_d, "b (h w) c -> b h w c", h=H, w=W)
+    select_inds = rearrange(select_inds, "b (h w) -> b h w", h=H, w=W)
+
+  # [..., N_rays, 3]
+  return rays_o, rays_d, select_inds
+
+
+def parse_intrinsic(intr):
+  """
+
+  :param intr: (3, 3)
+  :return:
+  """
+
+  fx = intr[0, 0]
+  fy = intr[1, 1]
+  cx = intr[0, 2]
+  cy = intr[1, 2]
+  return fx, fy, cx, cy
+
+
+def get_rays_by_intr_and_extr(
+        intrinsics,
+        c2w,
+        H: int,
+        W: int,
+        N_rays: int = -1,
+        flatten=False,
+        normalize_rays_d=False,
+      **kwargs):
+  """
+  Support backprop.
+
+  :param intrinsics: (3, 3)
+  :param c2w: (b, 4, 4)
+  :param H:
+  :param W:
+  :param N_rays:
+  :param kwargs:
+  :return:
+  """
+
+  device = c2w.device
+  prefix = c2w.shape[:-2]  # [b, ]
+
+  fx, fy, cx, cy = parse_intrinsic(intr=intrinsics)
+
+  dirs, select_inds = _get_direction(H=H, W=W, focal_x=fx, focal_y=fy, center_x=cx, center_y=cy,
+                                     prefix_dims=prefix, device=device, N_rays=N_rays,
+                                     normalize_rays_d=normalize_rays_d)
+
+  # (b, 3, 3)
+  rot_m = c2w[..., :3, :3]
+  # (b, 3)
+  trans = c2w[..., :3, 3]
+
+  # rotation: matrix multiplication, [..., N_rays, 1, 3] * [..., 1, 3, 3]
+  rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :, :], dim=-1)
+  rays_o = trans[..., None, :].expand_as(rays_d)
+
+  if not flatten and N_rays < 0:
+    rays_o = rearrange(rays_o, "b (h w) c -> b h w c", h=H, w=W)
+    rays_d = rearrange(rays_d, "b (h w) c -> b h w c", h=H, w=W)
+    select_inds = rearrange(select_inds, "b (h w) -> b h w", h=H, w=W)
+
+  return rays_o, rays_d, select_inds
+
+def get_focal(f,
+              H,
+              W,
+              intr_repr='square') -> Tuple[torch.Tensor, torch.Tensor]:
+  """
+
+  :param f:
+  :param H:
+  :param W:
+  :param intr_repr:
+  :return: (fx, fy)
+  """
+  if intr_repr == 'square':
+    f = f ** 2
+  elif intr_repr == 'ratio':
+    f = f
+  elif intr_repr == 'exp':
+    f = torch.exp(f)
+  else:
+    raise RuntimeError("Please choose intr_repr")
+  fx, fy = f
+  fx = fx * W
+  fy = fy * H
+  return fx, fy
+
+
+def get_rotation_matrix(rot,
+                        representation='quaternion'):
+  """
+
+  :param rot: (b, 3)
+  :param representation: ['axis-angle', ]
+  :return: rot_m: (b, 3, 3)
+  """
+  if representation == 'axis-angle':
+    assert rot.shape[-1] == 3
+    # pytorch3d's implementation: axis-angle -> quaternion -> rotation matrix
+    rot_m = tr3d.axis_angle_to_matrix(rot)
+  elif representation == 'quaternion':
+    assert rot.shape[-1] == 4
+    quat = F.normalize(rot)
+    rot_m = tr3d.quaternion_to_matrix(quat)  # [...,3,3]
+  elif representation == 'rotation6D':
+    assert rot.shape[-1] == 6
+    rot_m = tr3d.rotation_6d_to_matrix(rot)
+  else:
+    raise RuntimeError("Please choose representation.")
+  return rot_m
+
+
+def get_camera2world(rot,
+                     trans,
+                     representation='quaternion'):
+  """
+
+  :param rot: (b, 3)
+  :param trans: (b, 3)
+  :param representation: ['axis-angle', ]
+  :return: homo_m: (b, 4, 4)
+  """
+  assert rot.shape[:-1] == trans.shape[:-1]
+  prefix = rot.shape[:-1]
+  rot_m = get_rotation_matrix(rot, representation)
+  tmp = torch.cat((rot_m.view(*prefix, 3, 3), trans.view(*prefix, 3, 1)), dim=-1)
+  extend = torch.zeros(*prefix, 1, 4).to(rot.device)
+  extend[..., 0, 3] = 1.
+  homo_m = torch.cat((tmp, extend), dim=-2)  # [...,4,4]
+
+  return homo_m  # [...,4,4]
+
+
+class CamParams(nn.Module):
+  def __init__(self,
+               phi,
+               t,
+               f,
+               H0=None,
+               W0=None,
+               so3_repr=None,
+               intr_repr=None,
+               freeze_intr=False,
+               normalize_rays_d=False):
+    super().__init__()
+    # self.extra_attr_keys = []
+    # self.register_extra_attr('so3_repr', so3_repr)
+    # self.register_extra_attr('intr_repr', intr_repr)
+    # self.register_extra_attr('H0', H0)  # used to calc focal length
+    # self.register_extra_attr('W0', W0)  # used to calc focal length
+
+    self.so3_repr = so3_repr
+    self.intr_repr = intr_repr
+    self.H0 = H0
+    self.W0 = W0
+    self.freeze_intr = freeze_intr
+    self.normalize_rays_d = normalize_rays_d
+
+    self.phi = nn.Parameter(phi)
+    self.t = nn.Parameter(t) # initial value: 0
+    self.f = nn.Parameter(f)
+    pass
+
+  @staticmethod
+  def from_config(num_imgs=1,
+                  H0: float = 1000,
+                  W0: float = 1000,
+                  so3_repr: str = 'axis-angle',
+                  intr_repr: str = 'square',
+                  initial_fov: float = 12,
+                  freeze_intr=True,
+                  normalize_rays_d=True):
+    """
+    # Camera parameters to optimize: phi, t, f
+    # phi, t here is for camera2world
+
+    :param num_imgs:
+    :param H0:
+    :param W0:
+    :param so3_repr:
+    :param intr_repr:
+    :param initial_fov:
+    :return:
+    """
+
+
+    if so3_repr == 'quaternion':
+      phi = torch.tensor([1., 0., 0., 0.])
+
+    elif so3_repr == 'axis-angle':
+      phi = torch.tensor([0., 0., 0.])
+
+    elif so3_repr == 'rotation6D':
+      phi = torch.tensor([1., 0., 0., 0., 1., 0.])
+
+    else:
+      raise RuntimeError("Please choose representation")
+
+    phi = phi[None, :].expand(num_imgs, -1)
+
+    t = torch.zeros(num_imgs, 3)
+    sx = 1. / np.tan((.5 * initial_fov * np.pi / 180.))
+    sy = 1. / np.tan((.5 * initial_fov * np.pi / 180.))
+    f = torch.tensor([sx, sy])
+
+    if intr_repr == 'square':
+      f = torch.sqrt(f)
+    elif intr_repr == 'ratio':
+      pass
+    elif intr_repr == 'exp':
+      f = torch.log(f)
+    else:
+      raise RuntimeError("Please choose intr_repr")
+
+    m = CamParams(phi=phi.contiguous(),
+                  t=t.contiguous(),
+                  f=f.contiguous(),
+                  H0=H0,
+                  W0=W0,
+                  so3_repr=so3_repr,
+                  intr_repr=intr_repr,
+                  freeze_intr=freeze_intr,
+                  normalize_rays_d=normalize_rays_d)
+    return m
+
+  @staticmethod
+  def from_state_dict(state_dict):
+    m = CamParams(**state_dict)
+    return m
+
+  def forward(self,
+              indices=None,
+              mode='default'):
+    """
+
+    :param indices:
+    :param mode: get_intrinsic,
+    :return:
+
+    """
+    if mode == 'default':
+      fx, fy = self.get_focal()
+      return self.phi[indices], self.t[indices], fx, fy
+    elif mode == 'get_intrinsic':
+      if self.freeze_intr:
+        self.f.requires_grad_(False)
+      intr = self.get_intrinsic()
+      return intr
+    else:
+      raise NotImplementedError
+
+  def _get_random_pose(self,
+                       bs,
+                       r=1,
+                       h_stddev=0.3,
+                       v_stddev=0.155,
+                       h_mean=np.pi * 0.5,
+                       v_mean=np.pi * 0.5,
+                       sample_dist='gaussian',
+                       device='cuda'):
+    """
+    < opencv / colmap convention, standard pinhole camera >
+    the camera is facing [+z] direction, x right, y downwards
+          z
+        ↗
+      o------> x
+      ↓
+      y
+
+    :param bs:
+    :param r:
+    :param h_stddev:
+    :param v_stddev:
+    :param h_mean:
+    :param v_mean:
+    :param sample_dist:
+    :param device:
+    :return
+
+    - c2ws: (b, 4, 4)
+
+    """
+
+    camera_origin, pitch, yaw = geometry_tensor.sample_camera_positions(bs=bs,
+                                                                        r=r,
+                                                                        horizontal_stddev=h_stddev,
+                                                                        vertical_stddev=v_stddev,
+                                                                        horizontal_mean=h_mean,
+                                                                        vertical_mean=v_mean,
+                                                                        mode=sample_dist,
+                                                                        device=device)
+
+    # to opencv coordinate
+    # camera_origin[:, 1] *= -1
+    # camera_origin[:, 2] *= -1
+
+    up = torch.zeros(bs, 3, device=device)
+    up[:, 1] = 1
+    focus_in_world = torch.zeros(bs, 3, device=device)
+
+    # c2ws = geometry_tensor.look_at(cam_location=camera_origin, point=focus_in_world, up=up)
+    # Cam points in positive -z direction
+    forward_z = geometry_tensor.normalize(camera_origin - focus_in_world)
+    c2ws = geometry_tensor.view_matrix(forward_z, up, camera_origin)
+
+    return c2ws
+
+  def get_rays_random_pose(self,
+                           device,
+                           bs,
+                           # pixel coordinate to camera coordinate
+                           intr=None,
+                           H=None,
+                           W=None,
+                           # for random camera pose
+                           r=1,
+                           h_stddev=0.3,
+                           v_stddev=0.155,
+                           h_mean=np.pi * 0.5,
+                           v_mean=np.pi * 0.5,
+                           sample_dist='gaussian',
+                           # for rays
+                           N_rays: int = -1,
+                           **kwargs):
+    """
+    :param intr: (3, 3)
+
+    :return
+
+    - rays_o: (b, H, W, 3)
+    - rays_d: (b, H, W, 3)
+    - select_inds: (b, H, W)
+    """
+
+    if intr is None:
+      H, W = self.H0, self.W0
+      intr = self.get_intrinsic(H, W, device=device)
+    else:
+      if H is None and W is None:
+        H, W = self.H0, self.W0
+      elif H is None or W is None:
+        assert 0
+
+    c2ws = self._get_random_pose(bs=bs,
+                                 r=r,
+                                 h_stddev=h_stddev,
+                                 v_stddev=v_stddev,
+                                 h_mean=h_mean,
+                                 v_mean=v_mean,
+                                 sample_dist=sample_dist,
+                                 device=device)
+
+    rays_o, rays_d, select_inds = get_rays_by_intr_and_extr(
+      intrinsics=intr,
+      c2w=c2ws,
+      H=H,
+      W=W,
+      N_rays=N_rays,
+      flatten=False,
+      normalize_rays_d=self.normalize_rays_d)
+
+    return rays_o, rays_d, select_inds
+
+  def get_focal(self) -> Tuple[torch.Tensor, torch.Tensor]:
+    """
+
+    :return: (fx, fy)
+    """
+    return get_focal(f=self.f, H=self.H0, W=self.W0, intr_repr=self.intr_repr)
+
+  @torch.no_grad()
+  def get_rays_of_pose_avg(self,
+                           H,
+                           W,
+                           bs=1):
+
+    intr = self.get_intrinsic(H, W)
+    c2ws = self.poses_avg()
+    c2ws = c2ws.expand([bs, *c2ws.shape])
+
+    rays_o, rays_d, select_inds = get_rays_by_intr_and_extr(intrinsics=intr,
+                                                            c2w=c2ws,
+                                                            H=H,
+                                                            W=W,
+                                                            N_rays=-1,
+                                                            flatten=False)
+
+    return rays_o, rays_d
+
+  @torch.no_grad()
+  def poses_avg(self):
+
+    c2ws = self.get_camera2worlds()
+    c2w_center = geometry_tensor.poses_avg(c2ws)
+    return c2w_center
+
+  def get_camera2worlds(self):
+    """
+    Support backprop.
+
+    :return:
+    """
+    c2ws = get_camera2world(self.phi, self.t, self.so3_repr)
+    return c2ws
+
+  def get_intrinsic(self,
+                    new_H=None,
+                    new_W=None,
+                    **kwargs):
+    """
+    Support backprop.
+
+    :param new_H:
+    :param new_W:
+    :return: intr: (3, 3)
+            [[fx, 0, cx]
+             [0, fy, cy]
+             [0, 0, 1]]
+    """
+    scale_x = new_W / self.W0 if new_W is not None else 1.
+    scale_y = new_H / self.H0 if new_H is not None else 1.
+
+    fx, fy = self.get_focal()
+
+    intr = torch.eye(3, device=fx.device)
+    cx = self.W0 / 2.
+    cy = self.H0 / 2.
+    # OK with grad: with produce grad_fn=<CopySlices>
+    intr[0, 0] = fx * scale_x
+    intr[1, 1] = fy * scale_y
+    intr[0, 2] = cx * scale_x
+    intr[1, 2] = cy * scale_y
+    return intr
+
+  def get_approx_bounds(self, near: float, far: float):
+    fx, fy = get_focal(self.f.data.cpu(), self.H0, self.W0, self.intr_repr)
+    rays_o, rays_d, _ = get_rays(self.phi.data.cpu(), self.t.data.cpu(), fx, fy, self.H0, self.W0, -1, self.so3_repr)
+    rays_e = rays_o + rays_d * (far - near)
+    rays_o = rays_o.reshape(-1, 3)
+    rays_e = rays_e.reshape(-1, 3)
+    all_points = np.concatenate([rays_o, rays_e], axis=0)
+    min_points = np.min(all_points, axis=0)
+    max_points = np.max(all_points, axis=0)
+    return min_points, max_points
+
+  # def register_extra_attr(self, k, v):
+  #   self.__dict__[k] = v
+  #   self.extra_attr_keys.append(k)
+
+  # def load_state_dict(self,
+  #                     state_dict,
+  #                     strict: bool = True):
+  #   # Load extra non-tensor parameters
+  #   for k in self.extra_attr_keys:
+  #     assert k in state_dict, 'could not found key: [{}] in state_dict'.format(k)
+  #     self.__dict__[k] = state_dict[k]
+  #   # Notice: DO NOT deep copy. we do not want meaningless memory usage
+  #   nn_statedict = {}
+  #   for k, v in state_dict.items():
+  #     if k not in self.extra_attr_keys:
+  #       nn_statedict[k] = v
+  #   return super().load_state_dict(nn_statedict, strict=strict)
+  #
+  # def state_dict(self):
+  #   sdict = super().state_dict()
+  #   for k in self.extra_attr_keys:
+  #     sdict[k] = self.__dict__[k]
+  #   return sdict
+
+
+
+
+
+
+
+
+# -----------------
+# camera plotting utils
+# -----------------
+
+def about2index(about):
+  if len(about) != 2:
+    raise ValueError("Convention must have 2 letters.")
+  if about[0] == about[1]:
+    raise ValueError(f"Invalid convention {about}.")
+  for letter in about:
+    if letter not in ("x", "y", "z"):
+      raise ValueError(f"Invalid letter {letter} in convention string.")
+  letter2index = {'x': 0, 'y': 1, 'z': 2}
+  i0 = letter2index[about[0]]
+  i1 = letter2index[about[1]]
+  return i0, i1
+
+
+def plot_cam_trans(cam_param: CamParams, about='xy', return_img=False):
+  # --------
+  # get about index
+  i0, i1 = about2index(about)
+
+  fig = plt.figure()
+  ax = fig.add_subplot(111)
+  t = cam_param.t.data.cpu()
+  # x, y, z = t.unbind(-1)
+  t1, t2 = t[..., i0].numpy(), t[..., i1].numpy()
+  ax.plot(t1, t2, '^-')
+
+  if return_img:
+    return utils.figure_to_image(fig)
+  else:
+    return fig
+
+
+def plot_cam_rot(cam_param: CamParams, representation: str = 'quaternion', about='xy'):
+  # --------
+  # get about index
+  i0, i1 = about2index(about)
+
+  # ---------
+  # plot
+  fig = plt.figure()
+  ax = fig.add_subplot(111)
+  R = cam_param.phi.data.cpu()
+  rot_m = get_rotation_matrix(R, representation)
+  euler = tr3d.matrix_to_euler_angles(rot_m, 'XYZ')
+  # rx, ry, rz = euler.unbind(-1)
+  r1, r2 = euler[..., i0].numpy(), euler[..., i1].numpy()
+  ax.plot(r1, r2, '^-')
+  return fig
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/nerf/cam_params_v1.py` & `tl2-0.1.1/tl2/proj/pytorch/torch_utils.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,731 +1,644 @@
-import utils
+import pathlib
+import datetime
+import traceback
+import pprint
+import logging
+import os
+import argparse
+import random
 import numpy as np
-from typing import Tuple
-import matplotlib.pyplot as plt
 from einops import rearrange
 
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
+import torch.optim as optim
+import torchvision.transforms.functional as tv_f
+from torchvision.datasets import ImageFolder as ImageFolder_base
+
+from tl2.proj.fvcore.checkpoint import Checkpointer
+
+from .ddp.ddp_utils import parser_local_rank, is_distributed
+
+
+def init_seeds(seed=0,
+               rank=0,
+               # cuda_deterministic=True
+               ):
+  seed = seed + rank
+  print(f"{rank}: seed={seed}")
+
+  random.seed(seed)
+  np.random.seed(seed)
+  torch.manual_seed(seed)
+
+  if torch.cuda.is_available():
+    torch.cuda.manual_seed_all(seed)
+
+  # if cuda_deterministic:
+  #   torch.backends.cudnn.deterministic = True
+  #   torch.backends.cudnn.benchmark = False
+  # else:  # faster, less reproducible
+  #   torch.backends.cudnn.deterministic = False
+  #   torch.backends.cudnn.benchmark = True
+  pass
+
+
+def requires_grad(model, flag=True):
+  for p in model.parameters():
+    p.requires_grad = flag
+
+
+def print_number_params(models_dict,
+                        logger=None,
+                        add_info=""):
+  """
+    self.module_name_list = []
+    logger = logging.getLogger('tl')
+    models_dict = {}
+    for name in self.module_name_list:
+      models_dict[name] = getattr(self, name)
+    models_dict['G'] = self
+    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
+    logger.info(self)
+
+  :param models_dict:
+  :param logger:
+  :param add_info:
+  :return:
+  """
+  print()
+  if logger is None:
+    logger = logging.getLogger('tl')
+    print_func = logger.info
+  elif hasattr(logger, 'info'):
+    print_func = logger.info
+  else:
+    print_func = logger
+
+  for label, model in models_dict.items():
+    if model is None:
+      # logger.info(f'Number of params in {label}:\t 0M')
+      print_func(f'{label + ":":<40} '
+                 f"{'paras:'} {0:10.6f}M  {add_info}")
+    else:
+      num_params = sum([p.data.nelement() for p in model.parameters()]) / 1e6
+      num_bufs = sum([p.data.nelement() for p in model.buffers()]) / 1e6
 
-from pytorch3d import transforms as tr3d
-from . import geometry_tensor
+      print_func(f'{label + ":":<40} '
+                 f"{'paras:'} {num_params:10.6f}M"
+                 f"{'':<1} bufs: {str(num_bufs)}M  {add_info}",
+                 )
+
+def save_models(save_dir,
+                model_dict,
+                info_msg=None,
+                cfg=None,
+                msg_mode='w',
+                save_module=True):
+  """
+  def save_models(
+      unet,
+      state_dict,
+      info_msg,
+      saved_dir=None):
+
+    model_dict = {
+        'unet': unet,
+    }
+
+    if saved_dir is None:
+        ckpt_max2keep = tl2_utils.MaxToKeep.get_named_max_to_keep(name='ckpt', use_circle_number=True)
+        saved_dir = ckpt_max2keep.step_and_ret_circle_dir(global_cfg.tl_ckptdir)
+    os.makedirs(saved_dir, exist_ok=True)
+
+    global_cfg.dump_to_file_with_command(f"{saved_dir}/config_command.yaml", global_cfg.tl_command)
+
+    torch_utils.save_models(save_dir=saved_dir, model_dict=model_dict)
+    tl2_utils.write_info_msg(saved_dir, info_msg)
+
+    return saved_dir
+
+  Args:
+    save_dir:
+    model_dict:
+    info_msg:
+    cfg:
+    msg_mode:
+
+  Returns:
+
+  """
+  os.makedirs(save_dir, exist_ok=True)
+  for name, model in model_dict.items():
+    if hasattr(model, 'state_dict'):
+      # module and optim
+      torch.save(model.state_dict(), f"{save_dir}/{name}.pth")
+      if isinstance(model, nn.Module) and save_module:
+        torch.save(model, f"{save_dir}/{name}_model.pth")
+    else:
+      # dict
+      torch.save(model, f"{save_dir}/{name}.pth")
 
+  if info_msg is not None:
+    with open(f"{save_dir}/0info.txt", msg_mode) as f:
+      f.write(f"{info_msg}\n")
+
+  if cfg is not None:
+    cfg.dump_to_file_with_command(f"{save_dir}/config_command.yaml", cfg.tl_command)
+
+  pass
+
+def load_models(save_dir,
+                model_dict,
+                rank=0,
+                verbose=True,
+                **kwargs):
+  logger = logging.getLogger('tl')
+  logger.info(f"Loading models from {save_dir}\n"
+              f"models: {model_dict.keys()}")
+
+  map_location = lambda storage, loc: storage.cuda(rank)
+
+  for name, model in model_dict.items():
+    ckpt_path = f"{save_dir}/{name}.pth"
+    if not os.path.exists(ckpt_path):
+      logger.info(f"Do not exist, skip load {ckpt_path}!")
+      continue
+    # if isinstance(model, torch.nn.Module):
+    #   model_ckpt = Checkpointer(model=model)
+    #   model_ckpt.load_state_dict_from_file(ckpt_path)
+    #   del model_ckpt
+    #   torch.cuda.empty_cache()
+    if hasattr(model, 'load_state_dict'):
+      logger.info(f"Loading {name:<40}: load_state_dict")
+      loaded_state = torch.load(ckpt_path, map_location=map_location)
+      if isinstance(model, nn.Module):
+        if verbose:
+          model_ckpt = Checkpointer(model=model)
+          model_ckpt.load_state_dict(loaded_state)
+          del model_ckpt
+        else:
+          ret = model.load_state_dict(loaded_state, strict=False)
+          logger.info(pprint.pformat(ret))
+
+      elif isinstance(model, optim.Optimizer):
+        try:
+          ret = model.load_state_dict(loaded_state)
+        except:
+          logger.info(traceback.format_exc())
+      else:
+        ret = model.load_state_dict(loaded_state)
+      del loaded_state
+      torch.cuda.empty_cache()
+    else:
+      logger.info(f"Loading {name:<40}: update")
+      loaded_state = torch.load(ckpt_path, map_location=map_location)
+      model.update(loaded_state)
+      del loaded_state
+      torch.cuda.empty_cache()
+  pass
+
+def torch_load(model_path, rank, verbose=True):
+  map_location = lambda storage, loc: storage.cuda(rank)
+  loaded_model = torch.load(model_path, map_location=map_location)
+  if verbose:
+    logging.getLogger('tl').info(f"Load model: {model_path}")
+  return loaded_model
+
+
+def set_optimizer_lr(optimizer,
+                     lr):
+  for param_group in optimizer.param_groups:
+    param_group['lr'] = lr
+  pass
+
+def mul_optimizer_lr(optimizer,
+                     lr_mul):
+  for param_group in optimizer.param_groups:
+    param_group['lr'] = param_group['initial_lr'] * lr_mul
+  pass
+
+
+def get_optimizer_lr(optimizer,
+                     return_all=True):
+  lr = []
+  for param_group in optimizer.param_groups:
+    lr.append(param_group['lr'])
+    if not return_all:
+      break
+  if len(lr) == 1:
+    return lr[0]
+  else:
+    return lr
 
-def _get_mesh_xy(H,
-                 W,
-                 device,
-                 prefix_dims=[] # broadcast for batch
-                 ):
+
+def select_indices(bs,
+                   dim,
+                   device,
+                   num_samples,
+                   replacement=False):
+
+  select_inds = torch.multinomial(torch.ones(*[bs, dim], device=device),
+                                  num_samples=num_samples, replacement=replacement)
+  return select_inds
+
+
+def batch_random_split_indices(
+      bs,
+      num_points,
+      grad_points,
+      device
+):
+  rand_idx_list = []
+  for i in range(bs):
+    rand_idx = torch.randperm(num_points, device=device)
+    rand_idx_list.append(rand_idx)
+
+  batch_rand_idx = torch.stack(rand_idx_list, dim=0)
+
+  idx_grad = batch_rand_idx[:, 0:grad_points]
+  idx_no_grad = batch_rand_idx[:, grad_points:]
+
+  return idx_grad, idx_no_grad
+
+def batch_gather_points(points,
+                        idx_grad):
   """
 
-  :param H:
-  :param W:
-  :param device:
-  :param prefix_dims:
+  :param points: (b, n, c) or (b, n, s, c)
+  :param idx_grad: (b, Ngrad)
   :return:
-  - x: (b, HxW)
-  - y: (b, HxW)
   """
-  y, x = torch.meshgrid(torch.linspace(0, H - 1, H, device=device),
-                        torch.linspace(0, W - 1, W, device=device))
+  if points.dim() == 4:
+    idx_grad = rearrange(idx_grad, "b n -> b n 1 1")
+    idx_grad = idx_grad.expand(points.shape[0], -1, points.shape[-2], points.shape[-1])
+    sampled_points = torch.gather(points, dim=1, index=idx_grad, sparse_grad=False)
+  elif points.dim() == 3:
+    idx_grad = rearrange(idx_grad, "b n -> b n 1")
+    idx_grad = idx_grad.expand(points.shape[0], -1, points.shape[-1])
+    sampled_points = torch.gather(points, dim=1, index=idx_grad, sparse_grad=False)
+  else:
+    assert 0
+  return sampled_points
 
-  x = x.reshape([*[1] * len(prefix_dims), H * W]).expand([*prefix_dims, H * W]) + 0.5
-  y = y.reshape([*[1] * len(prefix_dims), H * W]).expand([*prefix_dims, H * W]) + 0.5
 
-  return x, y
+def gather_points(points,
+                  sample_idx,
+                  dim):
+  """
 
-def _get_mesh_xy_deprecated(H,
-                            W,
-                            device,
-                            prefix_dims=[] # broadcast for batch
-                            ):
-  i, j = torch.meshgrid(torch.linspace(0, W - 1, W, device=device), torch.linspace(0, H - 1, H, device=device))
-  i = i.t().reshape([*len(prefix_dims) * [1], H * W]).expand([*prefix_dims, H * W])
-  j = j.t().reshape([*len(prefix_dims) * [1], H * W]).expand([*prefix_dims, H * W])
-
-  return i, j
+  :param points: (b c h w)
+  :param idx_grad: (b, n)
+  :return:
+  """
 
+  rearrange_shape = ['1'] * len(points.shape)
+  rearrange_shape[0] = 'b'
+  rearrange_shape[dim] = 'n'
+  rearrange_shape_str = ' '.join(rearrange_shape)
 
-def _select_pixels(x,
-                   y,
-                   H,
-                   W,
-                   device,
-                   N_rays=-1,
-                   prefix_dims=[]):
+  expand_shape = list(points.shape)
+  expand_shape[dim] = -1
 
-  if N_rays > 0 and N_rays < H * W:
-    select_inds = torch.multinomial(torch.ones(*[*prefix_dims, H*W], device=device),
-                                    num_samples=N_rays, replacement=False)
-    # select_inds = torch.from_numpy(
-    #   np.random.choice(H * W, size=[*prefix_dims, N_rays], replace=False)
-    # ).to(device)
+  sample_idx = rearrange(sample_idx, f"b n -> {rearrange_shape_str}")
+  sample_idx = sample_idx.expand(*expand_shape)
+  sampled_points = torch.gather(points, dim=dim, index=sample_idx, sparse_grad=False)
 
-    x = torch.gather(x, dim=-1, index=select_inds)
-    y = torch.gather(y, dim=-1, index=select_inds)
-  else:
-    select_inds = torch.arange(H * W).to(device)
-    if len(prefix_dims) > 0:
-      select_inds = select_inds[None,].expand([*prefix_dims, -1])
-
-  return x, y, select_inds
+  return sampled_points
 
 
-def _get_direction(H,
-                   W,
-                   focal_x,
-                   focal_y,
-                   fov,
-                   prefix_dims,
-                   device,
-                   N_rays=-1,
-                   center_x=None,
-                   center_y=None):
+def batch_scatter_points(idx_grad,
+                         points_grad,
+                         idx_no_grad,
+                         points_no_grad,
+                         dim):
   """
 
-  :param H:
-  :param W:
-  :param focal_x:
-  :param focal_y:
-  :param prefix_dims: [b, ]
-  :param device:
-  :param N_rays:
+  :param idx_grad: (b, Ngrad)
+  :param points_grad: (b, N) or (b, N, c)
+  :param idx_no_grad:
+  :param points_no_grad:
+  :param num_points:
   :return:
   """
-  # [b, HxW]
-  x, y = _get_mesh_xy(H=H, W=W, device=device, prefix_dims=prefix_dims)
-  # x, y = _get_mesh_xy_deprecated(H=H, W=W, device=device, prefix_dims=prefix)
-  # assert (i == x).all()
-  # assert (j == y).all()
-
-  # [b, N_rays], pixel coordinate
-  x, y, select_inds = _select_pixels(x=x, y=y, H=H, W=W, N_rays=N_rays, device=device, prefix_dims=prefix_dims)
-
-  if center_x is None:
-    center_x = W / 2.
-  if center_y is None:
-    center_y = H / 2.
-
-  # [..., N_rays, 3], axes orientations : x right, y downwards, z positive, pixel coordinate to camera coordinate
-  z = torch.ones_like(x, device=device) / np.tan(np.deg2rad(fov) / 2)
-  dirs = torch.stack([(x - center_x) / focal_x,
-                      (y - center_y) / focal_y,
-                      z], dim=-1)
-  dirs = torch.nn.functional.normalize(dirs, p=2, dim=-1)
-
-  return dirs, select_inds
-
-
-def get_rays(
-      rot: torch.Tensor,
-      trans: torch.Tensor,
-      focal_x: torch.Tensor,
-      focal_y: torch.Tensor,
-      H: int,
-      W: int,
-      N_rays: int = -1,
-      representation='axis-angle', # 'quaternion'
-      flatten=True,
-      **kwargs):
-  """
-  < opencv / colmap convention, standard pinhole camera >
-    the camera is facing [+z] direction, x right, y downwards
-                  z
-                ↗
-              o------> x
-              ↓
-              y
-  :param rot: (b, 3)
-  :param trans: (b, 3)
-  :param focal_x: ()
-  :param focal_y: ()
-  :param H:
-  :param W:
-  :param N_rays: -1: all
-  :param representation:
 
-  :return
+  output_shape = list(points_grad.shape)
+  output_shape[dim] += points_no_grad.shape[dim]
 
-  - rays_o: (b, N_rays, 3)
-  - rays_d: (b, N_rays, 3)
-  - select_inds: (b, N_rays)
-  """
-
-  device = rot.device
-  assert rot.shape[:-1] == trans.shape[:-1]
-  prefix = rot.shape[:-1]  # [b, ]
-
-  dirs, select_inds = _get_direction(H=H, W=W, focal_x=focal_x, focal_y=focal_y,
-                                     prefix_dims=prefix, device=device, N_rays=N_rays)
-
-  # ---------
-  # Translate camera frame's origin to the world frame. It is the origin of all rays.
-  # ---------
-
-  if representation == 'quaternion':
-    # rot: [..., 4], trans: [..., 3]
-    assert rot.shape[-1] == 4
-    quat = tr3d.standardize_quaternion(F.normalize(rot, dim=-1))
-    rays_d = tr3d.quaternion_apply(quat[..., None, :], dirs)
-    rays_o = trans[..., None, :].expand_as(rays_d)
-
-  elif representation == 'axis-angle':
-    # original paper, rot: [..., 3], trans: [..., 3]
-    assert rot.shape[-1] == 3
-    ## pytorch 3d implementation: axis-angle --> quaternion -->matrix, [..., 3, 3]
-    rot_m = tr3d.axis_angle_to_matrix(rot)
-    # rotation: matrix multiplication, [..., N_rays, 1, 3] * [..., 1, 3, 3]
-    rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :3, :3], dim=-1)
-    rays_o = trans[..., None, :].expand_as(rays_d)
-
-  elif representation == 'rotation6D':
-    assert rot.shape[-1] == 6
-    rot_m = tr3d.rotation_6d_to_matrix(rot)
-    # rotation: matrix multiplication
-    # rays_d = rot_m.view(*prefix, 1, 3, 3)\
-    #     .expand([*prefix, N_rays, 3, 3]).flatten(0,-3).bmm(
-    #     dirs.flatten(0, -2).view([-1, 3, 1]))
-    # [..., N_rays, 1, 3] * [..., 1, 3, 3]
-    rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :3, :3], dim=-1)
-    rays_o = trans[..., None, :].expand_as(rays_d)
+  points_all = torch.zeros(*output_shape, device=points_grad.device, dtype=points_grad.dtype)
 
-  else:
-    raise RuntimeError("please choose representation")
+  rearrange_shape = ['1'] * len(output_shape)
+  rearrange_shape[0] = 'b'
+  rearrange_shape[dim] = 'n'
+  rearrange_shape_str = ' '.join(rearrange_shape)
 
-  if not flatten and N_rays < 0:
-    rays_o = rearrange(rays_o, "b (h w) c -> b h w c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b (h w) c -> b h w c", h=H, w=W)
-    select_inds = rearrange(select_inds, "b (h w) -> b h w", h=H, w=W)
+  expand_shape = output_shape
+  expand_shape[dim] = -1
 
-  # [..., N_rays, 3]
-  return rays_o, rays_d, select_inds
+  idx_grad = rearrange(idx_grad, f"b n -> {rearrange_shape_str}")
+  idx_grad_out = idx_grad.expand(*expand_shape)
+  points_all.scatter_(dim=dim, index=idx_grad_out, src=points_grad)
 
+  idx_no_grad = rearrange(idx_no_grad, f"b n -> {rearrange_shape_str}")
+  idx_no_grad_out = idx_no_grad.expand(expand_shape)
+  points_all.scatter_(dim=dim, index=idx_no_grad_out, src=points_no_grad)
 
-def parse_intrinsic(intr):
-  """
+  return points_all
 
-  :param intr: (3, 3)
-  :return:
+def get_grad_norm_string(named_params,
+                         norm_type=2):
+
+  named_params = list(named_params)
+
+  parameters = [p for p in named_params if p[1].grad is not None]
+  norm_type = float(norm_type)
+
+  ret_str = ""
+  for name, p in parameters:
+    param_norm = torch.norm(p.grad.detach(), p=norm_type)
+    ret_str += f"{name}: {param_norm}\n"
+
+  return ret_str
+
+
+def get_grad_norm_total(params,
+                        norm_type=2):
   """
 
-  fx = intr[0, 0]
-  fy = intr[1, 1]
-  cx = intr[0, 2]
-  cy = intr[1, 2]
-  return fx, fy, cx, cy
-
-
-def get_rays_by_intr_and_extr(
-        intrinsics,
-        c2w,
-        H: int,
-        W: int,
-        fov,
-        N_rays: int = -1,
-        flatten=False,
-        **kwargs):
-  """
-  Support backprop.
-
-  :param intrinsics: (3, 3)
-  :param c2w: (b, 4, 4)
-  :param H:
-  :param W:
-  :param N_rays:
-  :param kwargs:
+  :param params:
+  :param norm_type:
   :return:
   """
 
-  device = c2w.device
-  prefix = c2w.shape[:-2]  # [b, ]
+  params = list(params)
 
-  fx, fy, cx, cy = parse_intrinsic(intr=intrinsics)
+  parameters = [p for p in params if p.grad is not None]
 
-  dirs, select_inds = _get_direction(H=H, W=W, focal_x=fx, focal_y=fy, fov=fov,
-                                     center_x=cx, center_y=cy,
-                                     prefix_dims=prefix, device=device, N_rays=N_rays)
-
-  # (b, 3, 3)
-  rot_m = c2w[..., :3, :3]
-  # (b, 3)
-  trans = c2w[..., :3, 3]
-
-  # rotation: matrix multiplication, [..., N_rays, 1, 3] * [..., 1, 3, 3]
-  rays_d = torch.sum(dirs[..., None, :] * rot_m[..., None, :, :], dim=-1)
-  rays_o = trans[..., None, :].expand_as(rays_d)
-
-  if not flatten and N_rays < 0:
-    rays_o = rearrange(rays_o, "b (h w) c -> b h w c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b (h w) c -> b h w c", h=H, w=W)
-    select_inds = rearrange(select_inds, "b (h w) -> b h w", h=H, w=W)
-
-  return rays_o, rays_d, select_inds
-
-def get_focal(f,
-              H,
-              W,
-              intr_repr='square') -> Tuple[torch.Tensor, torch.Tensor]:
-  """
-
-  :param f:
-  :param H:
-  :param W:
-  :param intr_repr:
-  :return: (fx, fy)
-  """
-  if intr_repr == 'square':
-    f = f ** 2
-  elif intr_repr == 'ratio':
-    f = f
-  elif intr_repr == 'exp':
-    f = torch.exp(f)
-  else:
-    raise RuntimeError("Please choose intr_repr")
-  fx, fy = f
-  fx = fx * W
-  fy = fy * H
-  return fx, fy
-
-
-def get_rotation_matrix(rot,
-                        representation='quaternion'):
-  """
-
-  :param rot: (b, 3)
-  :param representation: ['axis-angle', ]
-  :return: rot_m: (b, 3, 3)
-  """
-  if representation == 'axis-angle':
-    assert rot.shape[-1] == 3
-    # pytorch3d's implementation: axis-angle -> quaternion -> rotation matrix
-    rot_m = tr3d.axis_angle_to_matrix(rot)
-  elif representation == 'quaternion':
-    assert rot.shape[-1] == 4
-    quat = F.normalize(rot)
-    rot_m = tr3d.quaternion_to_matrix(quat)  # [...,3,3]
-  elif representation == 'rotation6D':
-    assert rot.shape[-1] == 6
-    rot_m = tr3d.rotation_6d_to_matrix(rot)
-  else:
-    raise RuntimeError("Please choose representation.")
-  return rot_m
+  if len(parameters) == 0:
+    return -1
 
+  norm_type = float(norm_type)
+  device = parameters[0].grad.device
 
-def get_camera2world(rot,
-                     trans,
-                     representation='quaternion'):
-  """
-
-  :param rot: (b, 3)
-  :param trans: (b, 3)
-  :param representation: ['axis-angle', ]
-  :return: homo_m: (b, 4, 4)
-  """
-  assert rot.shape[:-1] == trans.shape[:-1]
-  prefix = rot.shape[:-1]
-  rot_m = get_rotation_matrix(rot, representation)
-  tmp = torch.cat((rot_m.view(*prefix, 3, 3), trans.view(*prefix, 3, 1)), dim=-1)
-  extend = torch.zeros(*prefix, 1, 4).to(rot.device)
-  extend[..., 0, 3] = 1.
-  homo_m = torch.cat((tmp, extend), dim=-2)  # [...,4,4]
-
-  return homo_m  # [...,4,4]
-
-
-class CamParams(nn.Module):
-  def __init__(self,
-               phi,
-               t,
-               f,
-               initial_fov,
-               H0=None,
-               W0=None,
-               so3_repr=None,
-               intr_repr=None,
-               freeze_intr=True):
-    super().__init__()
-    # self.extra_attr_keys = []
-    # self.register_extra_attr('so3_repr', so3_repr)
-    # self.register_extra_attr('intr_repr', intr_repr)
-    # self.register_extra_attr('H0', H0)  # used to calc focal length
-    # self.register_extra_attr('W0', W0)  # used to calc focal length
-
-    self.so3_repr = so3_repr
-    self.intr_repr = intr_repr
-    self.H0 = H0
-    self.W0 = W0
-    self.freeze_intr = freeze_intr
-    self.initial_fov = initial_fov
-
-    self.phi = nn.Parameter(phi)
-    self.t = nn.Parameter(t) # initial value: 0
-    self.f = nn.Parameter(f)
-    pass
+  total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]),
+                          norm_type).item()
 
-  @staticmethod
-  def from_config(num_imgs: int,
-                  initial_fov,
-                  fxfy=(0.5, 0.5),
-                  H0: float = 1000,
-                  W0: float = 1000,
-                  so3_repr: str = 'axis-angle',
-                  intr_repr: str = 'square',
-                  freeze_intr=True):
-    """
-    # Camera parameters to optimize: phi, t, f
-    # phi, t here is for camera2world
+  return total_norm
 
-    :param num_imgs:
-    :param H0:
-    :param W0:
-    :param so3_repr:
-    :param intr_repr:
-    :param initial_fov:
-    :return:
-    """
 
-    if so3_repr == 'quaternion':
-      phi = torch.tensor([1., 0., 0., 0.])
+def img_clamp_norm(img,
+                   low,
+                   high,
+                   mean0=False):
+  """
 
-    elif so3_repr == 'axis-angle':
-      phi = torch.tensor([0., 0., 0.])
+  :param img:
+  :param low:
+  :param high:
+  :return
 
-    elif so3_repr == 'rotation6D':
-      phi = torch.tensor([1., 0., 0., 0., 1., 0.])
+  - img_tensor: [0, 1]
 
-    else:
-      raise RuntimeError("Please choose representation")
+  """
+  img = img.clone()
+  img.clamp_(min=low, max=high)
+  img.sub_(low).div_(max(high - low, 1e-5))
+  if mean0:
+    img = img * 2. - 1
+  return img
+
+
+def img_tensor_to_pil(frame_tensor,
+                      low=-1,
+                      high=1,
+                      ):
+  frame_tensor = frame_tensor.squeeze()
+  frame_tensor = img_clamp_norm(frame_tensor, low=low, high=high)
+  img_pil = tv_f.to_pil_image(frame_tensor)
+  return img_pil
+
+
+class ImageFolder(ImageFolder_base):
+  def __init__(self, *args, **kwargs):
+    """A generic data loader where the images are arranged in this way by default: ::
+
+        root/dog/xxx.png
+        root/dog/xxy.png
+        root/dog/[...]/xxz.png
+
+        root/cat/123.png
+        root/cat/nsdf3.png
+        root/cat/[...]/asd932_.png
+
+    This class inherits from :class:`~torchvision.datasets.DatasetFolder` so
+    the same methods can be overridden to customize the dataset.
+
+    Args:
+        root (string): Root directory path.
+        transform (callable, optional): A function/transform that  takes in an PIL image
+            and returns a transformed version. E.g, ``transforms.RandomCrop``
+        target_transform (callable, optional): A function/transform that takes in the
+            target and transforms it.
+        loader (callable, optional): A function to load an image given its path.
+        is_valid_file (callable, optional): A function that takes path of an Image file
+            and check if the file is a valid file (used to check of corrupt files)
+
+     Attributes:
+        classes (list): List of the class names sorted alphabetically.
+        class_to_idx (dict): Dict with items (class_name, class_index).
+        imgs (list): List of (image path, class_index) tuples
+    """
+    super(ImageFolder, self).__init__(*args, **kwargs)
+    pass
 
-    phi = phi[None, :].expand(num_imgs, -1)
-    t = torch.zeros(num_imgs, 3)
+  def sample_partial_samples(self, N_samples):
+    self.samples = self.samples[:N_samples]
+    self.targets = [s[1] for s in self.samples]
+    self.imgs = self.samples
+    pass
 
-    # sx = 0.5 / np.tan((.5 * initial_fov * np.pi / 180.))
-    # sy = 0.5 / np.tan((.5 * initial_fov * np.pi / 180.))
-    # f = torch.tensor([sx, sy])
-    f = torch.tensor(fxfy)
-
-    if intr_repr == 'square':
-      f = torch.sqrt(f)
-    elif intr_repr == 'ratio':
-      pass
-    elif intr_repr == 'exp':
-      f = torch.log(f)
-    else:
-      raise RuntimeError("Please choose intr_repr")
 
-    m = CamParams(phi=phi.contiguous(),
-                  t=t.contiguous(),
-                  f=f.contiguous(),
-                  initial_fov=initial_fov,
-                  H0=H0,
-                  W0=W0,
-                  so3_repr=so3_repr,
-                  intr_repr=intr_repr,
-                  freeze_intr=freeze_intr)
-    return m
-
-  @staticmethod
-  def from_state_dict(state_dict):
-    m = CamParams(**state_dict)
-    return m
-
-  def forward(self,
-              indices=None,
-              mode='default'):
-    """
+def ema_accumulate(model1,
+                   model2,
+                   decay=0.999):
+  """
+  Exponential moving average for generator weights
+
+  :param model1:
+  :param model2:
+  :param decay:
+  :return:
+  """
+  if isinstance(model1, nn.Module):
+    par1 = dict(model1.state_dict())
+  else:
+    par1 = model1
 
-    :param indices:
-    :param mode: get_intrinsic,
-    :return:
+  if isinstance(model2, nn.Module):
+    par2 = dict(model2.state_dict())
+  else:
+    par2 = model2
 
-    """
-    if mode == 'default':
-      fx, fy = self.get_focal()
-      return self.phi[indices], self.t[indices], fx, fy
-    elif mode == 'get_intrinsic':
-      if self.freeze_intr:
-        self.f.requires_grad_(False)
-      intr = self.get_intrinsic()
-      return intr
-    else:
-      raise NotImplementedError
+  with torch.no_grad():
+    for k in par1.keys():
+      par1[k].data.mul_(decay).add_(par2[k].data, alpha=1 - decay)
 
-  def _get_random_pose(self,
-                       bs,
-                       r=1,
-                       h_stddev=0.3,
-                       v_stddev=0.155,
-                       h_mean=np.pi * 0.5,
-                       v_mean=np.pi * 0.5,
-                       sample_dist='gaussian',
-                       device='cuda'):
-    """
-    < opencv / colmap convention, standard pinhole camera >
-    the camera is facing [+z] direction, x right, y downwards
-          z
-        ↗
-      o------> x
-      ↓
-      y
-
-    :param bs:
-    :param r:
-    :param h_stddev:
-    :param v_stddev:
-    :param h_mean:
-    :param v_mean:
-    :param sample_dist:
-    :param device:
-    :return
 
-    - c2ws: (b, 4, 4)
+def get_gather_sample_idx(batch,
+                          N_size,
+                          N_samples,
+                          device):
+  window_size = N_size // N_samples
 
-    """
+  base_idx = torch.arange(N_samples, device=device, dtype=torch.int64) * window_size
+  base_idx = base_idx.unsqueeze(0).repeat([batch, 1])
 
-    camera_origin, pitch, yaw = geometry_tensor.sample_camera_positions(bs=bs,
-                                                                        r=r,
-                                                                        horizontal_stddev=h_stddev,
-                                                                        vertical_stddev=v_stddev,
-                                                                        horizontal_mean=h_mean,
-                                                                        vertical_mean=v_mean,
-                                                                        mode=sample_dist,
-                                                                        device=device)
-
-    # to opencv coordinate
-    camera_origin[:, 1] *= -1
-    camera_origin[:, 2] *= -1
-
-    up = torch.zeros(bs, 3, device=device)
-    up[:, 1] = 1
-    focus_in_world = torch.zeros(bs, 3, device=device)
-
-    c2ws = geometry_tensor.look_at(cam_location=camera_origin, point=focus_in_world, up=up)
-
-    return c2ws
-
-  def get_rays_random_pose(self,
-                           device,
-                           bs,
-                           # pixel coordinate to camera coordinate
-                           intr=None,
-                           H=None,
-                           W=None,
-                           fov=None,
-                           # for random camera pose
-                           r=1,
-                           h_stddev=0.3,
-                           v_stddev=0.155,
-                           h_mean=np.pi * 0.5,
-                           v_mean=np.pi * 0.5,
-                           sample_dist='gaussian',
-                           # for rays
-                           N_rays: int = -1,
-                           **kwargs):
-    """
-    :param intr: (3, 3)
+  shift_idx = torch.randint_like(base_idx, window_size)
+  sample_idx = base_idx + shift_idx
 
-    :return
+  return sample_idx
 
-    - rays_o: (b, H, W, 3)
-    - rays_d: (b, H, W, 3)
-    - select_inds: (b, H, W)
-    """
+def sample_image_sub_pixels(images,
+                            N_h_pixels,
+                            N_w_pixels,
+                            device,
+                            sample_idx_h=None,
+                            sample_idx_w=None):
+  B, C, H, W = images.shape
 
-    if intr is None:
-      H, W = self.H0, self.W0
-      intr = self.get_intrinsic(H, W, device=device)
-    else:
-      if H is None and W is None:
-        H, W = self.H0, self.W0
-      elif H is None or W is None:
-        assert 0
-
-    c2ws = self._get_random_pose(bs=bs,
-                                 r=r,
-                                 h_stddev=h_stddev,
-                                 v_stddev=v_stddev,
-                                 h_mean=h_mean,
-                                 v_mean=v_mean,
-                                 sample_dist=sample_dist,
-                                 device=device)
-
-    if fov is None:
-      fov = self.initial_fov
-
-    rays_o, rays_d, select_inds = get_rays_by_intr_and_extr(
-      intrinsics=intr,
-      c2w=c2ws,
-      H=H,
-      W=W,
-      fov=fov,
-      N_rays=N_rays,
-      flatten=False)
+  if sample_idx_h is None:
+    sample_idx_h = get_gather_sample_idx(batch=B, N_size=H, N_samples=N_h_pixels, device=device)
+  image_h = gather_points(points=images, sample_idx=sample_idx_h, dim=2)
 
-    return rays_o, rays_d, select_inds
+  if sample_idx_w is None:
+    sample_idx_w = get_gather_sample_idx(batch=B, N_size=W, N_samples=N_w_pixels, device=device)
+  image_hw = gather_points(points=image_h, sample_idx=sample_idx_w, dim=3)
 
-  def get_focal(self) -> Tuple[torch.Tensor, torch.Tensor]:
-    """
+  return image_hw
 
-    :return: (fx, fy)
-    """
-    return get_focal(f=self.f, H=self.H0, W=self.W0, intr_repr=self.intr_repr)
-
-  @torch.no_grad()
-  def get_rays_of_pose_avg(self,
-                           H,
-                           W,
-                           bs=1):
-
-    intr = self.get_intrinsic(H, W)
-    c2ws = self.poses_avg()
-    c2ws = c2ws.expand([bs, *c2ws.shape])
-
-    rays_o, rays_d, select_inds = get_rays_by_intr_and_extr(intrinsics=intr,
-                                                            c2w=c2ws,
-                                                            H=H,
-                                                            W=W,
-                                                            N_rays=-1,
-                                                            flatten=False)
-
-    return rays_o, rays_d
-
-  @torch.no_grad()
-  def poses_avg(self):
-
-    c2ws = self.get_camera2worlds()
-    c2w_center = geometry_tensor.poses_avg(c2ws)
-    return c2w_center
 
-  def get_camera2worlds(self):
-    """
-    Support backprop.
+def get_gather_sample_idx_patch(batch,
+                                all_size,
+                                patch_size,
+                                device):
 
-    :return:
-    """
-    c2ws = get_camera2world(self.phi, self.t, self.so3_repr)
-    return c2ws
+  shift_idx = torch.arange(patch_size, device=device, dtype=torch.int64)
+  shift_idx = shift_idx.unsqueeze(0).repeat([batch, 1])
 
-  def get_intrinsic(self,
-                    new_H=None,
-                    new_W=None,
-                    **kwargs):
-    """
-    Support backprop.
+  base_idx = torch.randint(0, all_size - patch_size + 1, [batch, 1], device=device)
+  sample_idx = base_idx + shift_idx
 
-    :param new_H:
-    :param new_W:
-    :return: intr: (3, 3)
-            [[fx, 0, cx]
-             [0, fy, cy]
-             [0, 0, 1]]
-    """
-    scale_x = new_W / self.W0 if new_W is not None else 1.
-    scale_y = new_H / self.H0 if new_H is not None else 1.
+  return sample_idx
 
-    fx, fy = self.get_focal()
+def sample_image_patch(images,
+                       patch_size_h,
+                       patch_size_w,
+                       device,
+                       sample_idx_h=None,
+                       sample_idx_w=None):
+  B, C, H, W = images.shape
 
-    intr = torch.eye(3, device=fx.device)
-    cx = self.W0 / 2.
-    cy = self.H0 / 2.
-    # OK with grad: with produce grad_fn=<CopySlices>
-    intr[0, 0] = fx * scale_x
-    intr[1, 1] = fy * scale_y
-    intr[0, 2] = cx * scale_x
-    intr[1, 2] = cy * scale_y
-    return intr
-
-  def get_approx_bounds(self, near: float, far: float):
-    fx, fy = get_focal(self.f.data.cpu(), self.H0, self.W0, self.intr_repr)
-    rays_o, rays_d, _ = get_rays(self.phi.data.cpu(), self.t.data.cpu(), fx, fy, self.H0, self.W0, -1, self.so3_repr)
-    rays_e = rays_o + rays_d * (far - near)
-    rays_o = rays_o.reshape(-1, 3)
-    rays_e = rays_e.reshape(-1, 3)
-    all_points = np.concatenate([rays_o, rays_e], axis=0)
-    min_points = np.min(all_points, axis=0)
-    max_points = np.max(all_points, axis=0)
-    return min_points, max_points
-
-  # def register_extra_attr(self, k, v):
-  #   self.__dict__[k] = v
-  #   self.extra_attr_keys.append(k)
-
-  # def load_state_dict(self,
-  #                     state_dict,
-  #                     strict: bool = True):
-  #   # Load extra non-tensor parameters
-  #   for k in self.extra_attr_keys:
-  #     assert k in state_dict, 'could not found key: [{}] in state_dict'.format(k)
-  #     self.__dict__[k] = state_dict[k]
-  #   # Notice: DO NOT deep copy. we do not want meaningless memory usage
-  #   nn_statedict = {}
-  #   for k, v in state_dict.items():
-  #     if k not in self.extra_attr_keys:
-  #       nn_statedict[k] = v
-  #   return super().load_state_dict(nn_statedict, strict=strict)
-  #
-  # def state_dict(self):
-  #   sdict = super().state_dict()
-  #   for k in self.extra_attr_keys:
-  #     sdict[k] = self.__dict__[k]
-  #   return sdict
+  if sample_idx_h is None:
+    sample_idx_h = get_gather_sample_idx_patch(batch=B, all_size=H, patch_size=patch_size_h, device=device)
+  image_h = gather_points(points=images, sample_idx=sample_idx_h, dim=2)
 
+  if sample_idx_w is None:
+    sample_idx_w = get_gather_sample_idx_patch(batch=B, all_size=W, patch_size=patch_size_w, device=device)
+  image_hw = gather_points(points=image_h, sample_idx=sample_idx_w, dim=3)
 
+  return image_hw
 
 
+def sample_noises(bs,
+                  noise_dim,
+                  device,
+                  N_samples=1,
+                  seed=None):
+  if seed is not None:
+    z_samples = np.random.RandomState(seed).randn(N_samples, bs, noise_dim)
+    z_samples = torch.from_numpy(z_samples).to(device).to(torch.float32)
 
+  else:
+    z_samples = torch.randn(N_samples, bs, noise_dim, device=device)
 
+  z_samples = z_samples.unbind(dim=0)
 
+  return z_samples
 
-# -----------------
-# camera plotting utils
-# -----------------
-
-def about2index(about):
-  if len(about) != 2:
-    raise ValueError("Convention must have 2 letters.")
-  if about[0] == about[1]:
-    raise ValueError(f"Invalid convention {about}.")
-  for letter in about:
-    if letter not in ("x", "y", "z"):
-      raise ValueError(f"Invalid letter {letter} in convention string.")
-  letter2index = {'x': 0, 'y': 1, 'z': 2}
-  i0 = letter2index[about[0]]
-  i1 = letter2index[about[1]]
-  return i0, i1
-
-
-def plot_cam_trans(cam_param: CamParams, about='xy', return_img=False):
-  # --------
-  # get about index
-  i0, i1 = about2index(about)
-
-  fig = plt.figure()
-  ax = fig.add_subplot(111)
-  t = cam_param.t.data.cpu()
-  # x, y, z = t.unbind(-1)
-  t1, t2 = t[..., i0].numpy(), t[..., i1].numpy()
-  ax.plot(t1, t2, '^-')
 
-  if return_img:
-    return utils.figure_to_image(fig)
+def batch_transform(M,
+                    points,
+                    pad_ones=True):
+  """
+  
+  :param M: (..., 4, 4)
+  :param points: (..., 3)
+  :param pad_ones:
+  :return:
+  """
+  to_numpy = False
+  if isinstance(points, np.ndarray):
+    M = torch.from_numpy(M)
+    points = torch.from_numpy(points)
+    to_numpy = True
+  
+  assert M.ndim == points.ndim + 1
+  
+  if pad_ones:
+    homo = torch.ones((*points.shape[:-1], 1), dtype=points.dtype, device=points.device)
   else:
-    return fig
+    homo = torch.zeros((*points.shape[:-1], 1), dtype=points.dtype, device=points.device)
+  v_homo = torch.cat((points, homo), dim=-1)
+  v_homo = torch.matmul(M, v_homo.unsqueeze(-1))
+  v_ = v_homo[..., :3, 0]
+  
+  if to_numpy:
+    v_ = v_.cpu().numpy()
+    
+  return v_
+
+
+def masked_select(input,
+                  mask):
+  """
+  
+  :param input: (b, N, c)
+  :param mask: (b, N, 1)
+  :return:
+  """
+  b, _, c = input.shape
+  matched = torch.masked_select(input, mask).view(b, -1, c)
+
+  return matched
 
 
-def plot_cam_rot(cam_param: CamParams, representation: str = 'quaternion', about='xy'):
-  # --------
-  # get about index
-  i0, i1 = about2index(about)
-
-  # ---------
-  # plot
-  fig = plt.figure()
-  ax = fig.add_subplot(111)
-  R = cam_param.phi.data.cpu()
-  rot_m = get_rotation_matrix(R, representation)
-  euler = tr3d.matrix_to_euler_angles(rot_m, 'XYZ')
-  # rx, ry, rz = euler.unbind(-1)
-  r1, r2 = euler[..., i0].numpy(), euler[..., i1].numpy()
-  ax.plot(r1, r2, '^-')
-  return fig
+def date_modified(path=__file__):
+  # return human-readable file modification date, i.e. '2021-3-26'
+  t = datetime.datetime.fromtimestamp(pathlib.Path(path).stat().st_mtime)
+  return f'{t.year}-{t.month}-{t.day}'
+  
+def get_gpu_info(verbose=True):
+  
+  s = f'\nGPU 🚀 {date_modified()} torch {torch.__version__} '
+  n = torch.cuda.device_count()
+  space = ' ' * len(s)
+
+  for i, d in enumerate(range(n)):
+    p = torch.cuda.get_device_properties(i)
+    s += f"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / 1024 ** 3:.2f}GB)\n"
+  
+  if verbose:
+    logging.getLogger('tl').info(s)
+    
+  return s
+
+
+def get_gpu_memory_GB():
+  p = torch.cuda.get_device_properties(0)
+  return p.total_memory / 1024 ** 3
+
+
+def check_grad_non_nan(G):
+  for name, param in G.named_parameters():
+    if param.grad is not None:
+      assert not torch.isnan(param.grad).any(), name
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/nerf/geometry.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/nerf/geometry.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,93 +1,93 @@
-import numpy as np
-
-def normalize(vec):
-    return vec / (np.linalg.norm(vec, axis=-1, keepdims=True) + 1e-9)
-
-
-def poses_avg(poses):
-    center = poses[:, :3, 3].mean(0)
-    forward = poses[:, :3, 2].sum(0)
-    up = poses[:, :3, 1].sum(0)
-    c2w = view_matrix(forward, up, center)
-    return c2w
-
-
-""" All following opencv convenrtion
-    < opencv / colmap convention, standard pinhole camera >
-    the camera is facing [+z] direction, x right, y downwards
-                z
-               ↗
-            o------> x
-            ↓ 
-            y
-"""
-
-
-def look_at(
-    cam_location: np.ndarray, 
-    point: np.ndarray, 
-    up=np.array([0., -1., 0.])          # openCV convention
-    # up=np.array([0., 1., 0.])         # openGL convention
-    ):
-    # Cam points in positive z direction
-    forward = normalize(point - cam_location)     # openCV convention
-    # forward = normalize(cam_location - point)   # openGL convention
-    return view_matrix(forward, up, cam_location)
-
-
-def view_matrix(
-      forward: np.ndarray,
-      up: np.ndarray,
-      cam_location: np.ndarray):
-
-    rot_z = normalize(forward)
-    rot_x = normalize(np.cross(up, rot_z))
-    rot_y = normalize(np.cross(rot_z, rot_x))
-    mat = np.stack((rot_x, rot_y, rot_z, cam_location), axis=-1)
-    hom_vec = np.array([[0., 0., 0., 1.]])
-    if len(mat.shape) > 2:
-        hom_vec = np.tile(hom_vec, [mat.shape[0], 1, 1])
-    mat = np.concatenate((mat, hom_vec), axis=-2)
-    return mat
-
-
-def c2w_track_spiral(c2w,
-                     up_vec,
-                     rads,
-                     focus: float,
-                     zrate: float,
-                     rots: int,
-                     N: int,
-                     **kwargs):
-    # TODO: support zdelta
-    """generate camera to world matrices of spiral track, looking at the same point [0,0,focus]
-
-    Args:
-        c2w ([4,4] or [3,4]):   camera to world matrix (of the spiral center,
-                                  with average rotation and average translation)
-        up_vec ([3,]):          vector pointing up
-        rads ([3,]):            radius of x,y,z direction, of the spiral track
-        # zdelta ([float]):       total delta z that is allowed to change 
-        focus (float):          a focus value (to be looked at) (in camera coordinates)
-        zrate ([float]):        a factor multiplied to z's angle
-        rots ([int]):           number of rounds to rotate
-        N ([int]):              number of total views
-    """
-
-    c2w_tracks = []
-    rads = np.array(list(rads) + [1.])
-    
-    # focus_in_cam = np.array([0, 0, -focus, 1.])   # openGL convention
-    focus_in_cam = np.array([0, 0, focus, 1.])      # openCV convention
-    focus_in_world = np.dot(c2w[:3, :4], focus_in_cam)
-
-    for theta in np.linspace(0., 2. * np.pi * rots, N+1)[:-1]:
-        cam_location = np.dot(
-            c2w[:3, :4], 
-            # np.array([np.cos(theta), -np.sin(theta), -np.sin(theta*zrate), 1.]) * rads    # openGL convention
-            np.array([np.cos(theta), np.sin(theta), np.sin(theta*zrate), 1.]) * rads        # openCV convention
-        )
-        c2w_i = look_at(cam_location, focus_in_world, up=up_vec)
-        c2w_tracks.append(c2w_i)
-
+import numpy as np
+
+def normalize(vec):
+    return vec / (np.linalg.norm(vec, axis=-1, keepdims=True) + 1e-9)
+
+
+def poses_avg(poses):
+    center = poses[:, :3, 3].mean(0)
+    forward = poses[:, :3, 2].sum(0)
+    up = poses[:, :3, 1].sum(0)
+    c2w = view_matrix(forward, up, center)
+    return c2w
+
+
+""" All following opencv convenrtion
+    < opencv / colmap convention, standard pinhole camera >
+    the camera is facing [+z] direction, x right, y downwards
+                z
+               ↗
+            o------> x
+            ↓ 
+            y
+"""
+
+
+def look_at(
+    cam_location: np.ndarray, 
+    point: np.ndarray, 
+    up=np.array([0., -1., 0.])          # openCV convention
+    # up=np.array([0., 1., 0.])         # openGL convention
+    ):
+    # Cam points in positive z direction
+    forward = normalize(point - cam_location)     # openCV convention
+    # forward = normalize(cam_location - point)   # openGL convention
+    return view_matrix(forward, up, cam_location)
+
+
+def view_matrix(
+      forward: np.ndarray,
+      up: np.ndarray,
+      cam_location: np.ndarray):
+
+    rot_z = normalize(forward)
+    rot_x = normalize(np.cross(up, rot_z))
+    rot_y = normalize(np.cross(rot_z, rot_x))
+    mat = np.stack((rot_x, rot_y, rot_z, cam_location), axis=-1)
+    hom_vec = np.array([[0., 0., 0., 1.]])
+    if len(mat.shape) > 2:
+        hom_vec = np.tile(hom_vec, [mat.shape[0], 1, 1])
+    mat = np.concatenate((mat, hom_vec), axis=-2)
+    return mat
+
+
+def c2w_track_spiral(c2w,
+                     up_vec,
+                     rads,
+                     focus: float,
+                     zrate: float,
+                     rots: int,
+                     N: int,
+                     **kwargs):
+    # TODO: support zdelta
+    """generate camera to world matrices of spiral track, looking at the same point [0,0,focus]
+
+    Args:
+        c2w ([4,4] or [3,4]):   camera to world matrix (of the spiral center,
+                                  with average rotation and average translation)
+        up_vec ([3,]):          vector pointing up
+        rads ([3,]):            radius of x,y,z direction, of the spiral track
+        # zdelta ([float]):       total delta z that is allowed to change 
+        focus (float):          a focus value (to be looked at) (in camera coordinates)
+        zrate ([float]):        a factor multiplied to z's angle
+        rots ([int]):           number of rounds to rotate
+        N ([int]):              number of total views
+    """
+
+    c2w_tracks = []
+    rads = np.array(list(rads) + [1.])
+    
+    # focus_in_cam = np.array([0, 0, -focus, 1.])   # openGL convention
+    focus_in_cam = np.array([0, 0, focus, 1.])      # openCV convention
+    focus_in_world = np.dot(c2w[:3, :4], focus_in_cam)
+
+    for theta in np.linspace(0., 2. * np.pi * rots, N+1)[:-1]:
+        cam_location = np.dot(
+            c2w[:3, :4], 
+            # np.array([np.cos(theta), -np.sin(theta), -np.sin(theta*zrate), 1.]) * rads    # openGL convention
+            np.array([np.cos(theta), np.sin(theta), np.sin(theta*zrate), 1.]) * rads        # openCV convention
+        )
+        c2w_i = look_at(cam_location, focus_in_world, up=up_vec)
+        c2w_tracks.append(c2w_i)
+
     return c2w_tracks
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/nerf/geometry_tensor.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/nerf/geometry_tensor.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,201 +1,201 @@
-import numpy as np
-import math
-import random
-
-import torch
-import torch.nn.functional as F
-
-
-def _truncated_normal(tensor, mean=0, std=1):
-  size = tensor.shape
-  tmp = tensor.new_empty(size + (4,)).normal_()
-  valid = (tmp < 2) & (tmp > -2)
-  ind = valid.max(-1, keepdim=True)[1]
-  tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))
-  tensor.data.mul_(std).add_(mean)
-  return tensor
-
-
-def sample_camera_positions(device,
-                            bs=1,
-                            r=1,
-                            horizontal_stddev=1,
-                            vertical_stddev=1,
-                            horizontal_mean=math.pi * 0.5,
-                            vertical_mean=math.pi * 0.5,
-                            mode='normal'):
-  """
-  Samples bs random locations along a sphere of radius r. Uses the specified distribution.
-       y
-      |
-      --> x
-    /
-   z
-  :param device:
-  :param bs:
-  :param r:
-  :param horizontal_stddev: yaw std
-  :param vertical_stddev: pitch std
-  :param horizontal_mean:
-  :param vertical_mean:
-  :param mode:
-  :return
-
-  - output_points: (bs, 3), camera positions
-  - phi: (bs, 1), pitch in radians [0, pi]
-  - theta: (bs, 1), yaw in radians [-pi, pi]
-  """
-
-  if mode == 'uniform':
-    theta = (torch.rand((bs, 1), device=device) - 0.5) \
-            * 2 * horizontal_stddev \
-            + horizontal_mean
-    phi = (torch.rand((bs, 1), device=device) - 0.5) \
-          * 2 * vertical_stddev \
-          + vertical_mean
-
-  elif mode == 'normal' or mode == 'gaussian':
-    theta = torch.randn((bs, 1), device=device) \
-            * horizontal_stddev \
-            + horizontal_mean
-    phi = torch.randn((bs, 1), device=device) \
-          * vertical_stddev \
-          + vertical_mean
-
-  elif mode == 'hybrid':
-    if random.random() < 0.5:
-      theta = (torch.rand((bs, 1), device=device) - 0.5) \
-              * 2 * horizontal_stddev * 2 \
-              + horizontal_mean
-      phi = (torch.rand((bs, 1), device=device) - 0.5) \
-            * 2 * vertical_stddev * 2 \
-            + vertical_mean
-    else:
-      theta = torch.randn((bs, 1), device=device) * horizontal_stddev + horizontal_mean
-      phi = torch.randn((bs, 1), device=device) * vertical_stddev + vertical_mean
-
-  elif mode == 'truncated_gaussian':
-    theta = _truncated_normal(torch.zeros((bs, 1), device=device)) \
-            * horizontal_stddev \
-            + horizontal_mean
-    phi = _truncated_normal(torch.zeros((bs, 1), device=device)) \
-          * vertical_stddev \
-          + vertical_mean
-
-  elif mode == 'spherical_uniform':
-    theta = (torch.rand((bs, 1), device=device) - .5) \
-            * 2 * horizontal_stddev \
-            + horizontal_mean
-    v_stddev, v_mean = vertical_stddev / math.pi, vertical_mean / math.pi
-    v = ((torch.rand((bs, 1), device=device) - .5) * 2 * v_stddev + v_mean)
-    v = torch.clamp(v, 1e-5, 1 - 1e-5)
-    phi = torch.arccos(1 - 2 * v)
-
-  elif mode == 'mean':
-    # Just use the mean.
-    theta = torch.ones((bs, 1), device=device, dtype=torch.float) * horizontal_mean
-    phi = torch.ones((bs, 1), device=device, dtype=torch.float) * vertical_mean
-  else:
-    assert 0
-
-  phi = torch.clamp(phi, 1e-5, math.pi - 1e-5)
-
-  output_points = torch.zeros((bs, 3), device=device) # (bs, 3)
-  output_points[:, 0:1] = r * torch.sin(phi) * torch.cos(theta) # x
-  output_points[:, 2:3] = r * torch.sin(phi) * torch.sin(theta) # z
-  output_points[:, 1:2] = r * torch.cos(phi) # y
-
-  return output_points, phi, theta
-
-
-def normalize(vec):
-    # return vec / (vec.norm(dim=-1, keepdim=True) + 1e-9)
-    return F.normalize(vec, p=2, dim=-1)
-
-
-def poses_avg(poses):
-    center = poses[:, :3, 3].mean(0)
-    forward = poses[:, :3, 2].sum(0)
-    up = poses[:, :3, 1].sum(0)
-    c2w = view_matrix(forward, up, center)
-    return c2w
-
-
-""" All following opencv convenrtion
-    < opencv / colmap convention, standard pinhole camera >
-    the camera is facing [+z] direction, x right, y downwards
-                z
-               ↗
-            o------> x
-            ↓ 
-            y
-"""
-
-
-def look_at(
-    cam_location,
-    point,
-    up):
-    # Cam points in positive z direction
-    forward = normalize(point - cam_location)     # openCV convention
-
-    return view_matrix(forward, up, cam_location)
-
-
-def view_matrix(
-      forward,
-      up,
-      cam_location):
-
-    rot_z = normalize(forward)
-    rot_x = normalize(torch.cross(up, rot_z))
-    rot_y = normalize(torch.cross(rot_z, rot_x))
-    mat = torch.stack((rot_x, rot_y, rot_z, cam_location), dim=-1)
-
-    hom_vec = torch.tensor([[0., 0., 0., 1.]], device=forward.device)
-    if len(mat.shape) > 2:
-        hom_vec = hom_vec.expand([mat.shape[0], -1, -1])
-    mat = torch.cat((mat, hom_vec), dim=-2)
-    return mat
-
-
-def c2w_track_spiral(c2w,
-                     up_vec,
-                     rads,
-                     focus: float,
-                     zrate: float,
-                     rots: int,
-                     N: int,
-                     **kwargs):
-    # TODO: support zdelta
-    """generate camera to world matrices of spiral track, looking at the same point [0,0,focus]
-
-    Args:
-        c2w ([4,4] or [3,4]):   camera to world matrix (of the spiral center,
-                                  with average rotation and average translation)
-        up_vec ([3,]):          vector pointing up
-        rads ([3,]):            radius of x,y,z direction, of the spiral track
-        # zdelta ([float]):       total delta z that is allowed to change 
-        focus (float):          a focus value (to be looked at) (in camera coordinates)
-        zrate ([float]):        a factor multiplied to z's angle
-        rots ([int]):           number of rounds to rotate
-        N ([int]):              number of total views
-    """
-
-    c2w_tracks = []
-    rads = np.array(list(rads) + [1.])
-    
-    # focus_in_cam = np.array([0, 0, -focus, 1.])   # openGL convention
-    focus_in_cam = np.array([0, 0, focus, 1.])      # openCV convention
-    focus_in_world = np.dot(c2w[:3, :4], focus_in_cam)
-
-    for theta in np.linspace(0., 2. * np.pi * rots, N+1)[:-1]:
-        cam_location = np.dot(
-            c2w[:3, :4], 
-            # np.array([np.cos(theta), -np.sin(theta), -np.sin(theta*zrate), 1.]) * rads    # openGL convention
-            np.array([np.cos(theta), np.sin(theta), np.sin(theta*zrate), 1.]) * rads        # openCV convention
-        )
-        c2w_i = look_at(cam_location, focus_in_world, up=up_vec)
-        c2w_tracks.append(c2w_i)
-
+import numpy as np
+import math
+import random
+
+import torch
+import torch.nn.functional as F
+
+
+def _truncated_normal(tensor, mean=0, std=1):
+  size = tensor.shape
+  tmp = tensor.new_empty(size + (4,)).normal_()
+  valid = (tmp < 2) & (tmp > -2)
+  ind = valid.max(-1, keepdim=True)[1]
+  tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))
+  tensor.data.mul_(std).add_(mean)
+  return tensor
+
+
+def sample_camera_positions(device,
+                            bs=1,
+                            r=1,
+                            horizontal_stddev=1,
+                            vertical_stddev=1,
+                            horizontal_mean=math.pi * 0.5,
+                            vertical_mean=math.pi * 0.5,
+                            mode='normal'):
+  """
+  Samples bs random locations along a sphere of radius r. Uses the specified distribution.
+       y
+      |
+      --> x
+    /
+   z
+  :param device:
+  :param bs:
+  :param r:
+  :param horizontal_stddev: yaw std
+  :param vertical_stddev: pitch std
+  :param horizontal_mean:
+  :param vertical_mean:
+  :param mode:
+  :return
+
+  - output_points: (bs, 3), camera positions
+  - phi: (bs, 1), pitch in radians [0, pi]
+  - theta: (bs, 1), yaw in radians [-pi, pi]
+  """
+
+  if mode == 'uniform':
+    theta = (torch.rand((bs, 1), device=device) - 0.5) \
+            * 2 * horizontal_stddev \
+            + horizontal_mean
+    phi = (torch.rand((bs, 1), device=device) - 0.5) \
+          * 2 * vertical_stddev \
+          + vertical_mean
+
+  elif mode == 'normal' or mode == 'gaussian':
+    theta = torch.randn((bs, 1), device=device) \
+            * horizontal_stddev \
+            + horizontal_mean
+    phi = torch.randn((bs, 1), device=device) \
+          * vertical_stddev \
+          + vertical_mean
+
+  elif mode == 'hybrid':
+    if random.random() < 0.5:
+      theta = (torch.rand((bs, 1), device=device) - 0.5) \
+              * 2 * horizontal_stddev * 2 \
+              + horizontal_mean
+      phi = (torch.rand((bs, 1), device=device) - 0.5) \
+            * 2 * vertical_stddev * 2 \
+            + vertical_mean
+    else:
+      theta = torch.randn((bs, 1), device=device) * horizontal_stddev + horizontal_mean
+      phi = torch.randn((bs, 1), device=device) * vertical_stddev + vertical_mean
+
+  elif mode == 'truncated_gaussian':
+    theta = _truncated_normal(torch.zeros((bs, 1), device=device)) \
+            * horizontal_stddev \
+            + horizontal_mean
+    phi = _truncated_normal(torch.zeros((bs, 1), device=device)) \
+          * vertical_stddev \
+          + vertical_mean
+
+  elif mode == 'spherical_uniform':
+    theta = (torch.rand((bs, 1), device=device) - .5) \
+            * 2 * horizontal_stddev \
+            + horizontal_mean
+    v_stddev, v_mean = vertical_stddev / math.pi, vertical_mean / math.pi
+    v = ((torch.rand((bs, 1), device=device) - .5) * 2 * v_stddev + v_mean)
+    v = torch.clamp(v, 1e-5, 1 - 1e-5)
+    phi = torch.arccos(1 - 2 * v)
+
+  elif mode == 'mean':
+    # Just use the mean.
+    theta = torch.ones((bs, 1), device=device, dtype=torch.float) * horizontal_mean
+    phi = torch.ones((bs, 1), device=device, dtype=torch.float) * vertical_mean
+  else:
+    assert 0
+
+  phi = torch.clamp(phi, 1e-5, math.pi - 1e-5)
+
+  output_points = torch.zeros((bs, 3), device=device) # (bs, 3)
+  output_points[:, 0:1] = r * torch.sin(phi) * torch.cos(theta) # x
+  output_points[:, 2:3] = r * torch.sin(phi) * torch.sin(theta) # z
+  output_points[:, 1:2] = r * torch.cos(phi) # y
+
+  return output_points, phi, theta
+
+
+def normalize(vec):
+    # return vec / (vec.norm(dim=-1, keepdim=True) + 1e-9)
+    return F.normalize(vec, p=2, dim=-1)
+
+
+def poses_avg(poses):
+    center = poses[:, :3, 3].mean(0)
+    forward = poses[:, :3, 2].sum(0)
+    up = poses[:, :3, 1].sum(0)
+    c2w = view_matrix(forward, up, center)
+    return c2w
+
+
+""" All following opencv convenrtion
+    < opencv / colmap convention, standard pinhole camera >
+    the camera is facing [+z] direction, x right, y downwards
+                z
+               ↗
+            o------> x
+            ↓ 
+            y
+"""
+
+
+def look_at(
+    cam_location,
+    point,
+    up):
+    # Cam points in positive z direction
+    forward = normalize(point - cam_location)     # openCV convention
+
+    return view_matrix(forward, up, cam_location)
+
+
+def view_matrix(
+      forward,
+      up,
+      cam_location):
+
+    rot_z = normalize(forward)
+    rot_x = normalize(torch.cross(up, rot_z))
+    rot_y = normalize(torch.cross(rot_z, rot_x))
+    mat = torch.stack((rot_x, rot_y, rot_z, cam_location), dim=-1)
+
+    hom_vec = torch.tensor([[0., 0., 0., 1.]], device=forward.device)
+    if len(mat.shape) > 2:
+        hom_vec = hom_vec.expand([mat.shape[0], -1, -1])
+    mat = torch.cat((mat, hom_vec), dim=-2)
+    return mat
+
+
+def c2w_track_spiral(c2w,
+                     up_vec,
+                     rads,
+                     focus: float,
+                     zrate: float,
+                     rots: int,
+                     N: int,
+                     **kwargs):
+    # TODO: support zdelta
+    """generate camera to world matrices of spiral track, looking at the same point [0,0,focus]
+
+    Args:
+        c2w ([4,4] or [3,4]):   camera to world matrix (of the spiral center,
+                                  with average rotation and average translation)
+        up_vec ([3,]):          vector pointing up
+        rads ([3,]):            radius of x,y,z direction, of the spiral track
+        # zdelta ([float]):       total delta z that is allowed to change 
+        focus (float):          a focus value (to be looked at) (in camera coordinates)
+        zrate ([float]):        a factor multiplied to z's angle
+        rots ([int]):           number of rounds to rotate
+        N ([int]):              number of total views
+    """
+
+    c2w_tracks = []
+    rads = np.array(list(rads) + [1.])
+    
+    # focus_in_cam = np.array([0, 0, -focus, 1.])   # openGL convention
+    focus_in_cam = np.array([0, 0, focus, 1.])      # openCV convention
+    focus_in_world = np.dot(c2w[:3, :4], focus_in_cam)
+
+    for theta in np.linspace(0., 2. * np.pi * rots, N+1)[:-1]:
+        cam_location = np.dot(
+            c2w[:3, :4], 
+            # np.array([np.cos(theta), -np.sin(theta), -np.sin(theta*zrate), 1.]) * rads    # openGL convention
+            np.array([np.cos(theta), np.sin(theta), np.sin(theta*zrate), 1.]) * rads        # openCV convention
+        )
+        c2w_i = look_at(cam_location, focus_in_world, up=up_vec)
+        c2w_tracks.append(c2w_i)
+
     return c2w_tracks
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/nerf/test_cam_params.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/nerf/test_cam_params.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,508 +1,508 @@
-import os
-import sys
-import unittest
-import argparse
-
-from einops import rearrange
-
-
-class Testing_cam_params(unittest.TestCase):
-
-  def test_CamParams_from_config(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from . import cam_params
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=20,
-                                                 H0=756,
-                                                 W0=1008,
-                                                 so3_repr='axis-angle',
-                                                 intr_repr='square',
-                                                 initial_fov=53.13)
-
-    R, t, fx, fy = cam_param(0)
-
-    pass
-
-  def test_get_rays(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from . import cam_params
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=20,
-                                                 H0=756,
-                                                 W0=1008,
-                                                 so3_repr='axis-angle',
-                                                 intr_repr='square',
-                                                 initial_fov=53.13).cuda()
-
-    idx = [0, 1]
-    R, t, fx, fy = cam_param(idx)
-
-    H = 756
-    W = 1008
-    N_rays = 1024
-    # N_rays = -1
-    so3_representation = 'axis-angle'
-
-    rays_o, rays_d, select_inds = cam_params.get_rays(
-      rot=R,
-      trans=t,
-      focal_x=fx,
-      focal_y=fy,
-      H=H,
-      W=W,
-      N_rays=N_rays,
-      representation=so3_representation)
-
-    c2ws = cam_param.get_camera2worlds().data.cpu().numpy()
-
-    intr = cam_param.get_intrinsic(H, W).data.cpu().numpy()
-
-    pass
-
-  def test_get_rays_by_intr_and_extr(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from . import cam_params
-
-    num_imgs = 4
-    H = 756
-    W = 1008
-    # N_rays = 1024
-    N_rays = -1
-    so3_representation = 'axis-angle'
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=num_imgs,
-                                                 H0=H,
-                                                 W0=W,
-                                                 so3_repr='axis-angle',
-                                                 intr_repr='square',
-                                                 initial_fov=53.13).cuda()
-
-    idx = list(range(num_imgs))
-    R, t, fx, fy = cam_param(idx)
-
-    rays_o, rays_d, select_inds = cam_params.get_rays(
-      rot=R,
-      trans=t,
-      focal_x=fx,
-      focal_y=fy,
-      H=H,
-      W=W,
-      N_rays=N_rays,
-      representation=so3_representation)
-
-
-    c2ws = cam_param.get_camera2worlds()
-    intr = cam_param.get_intrinsic(H, W)
-
-    rays_o1, rays_d1, select_inds1 = cam_params.get_rays_by_intr_and_extr(intrinsics=intr,
-                                                                          c2w=c2ws,
-                                                                          H=H,
-                                                                          W=W,
-                                                                          N_rays=N_rays)
-
-    assert (rays_o - rays_o1).sum() < 1e-6
-    assert (rays_d - rays_d1).sum() < 1e-6
-    assert (select_inds == select_inds1).all()
-
-    pass
-
-  def test_get_pose_avg(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from . import cam_params
-
-    num_imgs = 4
-    H = 756
-    W = 1008
-    # N_rays = 1024
-    N_rays = -1
-    so3_representation = 'axis-angle'
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=num_imgs,
-                                                 H0=H,
-                                                 W0=W,
-                                                 so3_repr='axis-angle',
-                                                 intr_repr='square',
-                                                 initial_fov=53.13).cuda()
-
-
-
-    # c2ws = cam_param.get_camera2worlds()
-    intr = cam_param.get_intrinsic(H, W)
-    c2ws = cam_param.poses_avg()[None, ...]
-
-    rays_o1, rays_d1, select_inds1 = cam_params.get_rays_by_intr_and_extr(intrinsics=intr,
-                                                                          c2w=c2ws,
-                                                                          H=H,
-                                                                          W=W,
-                                                                          N_rays=N_rays)
-
-
-
-    pass
-
-  def test_get_rays_random_pose(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from . import volume_rendering
-    from . import cam_params
-
-    num_imgs = 4
-    H = 756
-    W = 1008
-    # N_rays = 1024
-    N_rays = -1
-    so3_representation = 'axis-angle'
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=num_imgs,
-                                                 H0=H,
-                                                 W0=W,
-                                                 so3_repr='axis-angle',
-                                                 intr_repr='square',
-                                                 initial_fov=53.13,
-                                                 freeze_intr=True).cuda()
-
-    intr = cam_param(mode='get_intrinsic')
-
-    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-        device='cuda',
-        bs=num_imgs,
-        intr=intr,
-        h_stddev=0.3,
-        v_stddev=0.155,
-    )
-
-    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
-
-    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                        rays_d=rays_d,
-                                                        near=0.5,
-                                                        far=1.5,
-                                                        N_samples=24,
-                                                        perturb=0)
-
-    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
-    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
-    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
-    pass
-
-
-class Testing_cam_params_v1(unittest.TestCase):
-
-    def test_get_rays_random_pose(self, debug=True):
-        """
-        Usage:
-
-            # export CUDA_VISIBLE_DEVICES=$cuda_devices
-            # export RUN_NUM=$run_num
-
-            export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-            export PORT=12345
-            export TIME_STR=1
-            export PYTHONPATH=.
-            python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-              Testing_Launch_v1().test_launch_ddp(debug=False)" \
-              --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-              --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-              # --tl_outdir results/$resume_dir
-
-        :return:
-        """
-        if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-            os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-        if 'TIME_STR' not in os.environ:
-            os.environ['TIME_STR'] = '0'
-        if 'RUN_NUM' not in os.environ:
-            os.environ['RUN_NUM'] = '0'
-        from tl2 import tl2_utils
-        from tl2.launch.launch_utils import \
-            (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-        tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-        tl_opts = ' '.join(tl_opts_list)
-        print(f'tl_opts:\n {tl_opts}')
-
-        if debug:
-            # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-            pass
-        command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-        resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-                 tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-        argv_str = f"""
-                    --tl_config_file none
-                    --tl_command none
-                    --tl_outdir {outdir}
-                    {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                    --tl_opts {tl_opts}
-                    """
-        args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-        from . import volume_rendering
-        from . import cam_params_v1
-
-        num_imgs = 4
-        H = 756
-        W = 1008
-        # N_rays = 1024
-        N_rays = -1
-        so3_representation = 'axis-angle'
-
-        cam_param = cam_params_v1.CamParams.from_config(num_imgs=num_imgs,
-                                                        initial_fov=12,
-                                                        H0=H,
-                                                        W0=W,
-                                                        so3_repr='axis-angle',
-                                                        intr_repr='square',
-                                                        freeze_intr=True).cuda()
-
-        intr = cam_param(mode='get_intrinsic')
-
-        rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-            device='cuda',
-            bs=num_imgs,
-            intr=intr,
-            h_stddev=0,
-            v_stddev=0,)
-
-        rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
-        rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
-
-        z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                            rays_d=rays_d,
-                                                            near=0.88,
-                                                            far=1.12,
-                                                            N_samples=24,
-                                                            perturb=0)
-
-        pass
-
-
+import os
+import sys
+import unittest
+import argparse
+
+from einops import rearrange
+
+
+class Testing_cam_params(unittest.TestCase):
+
+  def test_CamParams_from_config(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from . import cam_params
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=20,
+                                                 H0=756,
+                                                 W0=1008,
+                                                 so3_repr='axis-angle',
+                                                 intr_repr='square',
+                                                 initial_fov=53.13)
+
+    R, t, fx, fy = cam_param(0)
+
+    pass
+
+  def test_get_rays(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from . import cam_params
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=20,
+                                                 H0=756,
+                                                 W0=1008,
+                                                 so3_repr='axis-angle',
+                                                 intr_repr='square',
+                                                 initial_fov=53.13).cuda()
+
+    idx = [0, 1]
+    R, t, fx, fy = cam_param(idx)
+
+    H = 756
+    W = 1008
+    N_rays = 1024
+    # N_rays = -1
+    so3_representation = 'axis-angle'
+
+    rays_o, rays_d, select_inds = cam_params.get_rays(
+      rot=R,
+      trans=t,
+      focal_x=fx,
+      focal_y=fy,
+      H=H,
+      W=W,
+      N_rays=N_rays,
+      representation=so3_representation)
+
+    c2ws = cam_param.get_camera2worlds().data.cpu().numpy()
+
+    intr = cam_param.get_intrinsic(H, W).data.cpu().numpy()
+
+    pass
+
+  def test_get_rays_by_intr_and_extr(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from . import cam_params
+
+    num_imgs = 4
+    H = 756
+    W = 1008
+    # N_rays = 1024
+    N_rays = -1
+    so3_representation = 'axis-angle'
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=num_imgs,
+                                                 H0=H,
+                                                 W0=W,
+                                                 so3_repr='axis-angle',
+                                                 intr_repr='square',
+                                                 initial_fov=53.13).cuda()
+
+    idx = list(range(num_imgs))
+    R, t, fx, fy = cam_param(idx)
+
+    rays_o, rays_d, select_inds = cam_params.get_rays(
+      rot=R,
+      trans=t,
+      focal_x=fx,
+      focal_y=fy,
+      H=H,
+      W=W,
+      N_rays=N_rays,
+      representation=so3_representation)
+
+
+    c2ws = cam_param.get_camera2worlds()
+    intr = cam_param.get_intrinsic(H, W)
+
+    rays_o1, rays_d1, select_inds1 = cam_params.get_rays_by_intr_and_extr(intrinsics=intr,
+                                                                          c2w=c2ws,
+                                                                          H=H,
+                                                                          W=W,
+                                                                          N_rays=N_rays)
+
+    assert (rays_o - rays_o1).sum() < 1e-6
+    assert (rays_d - rays_d1).sum() < 1e-6
+    assert (select_inds == select_inds1).all()
+
+    pass
+
+  def test_get_pose_avg(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from . import cam_params
+
+    num_imgs = 4
+    H = 756
+    W = 1008
+    # N_rays = 1024
+    N_rays = -1
+    so3_representation = 'axis-angle'
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=num_imgs,
+                                                 H0=H,
+                                                 W0=W,
+                                                 so3_repr='axis-angle',
+                                                 intr_repr='square',
+                                                 initial_fov=53.13).cuda()
+
+
+
+    # c2ws = cam_param.get_camera2worlds()
+    intr = cam_param.get_intrinsic(H, W)
+    c2ws = cam_param.poses_avg()[None, ...]
+
+    rays_o1, rays_d1, select_inds1 = cam_params.get_rays_by_intr_and_extr(intrinsics=intr,
+                                                                          c2w=c2ws,
+                                                                          H=H,
+                                                                          W=W,
+                                                                          N_rays=N_rays)
+
+
+
+    pass
+
+  def test_get_rays_random_pose(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from . import volume_rendering
+    from . import cam_params
+
+    num_imgs = 4
+    H = 756
+    W = 1008
+    # N_rays = 1024
+    N_rays = -1
+    so3_representation = 'axis-angle'
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=num_imgs,
+                                                 H0=H,
+                                                 W0=W,
+                                                 so3_repr='axis-angle',
+                                                 intr_repr='square',
+                                                 initial_fov=53.13,
+                                                 freeze_intr=True).cuda()
+
+    intr = cam_param(mode='get_intrinsic')
+
+    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+        device='cuda',
+        bs=num_imgs,
+        intr=intr,
+        h_stddev=0.3,
+        v_stddev=0.155,
+    )
+
+    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
+    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
+
+    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                        rays_d=rays_d,
+                                                        near=0.5,
+                                                        far=1.5,
+                                                        N_samples=24,
+                                                        perturb=0)
+
+    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
+    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
+    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
+    pass
+
+
+class Testing_cam_params_v1(unittest.TestCase):
+
+    def test_get_rays_random_pose(self, debug=True):
+        """
+        Usage:
+
+            # export CUDA_VISIBLE_DEVICES=$cuda_devices
+            # export RUN_NUM=$run_num
+
+            export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+            export PORT=12345
+            export TIME_STR=1
+            export PYTHONPATH=.
+            python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+              Testing_Launch_v1().test_launch_ddp(debug=False)" \
+              --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+              --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+              # --tl_outdir results/$resume_dir
+
+        :return:
+        """
+        if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+            os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+        if 'TIME_STR' not in os.environ:
+            os.environ['TIME_STR'] = '0'
+        if 'RUN_NUM' not in os.environ:
+            os.environ['RUN_NUM'] = '0'
+        from tl2 import tl2_utils
+        from tl2.launch.launch_utils import \
+            (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+        tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+        tl_opts = ' '.join(tl_opts_list)
+        print(f'tl_opts:\n {tl_opts}')
+
+        if debug:
+            # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+            pass
+        command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+        resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+                 tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+        argv_str = f"""
+                    --tl_config_file none
+                    --tl_command none
+                    --tl_outdir {outdir}
+                    {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                    --tl_opts {tl_opts}
+                    """
+        args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+        from . import volume_rendering
+        from . import cam_params_v1
+
+        num_imgs = 4
+        H = 756
+        W = 1008
+        # N_rays = 1024
+        N_rays = -1
+        so3_representation = 'axis-angle'
+
+        cam_param = cam_params_v1.CamParams.from_config(num_imgs=num_imgs,
+                                                        initial_fov=12,
+                                                        H0=H,
+                                                        W0=W,
+                                                        so3_repr='axis-angle',
+                                                        intr_repr='square',
+                                                        freeze_intr=True).cuda()
+
+        intr = cam_param(mode='get_intrinsic')
+
+        rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+            device='cuda',
+            bs=num_imgs,
+            intr=intr,
+            h_stddev=0,
+            v_stddev=0,)
+
+        rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
+        rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
+
+        z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                            rays_d=rays_d,
+                                                            near=0.88,
+                                                            far=1.12,
+                                                            N_samples=24,
+                                                            perturb=0)
+
+        pass
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/nerf/test_volume_rendering.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/nerf/test_volume_rendering.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,325 +1,325 @@
-import os
-import sys
-import unittest
-import argparse
-
-
-class Testing_volume_rendering(unittest.TestCase):
-
-  def test_ray_sample_points(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from . import cam_params
-    from . import volume_rendering
-
-    num_imgs = 20
-    H = 756
-    W = 1008
-    N_rays = 1024
-    # N_rays = -1
-    so3_representation = 'axis-angle'
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=num_imgs,
-                                                 H0=H,
-                                                 W0=W,
-                                                 so3_repr=so3_representation,
-                                                 intr_repr='square',
-                                                 initial_fov=53.13).cuda()
-
-    idx = [0, 1]
-    R, t, fx, fy = cam_param(idx)
-
-    rays_o, rays_d, select_inds = cam_params.get_rays(
-      rot=R,
-      trans=t,
-      focal_x=fx,
-      focal_y=fy,
-      H=H,
-      W=W,
-      N_rays=N_rays,
-      representation=so3_representation)
-
-    near = 0
-    far = 1
-    N_samples = 128
-    batched = True
-    lindisp = False
-    perturb = 1.
-
-    z_vals, pts =  volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                      rays_d=rays_d,
-                                                      near=near,
-                                                      far=far,
-                                                      N_samples=N_samples,
-                                                      batched=batched,
-                                                      lindisp=lindisp,
-                                                      perturb=perturb)
-
-
-    pass
-
-  def test_ray_integration(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import torch
-    from . import cam_params
-    from . import volume_rendering
-
-    num_imgs = 20
-    bs = 2
-    H = 756
-    W = 1008
-    N_rays = 1024
-    # N_rays = -1
-    so3_representation = 'axis-angle'
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=num_imgs,
-                                                 H0=H,
-                                                 W0=W,
-                                                 so3_repr=so3_representation,
-                                                 intr_repr='square',
-                                                 initial_fov=53.13).cuda()
-
-    idx = list(range(bs))
-    R, t, fx, fy = cam_param(idx)
-
-    rays_o, rays_d, select_inds = cam_params.get_rays(
-      rot=R,
-      trans=t,
-      focal_x=fx,
-      focal_y=fy,
-      H=H,
-      W=W,
-      N_rays=N_rays,
-      representation=so3_representation)
-
-    near = 0
-    far = 1
-    N_samples = 128
-    batched = True
-    lindisp = False
-    perturb = 1.
-
-    z_vals, pts = volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                     rays_d=rays_d,
-                                                     near=near,
-                                                     far=far,
-                                                     N_samples=N_samples,
-                                                     batched=batched,
-                                                     lindisp=lindisp,
-                                                     perturb=perturb)
-
-    raw_rgb = torch.rand(bs, N_rays, N_samples, 3).cuda()
-    raw_sigma = torch.rand(bs, N_rays, N_samples).cuda()
-
-    volume_rendering.ray_integration(raw_rgb=raw_rgb,
-                                     raw_sigma=raw_sigma,
-                                     z_vals=z_vals,
-                                     rays_d=rays_d,
-                                     raw_noise_std=1.0)
-
-    pass
-
-  def test_sample_pdf(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import torch
-    from . import cam_params
-    from . import volume_rendering
-
-    num_imgs = 20
-    bs = 2
-    H = 756
-    W = 1008
-    N_rays = 1024
-    # N_rays = -1
-    so3_representation = 'axis-angle'
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=num_imgs,
-                                                 H0=H,
-                                                 W0=W,
-                                                 so3_repr=so3_representation,
-                                                 intr_repr='square',
-                                                 initial_fov=53.13).cuda()
-
-    idx = list(range(bs))
-    R, t, fx, fy = cam_param(idx)
-
-    rays_o, rays_d, select_inds = cam_params.get_rays(
-      rot=R,
-      trans=t,
-      focal_x=fx,
-      focal_y=fy,
-      H=H,
-      W=W,
-      N_rays=N_rays,
-      representation=so3_representation)
-
-    near = 0
-    far = 1
-    N_samples = 128
-    batched = True
-    lindisp = False
-    # perturb = 1.
-    perturb = 0.
-
-    z_vals, pts = volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                     rays_d=rays_d,
-                                                     near=near,
-                                                     far=far,
-                                                     N_samples=N_samples,
-                                                     batched=batched,
-                                                     lindisp=lindisp,
-                                                     perturb=perturb)
-
-    raw_rgb = torch.rand(bs, N_rays, N_samples, 3).cuda()
-    raw_sigma = torch.rand(bs, N_rays, N_samples).cuda()
-
-    *_, weights = volume_rendering.ray_integration(raw_rgb=raw_rgb,
-                                                   raw_sigma=raw_sigma,
-                                                   z_vals=z_vals,
-                                                   rays_d=rays_d,
-                                                   raw_noise_std=1.0)
-
-    z_vals_mid = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])
-    samples = volume_rendering.sample_pdf(bins=z_vals_mid,
-                                          weights=weights[..., 1:-1],
-                                          N_importance=N_samples,
-                                          det=(perturb == 0.))
-
+import os
+import sys
+import unittest
+import argparse
+
+
+class Testing_volume_rendering(unittest.TestCase):
+
+  def test_ray_sample_points(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from . import cam_params
+    from . import volume_rendering
+
+    num_imgs = 20
+    H = 756
+    W = 1008
+    N_rays = 1024
+    # N_rays = -1
+    so3_representation = 'axis-angle'
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=num_imgs,
+                                                 H0=H,
+                                                 W0=W,
+                                                 so3_repr=so3_representation,
+                                                 intr_repr='square',
+                                                 initial_fov=53.13).cuda()
+
+    idx = [0, 1]
+    R, t, fx, fy = cam_param(idx)
+
+    rays_o, rays_d, select_inds = cam_params.get_rays(
+      rot=R,
+      trans=t,
+      focal_x=fx,
+      focal_y=fy,
+      H=H,
+      W=W,
+      N_rays=N_rays,
+      representation=so3_representation)
+
+    near = 0
+    far = 1
+    N_samples = 128
+    batched = True
+    lindisp = False
+    perturb = 1.
+
+    z_vals, pts =  volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                      rays_d=rays_d,
+                                                      near=near,
+                                                      far=far,
+                                                      N_samples=N_samples,
+                                                      batched=batched,
+                                                      lindisp=lindisp,
+                                                      perturb=perturb)
+
+
+    pass
+
+  def test_ray_integration(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import torch
+    from . import cam_params
+    from . import volume_rendering
+
+    num_imgs = 20
+    bs = 2
+    H = 756
+    W = 1008
+    N_rays = 1024
+    # N_rays = -1
+    so3_representation = 'axis-angle'
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=num_imgs,
+                                                 H0=H,
+                                                 W0=W,
+                                                 so3_repr=so3_representation,
+                                                 intr_repr='square',
+                                                 initial_fov=53.13).cuda()
+
+    idx = list(range(bs))
+    R, t, fx, fy = cam_param(idx)
+
+    rays_o, rays_d, select_inds = cam_params.get_rays(
+      rot=R,
+      trans=t,
+      focal_x=fx,
+      focal_y=fy,
+      H=H,
+      W=W,
+      N_rays=N_rays,
+      representation=so3_representation)
+
+    near = 0
+    far = 1
+    N_samples = 128
+    batched = True
+    lindisp = False
+    perturb = 1.
+
+    z_vals, pts = volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                     rays_d=rays_d,
+                                                     near=near,
+                                                     far=far,
+                                                     N_samples=N_samples,
+                                                     batched=batched,
+                                                     lindisp=lindisp,
+                                                     perturb=perturb)
+
+    raw_rgb = torch.rand(bs, N_rays, N_samples, 3).cuda()
+    raw_sigma = torch.rand(bs, N_rays, N_samples).cuda()
+
+    volume_rendering.ray_integration(raw_rgb=raw_rgb,
+                                     raw_sigma=raw_sigma,
+                                     z_vals=z_vals,
+                                     rays_d=rays_d,
+                                     raw_noise_std=1.0)
+
+    pass
+
+  def test_sample_pdf(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import torch
+    from . import cam_params
+    from . import volume_rendering
+
+    num_imgs = 20
+    bs = 2
+    H = 756
+    W = 1008
+    N_rays = 1024
+    # N_rays = -1
+    so3_representation = 'axis-angle'
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=num_imgs,
+                                                 H0=H,
+                                                 W0=W,
+                                                 so3_repr=so3_representation,
+                                                 intr_repr='square',
+                                                 initial_fov=53.13).cuda()
+
+    idx = list(range(bs))
+    R, t, fx, fy = cam_param(idx)
+
+    rays_o, rays_d, select_inds = cam_params.get_rays(
+      rot=R,
+      trans=t,
+      focal_x=fx,
+      focal_y=fy,
+      H=H,
+      W=W,
+      N_rays=N_rays,
+      representation=so3_representation)
+
+    near = 0
+    far = 1
+    N_samples = 128
+    batched = True
+    lindisp = False
+    # perturb = 1.
+    perturb = 0.
+
+    z_vals, pts = volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                     rays_d=rays_d,
+                                                     near=near,
+                                                     far=far,
+                                                     N_samples=N_samples,
+                                                     batched=batched,
+                                                     lindisp=lindisp,
+                                                     perturb=perturb)
+
+    raw_rgb = torch.rand(bs, N_rays, N_samples, 3).cuda()
+    raw_sigma = torch.rand(bs, N_rays, N_samples).cuda()
+
+    *_, weights = volume_rendering.ray_integration(raw_rgb=raw_rgb,
+                                                   raw_sigma=raw_sigma,
+                                                   z_vals=z_vals,
+                                                   rays_d=rays_d,
+                                                   raw_noise_std=1.0)
+
+    z_vals_mid = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])
+    samples = volume_rendering.sample_pdf(bins=z_vals_mid,
+                                          weights=weights[..., 1:-1],
+                                          N_importance=N_samples,
+                                          det=(perturb == 0.))
+
     pass
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/nerf/volume_rendering.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/nerf/volume_rendering.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,590 +1,590 @@
-from tqdm import tqdm
-from typing import Dict, Optional
-
-import torch.nn.functional as F
-import torch
-
-
-# torch.autograd.set_detect_anomaly(True)
-
-def get_fine_points(z_vals,
-                    rays_o,
-                    rays_d,
-                    raw_sigma,
-                    N_importance,
-                    perturb,
-                    raw_noise_std=1.0,
-                    sigma_clamp_mode='relu',  # [relu, softplus]
-                    eps=1e-10):
-  """
-
-  :param z_vals: (b, N_rays, N_samples)
-  :param rays_o:
-  :param rays_d: (b, N_rays, 3)
-  :param raw_sigma: (b, N_rays, N_samples)
-  :param N_importance:
-  :param perturb:
-  :param raw_noise_std:
-  :param sigma_clamp_mode:
-  :param eps:
-  :return
-
-  - fine_z_vals:
-  - fine_points:
-
-  """
-
-  v_weights, opacity_alpha = compute_weights(z_vals=z_vals,
-                                             rays_d=rays_d,
-                                             raw_sigma=raw_sigma,
-                                             raw_noise_std=raw_noise_std,
-                                             sigma_clamp_mode=sigma_clamp_mode,
-                                             eps=eps)
-
-  z_vals_mid = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])
-
-  # [(B,) N_rays, N_samples]
-  fine_z_vals = sample_pdf(
-    z_vals_mid,
-    v_weights[..., 1:-1],
-    N_importance,
-    det=perturb)
-
-  # [(B,) N_rays, N_samples, 3]
-  fine_points = get_points(rays_o=rays_o, rays_d=rays_d, z_vals=fine_z_vals)
-
-  return fine_z_vals, fine_points
-
-
-def get_viewdirs(rays_d):
-  """
-
-  :param rays_d: (..., 3)
-  :return:
-
-  - viewdirs: (..., 3)
-
-  """
-  viewdirs = (rays_d / torch.norm(rays_d, dim=-1, keepdim=True))
-  return viewdirs
-
-
-def perturb_z_vals(z_vals):
-  """
-  Stratified samples.
-
-  :param z_vals: (b, N_rays, N_samples)
-  :return:
-  """
-  # get intervals between samples
-  mids = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])
-  upper = torch.cat([mids, z_vals[..., -1:]], -1)
-  lower = torch.cat([z_vals[..., :1], mids], -1)
-  # stratified samples in those intervals
-  t_rand = torch.rand(z_vals.shape, device=z_vals.device)
-  z_vals = lower + (upper - lower) * t_rand
-
-  return z_vals
-
-
-def get_points(rays_o,
-               rays_d,
-               z_vals):
-  """
-
-  :param rays_o: (b, N_rays, 3)
-  :param rays_d: (b, N_rays, 3)
-  :param z_vals: (b, N_rays, N_samples)
-
-  :return:
-
-  - points: (b, N_rays, N_samples, 3)
-  """
-  points = (rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None])
-  return points
-
-
-def ray_sample_points(
-      rays_o: torch.Tensor,
-      rays_d: torch.Tensor,
-      near: float,
-      far: float,
-      N_samples: int,
-      batched=True,
-      lindisp=False,
-      perturb: float = 0.0,
-      **kwargs):
-  """
-
-  :param rays_o: [(B,) N_rays, 3]
-  :param rays_d: [(B,) N_rays, 3]
-  :param near:
-  :param far:
-  :param N_samples:
-  :param batched:
-  :param lindisp:
-  :param perturb: 0, 1
-  :param kwargs:
-  :return
-
-  - z_vals: [(B,) N_rays, N_samples]
-  - pts:    [(B,) N_rays, N_samples, 3]
-
-  """
-
-  if batched:
-    B, N_rays, _ = rays_o.shape
-    prefix_sh = [B, N_rays, N_samples]
-  else:
-    N_rays, _ = rays_o.shape
-    prefix_sh = [N_rays, N_samples]
-
-  device = rays_o.device
-  t_vals = torch.linspace(0.0, 1.0, steps=N_samples, device=device)
-  if not lindisp:
-    z_vals = near * (1.0 - t_vals) + far * (t_vals)
-  else:
-    z_vals = 1.0 / (1.0 / near * (1.0 - t_vals) + 1.0 / far * (t_vals))
-  z_vals = z_vals.expand(prefix_sh)
-
-  if perturb:
-    z_vals = perturb_z_vals(z_vals=z_vals)
-
-  # [(B,) N_rays, N_samples, 3]
-  # pts = (rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None])
-  pts = get_points(rays_o=rays_o, rays_d=rays_d, z_vals=z_vals)
-
-  return z_vals, pts
-
-
-def compute_weights(z_vals,
-                    rays_d,
-                    raw_sigma,
-                    raw_noise_std=1.0,
-                    sigma_clamp_mode='relu',  # [relu, softplus]
-                    eps=1e-10):
-  """
-  # Infer the weights of coarse points and do hierarchical sampling;
-
-  :param z_vals: (b, N_rays, N_samples)
-  :param rays_d: (b, N_rays, 3)
-  :param raw_sigma: (b, N_rays, N_samples)
-  :param raw_noise_std: 0, 1.
-  :param sigma_clamp_mode:
-  :return:
-  - visibility_weights: (b, N_rays, N_samples)
-  - opacity_alpha: (b, N_rays, N_samples)
-  """
-  device = raw_sigma.device
-
-  if sigma_clamp_mode == 'relu':
-    clamp_fn = F.relu_
-  elif sigma_clamp_mode == 'softplus':
-    clamp_fn = F.softplus
-  else:
-    raise RuntimeError("wrong clamp_fn")
-
-  dists = z_vals[..., 1:] - z_vals[..., :-1]
-  dists = torch.cat(
-    [
-      dists,
-      # 1e10 * torch.ones(dists[..., :1].shape).to(device)
-      1e2 * torch.ones(dists[..., :1].shape).to(device)  # use 1e2, as in nerf-w
-    ], dim=-1)
-
-  # rays_d is not normalized; (view_dir is normalized)
-  # rays_d contains information about the ratio of the length of all rays
-  # with respect to the ray on the principle point.
-  dists = dists * torch.norm(rays_d[..., None, :], dim=-1)
-
-  noise = torch.randn(raw_sigma.shape, device=device) * raw_noise_std
-  # alpha = 1 - exp[- relu(sigma + n) * delta]
-  opacity_alpha = 1.0 - torch.exp(-clamp_fn(raw_sigma + noise) * dists)
-
-  opacity_alpha_shifted = torch.cat(
-    [torch.ones([*opacity_alpha.shape[:-1], 1], device=device),
-     1.0 - opacity_alpha + eps], dim=-1)
-  T = torch.cumprod(opacity_alpha_shifted, dim=-1)[..., :-1]
-
-  visibility_weights = opacity_alpha * T
-
-  return visibility_weights, opacity_alpha
-
-
-def ray_integration(
-      raw_rgb: torch.Tensor,
-      raw_sigma: torch.Tensor,
-      z_vals: torch.Tensor,
-      rays_d: torch.Tensor,
-      # configs
-      raw_noise_std: float = 1.0,
-      white_bkgd=False,
-      sigma_clamp_mode='relu',  # [relu, softplus]
-      eps=1e-10,
-      **not_used_kwargs):
-  """
-  # Infer the weights of coarse points and do hierarchical sampling;
-
-  :param raw_rgb: (b, N_rays, N_samples, 3)
-  :param raw_sigma: (b, N_rays, N_samples)
-  :param z_vals: (b, N_rays, N_samples) for depth
-  :param rays_d: (b, N_rays, 3) for weights
-  :param raw_noise_std: 0, 1.  alpha = 1 - exp[- relu(sigma + n) * delta]
-  :param white_bkgd:
-  :param sigma_clamp_mode:
-  :param not_used_kwargs:
-
-  :return
-
-  - rgb_map: (b, N_rays, 3)
-  - depth_map: (b, N_rays)
-  - acc_map: (b, N_rays)
-  - disp_map: (b, N_rays)
-  - opacity_alpha: (b, N_rays, N_samples)
-  - visibility_weights: (b, N_rays, N_samples)
-
-  """
-
-  visibility_weights, opacity_alpha = compute_weights(z_vals=z_vals,
-                                                      rays_d=rays_d,
-                                                      raw_sigma=raw_sigma,
-                                                      raw_noise_std=raw_noise_std,
-                                                      sigma_clamp_mode=sigma_clamp_mode,
-                                                      eps=eps)
-
-  rgb_map = torch.sum(visibility_weights[..., None] * raw_rgb, -2)
-
-  ret_maps = {}
-
-  with torch.no_grad():
-    # depth_map = torch.sum(visibility_weights * z_vals, -1)
-    # NOTE: to get the correct depth map, the sum of weights must be 1!
-    depth_map = torch.sum(visibility_weights / (visibility_weights.sum(-1, keepdim=True) + eps) * z_vals, -1)
-    ret_maps['depth'] = depth_map
-
-    acc_map = torch.sum(visibility_weights, -1)
-    ret_maps['weights_sum'] = acc_map
-
-    # disp_map = 1.0 / torch.max(
-    #   1e-10 * torch.ones_like(depth_map),
-    #   # plus 1e-10 on the denominator to avoid nan
-    #   depth_map / (torch.sum(visibility_weights, -1) + eps), )
-    # ret_maps['disp'] = disp_map
-
-  if white_bkgd:
-    rgb_map = rgb_map + (1.0 - acc_map[..., None])
-
-  return rgb_map, ret_maps
-
-
-def sample_pdf(bins,
-               weights,
-               N_importance,
-               det=False,
-               eps=1e-5,
-               detach=True):
-  """
-  # Hierarchical sampling (section 5.2)
-
-  :param bins: (b, N_rays, N_bins)
-  :param weights: (b, N_rays, N_bins - 1)
-  :param N_importance:
-  :param det:
-  :param eps:
-
-  :return
-
-  - samples:
-
-  """
-  if detach:
-    weights = weights.detach()
-
-  device = weights.device
-  # Get pdf
-  weights = weights + 1e-5  # prevent nans
-  pdf = weights / torch.sum(weights, -1, keepdim=True)
-  cdf = torch.cumsum(pdf, -1)
-  cdf = torch.cat(
-    [torch.zeros_like(cdf[..., :1], device=device), cdf], -1
-  )  # (batch, len(bins))
-
-  # Take uniform samples
-  if det:
-    u = torch.linspace(0.0, 1.0, steps=N_importance, device=device)
-    u = u.expand(list(cdf.shape[:-1]) + [N_importance])
-  else:
-    u = torch.rand(list(cdf.shape[:-1]) + [N_importance], device=device)
-  u = u.contiguous()
-
-  # Invert CDF
-  inds = torch.searchsorted(cdf.detach(), u, right=False)
-
-  below = torch.clamp_min(inds - 1, 0)
-  above = torch.clamp_max(inds, cdf.shape[-1] - 1)
-  # (batch, N_importance, 2) ==> (B, batch, N_importance, 2)
-  inds_g = torch.stack([below, above], -1)
-
-  matched_shape = [*inds_g.shape[:-1], cdf.shape[-1]]  # fix prefix shape
-
-  cdf_g = torch.gather(cdf.unsqueeze(-2).expand(matched_shape), -1, inds_g)
-  bins_g = torch.gather(bins.unsqueeze(-2).expand(matched_shape), -1, inds_g)  # fix prefix shape
-
-  denom = cdf_g[..., 1] - cdf_g[..., 0]
-  denom[denom < eps] = 1
-  t = (u - cdf_g[..., 0]) / denom
-  samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])
-
-  return samples
-
-
-def volume_render(
-      rays_o: torch.Tensor,
-      rays_d: torch.Tensor,
-      near: float,
-      far: float,
-      network_fn,
-      network_fn_fine=None,
-      batched: bool = True,
-      batched_info: Optional[Dict] = None,
-      # configs
-      detailed_output: bool = False,
-      rayschunk: int = 32 * 1024,
-      ret_raw: bool = False,
-      use_viewdirs: bool = False,
-      use_fine_model: bool = False,
-      N_samples: int = 0,
-      N_importance: int = 0,
-      show_progress: bool = False,
-      **kwargs):
-  """ Do volume rendering
-  input:
-      rays_o: [(B,) N_rays, 3]					the starting point of each ray
-      rays_d: [(B,) N_rays, 3]					the direction of each ray (not normalized, z==1)
-      batched: whether the 0-dim is the batch-dim (B,)
-  output:
-      list
-      [0] rgb_map:    [(B,) N_rays, 3]            the rgb pixel value of each ray
-      [1] depth_map:  [(B,) N_rays]               the depth value of each ray
-      [2] dict of extra information,
-              acc_map:            [(B,) N_rays]
-              disp_map:           [(B,) N_rays]
-              opacity_alpha:      [(B,) N_rays, N_samples, ...]
-              visibility_weights: [(B,) N_rays, N_samples, ...]
-              raw.xxxx:           [(B,) N_rays, N_samples, ...]
-  """
-  use_fine_model = use_fine_model and network_fn_fine is not None
-
-  if batched:
-    DIM_RAYS = 1
-    DIM_SAMPLES = 2
-    B = rays_d.shape[0]  # batch_size
-    flat_vec_shape = [B, -1, 3]
-  else:
-    DIM_RAYS = 0
-    DIM_SAMPLES = 1
-    flat_vec_shape = [-1, 3]
-
-  rays_o = torch.reshape(rays_o, flat_vec_shape).float()
-  rays_d = torch.reshape(rays_d, flat_vec_shape).float()
-
-  # ---------------
-  # Render a ray chunk
-  # ---------------
-  def render_rayschunk(rays_o, rays_d):
-    if use_viewdirs:
-      # rays_d is not normalized; (view_dir is normalized)
-      # rays_d contains information about the ratio of the length of all rays with respect to the ray on the principle point.
-      viewdirs = (rays_d / torch.norm(rays_d, dim=-1, keepdim=True))
-    else:
-      viewdirs = None
-
-    # ---------------
-    # Sample points on the rays
-    # ---------------
-    z_vals, pts = ray_sample_points(
-      rays_o, rays_d, near, far, N_samples, batched=batched, **kwargs)
-
-    # ---------------
-    # Query network on the sampled points
-    # ---------------
-    # all in shape [(B,) N_rays, N_samples, ...]
-    coarse_raw = batchify_query_network(
-      pts, viewdirs, network_fn,
-      batched=batched,
-      batched_info=batched_info,
-      **kwargs)
-
-    # ---------------
-    # Importance sampling
-    # ---------------
-    if N_importance > 0:
-      # ---------------
-      # Infer the weights of coarse points and do hierarchical sampling
-      with torch.no_grad():
-        *_, v_weights = ray_integration(
-          coarse_raw['rgb'], coarse_raw['sigma'], z_vals, rays_d, **kwargs)
-        z_vals_mid = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])
-
-        # [(B,) N_rays, N_samples]
-        fine_z_vals = sample_pdf(
-          z_vals_mid,
-          v_weights[..., 1:-1],
-          N_importance,
-          det=(kwargs.get('perturb', 0) == 0.0))
-
-        # [(B,) N_rays, N_samples, 3]
-        # fine_pts = (rays_o[..., None, :] + rays_d[..., None, :] * fine_z_vals[..., :, None])
-        fine_pts = get_points(rays_o=rays_o, rays_d=rays_d, z_vals=fine_z_vals)
-        # end no_grad
-
-      # ---------------
-      # Qeury network on the importance sampled points
-      fine_raw = batchify_query_network(
-        fine_pts, viewdirs, network_fn_fine if use_fine_model else network_fn,
-        batched=batched,
-        batched_info=batched_info,
-        detailed_output=detailed_output,
-        **kwargs)
-
-      # Merge fine and coarse outputs
-      # Re-organize the raw output from near to far
-      all_z_vals = torch.cat([fine_z_vals, z_vals], dim=DIM_SAMPLES)
-      _, indices = torch.sort(all_z_vals, dim=DIM_SAMPLES)
-      all_z_vals = torch.gather(all_z_vals, DIM_SAMPLES, indices)
-      # all in shape [(B,) N_rays, N_samples+N_importance, ...]
-      all_raw = {}
-      for k in coarse_raw.keys():
-        val = torch.cat([fine_raw[k], coarse_raw[k]], dim=DIM_SAMPLES)
-        view_shape = [*indices.shape, *(len(val.shape) - len(indices.shape)) * [1]]
-        all_raw[k] = torch.gather(val, DIM_SAMPLES, indices.view(view_shape).expand_as(val))
-    else:
-      all_raw = coarse_raw
-      all_z_vals = z_vals
-
-    # ---------------
-    # ** The integration of volume rendering **
-    # ---------------
-    rgb_map, depth_map, acc_map, disp_map, opacity_alpha, visibility_weights = ray_integration(
-      raw_rgb=all_raw['rgb'], raw_sigma=all_raw['sigma'],
-      z_vals=all_z_vals, rays_d=rays_d, **kwargs)
-
-    ret = {
-      'rgb_map': rgb_map,
-      'depth_map': depth_map,
-    }
-
-    if detailed_output:
-      ret.update(
-        {
-          'acc_map': acc_map,
-          'disp_map': disp_map,
-          'opacity_alpha': opacity_alpha,
-          'visibility_weights': visibility_weights,
-        }
-      )
-
-    if ret_raw:
-      for k, v in all_raw.items():
-        ret['raw.{}'.format(k)] = v
-
-    return ret
-
-  ret = {}
-  for i in tqdm(range(0, rays_o.shape[DIM_RAYS], rayschunk), disable=not show_progress):
-    ret_i = render_rayschunk(
-      rays_o[:, i:i + rayschunk] if batched else rays_o[i:i + rayschunk],
-      rays_d[:, i:i + rayschunk] if batched else rays_d[i:i + rayschunk])
-    for k, v in ret_i.items():
-      if k not in ret:
-        ret[k] = []
-      ret[k].append(v)
-  for k, v in ret.items():
-    ret[k] = torch.cat(v, DIM_RAYS)
-
-  return ret['rgb_map'], ret['depth_map'], ret
-
-
-
-
-def batchify_query_network(
-      pts: torch.Tensor,
-      viewdirs: torch.Tensor,
-      network_fn,
-      batched: bool = True,
-      batched_info: Optional[Dict] = None,
-      # configs
-      netchunk: int = 1024 * 1024,
-      detailed_output: bool = False,
-      **not_used_kwargs):
-  *_, N_rays, N_samples, _ = pts.shape
-  if batched:
-    B = pts.shape[0]
-    DIM_TO_BATCHIFY = 1
-    prefix = [B, N_rays, N_samples]
-    flat_vec_shape = [B, N_rays * N_samples, 3]
-  else:
-    DIM_TO_BATCHIFY = 0
-    prefix = [N_rays, N_samples]
-    flat_vec_shape = [N_rays * N_samples, 3]
-  DIM_POST_SHAPE = DIM_TO_BATCHIFY + 1
-
-  def slice_chunk(x, ind, chunk):
-    if x is not None:
-      return x[:, ind:ind + chunk] if batched else x[ind:ind + chunk]
-    else:
-      return None
-
-  if viewdirs is not None:
-    viewdirs = viewdirs[..., None, :].expand(
-      pts.shape).reshape(flat_vec_shape)
-  pts = pts.reshape(flat_vec_shape)
-
-  raw_ret = {}
-  for i in range(0, pts.shape[DIM_TO_BATCHIFY], netchunk):
-    raw_ret_i = network_fn(
-      inputs=slice_chunk(pts, i, netchunk),
-      viewdirs=slice_chunk(viewdirs, i, netchunk),
-      batched_info=batched_info,
-      detailed_output=detailed_output
-    )
-    for k, v in raw_ret_i.items():
-      if k not in raw_ret:
-        raw_ret[k] = []
-      raw_ret[k].append(v)
-
-  # all in shape [(B,) N_rays, N_samples, ...]
-  for k, v in raw_ret.items():
-    v = torch.cat(v, DIM_TO_BATCHIFY)
-    raw_ret[k] = v.reshape([*prefix, *v.shape[DIM_POST_SHAPE:]])
-  return raw_ret
-
-
-
-
-
-
-if __name__ == "__main__":
-  H = 10
-  W = 10
-  # B, N_rays, N_samples, N_importance = [7, H * W, 32, 32, ]
-
-
-  # def dummy_network(inputs, viewdirs, batched_info, detailed_output):
-  #   N = inputs.shape[1]
-  #   return {
-  #     'rgb': torch.randn(B, N, 3),
-  #     'sigma': torch.randn(B, N),
-  #     'detail.dummya': torch.randn(B, N, 4, 4),
-  #     'detail.dummyb': torch.randn(B, N, 4, 2, 2),
-  #   }
-  #
-  #
-  # rays_o = torch.randn(B, H, W, 3)
-  # rays_d = torch.randn(B, H, W, 3)
-  # near = 0.0
-  # far = 1.0
-  # volume_render(
-  #   rays_o, rays_d, near, far, dummy_network,
-  #   N_samples=N_samples, N_importance=N_importance,
-  #   rayschunk=1000)
+from tqdm import tqdm
+from typing import Dict, Optional
+
+import torch.nn.functional as F
+import torch
+
+
+# torch.autograd.set_detect_anomaly(True)
+
+def get_fine_points(z_vals,
+                    rays_o,
+                    rays_d,
+                    raw_sigma,
+                    N_importance,
+                    perturb,
+                    raw_noise_std=1.0,
+                    sigma_clamp_mode='relu',  # [relu, softplus]
+                    eps=1e-10):
+  """
+
+  :param z_vals: (b, N_rays, N_samples)
+  :param rays_o:
+  :param rays_d: (b, N_rays, 3)
+  :param raw_sigma: (b, N_rays, N_samples)
+  :param N_importance:
+  :param perturb:
+  :param raw_noise_std:
+  :param sigma_clamp_mode:
+  :param eps:
+  :return
+
+  - fine_z_vals:
+  - fine_points:
+
+  """
+
+  v_weights, opacity_alpha = compute_weights(z_vals=z_vals,
+                                             rays_d=rays_d,
+                                             raw_sigma=raw_sigma,
+                                             raw_noise_std=raw_noise_std,
+                                             sigma_clamp_mode=sigma_clamp_mode,
+                                             eps=eps)
+
+  z_vals_mid = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])
+
+  # [(B,) N_rays, N_samples]
+  fine_z_vals = sample_pdf(
+    z_vals_mid,
+    v_weights[..., 1:-1],
+    N_importance,
+    det=perturb)
+
+  # [(B,) N_rays, N_samples, 3]
+  fine_points = get_points(rays_o=rays_o, rays_d=rays_d, z_vals=fine_z_vals)
+
+  return fine_z_vals, fine_points
+
+
+def get_viewdirs(rays_d):
+  """
+
+  :param rays_d: (..., 3)
+  :return:
+
+  - viewdirs: (..., 3)
+
+  """
+  viewdirs = (rays_d / torch.norm(rays_d, dim=-1, keepdim=True))
+  return viewdirs
+
+
+def perturb_z_vals(z_vals):
+  """
+  Stratified samples.
+
+  :param z_vals: (b, N_rays, N_samples)
+  :return:
+  """
+  # get intervals between samples
+  mids = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])
+  upper = torch.cat([mids, z_vals[..., -1:]], -1)
+  lower = torch.cat([z_vals[..., :1], mids], -1)
+  # stratified samples in those intervals
+  t_rand = torch.rand(z_vals.shape, device=z_vals.device)
+  z_vals = lower + (upper - lower) * t_rand
+
+  return z_vals
+
+
+def get_points(rays_o,
+               rays_d,
+               z_vals):
+  """
+
+  :param rays_o: (b, N_rays, 3)
+  :param rays_d: (b, N_rays, 3)
+  :param z_vals: (b, N_rays, N_samples)
+
+  :return:
+
+  - points: (b, N_rays, N_samples, 3)
+  """
+  points = (rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None])
+  return points
+
+
+def ray_sample_points(
+      rays_o: torch.Tensor,
+      rays_d: torch.Tensor,
+      near: float,
+      far: float,
+      N_samples: int,
+      batched=True,
+      lindisp=False,
+      perturb: float = 0.0,
+      **kwargs):
+  """
+
+  :param rays_o: [(B,) N_rays, 3]
+  :param rays_d: [(B,) N_rays, 3]
+  :param near:
+  :param far:
+  :param N_samples:
+  :param batched:
+  :param lindisp:
+  :param perturb: 0, 1
+  :param kwargs:
+  :return
+
+  - z_vals: [(B,) N_rays, N_samples]
+  - pts:    [(B,) N_rays, N_samples, 3]
+
+  """
+
+  if batched:
+    B, N_rays, _ = rays_o.shape
+    prefix_sh = [B, N_rays, N_samples]
+  else:
+    N_rays, _ = rays_o.shape
+    prefix_sh = [N_rays, N_samples]
+
+  device = rays_o.device
+  t_vals = torch.linspace(0.0, 1.0, steps=N_samples, device=device)
+  if not lindisp:
+    z_vals = near * (1.0 - t_vals) + far * (t_vals)
+  else:
+    z_vals = 1.0 / (1.0 / near * (1.0 - t_vals) + 1.0 / far * (t_vals))
+  z_vals = z_vals.expand(prefix_sh)
+
+  if perturb:
+    z_vals = perturb_z_vals(z_vals=z_vals)
+
+  # [(B,) N_rays, N_samples, 3]
+  # pts = (rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None])
+  pts = get_points(rays_o=rays_o, rays_d=rays_d, z_vals=z_vals)
+
+  return z_vals, pts
+
+
+def compute_weights(z_vals,
+                    rays_d,
+                    raw_sigma,
+                    raw_noise_std=1.0,
+                    sigma_clamp_mode='relu',  # [relu, softplus]
+                    eps=1e-10):
+  """
+  # Infer the weights of coarse points and do hierarchical sampling;
+
+  :param z_vals: (b, N_rays, N_samples)
+  :param rays_d: (b, N_rays, 3)
+  :param raw_sigma: (b, N_rays, N_samples)
+  :param raw_noise_std: 0, 1.
+  :param sigma_clamp_mode:
+  :return:
+  - visibility_weights: (b, N_rays, N_samples)
+  - opacity_alpha: (b, N_rays, N_samples)
+  """
+  device = raw_sigma.device
+
+  if sigma_clamp_mode == 'relu':
+    clamp_fn = F.relu_
+  elif sigma_clamp_mode == 'softplus':
+    clamp_fn = F.softplus
+  else:
+    raise RuntimeError("wrong clamp_fn")
+
+  dists = z_vals[..., 1:] - z_vals[..., :-1]
+  dists = torch.cat(
+    [
+      dists,
+      # 1e10 * torch.ones(dists[..., :1].shape).to(device)
+      1e2 * torch.ones(dists[..., :1].shape).to(device)  # use 1e2, as in nerf-w
+    ], dim=-1)
+
+  # rays_d is not normalized; (view_dir is normalized)
+  # rays_d contains information about the ratio of the length of all rays
+  # with respect to the ray on the principle point.
+  dists = dists * torch.norm(rays_d[..., None, :], dim=-1)
+
+  noise = torch.randn(raw_sigma.shape, device=device) * raw_noise_std
+  # alpha = 1 - exp[- relu(sigma + n) * delta]
+  opacity_alpha = 1.0 - torch.exp(-clamp_fn(raw_sigma + noise) * dists)
+
+  opacity_alpha_shifted = torch.cat(
+    [torch.ones([*opacity_alpha.shape[:-1], 1], device=device),
+     1.0 - opacity_alpha + eps], dim=-1)
+  T = torch.cumprod(opacity_alpha_shifted, dim=-1)[..., :-1]
+
+  visibility_weights = opacity_alpha * T
+
+  return visibility_weights, opacity_alpha
+
+
+def ray_integration(
+      raw_rgb: torch.Tensor,
+      raw_sigma: torch.Tensor,
+      z_vals: torch.Tensor,
+      rays_d: torch.Tensor,
+      # configs
+      raw_noise_std: float = 1.0,
+      white_bkgd=False,
+      sigma_clamp_mode='relu',  # [relu, softplus]
+      eps=1e-10,
+      **not_used_kwargs):
+  """
+  # Infer the weights of coarse points and do hierarchical sampling;
+
+  :param raw_rgb: (b, N_rays, N_samples, 3)
+  :param raw_sigma: (b, N_rays, N_samples)
+  :param z_vals: (b, N_rays, N_samples) for depth
+  :param rays_d: (b, N_rays, 3) for weights
+  :param raw_noise_std: 0, 1.  alpha = 1 - exp[- relu(sigma + n) * delta]
+  :param white_bkgd:
+  :param sigma_clamp_mode:
+  :param not_used_kwargs:
+
+  :return
+
+  - rgb_map: (b, N_rays, 3)
+  - depth_map: (b, N_rays)
+  - acc_map: (b, N_rays)
+  - disp_map: (b, N_rays)
+  - opacity_alpha: (b, N_rays, N_samples)
+  - visibility_weights: (b, N_rays, N_samples)
+
+  """
+
+  visibility_weights, opacity_alpha = compute_weights(z_vals=z_vals,
+                                                      rays_d=rays_d,
+                                                      raw_sigma=raw_sigma,
+                                                      raw_noise_std=raw_noise_std,
+                                                      sigma_clamp_mode=sigma_clamp_mode,
+                                                      eps=eps)
+
+  rgb_map = torch.sum(visibility_weights[..., None] * raw_rgb, -2)
+
+  ret_maps = {}
+
+  with torch.no_grad():
+    # depth_map = torch.sum(visibility_weights * z_vals, -1)
+    # NOTE: to get the correct depth map, the sum of weights must be 1!
+    depth_map = torch.sum(visibility_weights / (visibility_weights.sum(-1, keepdim=True) + eps) * z_vals, -1)
+    ret_maps['depth'] = depth_map
+
+    acc_map = torch.sum(visibility_weights, -1)
+    ret_maps['weights_sum'] = acc_map
+
+    # disp_map = 1.0 / torch.max(
+    #   1e-10 * torch.ones_like(depth_map),
+    #   # plus 1e-10 on the denominator to avoid nan
+    #   depth_map / (torch.sum(visibility_weights, -1) + eps), )
+    # ret_maps['disp'] = disp_map
+
+  if white_bkgd:
+    rgb_map = rgb_map + (1.0 - acc_map[..., None])
+
+  return rgb_map, ret_maps
+
+
+def sample_pdf(bins,
+               weights,
+               N_importance,
+               det=False,
+               eps=1e-5,
+               detach=True):
+  """
+  # Hierarchical sampling (section 5.2)
+
+  :param bins: (b, N_rays, N_bins)
+  :param weights: (b, N_rays, N_bins - 1)
+  :param N_importance:
+  :param det:
+  :param eps:
+
+  :return
+
+  - samples:
+
+  """
+  if detach:
+    weights = weights.detach()
+
+  device = weights.device
+  # Get pdf
+  weights = weights + 1e-5  # prevent nans
+  pdf = weights / torch.sum(weights, -1, keepdim=True)
+  cdf = torch.cumsum(pdf, -1)
+  cdf = torch.cat(
+    [torch.zeros_like(cdf[..., :1], device=device), cdf], -1
+  )  # (batch, len(bins))
+
+  # Take uniform samples
+  if det:
+    u = torch.linspace(0.0, 1.0, steps=N_importance, device=device)
+    u = u.expand(list(cdf.shape[:-1]) + [N_importance])
+  else:
+    u = torch.rand(list(cdf.shape[:-1]) + [N_importance], device=device)
+  u = u.contiguous()
+
+  # Invert CDF
+  inds = torch.searchsorted(cdf.detach(), u, right=False)
+
+  below = torch.clamp_min(inds - 1, 0)
+  above = torch.clamp_max(inds, cdf.shape[-1] - 1)
+  # (batch, N_importance, 2) ==> (B, batch, N_importance, 2)
+  inds_g = torch.stack([below, above], -1)
+
+  matched_shape = [*inds_g.shape[:-1], cdf.shape[-1]]  # fix prefix shape
+
+  cdf_g = torch.gather(cdf.unsqueeze(-2).expand(matched_shape), -1, inds_g)
+  bins_g = torch.gather(bins.unsqueeze(-2).expand(matched_shape), -1, inds_g)  # fix prefix shape
+
+  denom = cdf_g[..., 1] - cdf_g[..., 0]
+  denom[denom < eps] = 1
+  t = (u - cdf_g[..., 0]) / denom
+  samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])
+
+  return samples
+
+
+def volume_render(
+      rays_o: torch.Tensor,
+      rays_d: torch.Tensor,
+      near: float,
+      far: float,
+      network_fn,
+      network_fn_fine=None,
+      batched: bool = True,
+      batched_info: Optional[Dict] = None,
+      # configs
+      detailed_output: bool = False,
+      rayschunk: int = 32 * 1024,
+      ret_raw: bool = False,
+      use_viewdirs: bool = False,
+      use_fine_model: bool = False,
+      N_samples: int = 0,
+      N_importance: int = 0,
+      show_progress: bool = False,
+      **kwargs):
+  """ Do volume rendering
+  input:
+      rays_o: [(B,) N_rays, 3]					the starting point of each ray
+      rays_d: [(B,) N_rays, 3]					the direction of each ray (not normalized, z==1)
+      batched: whether the 0-dim is the batch-dim (B,)
+  output:
+      list
+      [0] rgb_map:    [(B,) N_rays, 3]            the rgb pixel value of each ray
+      [1] depth_map:  [(B,) N_rays]               the depth value of each ray
+      [2] dict of extra information,
+              acc_map:            [(B,) N_rays]
+              disp_map:           [(B,) N_rays]
+              opacity_alpha:      [(B,) N_rays, N_samples, ...]
+              visibility_weights: [(B,) N_rays, N_samples, ...]
+              raw.xxxx:           [(B,) N_rays, N_samples, ...]
+  """
+  use_fine_model = use_fine_model and network_fn_fine is not None
+
+  if batched:
+    DIM_RAYS = 1
+    DIM_SAMPLES = 2
+    B = rays_d.shape[0]  # batch_size
+    flat_vec_shape = [B, -1, 3]
+  else:
+    DIM_RAYS = 0
+    DIM_SAMPLES = 1
+    flat_vec_shape = [-1, 3]
+
+  rays_o = torch.reshape(rays_o, flat_vec_shape).float()
+  rays_d = torch.reshape(rays_d, flat_vec_shape).float()
+
+  # ---------------
+  # Render a ray chunk
+  # ---------------
+  def render_rayschunk(rays_o, rays_d):
+    if use_viewdirs:
+      # rays_d is not normalized; (view_dir is normalized)
+      # rays_d contains information about the ratio of the length of all rays with respect to the ray on the principle point.
+      viewdirs = (rays_d / torch.norm(rays_d, dim=-1, keepdim=True))
+    else:
+      viewdirs = None
+
+    # ---------------
+    # Sample points on the rays
+    # ---------------
+    z_vals, pts = ray_sample_points(
+      rays_o, rays_d, near, far, N_samples, batched=batched, **kwargs)
+
+    # ---------------
+    # Query network on the sampled points
+    # ---------------
+    # all in shape [(B,) N_rays, N_samples, ...]
+    coarse_raw = batchify_query_network(
+      pts, viewdirs, network_fn,
+      batched=batched,
+      batched_info=batched_info,
+      **kwargs)
+
+    # ---------------
+    # Importance sampling
+    # ---------------
+    if N_importance > 0:
+      # ---------------
+      # Infer the weights of coarse points and do hierarchical sampling
+      with torch.no_grad():
+        *_, v_weights = ray_integration(
+          coarse_raw['rgb'], coarse_raw['sigma'], z_vals, rays_d, **kwargs)
+        z_vals_mid = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])
+
+        # [(B,) N_rays, N_samples]
+        fine_z_vals = sample_pdf(
+          z_vals_mid,
+          v_weights[..., 1:-1],
+          N_importance,
+          det=(kwargs.get('perturb', 0) == 0.0))
+
+        # [(B,) N_rays, N_samples, 3]
+        # fine_pts = (rays_o[..., None, :] + rays_d[..., None, :] * fine_z_vals[..., :, None])
+        fine_pts = get_points(rays_o=rays_o, rays_d=rays_d, z_vals=fine_z_vals)
+        # end no_grad
+
+      # ---------------
+      # Qeury network on the importance sampled points
+      fine_raw = batchify_query_network(
+        fine_pts, viewdirs, network_fn_fine if use_fine_model else network_fn,
+        batched=batched,
+        batched_info=batched_info,
+        detailed_output=detailed_output,
+        **kwargs)
+
+      # Merge fine and coarse outputs
+      # Re-organize the raw output from near to far
+      all_z_vals = torch.cat([fine_z_vals, z_vals], dim=DIM_SAMPLES)
+      _, indices = torch.sort(all_z_vals, dim=DIM_SAMPLES)
+      all_z_vals = torch.gather(all_z_vals, DIM_SAMPLES, indices)
+      # all in shape [(B,) N_rays, N_samples+N_importance, ...]
+      all_raw = {}
+      for k in coarse_raw.keys():
+        val = torch.cat([fine_raw[k], coarse_raw[k]], dim=DIM_SAMPLES)
+        view_shape = [*indices.shape, *(len(val.shape) - len(indices.shape)) * [1]]
+        all_raw[k] = torch.gather(val, DIM_SAMPLES, indices.view(view_shape).expand_as(val))
+    else:
+      all_raw = coarse_raw
+      all_z_vals = z_vals
+
+    # ---------------
+    # ** The integration of volume rendering **
+    # ---------------
+    rgb_map, depth_map, acc_map, disp_map, opacity_alpha, visibility_weights = ray_integration(
+      raw_rgb=all_raw['rgb'], raw_sigma=all_raw['sigma'],
+      z_vals=all_z_vals, rays_d=rays_d, **kwargs)
+
+    ret = {
+      'rgb_map': rgb_map,
+      'depth_map': depth_map,
+    }
+
+    if detailed_output:
+      ret.update(
+        {
+          'acc_map': acc_map,
+          'disp_map': disp_map,
+          'opacity_alpha': opacity_alpha,
+          'visibility_weights': visibility_weights,
+        }
+      )
+
+    if ret_raw:
+      for k, v in all_raw.items():
+        ret['raw.{}'.format(k)] = v
+
+    return ret
+
+  ret = {}
+  for i in tqdm(range(0, rays_o.shape[DIM_RAYS], rayschunk), disable=not show_progress):
+    ret_i = render_rayschunk(
+      rays_o[:, i:i + rayschunk] if batched else rays_o[i:i + rayschunk],
+      rays_d[:, i:i + rayschunk] if batched else rays_d[i:i + rayschunk])
+    for k, v in ret_i.items():
+      if k not in ret:
+        ret[k] = []
+      ret[k].append(v)
+  for k, v in ret.items():
+    ret[k] = torch.cat(v, DIM_RAYS)
+
+  return ret['rgb_map'], ret['depth_map'], ret
+
+
+
+
+def batchify_query_network(
+      pts: torch.Tensor,
+      viewdirs: torch.Tensor,
+      network_fn,
+      batched: bool = True,
+      batched_info: Optional[Dict] = None,
+      # configs
+      netchunk: int = 1024 * 1024,
+      detailed_output: bool = False,
+      **not_used_kwargs):
+  *_, N_rays, N_samples, _ = pts.shape
+  if batched:
+    B = pts.shape[0]
+    DIM_TO_BATCHIFY = 1
+    prefix = [B, N_rays, N_samples]
+    flat_vec_shape = [B, N_rays * N_samples, 3]
+  else:
+    DIM_TO_BATCHIFY = 0
+    prefix = [N_rays, N_samples]
+    flat_vec_shape = [N_rays * N_samples, 3]
+  DIM_POST_SHAPE = DIM_TO_BATCHIFY + 1
+
+  def slice_chunk(x, ind, chunk):
+    if x is not None:
+      return x[:, ind:ind + chunk] if batched else x[ind:ind + chunk]
+    else:
+      return None
+
+  if viewdirs is not None:
+    viewdirs = viewdirs[..., None, :].expand(
+      pts.shape).reshape(flat_vec_shape)
+  pts = pts.reshape(flat_vec_shape)
+
+  raw_ret = {}
+  for i in range(0, pts.shape[DIM_TO_BATCHIFY], netchunk):
+    raw_ret_i = network_fn(
+      inputs=slice_chunk(pts, i, netchunk),
+      viewdirs=slice_chunk(viewdirs, i, netchunk),
+      batched_info=batched_info,
+      detailed_output=detailed_output
+    )
+    for k, v in raw_ret_i.items():
+      if k not in raw_ret:
+        raw_ret[k] = []
+      raw_ret[k].append(v)
+
+  # all in shape [(B,) N_rays, N_samples, ...]
+  for k, v in raw_ret.items():
+    v = torch.cat(v, DIM_TO_BATCHIFY)
+    raw_ret[k] = v.reshape([*prefix, *v.shape[DIM_POST_SHAPE:]])
+  return raw_ret
+
+
+
+
+
+
+if __name__ == "__main__":
+  H = 10
+  W = 10
+  # B, N_rays, N_samples, N_importance = [7, H * W, 32, 32, ]
+
+
+  # def dummy_network(inputs, viewdirs, batched_info, detailed_output):
+  #   N = inputs.shape[1]
+  #   return {
+  #     'rgb': torch.randn(B, N, 3),
+  #     'sigma': torch.randn(B, N),
+  #     'detail.dummya': torch.randn(B, N, 4, 4),
+  #     'detail.dummyb': torch.randn(B, N, 4, 2, 2),
+  #   }
+  #
+  #
+  # rays_o = torch.randn(B, H, W, 3)
+  # rays_d = torch.randn(B, H, W, 3)
+  # near = 0.0
+  # far = 1.0
+  # volume_render(
+  #   rays_o, rays_d, near, far, dummy_network,
+  #   N_samples=N_samples, N_importance=N_importance,
+  #   rayschunk=1000)
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/cips_net.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/cips_net.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,453 +1,453 @@
-import collections
-from collections import OrderedDict
-import numpy as np
-import logging
-from einops import rearrange
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-from tl2 import tl2_utils
-from tl2.proj.pytorch import torch_utils
-from tl2.proj.pytorch import init_func
-from tl2.proj.pytorch.pytorch_hook import VerboseModel
-from tl2.proj.fvcore import global_cfg
-
-
-class ModFC(nn.Module):
-  def __repr__(self): return f"{self.__class__.__name__}({self.repr})"
-
-  def __init__(
-        self,
-        in_channel,
-        out_channel,
-        kernel_size=1,
-        style_dim=None,
-        use_style_fc=True,
-        demodulate=True,
-        use_group_conv=False,
-        eps=1e-8,
-        **kwargs):
-    """
-
-    """
-    super().__init__()
-
-    self.repr = f"in_channel={in_channel}, out_channel={out_channel}, kernel_size={kernel_size}, " \
-                f"style_dim={style_dim}, use_style_fc={use_style_fc}, demodulate={demodulate}, " \
-                f"use_group_conv={use_group_conv}, eps={eps}"
-
-    self.eps = eps
-    self.in_channel = in_channel
-    self.out_channel = out_channel
-    self.kernel_size = kernel_size
-    self.style_dim = style_dim
-    self.use_style_fc = use_style_fc
-    self.demodulate = demodulate
-    self.use_group_conv = use_group_conv
-
-    self.padding = kernel_size // 2
-
-    if use_group_conv:
-      self.weight = nn.Parameter(torch.randn(1, out_channel, in_channel, kernel_size, kernel_size))
-      torch.nn.init.kaiming_normal_(self.weight[0], a=0.2, mode='fan_in', nonlinearity='leaky_relu')
-    else:
-      assert kernel_size == 1
-      self.weight = nn.Parameter(torch.randn(1, in_channel, out_channel))
-      torch.nn.init.kaiming_normal_(self.weight[0], a=0.2, mode='fan_in', nonlinearity='leaky_relu')
-
-    if use_style_fc:
-      # self.modulation = EqualLinear(style_dim, in_channel, bias_init=1, lr_mul=1., scale=scale)
-      self.modulation = nn.Linear(style_dim, in_channel)
-      self.modulation.apply(init_func.kaiming_leaky_init)
-      # self.modulation.weight.data.div_(0.01)
-    else:
-      self.style_dim = in_channel
-
-
-    pass
-
-  def forward_bmm(self,
-                  x,
-                  style,
-                  weight):
-    """
-
-    :param input: (b, in_c, h, w), (b, in_c), (b, n, in_c)
-    :param style: (b, in_c)
-    :return:
-    """
-    assert x.shape[0] == style.shape[0]
-    if x.dim() == 2:
-      input = rearrange(x, "b c -> b 1 c")
-    elif x.dim() == 3:
-      input = x
-    else:
-      assert 0
-
-    batch, N, in_channel = input.shape
-
-    if self.use_style_fc:
-      # style = self.sin(style)
-      style = self.modulation(style)
-      # style = self.norm(style)
-      style = style.view(-1, in_channel, 1)
-    else:
-      # style = self.norm(style)
-      style = rearrange(style, 'b c -> b c 1')
-      # style = style + 1.
-
-    # (1, in, out) * (b in 1) -> (b, in, out)
-    weight = weight * (style + 1)
-
-    if self.demodulate:
-      demod = torch.rsqrt(weight.pow(2).sum([1, ])).clamp_min(self.eps)  # (b, out)
-      weight = weight * demod.view(batch, 1, self.out_channel)  # (b, in, out) * (b, 1, out) -> (b, in, out)
-    # (b, n, in) * (b, in, out) -> (b, n, out)
-    out = torch.bmm(input, weight)
-
-    if x.dim() == 2:
-      out = rearrange(out, "b 1 c -> b c")
-    elif x.dim() == 3:
-      # out = rearrange(out, "b n c -> b n c")
-      pass
-    return out
-
-  def forward_group_conv(self,
-                         x,
-                         style):
-    """
-
-    :param input: (b, in_c, h, w), (b, in_c), (b, n, in_c)
-    :param style: (b, in_c)
-    :return:
-    """
-    assert x.shape[0] == style.shape[0]
-    if x.dim() == 2:
-      input = rearrange(x, "b c -> b c 1 1")
-    elif x.dim() == 3:
-      input = rearrange(x, "b n c -> b c n 1")
-    elif x.dim() == 4:
-      input = x
-    else:
-      assert 0
-
-    batch, in_channel, height, width = input.shape
-
-    if self.use_style_fc:
-      style = self.modulation(style).view(-1, 1, in_channel, 1, 1)
-      style = style + 1.
-    else:
-      style = rearrange(style, 'b c -> b 1 c 1 1')
-      # style = style + 1.
-    # (1, out, in, ks, ks) * (b, 1, in, 1, 1) -> (b, out, in, ks, ks)
-    weight = self.weight * style
-    if self.demodulate:
-      demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4])).clamp_min(self.eps) # (b, out)
-      weight = weight * demod.view(batch, self.out_channel, 1, 1, 1) # (b, out, in, ks, ks) * (b, out, 1, 1, 1)
-    # (b*out, in, ks, ks)
-    weight = weight.view(batch * self.out_channel, in_channel, self.kernel_size, self.kernel_size)
-    # (1, b*in, h, w)
-    input = input.reshape(1, batch * in_channel, height, width)
-    out = F.conv2d(input, weight, padding=self.padding, groups=batch)
-    _, _, height, width = out.shape
-    out = out.view(batch, self.out_channel, height, width)
-
-    if x.dim() == 2:
-      out = rearrange(out, "b c 1 1 -> b c")
-    elif x.dim() == 3:
-      out = rearrange(out, "b c n 1 -> b n c")
-
-    return out
-
-  def forward(self,
-              x,
-              style,
-              force_bmm=False):
-    """
-
-    :param input: (b, in_c, h, w), (b, in_c), (b, n, in_c)
-    :param style: (b, in_c)
-    :return:
-    """
-    if self.use_group_conv:
-      if force_bmm:
-        weight = rearrange(self.weight, "1 out in 1 1 -> 1 in out")
-        out = self.forward_bmm(x=x, style=style, weight=weight)
-      else:
-        out = self.forward_group_conv(x=x, style=style)
-    else:
-      out = self.forward_bmm(x=x, style=style, weight=self.weight)
-    return out
-
-
-class SkipLayer(nn.Module):
-  def __init__(self, ):
-    super(SkipLayer, self).__init__()
-
-  def forward(self, x0, x1):
-    # out = (x0 + x1) / math.pi
-    out = (x0 + x1)
-    return out
-
-
-class ModFCBlock(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               in_dim,
-               out_dim,
-               style_dim,
-               name_prefix,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = f"in_dim={in_dim}, " \
-                    f"out_dim={out_dim}, " \
-                    f"style_dim={style_dim})"
-
-    self.in_dim = in_dim
-    self.out_dim = out_dim
-    self.style_dim = style_dim
-    self.name_prefix = name_prefix
-
-    self.style_dim_dict = {}
-
-    self.mod1 = ModFC(in_channel=in_dim,
-                      out_channel=out_dim,
-                      style_dim=style_dim,
-                      use_style_fc=True,
-                      )
-    self.style_dim_dict[f'{name_prefix}_0'] = self.mod1.style_dim
-    self.act1 = nn.LeakyReLU(0.2, inplace=True)
-
-    self.mod2 = ModFC(in_channel=out_dim,
-                      out_channel=out_dim,
-                      style_dim=style_dim,
-                      use_style_fc=True,
-                      )
-    self.style_dim_dict[f'{name_prefix}_1'] = self.mod2.style_dim
-    self.act2 = nn.LeakyReLU(0.2, inplace=True)
-
-    self.skip = SkipLayer()
-    pass
-
-  def forward(self,
-              x,
-              style_dict,
-              skip=True):
-    """
-
-    :param x: (b, in_c, h, w), (b, in_c), (b, n, in_c)
-    :param style_dict:
-    :param skip:
-    :return:
-    """
-    x_orig = x
-
-    style = style_dict[f'{self.name_prefix}_0']
-    x = self.mod1(x, style)
-    x = self.act1(x)
-
-    style = style_dict[f'{self.name_prefix}_1']
-    x = self.mod2(x, style)
-    out = self.act2(x)
-
-    if skip and out.shape[-1] == x_orig.shape[-1]:
-      # out = (out + x_orig) / 1.41421
-      out = self.skip(out, x_orig)
-    return out
-
-
-class ToRGB(nn.Module):
-  def __init__(self,
-               in_dim,
-               dim_rgb=3):
-    super().__init__()
-    self.in_dim = in_dim
-    self.dim_rgb = dim_rgb
-
-    self.linear = nn.Linear(in_dim, dim_rgb)
-    pass
-
-  def forward(self,
-              input,
-              skip=None):
-
-    out = self.linear(input)
-
-    if skip is not None:
-      out = out + skip
-    return out
-
-
-def _frequency_init(freq):
-  def init(m):
-    with torch.no_grad():
-      if isinstance(m, nn.Linear):
-        num_input = m.weight.size(-1)
-        m.weight.uniform_(-np.sqrt(6 / num_input) / freq, np.sqrt(6 / num_input) / freq)
-
-  return init
-
-
-class CIPSNet(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               input_dim,
-               hidden_dim,
-               out_dim,
-               style_dim,
-               num_blocks,
-               device=None,
-               name_prefix='cips',
-               add_out_layer=False,
-               disable_to_rgb=False,
-               skip=True,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'input_dim': input_dim,
-      'hidden_dim': hidden_dim,
-      'out_dim': out_dim,
-      'style_dim': style_dim,
-      'num_blocks': num_blocks,
-      'add_out_layer': add_out_layer,
-      'disable_to_rgb': disable_to_rgb,
-      'skip': skip,
-    }, prefix_str=name_prefix)
-
-    self.device = device
-    self.name_prefix = name_prefix
-    self.num_blocks = num_blocks
-    self.disable_to_rgb = disable_to_rgb
-    self.skip = skip
-
-    if disable_to_rgb:
-      self.out_dim = hidden_dim
-    else:
-      self.out_dim = out_dim
-
-    self.channels = {}
-    for i in range(num_blocks):
-      self.channels[f"w_{name_prefix}_b{i}"] = hidden_dim
-
-    self.module_name_list = []
-
-    self.style_dim_dict = {}
-
-    _out_dim = input_dim
-
-    blocks = OrderedDict()
-    to_rbgs = OrderedDict()
-    for i, (name, channel) in enumerate(self.channels.items()):
-      _in_dim = _out_dim
-      _out_dim = channel
-
-      _block = ModFCBlock(in_dim=_in_dim,
-                          out_dim=_out_dim,
-                          style_dim=style_dim,
-                          name_prefix=f'{name}')
-      self.style_dim_dict.update(_block.style_dim_dict)
-      blocks[name] = _block
-
-      _to_rgb = ToRGB(in_dim=_out_dim, dim_rgb=out_dim)
-      to_rbgs[name] = _to_rgb
-
-    self.blocks = nn.ModuleDict(blocks)
-    self.module_name_list.append('blocks')
-
-    if self.disable_to_rgb:
-      self.to_rgbs = None
-    else:
-      self.to_rgbs = nn.ModuleDict(to_rbgs)
-      self.to_rgbs.apply(_frequency_init(100))
-      self.module_name_list.append('to_rgbs')
-
-    if add_out_layer:
-      out_layers = []
-      if out_dim > 3:
-        out_layers.append(nn.Linear(out_dim, 3))
-      out_layers.append(nn.Tanh())
-
-      self.out_layer = nn.Sequential(*out_layers)
-      self.out_layer.apply(_frequency_init(100))
-      self.module_name_list.append('out_layer')
-
-      self.hidden_layer = nn.Linear(input_dim, out_dim)
-      self.module_name_list.append('hidden_layer')
-
-    else:
-      self.out_layer = None
-
-    models_dict = {}
-    for name in self.module_name_list:
-      models_dict[name] = getattr(self, name)
-    models_dict[name_prefix] = self
-    logger = logging.getLogger('tl')
-    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
-    logger.info(self)
-    pass
-
-  def forward(self,
-              input,
-              style_dict,
-              block_end_index=None,
-              **kwargs):
-    """
-
-    :param input: points xyz, (b, num_points, 3)
-    :param style_dict:
-    :param kwargs:
-
-    :return
-
-    - out: (b, num_points, 4), rgb(3) + sigma(1)
-
-    """
-    if block_end_index is None:
-      block_end_index = self.num_blocks
-
-    x = input
-
-    if block_end_index > 0:
-      rgb = None
-      for idx, (name, block) in enumerate(self.blocks.items()):
-
-        if global_cfg.tl_debug:
-          VerboseModel.forward_verbose(block,
-                                       inputs_args=(x, style_dict, self.skip),
-                                       submodels=['mod1', 'mod2'],
-                                       name_prefix=f'{self.name_prefix}.b.{idx}.')
-        x = block(x, style_dict, skip=self.skip)
-
-        if self.disable_to_rgb:
-          rgb = x
-        else:
-          if global_cfg.tl_debug:
-            VerboseModel.forward_verbose(self.to_rgbs[name],
-                                         inputs_args=(x, rgb),
-                                         name_prefix=f'{self.name_prefix}.to_rgb.{idx}.')
-          rgb = self.to_rgbs[name](x, skip=rgb)
-
-        if idx + 1 >= block_end_index:
-          break
-    else:
-      rgb = self.hidden_layer(x)
-
-    if self.out_layer is not None:
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.out_layer,
-                                     inputs_args=(rgb, ),
-                                     name_prefix='out_layer.')
-      out = self.out_layer(rgb)
-    else:
-      out= rgb
-
-    return out
-
+import collections
+from collections import OrderedDict
+import numpy as np
+import logging
+from einops import rearrange
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from tl2 import tl2_utils
+from tl2.proj.pytorch import torch_utils
+from tl2.proj.pytorch import init_func
+from tl2.proj.pytorch.pytorch_hook import VerboseModel
+from tl2.proj.fvcore import global_cfg
+
+
+class ModFC(nn.Module):
+  def __repr__(self): return f"{self.__class__.__name__}({self.repr})"
+
+  def __init__(
+        self,
+        in_channel,
+        out_channel,
+        kernel_size=1,
+        style_dim=None,
+        use_style_fc=True,
+        demodulate=True,
+        use_group_conv=False,
+        eps=1e-8,
+        **kwargs):
+    """
+
+    """
+    super().__init__()
+
+    self.repr = f"in_channel={in_channel}, out_channel={out_channel}, kernel_size={kernel_size}, " \
+                f"style_dim={style_dim}, use_style_fc={use_style_fc}, demodulate={demodulate}, " \
+                f"use_group_conv={use_group_conv}, eps={eps}"
+
+    self.eps = eps
+    self.in_channel = in_channel
+    self.out_channel = out_channel
+    self.kernel_size = kernel_size
+    self.style_dim = style_dim
+    self.use_style_fc = use_style_fc
+    self.demodulate = demodulate
+    self.use_group_conv = use_group_conv
+
+    self.padding = kernel_size // 2
+
+    if use_group_conv:
+      self.weight = nn.Parameter(torch.randn(1, out_channel, in_channel, kernel_size, kernel_size))
+      torch.nn.init.kaiming_normal_(self.weight[0], a=0.2, mode='fan_in', nonlinearity='leaky_relu')
+    else:
+      assert kernel_size == 1
+      self.weight = nn.Parameter(torch.randn(1, in_channel, out_channel))
+      torch.nn.init.kaiming_normal_(self.weight[0], a=0.2, mode='fan_in', nonlinearity='leaky_relu')
+
+    if use_style_fc:
+      # self.modulation = EqualLinear(style_dim, in_channel, bias_init=1, lr_mul=1., scale=scale)
+      self.modulation = nn.Linear(style_dim, in_channel)
+      self.modulation.apply(init_func.kaiming_leaky_init)
+      # self.modulation.weight.data.div_(0.01)
+    else:
+      self.style_dim = in_channel
+
+
+    pass
+
+  def forward_bmm(self,
+                  x,
+                  style,
+                  weight):
+    """
+
+    :param input: (b, in_c, h, w), (b, in_c), (b, n, in_c)
+    :param style: (b, in_c)
+    :return:
+    """
+    assert x.shape[0] == style.shape[0]
+    if x.dim() == 2:
+      input = rearrange(x, "b c -> b 1 c")
+    elif x.dim() == 3:
+      input = x
+    else:
+      assert 0
+
+    batch, N, in_channel = input.shape
+
+    if self.use_style_fc:
+      # style = self.sin(style)
+      style = self.modulation(style)
+      # style = self.norm(style)
+      style = style.view(-1, in_channel, 1)
+    else:
+      # style = self.norm(style)
+      style = rearrange(style, 'b c -> b c 1')
+      # style = style + 1.
+
+    # (1, in, out) * (b in 1) -> (b, in, out)
+    weight = weight * (style + 1)
+
+    if self.demodulate:
+      demod = torch.rsqrt(weight.pow(2).sum([1, ])).clamp_min(self.eps)  # (b, out)
+      weight = weight * demod.view(batch, 1, self.out_channel)  # (b, in, out) * (b, 1, out) -> (b, in, out)
+    # (b, n, in) * (b, in, out) -> (b, n, out)
+    out = torch.bmm(input, weight)
+
+    if x.dim() == 2:
+      out = rearrange(out, "b 1 c -> b c")
+    elif x.dim() == 3:
+      # out = rearrange(out, "b n c -> b n c")
+      pass
+    return out
+
+  def forward_group_conv(self,
+                         x,
+                         style):
+    """
+
+    :param input: (b, in_c, h, w), (b, in_c), (b, n, in_c)
+    :param style: (b, in_c)
+    :return:
+    """
+    assert x.shape[0] == style.shape[0]
+    if x.dim() == 2:
+      input = rearrange(x, "b c -> b c 1 1")
+    elif x.dim() == 3:
+      input = rearrange(x, "b n c -> b c n 1")
+    elif x.dim() == 4:
+      input = x
+    else:
+      assert 0
+
+    batch, in_channel, height, width = input.shape
+
+    if self.use_style_fc:
+      style = self.modulation(style).view(-1, 1, in_channel, 1, 1)
+      style = style + 1.
+    else:
+      style = rearrange(style, 'b c -> b 1 c 1 1')
+      # style = style + 1.
+    # (1, out, in, ks, ks) * (b, 1, in, 1, 1) -> (b, out, in, ks, ks)
+    weight = self.weight * style
+    if self.demodulate:
+      demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4])).clamp_min(self.eps) # (b, out)
+      weight = weight * demod.view(batch, self.out_channel, 1, 1, 1) # (b, out, in, ks, ks) * (b, out, 1, 1, 1)
+    # (b*out, in, ks, ks)
+    weight = weight.view(batch * self.out_channel, in_channel, self.kernel_size, self.kernel_size)
+    # (1, b*in, h, w)
+    input = input.reshape(1, batch * in_channel, height, width)
+    out = F.conv2d(input, weight, padding=self.padding, groups=batch)
+    _, _, height, width = out.shape
+    out = out.view(batch, self.out_channel, height, width)
+
+    if x.dim() == 2:
+      out = rearrange(out, "b c 1 1 -> b c")
+    elif x.dim() == 3:
+      out = rearrange(out, "b c n 1 -> b n c")
+
+    return out
+
+  def forward(self,
+              x,
+              style,
+              force_bmm=False):
+    """
+
+    :param input: (b, in_c, h, w), (b, in_c), (b, n, in_c)
+    :param style: (b, in_c)
+    :return:
+    """
+    if self.use_group_conv:
+      if force_bmm:
+        weight = rearrange(self.weight, "1 out in 1 1 -> 1 in out")
+        out = self.forward_bmm(x=x, style=style, weight=weight)
+      else:
+        out = self.forward_group_conv(x=x, style=style)
+    else:
+      out = self.forward_bmm(x=x, style=style, weight=self.weight)
+    return out
+
+
+class SkipLayer(nn.Module):
+  def __init__(self, ):
+    super(SkipLayer, self).__init__()
+
+  def forward(self, x0, x1):
+    # out = (x0 + x1) / math.pi
+    out = (x0 + x1)
+    return out
+
+
+class ModFCBlock(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               in_dim,
+               out_dim,
+               style_dim,
+               name_prefix,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = f"in_dim={in_dim}, " \
+                    f"out_dim={out_dim}, " \
+                    f"style_dim={style_dim})"
+
+    self.in_dim = in_dim
+    self.out_dim = out_dim
+    self.style_dim = style_dim
+    self.name_prefix = name_prefix
+
+    self.style_dim_dict = {}
+
+    self.mod1 = ModFC(in_channel=in_dim,
+                      out_channel=out_dim,
+                      style_dim=style_dim,
+                      use_style_fc=True,
+                      )
+    self.style_dim_dict[f'{name_prefix}_0'] = self.mod1.style_dim
+    self.act1 = nn.LeakyReLU(0.2, inplace=True)
+
+    self.mod2 = ModFC(in_channel=out_dim,
+                      out_channel=out_dim,
+                      style_dim=style_dim,
+                      use_style_fc=True,
+                      )
+    self.style_dim_dict[f'{name_prefix}_1'] = self.mod2.style_dim
+    self.act2 = nn.LeakyReLU(0.2, inplace=True)
+
+    self.skip = SkipLayer()
+    pass
+
+  def forward(self,
+              x,
+              style_dict,
+              skip=True):
+    """
+
+    :param x: (b, in_c, h, w), (b, in_c), (b, n, in_c)
+    :param style_dict:
+    :param skip:
+    :return:
+    """
+    x_orig = x
+
+    style = style_dict[f'{self.name_prefix}_0']
+    x = self.mod1(x, style)
+    x = self.act1(x)
+
+    style = style_dict[f'{self.name_prefix}_1']
+    x = self.mod2(x, style)
+    out = self.act2(x)
+
+    if skip and out.shape[-1] == x_orig.shape[-1]:
+      # out = (out + x_orig) / 1.41421
+      out = self.skip(out, x_orig)
+    return out
+
+
+class ToRGB(nn.Module):
+  def __init__(self,
+               in_dim,
+               dim_rgb=3):
+    super().__init__()
+    self.in_dim = in_dim
+    self.dim_rgb = dim_rgb
+
+    self.linear = nn.Linear(in_dim, dim_rgb)
+    pass
+
+  def forward(self,
+              input,
+              skip=None):
+
+    out = self.linear(input)
+
+    if skip is not None:
+      out = out + skip
+    return out
+
+
+def _frequency_init(freq):
+  def init(m):
+    with torch.no_grad():
+      if isinstance(m, nn.Linear):
+        num_input = m.weight.size(-1)
+        m.weight.uniform_(-np.sqrt(6 / num_input) / freq, np.sqrt(6 / num_input) / freq)
+
+  return init
+
+
+class CIPSNet(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               input_dim,
+               hidden_dim,
+               out_dim,
+               style_dim,
+               num_blocks,
+               device=None,
+               name_prefix='cips',
+               add_out_layer=False,
+               disable_to_rgb=False,
+               skip=True,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'input_dim': input_dim,
+      'hidden_dim': hidden_dim,
+      'out_dim': out_dim,
+      'style_dim': style_dim,
+      'num_blocks': num_blocks,
+      'add_out_layer': add_out_layer,
+      'disable_to_rgb': disable_to_rgb,
+      'skip': skip,
+    }, prefix_str=name_prefix)
+
+    self.device = device
+    self.name_prefix = name_prefix
+    self.num_blocks = num_blocks
+    self.disable_to_rgb = disable_to_rgb
+    self.skip = skip
+
+    if disable_to_rgb:
+      self.out_dim = hidden_dim
+    else:
+      self.out_dim = out_dim
+
+    self.channels = {}
+    for i in range(num_blocks):
+      self.channels[f"w_{name_prefix}_b{i}"] = hidden_dim
+
+    self.module_name_list = []
+
+    self.style_dim_dict = {}
+
+    _out_dim = input_dim
+
+    blocks = OrderedDict()
+    to_rbgs = OrderedDict()
+    for i, (name, channel) in enumerate(self.channels.items()):
+      _in_dim = _out_dim
+      _out_dim = channel
+
+      _block = ModFCBlock(in_dim=_in_dim,
+                          out_dim=_out_dim,
+                          style_dim=style_dim,
+                          name_prefix=f'{name}')
+      self.style_dim_dict.update(_block.style_dim_dict)
+      blocks[name] = _block
+
+      _to_rgb = ToRGB(in_dim=_out_dim, dim_rgb=out_dim)
+      to_rbgs[name] = _to_rgb
+
+    self.blocks = nn.ModuleDict(blocks)
+    self.module_name_list.append('blocks')
+
+    if self.disable_to_rgb:
+      self.to_rgbs = None
+    else:
+      self.to_rgbs = nn.ModuleDict(to_rbgs)
+      self.to_rgbs.apply(_frequency_init(100))
+      self.module_name_list.append('to_rgbs')
+
+    if add_out_layer:
+      out_layers = []
+      if out_dim > 3:
+        out_layers.append(nn.Linear(out_dim, 3))
+      out_layers.append(nn.Tanh())
+
+      self.out_layer = nn.Sequential(*out_layers)
+      self.out_layer.apply(_frequency_init(100))
+      self.module_name_list.append('out_layer')
+
+      self.hidden_layer = nn.Linear(input_dim, out_dim)
+      self.module_name_list.append('hidden_layer')
+
+    else:
+      self.out_layer = None
+
+    models_dict = {}
+    for name in self.module_name_list:
+      models_dict[name] = getattr(self, name)
+    models_dict[name_prefix] = self
+    logger = logging.getLogger('tl')
+    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
+    logger.info(self)
+    pass
+
+  def forward(self,
+              input,
+              style_dict,
+              block_end_index=None,
+              **kwargs):
+    """
+
+    :param input: points xyz, (b, num_points, 3)
+    :param style_dict:
+    :param kwargs:
+
+    :return
+
+    - out: (b, num_points, 4), rgb(3) + sigma(1)
+
+    """
+    if block_end_index is None:
+      block_end_index = self.num_blocks
+
+    x = input
+
+    if block_end_index > 0:
+      rgb = None
+      for idx, (name, block) in enumerate(self.blocks.items()):
+
+        if global_cfg.tl_debug:
+          VerboseModel.forward_verbose(block,
+                                       inputs_args=(x, style_dict, self.skip),
+                                       submodels=['mod1', 'mod2'],
+                                       name_prefix=f'{self.name_prefix}.b.{idx}.')
+        x = block(x, style_dict, skip=self.skip)
+
+        if self.disable_to_rgb:
+          rgb = x
+        else:
+          if global_cfg.tl_debug:
+            VerboseModel.forward_verbose(self.to_rgbs[name],
+                                         inputs_args=(x, rgb),
+                                         name_prefix=f'{self.name_prefix}.to_rgb.{idx}.')
+          rgb = self.to_rgbs[name](x, skip=rgb)
+
+        if idx + 1 >= block_end_index:
+          break
+    else:
+      rgb = self.hidden_layer(x)
+
+    if self.out_layer is not None:
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.out_layer,
+                                     inputs_args=(rgb, ),
+                                     name_prefix='out_layer.')
+      out = self.out_layer(rgb)
+    else:
+      out= rgb
+
+    return out
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/grad_norm_layer.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/grad_norm_layer.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,70 +1,70 @@
-import torch
-import torch.nn as nn
-from torch import tensor
-from torch.nn import Module
-from torch.autograd import Function
-
-__all__ = ['GradNorm']
-
-
-class ScaleGrad_func(Function):
-  @staticmethod
-  def forward(ctx, input_, alpha_, debug_):
-    ctx.save_for_backward(input_, alpha_, debug_)
-    output = input_
-    return output
-
-  @staticmethod
-  def backward(ctx, grad_output):  # pragma: no cover
-    grad_input = None
-    _, alpha_, debug_ = ctx.saved_tensors
-    if ctx.needs_input_grad[0]:
-      # grad_input = -grad_output * alpha_
-      # grad_input = grad_output * alpha_
-
-      total_norm = torch.norm(grad_output).item()
-
-
-
-      if total_norm >= 1e-5:
-        # clip_coef = alpha_ / (total_norm + 1e-6)
-        clip_coef = alpha_ / total_norm
-        grad_input = grad_output * clip_coef
-        if debug_:
-          print(f"{total_norm} -> {alpha_}")
-
-      else:
-        grad_input = grad_output
-        if debug_:
-          print(f"{total_norm} -> unchanged")
-
-    return grad_input, None, None
-
-
-scalegrad = ScaleGrad_func.apply
-
-
-class GradNorm(Module):
-  def __repr__(self):
-    return f"{self.__class__.__name__}({self.repr_str})"
-
-  def __init__(self,
-               norm=1.,
-               *args,
-               **kwargs):
-    """
-    This layer has no parameters, and simply scale the gradient in the backward pass.
-
-    """
-    super(GradNorm, self).__init__(*args, **kwargs)
-
-    self.repr_str = f"norm={norm}"
-
-    self._norm = tensor(norm, requires_grad=False)
-    pass
-
-  def forward(self, input_, debug=False):
-
-    debug = tensor(debug, requires_grad=False)
-
-    return scalegrad(input_, self._norm, debug)
+import torch
+import torch.nn as nn
+from torch import tensor
+from torch.nn import Module
+from torch.autograd import Function
+
+__all__ = ['GradNorm']
+
+
+class ScaleGrad_func(Function):
+  @staticmethod
+  def forward(ctx, input_, alpha_, debug_):
+    ctx.save_for_backward(input_, alpha_, debug_)
+    output = input_
+    return output
+
+  @staticmethod
+  def backward(ctx, grad_output):  # pragma: no cover
+    grad_input = None
+    _, alpha_, debug_ = ctx.saved_tensors
+    if ctx.needs_input_grad[0]:
+      # grad_input = -grad_output * alpha_
+      # grad_input = grad_output * alpha_
+
+      total_norm = torch.norm(grad_output).item()
+
+
+
+      if total_norm >= 1e-5:
+        # clip_coef = alpha_ / (total_norm + 1e-6)
+        clip_coef = alpha_ / total_norm
+        grad_input = grad_output * clip_coef
+        if debug_:
+          print(f"{total_norm} -> {alpha_}")
+
+      else:
+        grad_input = grad_output
+        if debug_:
+          print(f"{total_norm} -> unchanged")
+
+    return grad_input, None, None
+
+
+scalegrad = ScaleGrad_func.apply
+
+
+class GradNorm(Module):
+  def __repr__(self):
+    return f"{self.__class__.__name__}({self.repr_str})"
+
+  def __init__(self,
+               norm=1.,
+               *args,
+               **kwargs):
+    """
+    This layer has no parameters, and simply scale the gradient in the backward pass.
+
+    """
+    super(GradNorm, self).__init__(*args, **kwargs)
+
+    self.repr_str = f"norm={norm}"
+
+    self._norm = tensor(norm, requires_grad=False)
+    pass
+
+  def forward(self, input_, debug=False):
+
+    debug = tensor(debug, requires_grad=False)
+
+    return scalegrad(input_, self._norm, debug)
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/grad_scale_layer.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/grad_scale_layer.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-from torch import tensor
-from torch.nn import Module
-from torch.autograd import Function
-
-__all__ = ['GradScale']
-
-
-class ScaleGrad_func(Function):
-  @staticmethod
-  def forward(ctx, input_, alpha_):
-    ctx.save_for_backward(input_, alpha_)
-    output = input_
-    return output
-
-  @staticmethod
-  def backward(ctx, grad_output):  # pragma: no cover
-    grad_input = None
-    _, alpha_ = ctx.saved_tensors
-    if ctx.needs_input_grad[0]:
-      # grad_input = -grad_output * alpha_
-      grad_input = grad_output * alpha_
-    return grad_input, None
-
-
-scalegrad = ScaleGrad_func.apply
-
-
-class GradScale(Module):
-  def __repr__(self):
-    return f"{self.__class__.__name__}({self.repr_str})"
-
-  def __init__(self,
-               alpha=1.,
-               *args,
-               **kwargs):
-    """
-    This layer has no parameters, and simply scale the gradient in the backward pass.
-
-    """
-    super(GradScale, self).__init__(*args, **kwargs)
-
-    self.repr_str = f"alpha={alpha:.3f}, alpha=1/{1 / alpha:.3f}"
-
-    self._alpha = tensor(alpha, requires_grad=False)
-    pass
-
-  def forward(self, input_):
-    return scalegrad(input_, self._alpha)
+from torch import tensor
+from torch.nn import Module
+from torch.autograd import Function
+
+__all__ = ['GradScale']
+
+
+class ScaleGrad_func(Function):
+  @staticmethod
+  def forward(ctx, input_, alpha_):
+    ctx.save_for_backward(input_, alpha_)
+    output = input_
+    return output
+
+  @staticmethod
+  def backward(ctx, grad_output):  # pragma: no cover
+    grad_input = None
+    _, alpha_ = ctx.saved_tensors
+    if ctx.needs_input_grad[0]:
+      # grad_input = -grad_output * alpha_
+      grad_input = grad_output * alpha_
+    return grad_input, None
+
+
+scalegrad = ScaleGrad_func.apply
+
+
+class GradScale(Module):
+  def __repr__(self):
+    return f"{self.__class__.__name__}({self.repr_str})"
+
+  def __init__(self,
+               alpha=1.,
+               *args,
+               **kwargs):
+    """
+    This layer has no parameters, and simply scale the gradient in the backward pass.
+
+    """
+    super(GradScale, self).__init__(*args, **kwargs)
+
+    self.repr_str = f"alpha={alpha:.3f}, alpha=1/{1 / alpha:.3f}"
+
+    self._alpha = tensor(alpha, requires_grad=False)
+    pass
+
+  def forward(self, input_):
+    return scalegrad(input_, self._alpha)
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/nerf_net.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/nerf_net.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,447 +1,447 @@
-import math
-import logging
-from collections import OrderedDict
-from einops import rearrange
-
-import torch
-import torch.nn as nn
-
-from tl2 import tl2_utils
-from tl2.proj.pytorch import init_func
-from tl2.proj.pytorch import torch_utils
-from tl2.proj.fvcore import global_cfg
-from tl2.proj.pytorch.pytorch_hook import VerboseModel
-
-from .cips_net import CIPSNet
-from .multi_head_mapping import MultiHeadMappingNetwork
-from .siren_net import SIRENNet_skip
-
-
-class UniformBoxWarp(nn.Module):
-  def __init__(self, scale_factor):
-    super().__init__()
-    self.scale_factor = scale_factor
-    pass
-
-  def forward(self, coordinates):
-    return coordinates * self.scale_factor
-
-
-class PosEmbedding(nn.Module):
-  def __init__(self,
-               N_freqs,
-               in_dim=3,
-               affine_dim=None,
-               xyz_affine=False,
-               append_xyz=False,
-               **kwargs):
-    """
-    Defines a function that embeds x to (x, sin(2^k x), cos(2^k x), ...)
-
-    :param max_logscale: 9
-    :param N_freqs: 10
-    :param logscale:
-    :param multi_pi:
-    """
-    super().__init__()
-
-    self.N_freqs = N_freqs
-    self.append_xyz = append_xyz
-
-    self.funcs = [torch.sin, torch.cos]
-
-    self.freqs = list(map(lambda x: 2**x * math.pi, range(N_freqs)))
-
-    if xyz_affine:
-      assert affine_dim is not None
-      self.affine_layer = nn.Linear(in_dim, affine_dim)
-      self.in_dim = affine_dim
-    else:
-      self.affine_layer = None
-      self.in_dim = in_dim
-    pass
-
-  def get_out_dim(self):
-    if self.append_xyz:
-      outdim = self.in_dim + self.in_dim * 2 * self.N_freqs
-    else:
-      outdim = self.in_dim * 2 * self.N_freqs
-    return outdim
-
-  def forward(self, x):
-    """
-    Inputs:
-        x: (B, 3)
-
-    Outputs:
-        out: (B, 2 * N_freqs * in_dim + in_dim)
-    """
-    if self.affine_layer is not None:
-      x = self.affine_layer(x)
-
-    out = []
-    if self.append_xyz:
-      out.append(x)
-    for func in self.funcs:
-      emb_list = list(map(lambda freq: func(freq * x), self.freqs))
-      out += emb_list
-
-    emb = torch.cat(out, -1)
-    return emb
-
-
-class NeRFNetwork_CIPS(nn.Module):
-  """Adds a UniformBoxWarp to scale input points to -1, 1"""
-
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               scale_factor=None,
-               PEF_cfg={},
-               use_PEF=True,
-               shape_net_cfg={},
-               app_net_cfg={},
-               name_prefix='nerf',
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'scale_factor': scale_factor,
-      'PEF_cfg': PEF_cfg,
-      'use_PEF': use_PEF,
-      'shape_net_cfg': shape_net_cfg,
-      'app_net_cfg': app_net_cfg,
-    })
-
-    self.name_prefix = name_prefix
-
-    self.module_name_list = []
-
-    if scale_factor is not None:
-      self.gridwarper = UniformBoxWarp(scale_factor=scale_factor)
-      self.module_name_list.append('gridwarper')
-    else:
-      self.gridwarper = None
-
-    if use_PEF:
-      self.xyz_emb = PosEmbedding(**PEF_cfg)
-      _in_dim = self.xyz_emb.get_out_dim()
-      self.module_name_list.append('xyz_emb')
-      # self.dir_emb = pigan_utils.PosEmbedding(max_logscale=3, N_freqs=4)
-      # dim_dir_emb = self.dir_emb.get_out_dim()
-    else:
-      self.xyz_emb = None
-      _in_dim = 3
-
-    _out_dim = shape_net_cfg['input_dim']
-    self.in_layer = nn.Linear(_in_dim, _out_dim)
-    self.module_name_list.append('in_layer')
-
-    # self.style_dim_dict = {}
-
-    self.shape_net = CIPSNet(**{
-      **shape_net_cfg,
-      'name_prefix': 'shape'
-    })
-    self.style_dim_dict_shape = self.shape_net.style_dim_dict
-    self.module_name_list.append('shape_net')
-
-    _in_dim = shape_net_cfg['out_dim']
-
-    self.sigma_layer = nn.Linear(_in_dim, 1)
-    # self.final_layer.apply(frequency_init(25))
-    self.module_name_list.append('sigma_layer')
-
-    self.app_net = CIPSNet(**{
-      **app_net_cfg,
-      'input_dim': _in_dim,
-      'name_prefix': 'app'
-    })
-    self.style_dim_dict_app = self.app_net.style_dim_dict
-    self.module_name_list.append('app_net')
-
-    _in_dim = app_net_cfg['out_dim']
-    self.out_dim = _in_dim
-
-    # self.color_layer_linear = nn.Sequential(
-    #   nn.Linear(_out_dim, rgb_dim),
-    # )
-    # self.color_layer_linear.apply(init_func.kaiming_leaky_init)
-    # self.module_name_list.append('color_layer_linear')
-
-    logger = logging.getLogger('tl')
-    models_dict = {}
-    for name in self.module_name_list:
-      models_dict[name] = getattr(self, name)
-    models_dict[name_prefix] = self
-    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
-    logger.info(self)
-    pass
-
-  def forward(self,
-              x,
-              style_dict,
-              ray_directions=None,
-              **kwargs):
-    """
-
-    :param x: points xyz, (b, num_points, 3)
-    :param style_dict:
-    :param ray_directions: (b, num_points, 3)
-    :param kwargs:
-    :return
-
-    - out: (b, num_points, rgb_dim + 1), rgb(rgb_dim) + sigma(1)
-
-    """
-
-    # scale xyz
-    if self.gridwarper is not None:
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.gridwarper,
-                                     inputs_args=(x,),
-                                     name_prefix="gridwarper.")
-      x = self.gridwarper(x)
-
-    if self.xyz_emb is not None:
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.xyz_emb,
-                                     inputs_args=(x, ),
-                                     name_prefix="xyz_emb.")
-      x = self.xyz_emb(x)
-
-    if global_cfg.tl_debug:
-      VerboseModel.forward_verbose(self.in_layer,
-                                   inputs_args=(x,),
-                                   name_prefix="in_layer.")
-    x = self.in_layer(x)
-
-    x = self.shape_net(x, style_dict)
-
-    if global_cfg.tl_debug:
-      VerboseModel.forward_verbose(self.sigma_layer,
-                                   inputs_args=(x, ),
-                                   name_prefix="sigma_layer")
-    sigma = self.sigma_layer(x)
-
-    x = self.app_net(x, style_dict)
-
-    out = torch.cat([x, sigma], dim=-1)
-    return out
-
-  # def print_number_params(self):
-  #   print()
-  #
-  #   pass
-  #
-  # def get_freq_phase(self, style_dict, name):
-  #   styles = style_dict[name]
-  #   styles = rearrange(styles, "b (n d) -> b d n", n=2)
-  #   frequencies, phase_shifts = styles.unbind(-1)
-  #   frequencies = frequencies * 15 + 30
-  #   return frequencies, phase_shifts
-  #
-  # def staged_forward(self,
-  #                    transformed_points,
-  #                    transformed_ray_directions_expanded,
-  #                    style_dict,
-  #                    max_points,
-  #                    num_steps,
-  #                    ):
-  #
-  #   batch_size, num_points, _ = transformed_points.shape
-  #
-  #   rgb_sigma_output = torch.zeros((batch_size, num_points, self.rgb_dim + 1),
-  #                                  device=self.device)
-  #   for b in range(batch_size):
-  #     head = 0
-  #     while head < num_points:
-  #       tail = head + max_points
-  #       rgb_sigma_output[b:b + 1, head:tail] = self(
-  #         input=transformed_points[b:b + 1, head:tail],  # (b, h x w x s, 3)
-  #         style_dict={name: style[b:b + 1] for name, style in style_dict.items()},
-  #         ray_directions=transformed_ray_directions_expanded[b:b + 1, head:tail])
-  #       head += max_points
-  #   rgb_sigma_output = rearrange(rgb_sigma_output, "b (hw s) rgb_sigma -> b hw s rgb_sigma", s=num_steps)
-  #   return rgb_sigma_output
-
-
-class NeRFNetwork_SIREN_skip(nn.Module):
-  """
-  shape app: SIRENNet_skip, cips_net
-
-  """
-
-
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               scale_factor=None,
-               PEF_cfg={},
-               use_PEF=True,
-               shape_net_cfg={},
-               app_net_cfg={},
-               name_prefix='nerf',
-               shape_block_end_index=None,
-               app_block_end_index=None,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'scale_factor': scale_factor,
-      'PEF_cfg': PEF_cfg,
-      'use_PEF': use_PEF,
-      'shape_net_cfg': shape_net_cfg,
-      'app_net_cfg': app_net_cfg,
-      'shape_block_end_index': shape_block_end_index,
-      'app_block_end_index': app_block_end_index,
-    })
-
-    self.shape_net_cfg = shape_net_cfg
-    self.app_net_cfg = app_net_cfg
-    self.name_prefix = name_prefix
-    self.shape_block_end_index = shape_block_end_index
-    self.app_block_end_index = app_block_end_index
-
-    self.module_name_list = []
-
-    if scale_factor is not None:
-      self.gridwarper = UniformBoxWarp(scale_factor=scale_factor)
-      self.module_name_list.append('gridwarper')
-    else:
-      self.gridwarper = None
-
-    if use_PEF:
-      self.xyz_emb = PosEmbedding(**PEF_cfg)
-      _in_dim = self.xyz_emb.get_out_dim()
-      self.module_name_list.append('xyz_emb')
-      # self.dir_emb = pigan_utils.PosEmbedding(max_logscale=3, N_freqs=4)
-      # dim_dir_emb = self.dir_emb.get_out_dim()
-    else:
-      self.xyz_emb = None
-      _in_dim = 3
-
-    # self.style_dim_dict = {}
-
-    self.shape_net = SIRENNet_skip(**{
-      **shape_net_cfg,
-      'name_prefix': 'shape'
-    })
-    self.style_dim_dict_shape = self.shape_net.style_dim_dict
-    self.module_name_list.append('shape_net')
-
-    _in_dim = shape_net_cfg['out_dim']
-
-    self.sigma_layer = nn.Linear(_in_dim, 1)
-    # self.final_layer.apply(frequency_init(25))
-    self.module_name_list.append('sigma_layer')
-
-    self.app_net = CIPSNet(**{
-      **app_net_cfg,
-      'input_dim': _in_dim,
-      'name_prefix': 'app'
-    })
-    self.style_dim_dict_app = self.app_net.style_dim_dict
-    self.module_name_list.append('app_net')
-
-    _in_dim = app_net_cfg['out_dim']
-    self.out_dim = _in_dim
-
-    # self.color_layer_linear = nn.Sequential(
-    #   nn.Linear(_out_dim, rgb_dim),
-    # )
-    # self.color_layer_linear.apply(init_func.kaiming_leaky_init)
-    # self.module_name_list.append('color_layer_linear')
-
-    logger = logging.getLogger('tl')
-    models_dict = {}
-    for name in self.module_name_list:
-      models_dict[name] = getattr(self, name)
-    models_dict[name_prefix] = self
-    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
-    logger.info(self)
-    pass
-
-  def forward(self,
-              x,
-              style_dict,
-              ray_directions=None,
-              **kwargs):
-    """
-
-    :param x: points xyz, (b, num_points, 3)
-    :param style_dict:
-    :param ray_directions: (b, num_points, 3)
-    :param kwargs:
-    :return
-
-    - out: (b, num_points, rgb_dim + 1), rgb(rgb_dim) + sigma(1)
-
-    """
-
-    # scale xyz
-    if self.gridwarper is not None:
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.gridwarper,
-                                     inputs_args=(x,),
-                                     name_prefix="gridwarper.")
-      x = self.gridwarper(x)
-
-    if self.xyz_emb is not None:
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.xyz_emb,
-                                     inputs_args=(x, ),
-                                     name_prefix="xyz_emb.")
-      x = self.xyz_emb(x)
-
-    x = self.shape_net(x, style_dict, block_end_index=self.shape_block_end_index)
-
-    if global_cfg.tl_debug:
-      VerboseModel.forward_verbose(self.sigma_layer,
-                                   inputs_args=(x, ),
-                                   name_prefix="sigma_layer")
-    sigma = self.sigma_layer(x)
-
-    x = self.app_net(x, style_dict, block_end_index=self.app_block_end_index)
-
-    out = torch.cat([x, sigma], dim=-1)
-    return out
-
-  # def print_number_params(self):
-  #   print()
-  #
-  #   pass
-  #
-  # def get_freq_phase(self, style_dict, name):
-  #   styles = style_dict[name]
-  #   styles = rearrange(styles, "b (n d) -> b d n", n=2)
-  #   frequencies, phase_shifts = styles.unbind(-1)
-  #   frequencies = frequencies * 15 + 30
-  #   return frequencies, phase_shifts
-  #
-  # def staged_forward(self,
-  #                    transformed_points,
-  #                    transformed_ray_directions_expanded,
-  #                    style_dict,
-  #                    max_points,
-  #                    num_steps,
-  #                    ):
-  #
-  #   batch_size, num_points, _ = transformed_points.shape
-  #
-  #   rgb_sigma_output = torch.zeros((batch_size, num_points, self.rgb_dim + 1),
-  #                                  device=self.device)
-  #   for b in range(batch_size):
-  #     head = 0
-  #     while head < num_points:
-  #       tail = head + max_points
-  #       rgb_sigma_output[b:b + 1, head:tail] = self(
-  #         input=transformed_points[b:b + 1, head:tail],  # (b, h x w x s, 3)
-  #         style_dict={name: style[b:b + 1] for name, style in style_dict.items()},
-  #         ray_directions=transformed_ray_directions_expanded[b:b + 1, head:tail])
-  #       head += max_points
-  #   rgb_sigma_output = rearrange(rgb_sigma_output, "b (hw s) rgb_sigma -> b hw s rgb_sigma", s=num_steps)
-  #   return rgb_sigma_output
-
+import math
+import logging
+from collections import OrderedDict
+from einops import rearrange
+
+import torch
+import torch.nn as nn
+
+from tl2 import tl2_utils
+from tl2.proj.pytorch import init_func
+from tl2.proj.pytorch import torch_utils
+from tl2.proj.fvcore import global_cfg
+from tl2.proj.pytorch.pytorch_hook import VerboseModel
+
+from .cips_net import CIPSNet
+from .multi_head_mapping import MultiHeadMappingNetwork
+from .siren_net import SIRENNet_skip
+
+
+class UniformBoxWarp(nn.Module):
+  def __init__(self, scale_factor):
+    super().__init__()
+    self.scale_factor = scale_factor
+    pass
+
+  def forward(self, coordinates):
+    return coordinates * self.scale_factor
+
+
+class PosEmbedding(nn.Module):
+  def __init__(self,
+               N_freqs,
+               in_dim=3,
+               affine_dim=None,
+               xyz_affine=False,
+               append_xyz=False,
+               **kwargs):
+    """
+    Defines a function that embeds x to (x, sin(2^k x), cos(2^k x), ...)
+
+    :param max_logscale: 9
+    :param N_freqs: 10
+    :param logscale:
+    :param multi_pi:
+    """
+    super().__init__()
+
+    self.N_freqs = N_freqs
+    self.append_xyz = append_xyz
+
+    self.funcs = [torch.sin, torch.cos]
+
+    self.freqs = list(map(lambda x: 2**x * math.pi, range(N_freqs)))
+
+    if xyz_affine:
+      assert affine_dim is not None
+      self.affine_layer = nn.Linear(in_dim, affine_dim)
+      self.in_dim = affine_dim
+    else:
+      self.affine_layer = None
+      self.in_dim = in_dim
+    pass
+
+  def get_out_dim(self):
+    if self.append_xyz:
+      outdim = self.in_dim + self.in_dim * 2 * self.N_freqs
+    else:
+      outdim = self.in_dim * 2 * self.N_freqs
+    return outdim
+
+  def forward(self, x):
+    """
+    Inputs:
+        x: (B, 3)
+
+    Outputs:
+        out: (B, 2 * N_freqs * in_dim + in_dim)
+    """
+    if self.affine_layer is not None:
+      x = self.affine_layer(x)
+
+    out = []
+    if self.append_xyz:
+      out.append(x)
+    for func in self.funcs:
+      emb_list = list(map(lambda freq: func(freq * x), self.freqs))
+      out += emb_list
+
+    emb = torch.cat(out, -1)
+    return emb
+
+
+class NeRFNetwork_CIPS(nn.Module):
+  """Adds a UniformBoxWarp to scale input points to -1, 1"""
+
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               scale_factor=None,
+               PEF_cfg={},
+               use_PEF=True,
+               shape_net_cfg={},
+               app_net_cfg={},
+               name_prefix='nerf',
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'scale_factor': scale_factor,
+      'PEF_cfg': PEF_cfg,
+      'use_PEF': use_PEF,
+      'shape_net_cfg': shape_net_cfg,
+      'app_net_cfg': app_net_cfg,
+    })
+
+    self.name_prefix = name_prefix
+
+    self.module_name_list = []
+
+    if scale_factor is not None:
+      self.gridwarper = UniformBoxWarp(scale_factor=scale_factor)
+      self.module_name_list.append('gridwarper')
+    else:
+      self.gridwarper = None
+
+    if use_PEF:
+      self.xyz_emb = PosEmbedding(**PEF_cfg)
+      _in_dim = self.xyz_emb.get_out_dim()
+      self.module_name_list.append('xyz_emb')
+      # self.dir_emb = pigan_utils.PosEmbedding(max_logscale=3, N_freqs=4)
+      # dim_dir_emb = self.dir_emb.get_out_dim()
+    else:
+      self.xyz_emb = None
+      _in_dim = 3
+
+    _out_dim = shape_net_cfg['input_dim']
+    self.in_layer = nn.Linear(_in_dim, _out_dim)
+    self.module_name_list.append('in_layer')
+
+    # self.style_dim_dict = {}
+
+    self.shape_net = CIPSNet(**{
+      **shape_net_cfg,
+      'name_prefix': 'shape'
+    })
+    self.style_dim_dict_shape = self.shape_net.style_dim_dict
+    self.module_name_list.append('shape_net')
+
+    _in_dim = shape_net_cfg['out_dim']
+
+    self.sigma_layer = nn.Linear(_in_dim, 1)
+    # self.final_layer.apply(frequency_init(25))
+    self.module_name_list.append('sigma_layer')
+
+    self.app_net = CIPSNet(**{
+      **app_net_cfg,
+      'input_dim': _in_dim,
+      'name_prefix': 'app'
+    })
+    self.style_dim_dict_app = self.app_net.style_dim_dict
+    self.module_name_list.append('app_net')
+
+    _in_dim = app_net_cfg['out_dim']
+    self.out_dim = _in_dim
+
+    # self.color_layer_linear = nn.Sequential(
+    #   nn.Linear(_out_dim, rgb_dim),
+    # )
+    # self.color_layer_linear.apply(init_func.kaiming_leaky_init)
+    # self.module_name_list.append('color_layer_linear')
+
+    logger = logging.getLogger('tl')
+    models_dict = {}
+    for name in self.module_name_list:
+      models_dict[name] = getattr(self, name)
+    models_dict[name_prefix] = self
+    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
+    logger.info(self)
+    pass
+
+  def forward(self,
+              x,
+              style_dict,
+              ray_directions=None,
+              **kwargs):
+    """
+
+    :param x: points xyz, (b, num_points, 3)
+    :param style_dict:
+    :param ray_directions: (b, num_points, 3)
+    :param kwargs:
+    :return
+
+    - out: (b, num_points, rgb_dim + 1), rgb(rgb_dim) + sigma(1)
+
+    """
+
+    # scale xyz
+    if self.gridwarper is not None:
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.gridwarper,
+                                     inputs_args=(x,),
+                                     name_prefix="gridwarper.")
+      x = self.gridwarper(x)
+
+    if self.xyz_emb is not None:
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.xyz_emb,
+                                     inputs_args=(x, ),
+                                     name_prefix="xyz_emb.")
+      x = self.xyz_emb(x)
+
+    if global_cfg.tl_debug:
+      VerboseModel.forward_verbose(self.in_layer,
+                                   inputs_args=(x,),
+                                   name_prefix="in_layer.")
+    x = self.in_layer(x)
+
+    x = self.shape_net(x, style_dict)
+
+    if global_cfg.tl_debug:
+      VerboseModel.forward_verbose(self.sigma_layer,
+                                   inputs_args=(x, ),
+                                   name_prefix="sigma_layer")
+    sigma = self.sigma_layer(x)
+
+    x = self.app_net(x, style_dict)
+
+    out = torch.cat([x, sigma], dim=-1)
+    return out
+
+  # def print_number_params(self):
+  #   print()
+  #
+  #   pass
+  #
+  # def get_freq_phase(self, style_dict, name):
+  #   styles = style_dict[name]
+  #   styles = rearrange(styles, "b (n d) -> b d n", n=2)
+  #   frequencies, phase_shifts = styles.unbind(-1)
+  #   frequencies = frequencies * 15 + 30
+  #   return frequencies, phase_shifts
+  #
+  # def staged_forward(self,
+  #                    transformed_points,
+  #                    transformed_ray_directions_expanded,
+  #                    style_dict,
+  #                    max_points,
+  #                    num_steps,
+  #                    ):
+  #
+  #   batch_size, num_points, _ = transformed_points.shape
+  #
+  #   rgb_sigma_output = torch.zeros((batch_size, num_points, self.rgb_dim + 1),
+  #                                  device=self.device)
+  #   for b in range(batch_size):
+  #     head = 0
+  #     while head < num_points:
+  #       tail = head + max_points
+  #       rgb_sigma_output[b:b + 1, head:tail] = self(
+  #         input=transformed_points[b:b + 1, head:tail],  # (b, h x w x s, 3)
+  #         style_dict={name: style[b:b + 1] for name, style in style_dict.items()},
+  #         ray_directions=transformed_ray_directions_expanded[b:b + 1, head:tail])
+  #       head += max_points
+  #   rgb_sigma_output = rearrange(rgb_sigma_output, "b (hw s) rgb_sigma -> b hw s rgb_sigma", s=num_steps)
+  #   return rgb_sigma_output
+
+
+class NeRFNetwork_SIREN_skip(nn.Module):
+  """
+  shape app: SIRENNet_skip, cips_net
+
+  """
+
+
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               scale_factor=None,
+               PEF_cfg={},
+               use_PEF=True,
+               shape_net_cfg={},
+               app_net_cfg={},
+               name_prefix='nerf',
+               shape_block_end_index=None,
+               app_block_end_index=None,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'scale_factor': scale_factor,
+      'PEF_cfg': PEF_cfg,
+      'use_PEF': use_PEF,
+      'shape_net_cfg': shape_net_cfg,
+      'app_net_cfg': app_net_cfg,
+      'shape_block_end_index': shape_block_end_index,
+      'app_block_end_index': app_block_end_index,
+    })
+
+    self.shape_net_cfg = shape_net_cfg
+    self.app_net_cfg = app_net_cfg
+    self.name_prefix = name_prefix
+    self.shape_block_end_index = shape_block_end_index
+    self.app_block_end_index = app_block_end_index
+
+    self.module_name_list = []
+
+    if scale_factor is not None:
+      self.gridwarper = UniformBoxWarp(scale_factor=scale_factor)
+      self.module_name_list.append('gridwarper')
+    else:
+      self.gridwarper = None
+
+    if use_PEF:
+      self.xyz_emb = PosEmbedding(**PEF_cfg)
+      _in_dim = self.xyz_emb.get_out_dim()
+      self.module_name_list.append('xyz_emb')
+      # self.dir_emb = pigan_utils.PosEmbedding(max_logscale=3, N_freqs=4)
+      # dim_dir_emb = self.dir_emb.get_out_dim()
+    else:
+      self.xyz_emb = None
+      _in_dim = 3
+
+    # self.style_dim_dict = {}
+
+    self.shape_net = SIRENNet_skip(**{
+      **shape_net_cfg,
+      'name_prefix': 'shape'
+    })
+    self.style_dim_dict_shape = self.shape_net.style_dim_dict
+    self.module_name_list.append('shape_net')
+
+    _in_dim = shape_net_cfg['out_dim']
+
+    self.sigma_layer = nn.Linear(_in_dim, 1)
+    # self.final_layer.apply(frequency_init(25))
+    self.module_name_list.append('sigma_layer')
+
+    self.app_net = CIPSNet(**{
+      **app_net_cfg,
+      'input_dim': _in_dim,
+      'name_prefix': 'app'
+    })
+    self.style_dim_dict_app = self.app_net.style_dim_dict
+    self.module_name_list.append('app_net')
+
+    _in_dim = app_net_cfg['out_dim']
+    self.out_dim = _in_dim
+
+    # self.color_layer_linear = nn.Sequential(
+    #   nn.Linear(_out_dim, rgb_dim),
+    # )
+    # self.color_layer_linear.apply(init_func.kaiming_leaky_init)
+    # self.module_name_list.append('color_layer_linear')
+
+    logger = logging.getLogger('tl')
+    models_dict = {}
+    for name in self.module_name_list:
+      models_dict[name] = getattr(self, name)
+    models_dict[name_prefix] = self
+    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
+    logger.info(self)
+    pass
+
+  def forward(self,
+              x,
+              style_dict,
+              ray_directions=None,
+              **kwargs):
+    """
+
+    :param x: points xyz, (b, num_points, 3)
+    :param style_dict:
+    :param ray_directions: (b, num_points, 3)
+    :param kwargs:
+    :return
+
+    - out: (b, num_points, rgb_dim + 1), rgb(rgb_dim) + sigma(1)
+
+    """
+
+    # scale xyz
+    if self.gridwarper is not None:
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.gridwarper,
+                                     inputs_args=(x,),
+                                     name_prefix="gridwarper.")
+      x = self.gridwarper(x)
+
+    if self.xyz_emb is not None:
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.xyz_emb,
+                                     inputs_args=(x, ),
+                                     name_prefix="xyz_emb.")
+      x = self.xyz_emb(x)
+
+    x = self.shape_net(x, style_dict, block_end_index=self.shape_block_end_index)
+
+    if global_cfg.tl_debug:
+      VerboseModel.forward_verbose(self.sigma_layer,
+                                   inputs_args=(x, ),
+                                   name_prefix="sigma_layer")
+    sigma = self.sigma_layer(x)
+
+    x = self.app_net(x, style_dict, block_end_index=self.app_block_end_index)
+
+    out = torch.cat([x, sigma], dim=-1)
+    return out
+
+  # def print_number_params(self):
+  #   print()
+  #
+  #   pass
+  #
+  # def get_freq_phase(self, style_dict, name):
+  #   styles = style_dict[name]
+  #   styles = rearrange(styles, "b (n d) -> b d n", n=2)
+  #   frequencies, phase_shifts = styles.unbind(-1)
+  #   frequencies = frequencies * 15 + 30
+  #   return frequencies, phase_shifts
+  #
+  # def staged_forward(self,
+  #                    transformed_points,
+  #                    transformed_ray_directions_expanded,
+  #                    style_dict,
+  #                    max_points,
+  #                    num_steps,
+  #                    ):
+  #
+  #   batch_size, num_points, _ = transformed_points.shape
+  #
+  #   rgb_sigma_output = torch.zeros((batch_size, num_points, self.rgb_dim + 1),
+  #                                  device=self.device)
+  #   for b in range(batch_size):
+  #     head = 0
+  #     while head < num_points:
+  #       tail = head + max_points
+  #       rgb_sigma_output[b:b + 1, head:tail] = self(
+  #         input=transformed_points[b:b + 1, head:tail],  # (b, h x w x s, 3)
+  #         style_dict={name: style[b:b + 1] for name, style in style_dict.items()},
+  #         ray_directions=transformed_ray_directions_expanded[b:b + 1, head:tail])
+  #       head += max_points
+  #   rgb_sigma_output = rearrange(rgb_sigma_output, "b (hw s) rgb_sigma -> b hw s rgb_sigma", s=num_steps)
+  #   return rgb_sigma_output
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/pigan_net.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/pigan_net.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,648 +1,648 @@
-import collections
-import math
-from collections import OrderedDict
-import numpy as np
-import logging
-from einops import rearrange
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-from tl2 import tl2_utils
-from tl2.proj.pytorch import torch_utils
-from tl2.proj.pytorch import init_func
-from tl2.proj.pytorch.pytorch_hook import VerboseModel
-from tl2.proj.fvcore import global_cfg
-
-from .grad_scale_layer import GradScale
-from .grad_norm_layer import GradNorm
-
-
-def _frequency_init(freq):
-  def init(m):
-    with torch.no_grad():
-      if isinstance(m, nn.Linear):
-        num_input = m.weight.size(-1)
-        m.weight.uniform_(-np.sqrt(6 / num_input) / freq, np.sqrt(6 / num_input) / freq)
-
-  return init
-
-def _first_layer_film_sine_init(m):
-  with torch.no_grad():
-    if isinstance(m, nn.Linear):
-      num_input = m.weight.size(-1)
-      m.weight.uniform_(-1 / num_input, 1 / num_input)
-
-
-class SinAct(nn.Module):
-  def __init__(self, ):
-    super(SinAct, self).__init__()
-
-  def forward(self, x):
-    return torch.sin(x)
-
-
-class ScaleGradient(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self, gamma=1.):
-    """
-    Keep the input unchanged but scale the gradients by gamma
-
-    :param gamma:
-    """
-    super(ScaleGradient, self).__init__()
-
-    self.repr_str = f"gamma={gamma:.3f}=1/{1/gamma:.3f}"
-
-    self.gamma = gamma
-    pass
-
-  def forward(self, x):
-    return self.gamma * x + (1 - self.gamma) * x.detach()
-
-
-class ScaleAct(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               gamma=15.,
-               beta=30.):
-    super(ScaleAct, self).__init__()
-
-    self.repr_str = f"gamma={gamma}, " \
-                    f"beta={beta}, "
-
-    self.gamma = gamma
-    self.beta = beta
-    pass
-
-  def forward(self, x):
-    out = x * self.gamma + self.beta
-    return out
-
-
-class ScaleLinear(nn.Linear):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               in_features,
-               out_features,
-               bias=True,
-               gamma=15.,
-               beta=30.,
-               scale_gradients=True,
-               **kwargs):
-    """
-    y = gamma * w * x + bias + beta
-
-    :param in_features:
-    :param out_features:
-    :param bias:
-    :param gamma:
-    :param beta:
-    :param scale_gradients:
-    :param kwargs:
-    """
-    super(ScaleLinear, self).__init__(in_features=in_features, out_features=out_features, bias=bias)
-
-    self.repr_str = f"in_features={in_features}, " \
-                    f"out_features={out_features}, " \
-                    f"bias={bias}, " \
-                    f"gamma={gamma}, " \
-                    f"beta={beta}, " \
-                    f"scale_gradients={scale_gradients}"
-
-    self.gamma = gamma
-    self.beta = beta
-    self.scale_gradients = scale_gradients
-
-    if scale_gradients:
-      self.weight_scale = GradScale(alpha=1/self.gamma)
-      self.input_scale = GradScale(alpha=1/self.gamma)
-
-    pass
-
-  def forward(self, x):
-
-    if self.scale_gradients:
-      x = self.input_scale(x)
-      _weight = self.weight_scale(self.weight)
-
-    else:
-      _weight = self.weight
-
-    if self.bias is not None:
-      return F.linear(x, _weight * self.gamma, self.bias + self.beta)
-    else:
-      return F.linear(x, _weight * self.gamma, self.bias)
-
-
-class ModFiLMLayer(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               in_dim,
-               out_dim,
-               style_dim,
-               freq_scale=15,
-               freq_shift=30,
-               scale_gradients=True,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = f"in_dim={in_dim}, " \
-                    f"out_dim={out_dim}, " \
-                    f"style_dim={style_dim}, " \
-                    f"freq_scale={freq_scale}, " \
-                    f"freq_shift={freq_shift}, " \
-                    f"scale_gradients={scale_gradients}"
-
-    if freq_scale == 1 and freq_shift == 0:
-      self.mod_freq = nn.Linear(style_dim, out_dim)
-      self.x_scale = None
-    else:
-      self.mod_freq = ScaleLinear(in_features=style_dim,
-                                  out_features=out_dim,
-                                  gamma=freq_scale,
-                                  beta=freq_shift,
-                                  scale_gradients=scale_gradients)
-      # if scale_gradients:
-      #   self.x_scale = GradScale(alpha=1 / freq_shift)
-      # else:
-      #   self.x_scale = None
-      self.x_scale = None
-
-    self.mod_freq.apply(init_func.kaiming_leaky_init)
-
-    self.mod_phase = nn.Linear(style_dim, out_dim)
-    self.mod_phase.apply(init_func.kaiming_leaky_init)
-
-    self.fc_layer = nn.Linear(in_dim, out_dim)
-    self.fc_layer.apply(_frequency_init(25))
-
-    self.sin_act = SinAct()
-    pass
-
-  def forward(self,
-              x,
-              style):
-    """
-
-    :param x: (b, n, in_dim) or (b, in_dim)
-    :param style: (b, style_dim)
-    :return
-
-    """
-
-    # (b, out_dim)
-    x = self.fc_layer(x)
-
-    # (b, style_dim) -> (b, out_dim)
-    freq = self.mod_freq(style)
-    phase = self.mod_phase(style)
-
-    if x.dim() == 3:
-      freq = freq.unsqueeze(1)
-      phase = phase.unsqueeze(1)
-    elif x.dim() == 2:
-      pass
-    else:
-      raise NotImplementedError
-
-    if self.x_scale is not None:
-      x = self.x_scale(x)
-
-    x = freq * x
-    return self.sin_act(x + phase)
-
-
-class PosEncoding(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               N_freqs,
-               in_dim=3,
-               xyz_affine=False,
-               affine_dim=None,
-               append_xyz=False,
-               **kwargs):
-    """
-    Defines a function that embeds x to (x, sin(2^k x), cos(2^k x), ...)
-
-    :param max_logscale: 9
-    :param N_freqs: 10
-    :param logscale:
-    :param multi_pi:
-    """
-    super().__init__()
-
-    self.repr_str = f"N_freqs={N_freqs}, " \
-                    f"in_dim={in_dim}, " \
-                    f"xyz_affine={xyz_affine}, " \
-                    f"affine_dim={affine_dim}, " \
-                    f"append_xyz={append_xyz}"
-
-    self.N_freqs = N_freqs
-    self.append_xyz = append_xyz
-
-    self.funcs = [torch.sin, torch.cos]
-
-    self.freqs = list(map(lambda x: 2**x * math.pi, range(N_freqs)))
-
-    if xyz_affine:
-      assert affine_dim is not None
-      self.affine_layer = nn.Linear(in_dim, affine_dim)
-      self.in_dim = affine_dim
-    else:
-      self.affine_layer = None
-      self.in_dim = in_dim
-    pass
-
-  def get_out_dim(self):
-    if self.append_xyz:
-      outdim = self.in_dim + self.in_dim * 2 * self.N_freqs
-    else:
-      outdim = self.in_dim * 2 * self.N_freqs
-    return outdim
-
-  def forward(self, x):
-    """
-    Inputs:
-        x: (B, 3)
-
-    Outputs:
-        out: (B, 2 * N_freqs * in_dim + in_dim)
-    """
-    if self.affine_layer is not None:
-      x = self.affine_layer(x)
-
-    out = []
-    if self.append_xyz:
-      out.append(x)
-    for func in self.funcs:
-      emb_list = list(map(lambda freq: func(freq * x), self.freqs))
-      out += emb_list
-
-    emb = torch.cat(out, -1)
-    return emb
-
-
-class ModSIREN_Net(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               input_dim,
-               hidden_dim,
-               out_dim,
-               style_dim,
-               N_layers,
-               device=None,
-               name_prefix='mod_siren',
-               use_pos_enc=False,
-               PEF_cfg={},
-               freq_scale=15,
-               freq_shift=30,
-               scale_gradients=False,
-               grad_norm_layer=True,
-               grad_norm=0.01,
-               grad_norm_idx=[],
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'input_dim': input_dim,
-      'hidden_dim': hidden_dim,
-      'out_dim': out_dim,
-      'style_dim': style_dim,
-      'N_layers': N_layers,
-      'use_pos_enc': use_pos_enc,
-      'PEF_cfg': PEF_cfg,
-      'freq_scale': freq_scale,
-      'freq_shift': freq_shift,
-      'scale_gradients': scale_gradients,
-      'grad_norm_layer': grad_norm_layer,
-      'grad_norm': grad_norm,
-      'grad_norm_idx': grad_norm_idx,
-    }, prefix_str=name_prefix)
-
-    self.device = device
-    self.N_layers = N_layers
-    self.name_prefix = name_prefix
-    self.out_dim = out_dim
-    self.grad_norm_layer = grad_norm_layer
-
-    self.module_name_list = []
-    self.style_dim_dict = {}
-
-    if use_pos_enc:
-      self.pos_enc_layer = PosEncoding(**PEF_cfg)
-      self.module_name_list.append('pos_enc_layer')
-      _out_dim = self.pos_enc_layer.get_out_dim()
-
-    else:
-      self.pos_enc_layer = None
-      _out_dim = input_dim
-
-    blocks = OrderedDict()
-    grad_norms = OrderedDict()
-    for idx in range(N_layers):
-      _in_dim = _out_dim
-      _out_dim = hidden_dim
-      if idx == N_layers - 1:
-        _out_dim = out_dim
-
-      _block = ModFiLMLayer(in_dim=_in_dim, out_dim=_out_dim, style_dim=style_dim,
-                            freq_scale=freq_scale, freq_shift=freq_shift,
-                            scale_gradients=scale_gradients)
-      # initialize
-      _block.fc_layer.apply(_frequency_init(25))
-      if idx == 0:
-        _block.fc_layer.apply(_first_layer_film_sine_init)
-
-      name = f"{name_prefix}_{idx}"
-      blocks[name] = _block
-      self.style_dim_dict[f"w_{name}"] = style_dim
-      self.module_name_list.append(f"blocks.{name}")
-
-      if grad_norm_layer and idx in grad_norm_idx:
-        grad_norms[name] = GradNorm(norm=grad_norm)
-
-    self.blocks = nn.ModuleDict(blocks)
-    self.module_name_list.append('blocks')
-
-    self.grad_norms = nn.ModuleDict(grad_norms)
-    self.module_name_list.append('grad_norms')
-
-    models_dict = {}
-    for name in self.module_name_list:
-      models_dict[name] = tl2_utils.attrgetter_default(object=self, attr=name)
-    models_dict[name_prefix] = self
-    logger = logging.getLogger('tl')
-    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
-    logger.info(self)
-    pass
-
-  def forward(self,
-              input,
-              style_dict,
-              block_end_index=None,
-              **kwargs):
-    """
-
-    :param input: points xyz, (b, num_points, 3)
-    :param style_dict:
-    :param kwargs:
-
-    :return
-
-    - out: (b, num_points, out_dim)
-
-    """
-    if block_end_index is None:
-      block_end_index = self.N_layers
-
-    if self.pos_enc_layer is not None:
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.pos_enc_layer,
-                                     inputs_args=(input, ),
-                                     name_prefix=f'{self.name_prefix}.pos_enc_layer.')
-      x = self.pos_enc_layer(input)
-    else:
-      x = input
-
-    for idx, (name, block) in enumerate(self.blocks.items()):
-      style_name = f"w_{name}"
-      style = style_dict[style_name]
-
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(block,
-                                     inputs_args=(x, style),
-                                     name_prefix=f'{self.name_prefix}.mod_film_{idx}.')
-      x = block(x, style)
-      if self.training and name in self.grad_norms:
-        x = self.grad_norms[name](x, debug=global_cfg.tl_debug)
-
-      if idx + 1 == block_end_index:
-        break
-
-    out = x
-    return out
-
-
-class piGAN_NeRF_Net(nn.Module):
-  """
-
-  """
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               shape_net_cfg={},
-               app_net_cfg={},
-               name_prefix='nerf',
-               shape_block_end_index=None,
-               app_block_end_index=None,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'shape_net_cfg': shape_net_cfg,
-      'app_net_cfg': app_net_cfg,
-      'shape_block_end_index': shape_block_end_index,
-      'app_block_end_index': app_block_end_index,
-    })
-
-    self.shape_net_cfg = shape_net_cfg
-    self.app_net_cfg = app_net_cfg
-    self.name_prefix = name_prefix
-    self.shape_block_end_index = shape_block_end_index
-    self.app_block_end_index = app_block_end_index
-
-    self.module_name_list = []
-
-    # self.style_dim_dict = {}
-
-    self.shape_net = ModSIREN_Net(**{
-      **shape_net_cfg,
-      'name_prefix': 'shape'
-    })
-    self.style_dim_dict_shape = self.shape_net.style_dim_dict
-    self.module_name_list.append('shape_net')
-
-    # _in_dim = shape_net_cfg['out_dim']
-    _in_dim = self.shape_net.out_dim
-
-    self.sigma_layer = nn.Linear(_in_dim, 1)
-    # self.final_layer.apply(frequency_init(25))
-    self.module_name_list.append('sigma_layer')
-
-    from . import cips_net
-
-    self.app_net = ModSIREN_Net(**{
-      **app_net_cfg,
-      'input_dim': _in_dim,
-      'name_prefix': 'app'
-    })
-    self.style_dim_dict_app = self.app_net.style_dim_dict
-    self.module_name_list.append('app_net')
-
-    self.out_dim = self.app_net.out_dim
-
-    logger = logging.getLogger('tl')
-    models_dict = {}
-    for name in self.module_name_list:
-      models_dict[name] = getattr(self, name)
-    models_dict[name_prefix] = self
-    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
-    logger.info(self)
-    pass
-
-  def forward(self,
-              x,
-              style_dict,
-              ray_directions=None,
-              **kwargs):
-    """
-
-    :param x: points xyz, (b, num_points, 3)
-    :param style_dict:
-    :param ray_directions: (b, num_points, 3)
-    :param kwargs:
-    :return
-
-    - out: (b, num_points, rgb_dim + 1), rgb(rgb_dim) + sigma(1)
-
-    """
-
-    # scale xyz
-
-    x = self.shape_net(x, style_dict, block_end_index=self.shape_block_end_index)
-
-    if global_cfg.tl_debug:
-      VerboseModel.forward_verbose(self.sigma_layer,
-                                   inputs_args=(x, ),
-                                   name_prefix="sigma_layer")
-    sigma = self.sigma_layer(x)
-
-    x = self.app_net(x, style_dict, block_end_index=self.app_block_end_index)
-
-    out = torch.cat([x, sigma], dim=-1)
-    return out
-
-
-class NeRF_Net(nn.Module):
-  """
-
-  """
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               shape_net_cfg={},
-               app_net_cfg={},
-               name_prefix='nerf',
-               shape_block_end_index=None,
-               app_block_end_index=None,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'shape_net_cfg': shape_net_cfg,
-      'app_net_cfg': app_net_cfg,
-      'shape_block_end_index': shape_block_end_index,
-      'app_block_end_index': app_block_end_index,
-    })
-
-    self.shape_net_cfg = shape_net_cfg
-    self.app_net_cfg = app_net_cfg
-    self.name_prefix = name_prefix
-    self.shape_block_end_index = shape_block_end_index
-    self.app_block_end_index = app_block_end_index
-
-    self.module_name_list = []
-
-    # self.style_dim_dict = {}
-
-    self.shape_net = ModSIREN_Net(**{
-      **shape_net_cfg,
-      'name_prefix': 'shape'
-    })
-    self.style_dim_dict_shape = self.shape_net.style_dim_dict
-    self.module_name_list.append('shape_net')
-
-    # _in_dim = shape_net_cfg['out_dim']
-    _in_dim = self.shape_net.out_dim
-
-    self.sigma_layer = nn.Linear(_in_dim, 1)
-    # self.final_layer.apply(frequency_init(25))
-    self.module_name_list.append('sigma_layer')
-
-    from . import cips_net
-
-    self.app_net = cips_net.CIPSNet(**{
-      **app_net_cfg,
-      'input_dim': _in_dim,
-      'name_prefix': 'app'
-    })
-    self.style_dim_dict_app = self.app_net.style_dim_dict
-    self.module_name_list.append('app_net')
-
-    # self.out_dim = app_net_cfg['out_dim']
-    self.out_dim = self.app_net.out_dim
-
-    # self.color_layer_linear = nn.Sequential(
-    #   nn.Linear(_out_dim, rgb_dim),
-    # )
-    # self.color_layer_linear.apply(init_func.kaiming_leaky_init)
-    # self.module_name_list.append('color_layer_linear')
-
-    logger = logging.getLogger('tl')
-    models_dict = {}
-    for name in self.module_name_list:
-      models_dict[name] = getattr(self, name)
-    models_dict[name_prefix] = self
-    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
-    logger.info(self)
-    pass
-
-  def forward(self,
-              x,
-              style_dict,
-              ray_directions=None,
-              **kwargs):
-    """
-
-    :param x: points xyz, (b, num_points, 3)
-    :param style_dict:
-    :param ray_directions: (b, num_points, 3)
-    :param kwargs:
-    :return
-
-    - out: (b, num_points, rgb_dim + 1), rgb(rgb_dim) + sigma(1)
-
-    """
-
-    # scale xyz
-
-    x = self.shape_net(x, style_dict, block_end_index=self.shape_block_end_index)
-
-    if global_cfg.tl_debug:
-      VerboseModel.forward_verbose(self.sigma_layer,
-                                   inputs_args=(x, ),
-                                   name_prefix="sigma_layer")
-    sigma = self.sigma_layer(x)
-
-    x = self.app_net(x, style_dict, block_end_index=self.app_block_end_index)
-
-    out = torch.cat([x, sigma], dim=-1)
-    return out
-
-
+import collections
+import math
+from collections import OrderedDict
+import numpy as np
+import logging
+from einops import rearrange
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from tl2 import tl2_utils
+from tl2.proj.pytorch import torch_utils
+from tl2.proj.pytorch import init_func
+from tl2.proj.pytorch.pytorch_hook import VerboseModel
+from tl2.proj.fvcore import global_cfg
+
+from .grad_scale_layer import GradScale
+from .grad_norm_layer import GradNorm
+
+
+def _frequency_init(freq):
+  def init(m):
+    with torch.no_grad():
+      if isinstance(m, nn.Linear):
+        num_input = m.weight.size(-1)
+        m.weight.uniform_(-np.sqrt(6 / num_input) / freq, np.sqrt(6 / num_input) / freq)
+
+  return init
+
+def _first_layer_film_sine_init(m):
+  with torch.no_grad():
+    if isinstance(m, nn.Linear):
+      num_input = m.weight.size(-1)
+      m.weight.uniform_(-1 / num_input, 1 / num_input)
+
+
+class SinAct(nn.Module):
+  def __init__(self, ):
+    super(SinAct, self).__init__()
+
+  def forward(self, x):
+    return torch.sin(x)
+
+
+class ScaleGradient(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self, gamma=1.):
+    """
+    Keep the input unchanged but scale the gradients by gamma
+
+    :param gamma:
+    """
+    super(ScaleGradient, self).__init__()
+
+    self.repr_str = f"gamma={gamma:.3f}=1/{1/gamma:.3f}"
+
+    self.gamma = gamma
+    pass
+
+  def forward(self, x):
+    return self.gamma * x + (1 - self.gamma) * x.detach()
+
+
+class ScaleAct(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               gamma=15.,
+               beta=30.):
+    super(ScaleAct, self).__init__()
+
+    self.repr_str = f"gamma={gamma}, " \
+                    f"beta={beta}, "
+
+    self.gamma = gamma
+    self.beta = beta
+    pass
+
+  def forward(self, x):
+    out = x * self.gamma + self.beta
+    return out
+
+
+class ScaleLinear(nn.Linear):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               in_features,
+               out_features,
+               bias=True,
+               gamma=15.,
+               beta=30.,
+               scale_gradients=True,
+               **kwargs):
+    """
+    y = gamma * w * x + bias + beta
+
+    :param in_features:
+    :param out_features:
+    :param bias:
+    :param gamma:
+    :param beta:
+    :param scale_gradients:
+    :param kwargs:
+    """
+    super(ScaleLinear, self).__init__(in_features=in_features, out_features=out_features, bias=bias)
+
+    self.repr_str = f"in_features={in_features}, " \
+                    f"out_features={out_features}, " \
+                    f"bias={bias}, " \
+                    f"gamma={gamma}, " \
+                    f"beta={beta}, " \
+                    f"scale_gradients={scale_gradients}"
+
+    self.gamma = gamma
+    self.beta = beta
+    self.scale_gradients = scale_gradients
+
+    if scale_gradients:
+      self.weight_scale = GradScale(alpha=1/self.gamma)
+      self.input_scale = GradScale(alpha=1/self.gamma)
+
+    pass
+
+  def forward(self, x):
+
+    if self.scale_gradients:
+      x = self.input_scale(x)
+      _weight = self.weight_scale(self.weight)
+
+    else:
+      _weight = self.weight
+
+    if self.bias is not None:
+      return F.linear(x, _weight * self.gamma, self.bias + self.beta)
+    else:
+      return F.linear(x, _weight * self.gamma, self.bias)
+
+
+class ModFiLMLayer(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               in_dim,
+               out_dim,
+               style_dim,
+               freq_scale=15,
+               freq_shift=30,
+               scale_gradients=True,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = f"in_dim={in_dim}, " \
+                    f"out_dim={out_dim}, " \
+                    f"style_dim={style_dim}, " \
+                    f"freq_scale={freq_scale}, " \
+                    f"freq_shift={freq_shift}, " \
+                    f"scale_gradients={scale_gradients}"
+
+    if freq_scale == 1 and freq_shift == 0:
+      self.mod_freq = nn.Linear(style_dim, out_dim)
+      self.x_scale = None
+    else:
+      self.mod_freq = ScaleLinear(in_features=style_dim,
+                                  out_features=out_dim,
+                                  gamma=freq_scale,
+                                  beta=freq_shift,
+                                  scale_gradients=scale_gradients)
+      # if scale_gradients:
+      #   self.x_scale = GradScale(alpha=1 / freq_shift)
+      # else:
+      #   self.x_scale = None
+      self.x_scale = None
+
+    self.mod_freq.apply(init_func.kaiming_leaky_init)
+
+    self.mod_phase = nn.Linear(style_dim, out_dim)
+    self.mod_phase.apply(init_func.kaiming_leaky_init)
+
+    self.fc_layer = nn.Linear(in_dim, out_dim)
+    self.fc_layer.apply(_frequency_init(25))
+
+    self.sin_act = SinAct()
+    pass
+
+  def forward(self,
+              x,
+              style):
+    """
+
+    :param x: (b, n, in_dim) or (b, in_dim)
+    :param style: (b, style_dim)
+    :return
+
+    """
+
+    # (b, out_dim)
+    x = self.fc_layer(x)
+
+    # (b, style_dim) -> (b, out_dim)
+    freq = self.mod_freq(style)
+    phase = self.mod_phase(style)
+
+    if x.dim() == 3:
+      freq = freq.unsqueeze(1)
+      phase = phase.unsqueeze(1)
+    elif x.dim() == 2:
+      pass
+    else:
+      raise NotImplementedError
+
+    if self.x_scale is not None:
+      x = self.x_scale(x)
+
+    x = freq * x
+    return self.sin_act(x + phase)
+
+
+class PosEncoding(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               N_freqs,
+               in_dim=3,
+               xyz_affine=False,
+               affine_dim=None,
+               append_xyz=False,
+               **kwargs):
+    """
+    Defines a function that embeds x to (x, sin(2^k x), cos(2^k x), ...)
+
+    :param max_logscale: 9
+    :param N_freqs: 10
+    :param logscale:
+    :param multi_pi:
+    """
+    super().__init__()
+
+    self.repr_str = f"N_freqs={N_freqs}, " \
+                    f"in_dim={in_dim}, " \
+                    f"xyz_affine={xyz_affine}, " \
+                    f"affine_dim={affine_dim}, " \
+                    f"append_xyz={append_xyz}"
+
+    self.N_freqs = N_freqs
+    self.append_xyz = append_xyz
+
+    self.funcs = [torch.sin, torch.cos]
+
+    self.freqs = list(map(lambda x: 2**x * math.pi, range(N_freqs)))
+
+    if xyz_affine:
+      assert affine_dim is not None
+      self.affine_layer = nn.Linear(in_dim, affine_dim)
+      self.in_dim = affine_dim
+    else:
+      self.affine_layer = None
+      self.in_dim = in_dim
+    pass
+
+  def get_out_dim(self):
+    if self.append_xyz:
+      outdim = self.in_dim + self.in_dim * 2 * self.N_freqs
+    else:
+      outdim = self.in_dim * 2 * self.N_freqs
+    return outdim
+
+  def forward(self, x):
+    """
+    Inputs:
+        x: (B, 3)
+
+    Outputs:
+        out: (B, 2 * N_freqs * in_dim + in_dim)
+    """
+    if self.affine_layer is not None:
+      x = self.affine_layer(x)
+
+    out = []
+    if self.append_xyz:
+      out.append(x)
+    for func in self.funcs:
+      emb_list = list(map(lambda freq: func(freq * x), self.freqs))
+      out += emb_list
+
+    emb = torch.cat(out, -1)
+    return emb
+
+
+class ModSIREN_Net(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               input_dim,
+               hidden_dim,
+               out_dim,
+               style_dim,
+               N_layers,
+               device=None,
+               name_prefix='mod_siren',
+               use_pos_enc=False,
+               PEF_cfg={},
+               freq_scale=15,
+               freq_shift=30,
+               scale_gradients=False,
+               grad_norm_layer=True,
+               grad_norm=0.01,
+               grad_norm_idx=[],
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'input_dim': input_dim,
+      'hidden_dim': hidden_dim,
+      'out_dim': out_dim,
+      'style_dim': style_dim,
+      'N_layers': N_layers,
+      'use_pos_enc': use_pos_enc,
+      'PEF_cfg': PEF_cfg,
+      'freq_scale': freq_scale,
+      'freq_shift': freq_shift,
+      'scale_gradients': scale_gradients,
+      'grad_norm_layer': grad_norm_layer,
+      'grad_norm': grad_norm,
+      'grad_norm_idx': grad_norm_idx,
+    }, prefix_str=name_prefix)
+
+    self.device = device
+    self.N_layers = N_layers
+    self.name_prefix = name_prefix
+    self.out_dim = out_dim
+    self.grad_norm_layer = grad_norm_layer
+
+    self.module_name_list = []
+    self.style_dim_dict = {}
+
+    if use_pos_enc:
+      self.pos_enc_layer = PosEncoding(**PEF_cfg)
+      self.module_name_list.append('pos_enc_layer')
+      _out_dim = self.pos_enc_layer.get_out_dim()
+
+    else:
+      self.pos_enc_layer = None
+      _out_dim = input_dim
+
+    blocks = OrderedDict()
+    grad_norms = OrderedDict()
+    for idx in range(N_layers):
+      _in_dim = _out_dim
+      _out_dim = hidden_dim
+      if idx == N_layers - 1:
+        _out_dim = out_dim
+
+      _block = ModFiLMLayer(in_dim=_in_dim, out_dim=_out_dim, style_dim=style_dim,
+                            freq_scale=freq_scale, freq_shift=freq_shift,
+                            scale_gradients=scale_gradients)
+      # initialize
+      _block.fc_layer.apply(_frequency_init(25))
+      if idx == 0:
+        _block.fc_layer.apply(_first_layer_film_sine_init)
+
+      name = f"{name_prefix}_{idx}"
+      blocks[name] = _block
+      self.style_dim_dict[f"w_{name}"] = style_dim
+      self.module_name_list.append(f"blocks.{name}")
+
+      if grad_norm_layer and idx in grad_norm_idx:
+        grad_norms[name] = GradNorm(norm=grad_norm)
+
+    self.blocks = nn.ModuleDict(blocks)
+    self.module_name_list.append('blocks')
+
+    self.grad_norms = nn.ModuleDict(grad_norms)
+    self.module_name_list.append('grad_norms')
+
+    models_dict = {}
+    for name in self.module_name_list:
+      models_dict[name] = tl2_utils.attrgetter_default(object=self, attr=name)
+    models_dict[name_prefix] = self
+    logger = logging.getLogger('tl')
+    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
+    logger.info(self)
+    pass
+
+  def forward(self,
+              input,
+              style_dict,
+              block_end_index=None,
+              **kwargs):
+    """
+
+    :param input: points xyz, (b, num_points, 3)
+    :param style_dict:
+    :param kwargs:
+
+    :return
+
+    - out: (b, num_points, out_dim)
+
+    """
+    if block_end_index is None:
+      block_end_index = self.N_layers
+
+    if self.pos_enc_layer is not None:
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.pos_enc_layer,
+                                     inputs_args=(input, ),
+                                     name_prefix=f'{self.name_prefix}.pos_enc_layer.')
+      x = self.pos_enc_layer(input)
+    else:
+      x = input
+
+    for idx, (name, block) in enumerate(self.blocks.items()):
+      style_name = f"w_{name}"
+      style = style_dict[style_name]
+
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(block,
+                                     inputs_args=(x, style),
+                                     name_prefix=f'{self.name_prefix}.mod_film_{idx}.')
+      x = block(x, style)
+      if self.training and name in self.grad_norms:
+        x = self.grad_norms[name](x, debug=global_cfg.tl_debug)
+
+      if idx + 1 == block_end_index:
+        break
+
+    out = x
+    return out
+
+
+class piGAN_NeRF_Net(nn.Module):
+  """
+
+  """
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               shape_net_cfg={},
+               app_net_cfg={},
+               name_prefix='nerf',
+               shape_block_end_index=None,
+               app_block_end_index=None,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'shape_net_cfg': shape_net_cfg,
+      'app_net_cfg': app_net_cfg,
+      'shape_block_end_index': shape_block_end_index,
+      'app_block_end_index': app_block_end_index,
+    })
+
+    self.shape_net_cfg = shape_net_cfg
+    self.app_net_cfg = app_net_cfg
+    self.name_prefix = name_prefix
+    self.shape_block_end_index = shape_block_end_index
+    self.app_block_end_index = app_block_end_index
+
+    self.module_name_list = []
+
+    # self.style_dim_dict = {}
+
+    self.shape_net = ModSIREN_Net(**{
+      **shape_net_cfg,
+      'name_prefix': 'shape'
+    })
+    self.style_dim_dict_shape = self.shape_net.style_dim_dict
+    self.module_name_list.append('shape_net')
+
+    # _in_dim = shape_net_cfg['out_dim']
+    _in_dim = self.shape_net.out_dim
+
+    self.sigma_layer = nn.Linear(_in_dim, 1)
+    # self.final_layer.apply(frequency_init(25))
+    self.module_name_list.append('sigma_layer')
+
+    from . import cips_net
+
+    self.app_net = ModSIREN_Net(**{
+      **app_net_cfg,
+      'input_dim': _in_dim,
+      'name_prefix': 'app'
+    })
+    self.style_dim_dict_app = self.app_net.style_dim_dict
+    self.module_name_list.append('app_net')
+
+    self.out_dim = self.app_net.out_dim
+
+    logger = logging.getLogger('tl')
+    models_dict = {}
+    for name in self.module_name_list:
+      models_dict[name] = getattr(self, name)
+    models_dict[name_prefix] = self
+    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
+    logger.info(self)
+    pass
+
+  def forward(self,
+              x,
+              style_dict,
+              ray_directions=None,
+              **kwargs):
+    """
+
+    :param x: points xyz, (b, num_points, 3)
+    :param style_dict:
+    :param ray_directions: (b, num_points, 3)
+    :param kwargs:
+    :return
+
+    - out: (b, num_points, rgb_dim + 1), rgb(rgb_dim) + sigma(1)
+
+    """
+
+    # scale xyz
+
+    x = self.shape_net(x, style_dict, block_end_index=self.shape_block_end_index)
+
+    if global_cfg.tl_debug:
+      VerboseModel.forward_verbose(self.sigma_layer,
+                                   inputs_args=(x, ),
+                                   name_prefix="sigma_layer")
+    sigma = self.sigma_layer(x)
+
+    x = self.app_net(x, style_dict, block_end_index=self.app_block_end_index)
+
+    out = torch.cat([x, sigma], dim=-1)
+    return out
+
+
+class NeRF_Net(nn.Module):
+  """
+
+  """
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               shape_net_cfg={},
+               app_net_cfg={},
+               name_prefix='nerf',
+               shape_block_end_index=None,
+               app_block_end_index=None,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'shape_net_cfg': shape_net_cfg,
+      'app_net_cfg': app_net_cfg,
+      'shape_block_end_index': shape_block_end_index,
+      'app_block_end_index': app_block_end_index,
+    })
+
+    self.shape_net_cfg = shape_net_cfg
+    self.app_net_cfg = app_net_cfg
+    self.name_prefix = name_prefix
+    self.shape_block_end_index = shape_block_end_index
+    self.app_block_end_index = app_block_end_index
+
+    self.module_name_list = []
+
+    # self.style_dim_dict = {}
+
+    self.shape_net = ModSIREN_Net(**{
+      **shape_net_cfg,
+      'name_prefix': 'shape'
+    })
+    self.style_dim_dict_shape = self.shape_net.style_dim_dict
+    self.module_name_list.append('shape_net')
+
+    # _in_dim = shape_net_cfg['out_dim']
+    _in_dim = self.shape_net.out_dim
+
+    self.sigma_layer = nn.Linear(_in_dim, 1)
+    # self.final_layer.apply(frequency_init(25))
+    self.module_name_list.append('sigma_layer')
+
+    from . import cips_net
+
+    self.app_net = cips_net.CIPSNet(**{
+      **app_net_cfg,
+      'input_dim': _in_dim,
+      'name_prefix': 'app'
+    })
+    self.style_dim_dict_app = self.app_net.style_dim_dict
+    self.module_name_list.append('app_net')
+
+    # self.out_dim = app_net_cfg['out_dim']
+    self.out_dim = self.app_net.out_dim
+
+    # self.color_layer_linear = nn.Sequential(
+    #   nn.Linear(_out_dim, rgb_dim),
+    # )
+    # self.color_layer_linear.apply(init_func.kaiming_leaky_init)
+    # self.module_name_list.append('color_layer_linear')
+
+    logger = logging.getLogger('tl')
+    models_dict = {}
+    for name in self.module_name_list:
+      models_dict[name] = getattr(self, name)
+    models_dict[name_prefix] = self
+    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
+    logger.info(self)
+    pass
+
+  def forward(self,
+              x,
+              style_dict,
+              ray_directions=None,
+              **kwargs):
+    """
+
+    :param x: points xyz, (b, num_points, 3)
+    :param style_dict:
+    :param ray_directions: (b, num_points, 3)
+    :param kwargs:
+    :return
+
+    - out: (b, num_points, rgb_dim + 1), rgb(rgb_dim) + sigma(1)
+
+    """
+
+    # scale xyz
+
+    x = self.shape_net(x, style_dict, block_end_index=self.shape_block_end_index)
+
+    if global_cfg.tl_debug:
+      VerboseModel.forward_verbose(self.sigma_layer,
+                                   inputs_args=(x, ),
+                                   name_prefix="sigma_layer")
+    sigma = self.sigma_layer(x)
+
+    x = self.app_net(x, style_dict, block_end_index=self.app_block_end_index)
+
+    out = torch.cat([x, sigma], dim=-1)
+    return out
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/siren_net.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/siren_net.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,721 +1,721 @@
-import collections
-import math
-from collections import OrderedDict
-import numpy as np
-import logging
-from einops import rearrange
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-from tl2 import tl2_utils
-from tl2.proj.pytorch import torch_utils
-from tl2.proj.pytorch import init_func
-from tl2.proj.pytorch.pytorch_hook import VerboseModel
-from tl2.proj.fvcore import global_cfg
-
-
-class SinAct(nn.Module):
-  def __init__(self, ):
-    super(SinAct, self).__init__()
-
-  def forward(self, x):
-    return torch.sin(x)
-
-
-class ScaleGradient(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self, gamma=1.):
-    """
-    Keep the input unchanged but scale the gradients by gamma
-
-    :param gamma:
-    """
-    super(ScaleGradient, self).__init__()
-
-    self.repr_str = f"gamma={gamma:.3f}=1/{1/gamma:.3f}"
-
-    self.gamma = gamma
-    pass
-
-  def forward(self, x):
-    return self.gamma * x + (1 - self.gamma) * x.detach()
-
-
-class ScaleAct(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               gamma=15.,
-               beta=30.):
-    super(ScaleAct, self).__init__()
-
-    self.repr_str = f"gamma={gamma}, " \
-                    f"beta={beta}, "
-
-    self.gamma = gamma
-    self.beta = beta
-    pass
-
-  def forward(self, x):
-    out = x * self.gamma + self.beta
-    return out
-
-
-class ModFiLMLayer(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               in_dim,
-               out_dim,
-               style_dim,
-               freq_scale=15,
-               freq_shift=30,
-               gradient_scale=None,
-               **kwargs):
-    super().__init__()
-
-    if gradient_scale is not None and isinstance(gradient_scale, str):
-      gradient_scale = eval(gradient_scale)
-
-    self.repr_str = f"in_dim={in_dim}, " \
-                    f"out_dim={out_dim}, " \
-                    f"style_dim={style_dim}, " \
-                    f"freq_scale={freq_scale}, " \
-                    f"freq_shift={freq_shift}, " \
-                    f"gradient_scale={gradient_scale}"
-
-    self.mod_freq = nn.Linear(style_dim, out_dim)
-    self.mod_freq.apply(init_func.kaiming_leaky_init)
-    if freq_scale == 1 and freq_shift == 0:
-      self.freq_scale = nn.Identity()
-    else:
-      self.freq_scale = ScaleAct(gamma=freq_scale, beta=freq_shift)
-
-    self.mod_phase = nn.Linear(style_dim, out_dim)
-    self.mod_phase.apply(init_func.kaiming_leaky_init)
-
-    self.fc_layer = nn.Linear(in_dim, out_dim)
-    self.fc_layer.apply(_frequency_init(25))
-
-    if gradient_scale is not None:
-      self.gradient_scale_layer = ScaleGradient(gamma=gradient_scale)
-    else:
-      self.gradient_scale_layer = None
-
-    self.sin_act = SinAct()
-    pass
-
-  def forward(self,
-              x,
-              style):
-    """
-
-    :param x: (b, n, in_dim) or (b, in_dim)
-    :param style: (b, style_dim)
-    :return
-
-    """
-
-    # (b, out_dim)
-    x = self.fc_layer(x)
-
-    # (b, style_dim) -> (b, out_dim)
-    freq = self.mod_freq(style)
-    freq = self.freq_scale(freq)
-
-    phase = self.mod_phase(style)
-
-    if x.dim() == 3:
-      freq = freq.unsqueeze(1)
-      phase = phase.unsqueeze(1)
-    elif x.dim() == 2:
-      pass
-    else:
-      raise NotImplementedError
-
-    x = freq * x
-
-    if self.gradient_scale_layer is not None:
-      _x = self.gradient_scale_layer(x)
-      x = _x
-
-    return self.sin_act(x + phase)
-
-
-def _frequency_init(freq):
-  def init(m):
-    with torch.no_grad():
-      if isinstance(m, nn.Linear):
-        num_input = m.weight.size(-1)
-        m.weight.uniform_(-np.sqrt(6 / num_input) / freq, np.sqrt(6 / num_input) / freq)
-
-  return init
-
-def _first_layer_film_sine_init(m):
-  with torch.no_grad():
-    if isinstance(m, nn.Linear):
-      num_input = m.weight.size(-1)
-      m.weight.uniform_(-1 / num_input, 1 / num_input)
-
-
-class SkipLayer(nn.Module):
-  def __init__(self, ):
-    super(SkipLayer, self).__init__()
-
-  def forward(self, x0, x1):
-    # out = (x0 + x1) / math.pi
-    out = (x0 + x1) / math.sqrt(2)
-    return out
-
-
-class ModFiLMBlock(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               in_dim,
-               out_dim,
-               style_dim,
-               name_prefix,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = f"in_dim={in_dim}, " \
-                    f"out_dim={out_dim}, " \
-                    f"style_dim={style_dim})"
-
-    self.in_dim = in_dim
-    self.out_dim = out_dim
-    self.style_dim = style_dim
-    self.name_prefix = name_prefix
-
-    self.style_dim_dict = {}
-
-    self.mod1 = ModFiLMLayer(in_dim=in_dim,
-                             out_dim=out_dim,
-                             style_dim=style_dim)
-    self.style_dim_dict[f'{name_prefix}_0'] = style_dim
-
-    self.mod2 = ModFiLMLayer(in_dim=out_dim,
-                             out_dim=out_dim,
-                             style_dim=style_dim)
-    self.style_dim_dict[f'{name_prefix}_1'] = style_dim
-
-    self.skip = SkipLayer()
-    pass
-
-  def forward(self,
-              x,
-              style_dict,
-              skip=True):
-    """
-
-    :param x: (b, in_c, h, w), (b, in_c), (b, n, in_c)
-    :param style_dict:
-    :param skip:
-    :return:
-    """
-    x_orig = x
-
-    style = style_dict[f'{self.name_prefix}_0']
-    x = self.mod1(x, style)
-
-    style = style_dict[f'{self.name_prefix}_1']
-    x = self.mod2(x, style)
-
-    out = x
-
-    if skip and out.shape[-1] == x_orig.shape[-1]:
-      # out = (out + x_orig) / 1.41421
-      out = self.skip(out, x_orig)
-    return out
-
-
-class ToRGB(nn.Module):
-  def __init__(self,
-               in_dim,
-               dim_rgb=3):
-    super().__init__()
-    self.in_dim = in_dim
-    self.dim_rgb = dim_rgb
-
-    self.linear = nn.Linear(in_dim, dim_rgb)
-    pass
-
-  def forward(self,
-              input,
-              skip=None):
-
-    out = self.linear(input)
-
-    if skip is not None:
-      out = out + skip
-    return out
-
-
-class SIRENNet_skip(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               input_dim,
-               hidden_dim,
-               out_dim,
-               style_dim,
-               num_blocks,
-               device=None,
-               name_prefix='siren_skip',
-               add_out_layer=False,
-               add_in_layer=False,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'input_dim': input_dim,
-      'hidden_dim': hidden_dim,
-      'out_dim': out_dim,
-      'style_dim': style_dim,
-      'num_blocks': num_blocks,
-      'add_out_layer': add_out_layer,
-      'add_in_layer': add_in_layer,
-    }, prefix_str=name_prefix)
-
-    self.device = device
-    self.name_prefix = name_prefix
-    self.num_blocks = num_blocks
-
-    self.module_name_list = []
-    self.style_dim_dict = {}
-
-    if add_in_layer:
-      _in_dim = input_dim
-      _out_dim = hidden_dim
-      self.in_layer = ModFiLMLayer(in_dim=_in_dim, out_dim=_out_dim, style_dim=style_dim)
-      self.in_layer.fc_layer.apply(_first_layer_film_sine_init)
-      name = f"{name_prefix}_in"
-      self.style_dim_dict[f"w_{name}"] = style_dim
-
-    else:
-      self.in_layer = None
-      _out_dim = input_dim
-
-    blocks = OrderedDict()
-    to_rbgs = OrderedDict()
-    for idx in range(num_blocks):
-
-      _in_dim = _out_dim
-      _out_dim = hidden_dim
-
-      name = f"{name_prefix}_b{idx}"
-      _block = ModFiLMBlock(in_dim=_in_dim,
-                            out_dim=_out_dim,
-                            style_dim=style_dim,
-                            name_prefix=f'w_{name}')
-      self.style_dim_dict.update(_block.style_dim_dict)
-      blocks[name] = _block
-
-      _to_rgb = ToRGB(in_dim=_out_dim, dim_rgb=out_dim)
-      to_rbgs[name] = _to_rgb
-
-    self.blocks = nn.ModuleDict(blocks)
-    self.to_rgbs = nn.ModuleDict(to_rbgs)
-    self.to_rgbs.apply(_frequency_init(100))
-    self.module_name_list.append('blocks')
-    self.module_name_list.append('to_rgbs')
-
-    if add_out_layer:
-      out_layers = []
-      if out_dim > 3:
-        out_layers.append(nn.Linear(out_dim, 3))
-      out_layers.append(nn.Tanh())
-
-      self.out_layer = nn.Sequential(*out_layers)
-      self.out_layer.apply(_frequency_init(100))
-      self.module_name_list.append('out_layer')
-    else:
-      self.out_layer = None
-
-    models_dict = {}
-    for name in self.module_name_list:
-      models_dict[name] = getattr(self, name)
-    models_dict[name_prefix] = self
-    logger = logging.getLogger('tl')
-    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
-    logger.info(self)
-    pass
-
-  def forward(self,
-              input,
-              style_dict,
-              block_end_index=None,
-              **kwargs):
-    """
-
-    :param input: points xyz, (b, num_points, 3)
-    :param style_dict:
-    :param kwargs:
-
-    :return
-
-    - out: (b, num_points, out_dim)
-
-    """
-    if block_end_index is None:
-      block_end_index = self.num_blocks
-
-    x = input
-
-    if self.in_layer is not None:
-      name = f"w_{self.name_prefix}_in"
-      _style = style_dict[name]
-
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.in_layer,
-                                     inputs_args=(x, _style),
-                                     name_prefix=f'{self.name_prefix}.in_layer.')
-      x = self.in_layer(x, _style)
-
-    rgb = None
-    for idx, (name, block) in enumerate(self.blocks.items()):
-
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(block,
-                                     inputs_args=(x, style_dict),
-                                     submodels=['mod1', 'mod2'],
-                                     name_prefix=f'{name}.')
-      x = block(x, style_dict)
-
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.to_rgbs[name],
-                                     inputs_args=(x, rgb),
-                                     name_prefix=f'{name}.to_rgb.')
-      rgb = self.to_rgbs[name](x, skip=rgb)
-
-      if idx + 1 == block_end_index:
-        break
-
-    if self.out_layer is not None:
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.out_layer,
-                                     inputs_args=(rgb, ),
-                                     name_prefix='out_layer.')
-      out = self.out_layer(rgb)
-    else:
-      out= rgb
-
-    return out
-
-
-class PosEncoding(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               N_freqs,
-               in_dim=3,
-               xyz_affine=False,
-               affine_dim=None,
-               append_xyz=False,
-               **kwargs):
-    """
-    Defines a function that embeds x to (x, sin(2^k x), cos(2^k x), ...)
-
-    :param max_logscale: 9
-    :param N_freqs: 10
-    :param logscale:
-    :param multi_pi:
-    """
-    super().__init__()
-
-    self.repr_str = f"N_freqs={N_freqs}, " \
-                    f"in_dim={in_dim}, " \
-                    f"xyz_affine={xyz_affine}, " \
-                    f"affine_dim={affine_dim}, " \
-                    f"append_xyz={append_xyz}"
-
-    self.N_freqs = N_freqs
-    self.append_xyz = append_xyz
-
-    self.funcs = [torch.sin, torch.cos]
-
-    self.freqs = list(map(lambda x: 2**x * math.pi, range(N_freqs)))
-
-    if xyz_affine:
-      assert affine_dim is not None
-      self.affine_layer = nn.Linear(in_dim, affine_dim)
-      self.in_dim = affine_dim
-    else:
-      self.affine_layer = None
-      self.in_dim = in_dim
-    pass
-
-  def get_out_dim(self):
-    if self.append_xyz:
-      outdim = self.in_dim + self.in_dim * 2 * self.N_freqs
-    else:
-      outdim = self.in_dim * 2 * self.N_freqs
-    return outdim
-
-  def forward(self, x):
-    """
-    Inputs:
-        x: (B, 3)
-
-    Outputs:
-        out: (B, 2 * N_freqs * in_dim + in_dim)
-    """
-    if self.affine_layer is not None:
-      x = self.affine_layer(x)
-
-    out = []
-    if self.append_xyz:
-      out.append(x)
-    for func in self.funcs:
-      emb_list = list(map(lambda freq: func(freq * x), self.freqs))
-      out += emb_list
-
-    emb = torch.cat(out, -1)
-    return emb
-
-
-class ModSIREN_Net(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               input_dim,
-               hidden_dim,
-               out_dim,
-               style_dim,
-               N_layers,
-               device=None,
-               name_prefix='siren',
-               use_pos_enc=False,
-               PEF_cfg={},
-               freq_scale=15,
-               freq_shift=30,
-               gradient_scale=None,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'input_dim': input_dim,
-      'hidden_dim': hidden_dim,
-      'out_dim': out_dim,
-      'style_dim': style_dim,
-      'N_layers': N_layers,
-      'use_pos_enc': use_pos_enc,
-      'PEF_cfg': PEF_cfg,
-      'freq_scale': freq_scale,
-      'freq_shift': freq_shift,
-      'gradient_scale': gradient_scale,
-    }, prefix_str=name_prefix)
-
-    self.device = device
-    self.N_layers = N_layers
-    self.name_prefix = name_prefix
-    self.out_dim = out_dim
-
-    self.module_name_list = []
-    self.style_dim_dict = {}
-
-    if use_pos_enc:
-      self.pos_enc_layer = PosEncoding(**PEF_cfg)
-      self.module_name_list.append('pos_enc_layer')
-      _out_dim = self.pos_enc_layer.get_out_dim()
-
-    else:
-      self.pos_enc_layer = None
-      _out_dim = input_dim
-
-    blocks = OrderedDict()
-    for idx in range(N_layers):
-      _in_dim = _out_dim
-      _out_dim = hidden_dim
-      if idx == N_layers - 1:
-        _out_dim = out_dim
-
-      _block = ModFiLMLayer(in_dim=_in_dim, out_dim=_out_dim, style_dim=style_dim,
-                            freq_scale=freq_scale, freq_shift=freq_shift,
-                            gradient_scale=gradient_scale)
-      # initialize
-      _block.fc_layer.apply(_frequency_init(25))
-      if idx == 0:
-        _block.fc_layer.apply(_first_layer_film_sine_init)
-
-      name = f"{name_prefix}_{idx}"
-      blocks[name] = _block
-      self.style_dim_dict[f"w_{name}"] = style_dim
-      self.module_name_list.append(f"blocks.{name}")
-
-    self.blocks = nn.ModuleDict(blocks)
-    self.module_name_list.append('blocks')
-
-    models_dict = {}
-    for name in self.module_name_list:
-      models_dict[name] = tl2_utils.attrgetter_default(object=self, attr=name)
-    models_dict[name_prefix] = self
-    logger = logging.getLogger('tl')
-    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
-    logger.info(self)
-    pass
-
-  def forward(self,
-              input,
-              style_dict,
-              block_end_index=None,
-              **kwargs):
-    """
-
-    :param input: points xyz, (b, num_points, 3)
-    :param style_dict:
-    :param kwargs:
-
-    :return
-
-    - out: (b, num_points, out_dim)
-
-    """
-    if block_end_index is None:
-      block_end_index = self.N_layers
-
-    if self.pos_enc_layer is not None:
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.pos_enc_layer,
-                                     inputs_args=(input, ),
-                                     name_prefix=f'{self.name_prefix}.pos_enc_layer.')
-      x = self.pos_enc_layer(input)
-    else:
-      x = input
-
-    for idx, (name, block) in enumerate(self.blocks.items()):
-      style_name = f"w_{name}"
-      style = style_dict[style_name]
-
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(block,
-                                     inputs_args=(x, style),
-                                     name_prefix=f'{self.name_prefix}.mod_film_{idx}.')
-      x = block(x, style)
-
-      if idx + 1 == block_end_index:
-        break
-
-    out = x
-    return out
-
-
-class NeRF_Net(nn.Module):
-  """
-
-  """
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               shape_net_cfg={},
-               app_net_cfg={},
-               name_prefix='nerf',
-               shape_block_end_index=None,
-               app_block_end_index=None,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'shape_net_cfg': shape_net_cfg,
-      'app_net_cfg': app_net_cfg,
-      'shape_block_end_index': shape_block_end_index,
-      'app_block_end_index': app_block_end_index,
-    })
-
-    self.shape_net_cfg = shape_net_cfg
-    self.app_net_cfg = app_net_cfg
-    self.name_prefix = name_prefix
-    self.shape_block_end_index = shape_block_end_index
-    self.app_block_end_index = app_block_end_index
-
-    self.module_name_list = []
-
-    # self.style_dim_dict = {}
-
-    self.shape_net = ModSIREN_Net(**{
-      **shape_net_cfg,
-      'name_prefix': 'shape'
-    })
-    self.style_dim_dict_shape = self.shape_net.style_dim_dict
-    self.module_name_list.append('shape_net')
-
-    # _in_dim = shape_net_cfg['out_dim']
-    _in_dim = self.shape_net.out_dim
-
-    self.sigma_layer = nn.Linear(_in_dim, 1)
-    # self.final_layer.apply(frequency_init(25))
-    self.module_name_list.append('sigma_layer')
-
-    from . import cips_net
-
-    self.app_net = cips_net.CIPSNet(**{
-      **app_net_cfg,
-      'input_dim': _in_dim,
-      'name_prefix': 'app'
-    })
-    self.style_dim_dict_app = self.app_net.style_dim_dict
-    self.module_name_list.append('app_net')
-
-    # self.out_dim = app_net_cfg['out_dim']
-    self.out_dim = self.app_net.out_dim
-
-    # self.color_layer_linear = nn.Sequential(
-    #   nn.Linear(_out_dim, rgb_dim),
-    # )
-    # self.color_layer_linear.apply(init_func.kaiming_leaky_init)
-    # self.module_name_list.append('color_layer_linear')
-
-    logger = logging.getLogger('tl')
-    models_dict = {}
-    for name in self.module_name_list:
-      models_dict[name] = getattr(self, name)
-    models_dict[name_prefix] = self
-    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
-    logger.info(self)
-    pass
-
-  def forward(self,
-              x,
-              style_dict,
-              ray_directions=None,
-              **kwargs):
-    """
-
-    :param x: points xyz, (b, num_points, 3)
-    :param style_dict:
-    :param ray_directions: (b, num_points, 3)
-    :param kwargs:
-    :return
-
-    - out: (b, num_points, rgb_dim + 1), rgb(rgb_dim) + sigma(1)
-
-    """
-
-    # scale xyz
-
-    x = self.shape_net(x, style_dict, block_end_index=self.shape_block_end_index)
-
-    if global_cfg.tl_debug:
-      VerboseModel.forward_verbose(self.sigma_layer,
-                                   inputs_args=(x, ),
-                                   name_prefix="sigma_layer")
-    sigma = self.sigma_layer(x)
-
-    x = self.app_net(x, style_dict, block_end_index=self.app_block_end_index)
-
-    out = torch.cat([x, sigma], dim=-1)
-    return out
-
-
+import collections
+import math
+from collections import OrderedDict
+import numpy as np
+import logging
+from einops import rearrange
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from tl2 import tl2_utils
+from tl2.proj.pytorch import torch_utils
+from tl2.proj.pytorch import init_func
+from tl2.proj.pytorch.pytorch_hook import VerboseModel
+from tl2.proj.fvcore import global_cfg
+
+
+class SinAct(nn.Module):
+  def __init__(self, ):
+    super(SinAct, self).__init__()
+
+  def forward(self, x):
+    return torch.sin(x)
+
+
+class ScaleGradient(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self, gamma=1.):
+    """
+    Keep the input unchanged but scale the gradients by gamma
+
+    :param gamma:
+    """
+    super(ScaleGradient, self).__init__()
+
+    self.repr_str = f"gamma={gamma:.3f}=1/{1/gamma:.3f}"
+
+    self.gamma = gamma
+    pass
+
+  def forward(self, x):
+    return self.gamma * x + (1 - self.gamma) * x.detach()
+
+
+class ScaleAct(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               gamma=15.,
+               beta=30.):
+    super(ScaleAct, self).__init__()
+
+    self.repr_str = f"gamma={gamma}, " \
+                    f"beta={beta}, "
+
+    self.gamma = gamma
+    self.beta = beta
+    pass
+
+  def forward(self, x):
+    out = x * self.gamma + self.beta
+    return out
+
+
+class ModFiLMLayer(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               in_dim,
+               out_dim,
+               style_dim,
+               freq_scale=15,
+               freq_shift=30,
+               gradient_scale=None,
+               **kwargs):
+    super().__init__()
+
+    if gradient_scale is not None and isinstance(gradient_scale, str):
+      gradient_scale = eval(gradient_scale)
+
+    self.repr_str = f"in_dim={in_dim}, " \
+                    f"out_dim={out_dim}, " \
+                    f"style_dim={style_dim}, " \
+                    f"freq_scale={freq_scale}, " \
+                    f"freq_shift={freq_shift}, " \
+                    f"gradient_scale={gradient_scale}"
+
+    self.mod_freq = nn.Linear(style_dim, out_dim)
+    self.mod_freq.apply(init_func.kaiming_leaky_init)
+    if freq_scale == 1 and freq_shift == 0:
+      self.freq_scale = nn.Identity()
+    else:
+      self.freq_scale = ScaleAct(gamma=freq_scale, beta=freq_shift)
+
+    self.mod_phase = nn.Linear(style_dim, out_dim)
+    self.mod_phase.apply(init_func.kaiming_leaky_init)
+
+    self.fc_layer = nn.Linear(in_dim, out_dim)
+    self.fc_layer.apply(_frequency_init(25))
+
+    if gradient_scale is not None:
+      self.gradient_scale_layer = ScaleGradient(gamma=gradient_scale)
+    else:
+      self.gradient_scale_layer = None
+
+    self.sin_act = SinAct()
+    pass
+
+  def forward(self,
+              x,
+              style):
+    """
+
+    :param x: (b, n, in_dim) or (b, in_dim)
+    :param style: (b, style_dim)
+    :return
+
+    """
+
+    # (b, out_dim)
+    x = self.fc_layer(x)
+
+    # (b, style_dim) -> (b, out_dim)
+    freq = self.mod_freq(style)
+    freq = self.freq_scale(freq)
+
+    phase = self.mod_phase(style)
+
+    if x.dim() == 3:
+      freq = freq.unsqueeze(1)
+      phase = phase.unsqueeze(1)
+    elif x.dim() == 2:
+      pass
+    else:
+      raise NotImplementedError
+
+    x = freq * x
+
+    if self.gradient_scale_layer is not None:
+      _x = self.gradient_scale_layer(x)
+      x = _x
+
+    return self.sin_act(x + phase)
+
+
+def _frequency_init(freq):
+  def init(m):
+    with torch.no_grad():
+      if isinstance(m, nn.Linear):
+        num_input = m.weight.size(-1)
+        m.weight.uniform_(-np.sqrt(6 / num_input) / freq, np.sqrt(6 / num_input) / freq)
+
+  return init
+
+def _first_layer_film_sine_init(m):
+  with torch.no_grad():
+    if isinstance(m, nn.Linear):
+      num_input = m.weight.size(-1)
+      m.weight.uniform_(-1 / num_input, 1 / num_input)
+
+
+class SkipLayer(nn.Module):
+  def __init__(self, ):
+    super(SkipLayer, self).__init__()
+
+  def forward(self, x0, x1):
+    # out = (x0 + x1) / math.pi
+    out = (x0 + x1) / math.sqrt(2)
+    return out
+
+
+class ModFiLMBlock(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               in_dim,
+               out_dim,
+               style_dim,
+               name_prefix,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = f"in_dim={in_dim}, " \
+                    f"out_dim={out_dim}, " \
+                    f"style_dim={style_dim})"
+
+    self.in_dim = in_dim
+    self.out_dim = out_dim
+    self.style_dim = style_dim
+    self.name_prefix = name_prefix
+
+    self.style_dim_dict = {}
+
+    self.mod1 = ModFiLMLayer(in_dim=in_dim,
+                             out_dim=out_dim,
+                             style_dim=style_dim)
+    self.style_dim_dict[f'{name_prefix}_0'] = style_dim
+
+    self.mod2 = ModFiLMLayer(in_dim=out_dim,
+                             out_dim=out_dim,
+                             style_dim=style_dim)
+    self.style_dim_dict[f'{name_prefix}_1'] = style_dim
+
+    self.skip = SkipLayer()
+    pass
+
+  def forward(self,
+              x,
+              style_dict,
+              skip=True):
+    """
+
+    :param x: (b, in_c, h, w), (b, in_c), (b, n, in_c)
+    :param style_dict:
+    :param skip:
+    :return:
+    """
+    x_orig = x
+
+    style = style_dict[f'{self.name_prefix}_0']
+    x = self.mod1(x, style)
+
+    style = style_dict[f'{self.name_prefix}_1']
+    x = self.mod2(x, style)
+
+    out = x
+
+    if skip and out.shape[-1] == x_orig.shape[-1]:
+      # out = (out + x_orig) / 1.41421
+      out = self.skip(out, x_orig)
+    return out
+
+
+class ToRGB(nn.Module):
+  def __init__(self,
+               in_dim,
+               dim_rgb=3):
+    super().__init__()
+    self.in_dim = in_dim
+    self.dim_rgb = dim_rgb
+
+    self.linear = nn.Linear(in_dim, dim_rgb)
+    pass
+
+  def forward(self,
+              input,
+              skip=None):
+
+    out = self.linear(input)
+
+    if skip is not None:
+      out = out + skip
+    return out
+
+
+class SIRENNet_skip(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               input_dim,
+               hidden_dim,
+               out_dim,
+               style_dim,
+               num_blocks,
+               device=None,
+               name_prefix='siren_skip',
+               add_out_layer=False,
+               add_in_layer=False,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'input_dim': input_dim,
+      'hidden_dim': hidden_dim,
+      'out_dim': out_dim,
+      'style_dim': style_dim,
+      'num_blocks': num_blocks,
+      'add_out_layer': add_out_layer,
+      'add_in_layer': add_in_layer,
+    }, prefix_str=name_prefix)
+
+    self.device = device
+    self.name_prefix = name_prefix
+    self.num_blocks = num_blocks
+
+    self.module_name_list = []
+    self.style_dim_dict = {}
+
+    if add_in_layer:
+      _in_dim = input_dim
+      _out_dim = hidden_dim
+      self.in_layer = ModFiLMLayer(in_dim=_in_dim, out_dim=_out_dim, style_dim=style_dim)
+      self.in_layer.fc_layer.apply(_first_layer_film_sine_init)
+      name = f"{name_prefix}_in"
+      self.style_dim_dict[f"w_{name}"] = style_dim
+
+    else:
+      self.in_layer = None
+      _out_dim = input_dim
+
+    blocks = OrderedDict()
+    to_rbgs = OrderedDict()
+    for idx in range(num_blocks):
+
+      _in_dim = _out_dim
+      _out_dim = hidden_dim
+
+      name = f"{name_prefix}_b{idx}"
+      _block = ModFiLMBlock(in_dim=_in_dim,
+                            out_dim=_out_dim,
+                            style_dim=style_dim,
+                            name_prefix=f'w_{name}')
+      self.style_dim_dict.update(_block.style_dim_dict)
+      blocks[name] = _block
+
+      _to_rgb = ToRGB(in_dim=_out_dim, dim_rgb=out_dim)
+      to_rbgs[name] = _to_rgb
+
+    self.blocks = nn.ModuleDict(blocks)
+    self.to_rgbs = nn.ModuleDict(to_rbgs)
+    self.to_rgbs.apply(_frequency_init(100))
+    self.module_name_list.append('blocks')
+    self.module_name_list.append('to_rgbs')
+
+    if add_out_layer:
+      out_layers = []
+      if out_dim > 3:
+        out_layers.append(nn.Linear(out_dim, 3))
+      out_layers.append(nn.Tanh())
+
+      self.out_layer = nn.Sequential(*out_layers)
+      self.out_layer.apply(_frequency_init(100))
+      self.module_name_list.append('out_layer')
+    else:
+      self.out_layer = None
+
+    models_dict = {}
+    for name in self.module_name_list:
+      models_dict[name] = getattr(self, name)
+    models_dict[name_prefix] = self
+    logger = logging.getLogger('tl')
+    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
+    logger.info(self)
+    pass
+
+  def forward(self,
+              input,
+              style_dict,
+              block_end_index=None,
+              **kwargs):
+    """
+
+    :param input: points xyz, (b, num_points, 3)
+    :param style_dict:
+    :param kwargs:
+
+    :return
+
+    - out: (b, num_points, out_dim)
+
+    """
+    if block_end_index is None:
+      block_end_index = self.num_blocks
+
+    x = input
+
+    if self.in_layer is not None:
+      name = f"w_{self.name_prefix}_in"
+      _style = style_dict[name]
+
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.in_layer,
+                                     inputs_args=(x, _style),
+                                     name_prefix=f'{self.name_prefix}.in_layer.')
+      x = self.in_layer(x, _style)
+
+    rgb = None
+    for idx, (name, block) in enumerate(self.blocks.items()):
+
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(block,
+                                     inputs_args=(x, style_dict),
+                                     submodels=['mod1', 'mod2'],
+                                     name_prefix=f'{name}.')
+      x = block(x, style_dict)
+
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.to_rgbs[name],
+                                     inputs_args=(x, rgb),
+                                     name_prefix=f'{name}.to_rgb.')
+      rgb = self.to_rgbs[name](x, skip=rgb)
+
+      if idx + 1 == block_end_index:
+        break
+
+    if self.out_layer is not None:
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.out_layer,
+                                     inputs_args=(rgb, ),
+                                     name_prefix='out_layer.')
+      out = self.out_layer(rgb)
+    else:
+      out= rgb
+
+    return out
+
+
+class PosEncoding(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               N_freqs,
+               in_dim=3,
+               xyz_affine=False,
+               affine_dim=None,
+               append_xyz=False,
+               **kwargs):
+    """
+    Defines a function that embeds x to (x, sin(2^k x), cos(2^k x), ...)
+
+    :param max_logscale: 9
+    :param N_freqs: 10
+    :param logscale:
+    :param multi_pi:
+    """
+    super().__init__()
+
+    self.repr_str = f"N_freqs={N_freqs}, " \
+                    f"in_dim={in_dim}, " \
+                    f"xyz_affine={xyz_affine}, " \
+                    f"affine_dim={affine_dim}, " \
+                    f"append_xyz={append_xyz}"
+
+    self.N_freqs = N_freqs
+    self.append_xyz = append_xyz
+
+    self.funcs = [torch.sin, torch.cos]
+
+    self.freqs = list(map(lambda x: 2**x * math.pi, range(N_freqs)))
+
+    if xyz_affine:
+      assert affine_dim is not None
+      self.affine_layer = nn.Linear(in_dim, affine_dim)
+      self.in_dim = affine_dim
+    else:
+      self.affine_layer = None
+      self.in_dim = in_dim
+    pass
+
+  def get_out_dim(self):
+    if self.append_xyz:
+      outdim = self.in_dim + self.in_dim * 2 * self.N_freqs
+    else:
+      outdim = self.in_dim * 2 * self.N_freqs
+    return outdim
+
+  def forward(self, x):
+    """
+    Inputs:
+        x: (B, 3)
+
+    Outputs:
+        out: (B, 2 * N_freqs * in_dim + in_dim)
+    """
+    if self.affine_layer is not None:
+      x = self.affine_layer(x)
+
+    out = []
+    if self.append_xyz:
+      out.append(x)
+    for func in self.funcs:
+      emb_list = list(map(lambda freq: func(freq * x), self.freqs))
+      out += emb_list
+
+    emb = torch.cat(out, -1)
+    return emb
+
+
+class ModSIREN_Net(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               input_dim,
+               hidden_dim,
+               out_dim,
+               style_dim,
+               N_layers,
+               device=None,
+               name_prefix='siren',
+               use_pos_enc=False,
+               PEF_cfg={},
+               freq_scale=15,
+               freq_shift=30,
+               gradient_scale=None,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'input_dim': input_dim,
+      'hidden_dim': hidden_dim,
+      'out_dim': out_dim,
+      'style_dim': style_dim,
+      'N_layers': N_layers,
+      'use_pos_enc': use_pos_enc,
+      'PEF_cfg': PEF_cfg,
+      'freq_scale': freq_scale,
+      'freq_shift': freq_shift,
+      'gradient_scale': gradient_scale,
+    }, prefix_str=name_prefix)
+
+    self.device = device
+    self.N_layers = N_layers
+    self.name_prefix = name_prefix
+    self.out_dim = out_dim
+
+    self.module_name_list = []
+    self.style_dim_dict = {}
+
+    if use_pos_enc:
+      self.pos_enc_layer = PosEncoding(**PEF_cfg)
+      self.module_name_list.append('pos_enc_layer')
+      _out_dim = self.pos_enc_layer.get_out_dim()
+
+    else:
+      self.pos_enc_layer = None
+      _out_dim = input_dim
+
+    blocks = OrderedDict()
+    for idx in range(N_layers):
+      _in_dim = _out_dim
+      _out_dim = hidden_dim
+      if idx == N_layers - 1:
+        _out_dim = out_dim
+
+      _block = ModFiLMLayer(in_dim=_in_dim, out_dim=_out_dim, style_dim=style_dim,
+                            freq_scale=freq_scale, freq_shift=freq_shift,
+                            gradient_scale=gradient_scale)
+      # initialize
+      _block.fc_layer.apply(_frequency_init(25))
+      if idx == 0:
+        _block.fc_layer.apply(_first_layer_film_sine_init)
+
+      name = f"{name_prefix}_{idx}"
+      blocks[name] = _block
+      self.style_dim_dict[f"w_{name}"] = style_dim
+      self.module_name_list.append(f"blocks.{name}")
+
+    self.blocks = nn.ModuleDict(blocks)
+    self.module_name_list.append('blocks')
+
+    models_dict = {}
+    for name in self.module_name_list:
+      models_dict[name] = tl2_utils.attrgetter_default(object=self, attr=name)
+    models_dict[name_prefix] = self
+    logger = logging.getLogger('tl')
+    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
+    logger.info(self)
+    pass
+
+  def forward(self,
+              input,
+              style_dict,
+              block_end_index=None,
+              **kwargs):
+    """
+
+    :param input: points xyz, (b, num_points, 3)
+    :param style_dict:
+    :param kwargs:
+
+    :return
+
+    - out: (b, num_points, out_dim)
+
+    """
+    if block_end_index is None:
+      block_end_index = self.N_layers
+
+    if self.pos_enc_layer is not None:
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.pos_enc_layer,
+                                     inputs_args=(input, ),
+                                     name_prefix=f'{self.name_prefix}.pos_enc_layer.')
+      x = self.pos_enc_layer(input)
+    else:
+      x = input
+
+    for idx, (name, block) in enumerate(self.blocks.items()):
+      style_name = f"w_{name}"
+      style = style_dict[style_name]
+
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(block,
+                                     inputs_args=(x, style),
+                                     name_prefix=f'{self.name_prefix}.mod_film_{idx}.')
+      x = block(x, style)
+
+      if idx + 1 == block_end_index:
+        break
+
+    out = x
+    return out
+
+
+class NeRF_Net(nn.Module):
+  """
+
+  """
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               shape_net_cfg={},
+               app_net_cfg={},
+               name_prefix='nerf',
+               shape_block_end_index=None,
+               app_block_end_index=None,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'shape_net_cfg': shape_net_cfg,
+      'app_net_cfg': app_net_cfg,
+      'shape_block_end_index': shape_block_end_index,
+      'app_block_end_index': app_block_end_index,
+    })
+
+    self.shape_net_cfg = shape_net_cfg
+    self.app_net_cfg = app_net_cfg
+    self.name_prefix = name_prefix
+    self.shape_block_end_index = shape_block_end_index
+    self.app_block_end_index = app_block_end_index
+
+    self.module_name_list = []
+
+    # self.style_dim_dict = {}
+
+    self.shape_net = ModSIREN_Net(**{
+      **shape_net_cfg,
+      'name_prefix': 'shape'
+    })
+    self.style_dim_dict_shape = self.shape_net.style_dim_dict
+    self.module_name_list.append('shape_net')
+
+    # _in_dim = shape_net_cfg['out_dim']
+    _in_dim = self.shape_net.out_dim
+
+    self.sigma_layer = nn.Linear(_in_dim, 1)
+    # self.final_layer.apply(frequency_init(25))
+    self.module_name_list.append('sigma_layer')
+
+    from . import cips_net
+
+    self.app_net = cips_net.CIPSNet(**{
+      **app_net_cfg,
+      'input_dim': _in_dim,
+      'name_prefix': 'app'
+    })
+    self.style_dim_dict_app = self.app_net.style_dim_dict
+    self.module_name_list.append('app_net')
+
+    # self.out_dim = app_net_cfg['out_dim']
+    self.out_dim = self.app_net.out_dim
+
+    # self.color_layer_linear = nn.Sequential(
+    #   nn.Linear(_out_dim, rgb_dim),
+    # )
+    # self.color_layer_linear.apply(init_func.kaiming_leaky_init)
+    # self.module_name_list.append('color_layer_linear')
+
+    logger = logging.getLogger('tl')
+    models_dict = {}
+    for name in self.module_name_list:
+      models_dict[name] = getattr(self, name)
+    models_dict[name_prefix] = self
+    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
+    logger.info(self)
+    pass
+
+  def forward(self,
+              x,
+              style_dict,
+              ray_directions=None,
+              **kwargs):
+    """
+
+    :param x: points xyz, (b, num_points, 3)
+    :param style_dict:
+    :param ray_directions: (b, num_points, 3)
+    :param kwargs:
+    :return
+
+    - out: (b, num_points, rgb_dim + 1), rgb(rgb_dim) + sigma(1)
+
+    """
+
+    # scale xyz
+
+    x = self.shape_net(x, style_dict, block_end_index=self.shape_block_end_index)
+
+    if global_cfg.tl_debug:
+      VerboseModel.forward_verbose(self.sigma_layer,
+                                   inputs_args=(x, ),
+                                   name_prefix="sigma_layer")
+    sigma = self.sigma_layer(x)
+
+    x = self.app_net(x, style_dict, block_end_index=self.app_block_end_index)
+
+    out = torch.cat([x, sigma], dim=-1)
+    return out
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/siren_net.yaml` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/siren_net.yaml`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,69 +1,69 @@
-_build_ModFiLMLayer:
-  in_dim: 128
-  out_dim: 256
-  style_dim: 512
-  gradient_scale: 1/15
-
-
-mapping_cfg: &mapping_cfg
-  z_dim: 256
-  hidden_dim: 256
-  base_layers: 4
-  head_layers: 0
-  add_norm: true
-  norm_out: true
-
-
-_build_ModFiLMBlock:
-  mapping_cfg: *mapping_cfg
-  in_dim: 128
-  out_dim: 128
-  style_dim: 256
-
-
-_build_SIRENNet_skip:
-  mapping_cfg: *mapping_cfg
-  input_dim: 3
-  hidden_dim: 128
-  out_dim: 64
-  style_dim: 256
-  num_blocks: 4
-  add_out_layer: false
-  add_in_layer: true
-
-
-_build_ModSIREN_Net:
-  mapping_cfg: *mapping_cfg
-  input_dim: 3
-  hidden_dim: 128
-  out_dim: 128
-  style_dim: 256
-  N_layers: 4
-  use_pos_enc: true
-  PEF_cfg:
-    N_freqs: 10
-
-_shape_cfg: &_shape_cfg
-  input_dim: 3
-  hidden_dim: 256
-  out_dim: 256
-  style_dim: 256
-  N_layers: 8
-  use_pos_enc: true
-  PEF_cfg:
-    N_freqs: 10
-
-_app_cfg: &_app_cfg
-#  input_dim: 64
-  hidden_dim: 128
-  out_dim: 32
-  style_dim: 256
-  num_blocks: 1
-  disable_to_rgb: true
-  skip: false
-
-_build_NeRF_Net:
-  shape_net_cfg: *_shape_cfg
-  app_net_cfg: *_app_cfg
-  shape_block_end_index: 3
-  app_block_end_index: 1
+_build_ModFiLMLayer:
+  in_dim: 128
+  out_dim: 256
+  style_dim: 512
+  gradient_scale: 1/15
+
+
+mapping_cfg: &mapping_cfg
+  z_dim: 256
+  hidden_dim: 256
+  base_layers: 4
+  head_layers: 0
+  add_norm: true
+  norm_out: true
+
+
+_build_ModFiLMBlock:
+  mapping_cfg: *mapping_cfg
+  in_dim: 128
+  out_dim: 128
+  style_dim: 256
+
+
+_build_SIRENNet_skip:
+  mapping_cfg: *mapping_cfg
+  input_dim: 3
+  hidden_dim: 128
+  out_dim: 64
+  style_dim: 256
+  num_blocks: 4
+  add_out_layer: false
+  add_in_layer: true
+
+
+_build_ModSIREN_Net:
+  mapping_cfg: *mapping_cfg
+  input_dim: 3
+  hidden_dim: 128
+  out_dim: 128
+  style_dim: 256
+  N_layers: 4
+  use_pos_enc: true
+  PEF_cfg:
+    N_freqs: 10
+
+_shape_cfg: &_shape_cfg
+  input_dim: 3
+  hidden_dim: 256
+  out_dim: 256
+  style_dim: 256
+  N_layers: 8
+  use_pos_enc: true
+  PEF_cfg:
+    N_freqs: 10
+
+_app_cfg: &_app_cfg
+#  input_dim: 64
+  hidden_dim: 128
+  out_dim: 32
+  style_dim: 256
+  num_blocks: 1
+  disable_to_rgb: true
+  skip: false
+
+_build_NeRF_Net:
+  shape_net_cfg: *_shape_cfg
+  app_net_cfg: *_app_cfg
+  shape_block_end_index: 3
+  app_block_end_index: 1
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,87 +1,87 @@
-_build_ModFiLMLayer:
-  in_dim: 128
-  out_dim: 256
-  style_dim: 512
-  gradient_scale: 1/15
-
-
-mapping_cfg: &mapping_cfg
-  z_dim: 256
-  hidden_dim: 256
-  base_layers: 4
-  head_layers: 0
-  add_norm: true
-  norm_out: true
-
-
-_build_ModFiLMBlock:
-  mapping_cfg: *mapping_cfg
-  in_dim: 128
-  out_dim: 128
-  style_dim: 256
-
-
-_build_SIRENNet_skip:
-  mapping_cfg: *mapping_cfg
-  input_dim: 3
-  hidden_dim: 128
-  out_dim: 64
-  style_dim: 256
-  num_blocks: 4
-  add_out_layer: false
-  add_in_layer: true
-
-
-shape_cfg: &shape_cfg
-  input_dim: 3
-  hidden_dim: 256
-  out_dim: 256
-  N_layers: 8
-
-_build_ShapeNet:
-  mapping_cfg: *mapping_cfg
-  shape_cfg: *shape_cfg
-
-
-app_cfg: &app_cfg
-  input_dim: 256
-  hidden_dim: 256
-  out_dim: 256
-  N_layers: 1
-
-_build_AppNet:
-  mapping_cfg: *mapping_cfg
-  shape_cfg: *app_cfg
-
-_build_NeRF_Net_kwargs: &_build_NeRF_Net_kwargs
-  shape_net_cfg: *shape_cfg
-  app_net_cfg: *app_cfg
-  shape_block_end_index: 8
-  app_block_end_index: 1
-
-
-_build_MappingNetwork:
-  z_dim: 256
-  map_hidden_dim: 256
-  map_output_dim: "9 * 256 * 2"
-  N_layers: 4
-
-_build_StyleMappingBaseNet:
-  z_dim: 256
-  hidden_dim: 256
-  N_layers: 3
-
-_build_StyleMappingShapeApp:
-  nerf_cfg: *_build_NeRF_Net_kwargs
-  mapping_cfg:
-    base_cfg:
-      z_dim: 256
-      hidden_dim: 256
-      N_layers: 3
-
-
-_build_NeRF_Net:
-  shape_net_cfg: *shape_cfg
-  app_net_cfg: *app_cfg
-  shape_block_end_index: 8
-  app_block_end_index: 1
+_build_ModFiLMLayer:
+  in_dim: 128
+  out_dim: 256
+  style_dim: 512
+  gradient_scale: 1/15
+
+
+mapping_cfg: &mapping_cfg
+  z_dim: 256
+  hidden_dim: 256
+  base_layers: 4
+  head_layers: 0
+  add_norm: true
+  norm_out: true
+
+
+_build_ModFiLMBlock:
+  mapping_cfg: *mapping_cfg
+  in_dim: 128
+  out_dim: 128
+  style_dim: 256
+
+
+_build_SIRENNet_skip:
+  mapping_cfg: *mapping_cfg
+  input_dim: 3
+  hidden_dim: 128
+  out_dim: 64
+  style_dim: 256
+  num_blocks: 4
+  add_out_layer: false
+  add_in_layer: true
+
+
+shape_cfg: &shape_cfg
+  input_dim: 3
+  hidden_dim: 256
+  out_dim: 256
+  N_layers: 8
+
+_build_ShapeNet:
+  mapping_cfg: *mapping_cfg
+  shape_cfg: *shape_cfg
+
+
+app_cfg: &app_cfg
+  input_dim: 256
+  hidden_dim: 256
+  out_dim: 256
+  N_layers: 1
+
+_build_AppNet:
+  mapping_cfg: *mapping_cfg
+  shape_cfg: *app_cfg
+
+_build_NeRF_Net_kwargs: &_build_NeRF_Net_kwargs
+  shape_net_cfg: *shape_cfg
+  app_net_cfg: *app_cfg
+  shape_block_end_index: 8
+  app_block_end_index: 1
+
+
+_build_MappingNetwork:
+  z_dim: 256
+  map_hidden_dim: 256
+  map_output_dim: "9 * 256 * 2"
+  N_layers: 4
+
+_build_StyleMappingBaseNet:
+  z_dim: 256
+  hidden_dim: 256
+  N_layers: 3
+
+_build_StyleMappingShapeApp:
+  nerf_cfg: *_build_NeRF_Net_kwargs
+  mapping_cfg:
+    base_cfg:
+      z_dim: 256
+      hidden_dim: 256
+      N_layers: 3
+
+
+_build_NeRF_Net:
+  shape_net_cfg: *shape_cfg
+  app_net_cfg: *app_cfg
+  shape_block_end_index: 8
+  app_block_end_index: 1
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/siren_net_v1.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/siren_net_v1.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,562 +1,562 @@
-import collections
-import math
-from collections import OrderedDict
-import numpy as np
-import logging
-from einops import rearrange
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-from tl2 import tl2_utils
-from tl2.proj.pytorch import torch_utils
-from tl2.proj.pytorch import init_func
-from tl2.proj.pytorch.pytorch_hook import VerboseModel
-from tl2.proj.fvcore import global_cfg
-
-
-class Sine(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               w0=20.):
-    super().__init__()
-
-    self.repr_str = f"w0={w0}"
-
-    self.w0 = w0
-    pass
-
-  def forward(self, x):
-    return torch.sin(self.w0 * x)
-
-
-class SirenLayer(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               input_dim,
-               hidden_dim,
-               use_bias=True,
-               w0=1.,
-               is_first=False, # for initialization
-               **kwargs):
-    super().__init__()
-    self.repr_str = f"input_dim={input_dim}, " \
-                    f"hidden_dim={hidden_dim}, " \
-                    f"use_bias={use_bias}, " \
-                    f"w0={w0}, " \
-                    f"is_first={is_first}"
-
-    self.layer = nn.Linear(input_dim, hidden_dim, bias=use_bias)
-    self.activation = Sine(w0)
-    self.is_first = is_first
-    self.input_dim = input_dim
-    self.w0 = w0
-    self.c = 6
-    self.reset_parameters()
-    pass
-
-  def reset_parameters(self):
-    with torch.no_grad():
-      dim = self.input_dim
-      w_std = (1 / dim) if self.is_first else (math.sqrt(self.c / dim) / self.w0)
-      self.layer.weight.uniform_(-w_std, w_std)
-      if self.layer.bias is not None:
-        self.layer.bias.uniform_(-w_std, w_std)
-    pass
-
-  def forward(self, x):
-    out = self.layer(x)
-    out = self.activation(out)
-    return out
-
-
-class ModSirenLayer(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               input_dim,
-               hidden_dim,
-               style_dim,
-               w0=1.,
-               is_first=False, # for initialization
-               **kwargs):
-    super().__init__()
-    self.repr_str = f"input_dim={input_dim}, " \
-                    f"hidden_dim={hidden_dim}, " \
-                    f"style_dim={style_dim}, " \
-                    f"w0={w0}, " \
-                    f"is_first={is_first}"
-
-    from . import cips_net
-    # self.layer = nn.Linear(input_dim, hidden_dim, bias=use_bias)
-    self.layer = cips_net.ModFC(in_channel=input_dim,
-                                out_channel=hidden_dim,
-                                style_dim=style_dim)
-
-    self.activation = Sine(w0)
-    self.is_first = is_first
-    self.input_dim = input_dim
-    self.w0 = w0
-    self.c = 6
-    self.reset_parameters()
-    pass
-
-  def reset_parameters(self):
-    with torch.no_grad():
-      dim = self.input_dim
-      w_std = (1 / dim) if self.is_first else (math.sqrt(self.c / dim) / self.w0)
-      self.layer.weight.uniform_(-w_std, w_std)
-      # if self.layer.bias is not None:
-      #   self.layer.bias.uniform_(-w_std, w_std)
-    pass
-
-  def forward(self,
-              x,
-              style):
-    out = self.layer(x, style)
-    out = self.activation(out)
-    return out
-
-
-class SkipLayer(nn.Module):
-  def __init__(self, ):
-    super(SkipLayer, self).__init__()
-
-  def forward(self, x0, x1):
-    # out = (x0 + x1) / math.pi
-    out = (x0 + x1) / math.sqrt(2)
-    return out
-
-
-class ModSirenBlock(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               in_dim,
-               out_dim,
-               style_dim,
-               name_prefix,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = f"in_dim={in_dim}, " \
-                    f"out_dim={out_dim}, " \
-                    f"style_dim={style_dim})"
-
-    self.in_dim = in_dim
-    self.out_dim = out_dim
-    self.style_dim = style_dim
-    self.name_prefix = name_prefix
-
-    self.style_dim_dict = {}
-
-    self.mod1 = ModSirenLayer(input_dim=in_dim,
-                              hidden_dim=out_dim,
-                              style_dim=style_dim)
-    self.style_dim_dict[f'{name_prefix}_0'] = style_dim
-
-    self.mod2 = ModSirenLayer(input_dim=out_dim,
-                              hidden_dim=out_dim,
-                              style_dim=style_dim)
-    self.style_dim_dict[f'{name_prefix}_1'] = style_dim
-
-    self.skip = SkipLayer()
-    pass
-
-  def forward(self,
-              x,
-              style_dict,
-              skip=True):
-    """
-
-    :param x: (b, in_c, h, w), (b, in_c), (b, n, in_c)
-    :param style_dict:
-    :param skip:
-    :return:
-    """
-    x_orig = x
-
-    style = style_dict[f'{self.name_prefix}_0']
-    x = self.mod1(x, style)
-
-    style = style_dict[f'{self.name_prefix}_1']
-    x = self.mod2(x, style)
-
-    out = x
-
-    if skip and out.shape[-1] == x_orig.shape[-1]:
-      # out = (out + x_orig) / 1.41421
-      out = self.skip(out, x_orig)
-    return out
-
-
-class ToRGB(nn.Module):
-  def __init__(self,
-               in_dim,
-               dim_rgb=3):
-    super().__init__()
-    self.in_dim = in_dim
-    self.dim_rgb = dim_rgb
-
-    self.linear = nn.Linear(in_dim, dim_rgb)
-    pass
-
-  def forward(self,
-              input,
-              skip=None):
-
-    out = self.linear(input)
-
-    if skip is not None:
-      out = out + skip
-    return out
-
-
-def _frequency_init(freq):
-  def init(m):
-    with torch.no_grad():
-      if isinstance(m, nn.Linear):
-        num_input = m.weight.size(-1)
-        m.weight.uniform_(-np.sqrt(6 / num_input) / freq, np.sqrt(6 / num_input) / freq)
-
-  return init
-
-
-class ModSIREN_Skip_Net(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               input_dim,
-               hidden_dim,
-               out_dim,
-               style_dim,
-               num_blocks,
-               device=None,
-               name_prefix='siren_skip',
-               add_out_layer=False,
-               add_in_layer=False,
-               in_layer_mode='siren', # [siren, mod_film]
-               xyz_sine_w=20,
-               disable_to_rgb=False,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'input_dim': input_dim,
-      'hidden_dim': hidden_dim,
-      'out_dim': out_dim,
-      'style_dim': style_dim,
-      'num_blocks': num_blocks,
-      'add_out_layer': add_out_layer,
-      'add_in_layer': add_in_layer,
-      'in_layer_mode': in_layer_mode,
-      'xyz_sine_w': xyz_sine_w,
-      'disable_to_rgb': disable_to_rgb,
-    }, prefix_str=name_prefix)
-
-    self.device = device
-    self.name_prefix = name_prefix
-    self.num_blocks = num_blocks
-    self.in_layer_mode = in_layer_mode
-    self.disable_to_rgb = disable_to_rgb
-
-    if disable_to_rgb:
-      self.out_dim = hidden_dim
-    else:
-      self.out_dim = out_dim
-
-    self.module_name_list = []
-    self.style_dim_dict = {}
-
-    if add_in_layer:
-      _in_dim = input_dim
-      _out_dim = hidden_dim
-
-      if in_layer_mode == 'siren':
-        self.in_layer = SirenLayer(input_dim=_in_dim, hidden_dim=_out_dim,
-                                   w0=xyz_sine_w, is_first=True)
-        self.module_name_list.append('in_layer')
-      elif in_layer_mode == 'mod_film':
-        from .siren_net import ModFiLMLayer, _first_layer_film_sine_init
-
-        self.in_layer = ModFiLMLayer(in_dim=_in_dim, out_dim=_out_dim, style_dim=style_dim)
-        self.in_layer.fc_layer.apply(_first_layer_film_sine_init)
-        name = f"{name_prefix}_in"
-        self.style_dim_dict[f"w_{name}"] = style_dim
-        self.module_name_list.append('in_layer')
-      else:
-        raise NotImplementedError
-    else:
-      self.in_layer = None
-      _out_dim = input_dim
-
-    blocks = OrderedDict()
-    to_rbgs = OrderedDict()
-    for idx in range(num_blocks):
-
-      _in_dim = _out_dim
-      _out_dim = hidden_dim
-
-      name = f"{name_prefix}_b{idx}"
-      _block = ModSirenBlock(in_dim=_in_dim,
-                             out_dim=_out_dim,
-                             style_dim=style_dim,
-                             name_prefix=f'w_{name}')
-      self.style_dim_dict.update(_block.style_dim_dict)
-      blocks[name] = _block
-
-      _to_rgb = ToRGB(in_dim=_out_dim, dim_rgb=out_dim)
-      to_rbgs[name] = _to_rgb
-
-    self.blocks = nn.ModuleDict(blocks)
-    self.module_name_list.append('blocks')
-
-    if disable_to_rgb:
-      self.to_rgbs = None
-    else:
-      self.to_rgbs = nn.ModuleDict(to_rbgs)
-      self.module_name_list.append('to_rgbs')
-
-
-    if add_out_layer:
-      out_layers = []
-      if out_dim > 3:
-        out_layers.append(nn.Linear(out_dim, 3))
-      out_layers.append(nn.Tanh())
-
-      self.out_layer = nn.Sequential(*out_layers)
-      self.out_layer.apply(_frequency_init(100))
-      self.module_name_list.append('out_layer')
-    else:
-      self.out_layer = None
-
-    models_dict = {}
-    for name in self.module_name_list:
-      models_dict[name] = getattr(self, name)
-    models_dict[name_prefix] = self
-    logger = logging.getLogger('tl')
-    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
-    logger.info(self)
-    pass
-
-  def forward(self,
-              input,
-              style_dict,
-              block_end_index=None,
-              **kwargs):
-    """
-
-    :param input: points xyz, (b, num_points, 3)
-    :param style_dict:
-    :param kwargs:
-
-    :return
-
-    - out: (b, num_points, out_dim)
-
-    """
-    if block_end_index is None:
-      block_end_index = self.num_blocks
-
-    x = input
-
-    if self.in_layer is not None:
-      if self.in_layer_mode == 'siren':
-        if global_cfg.tl_debug:
-          VerboseModel.forward_verbose(self.in_layer,
-                                       inputs_args=(x, ),
-                                       name_prefix=f'{self.name_prefix}.in_layer.')
-        x = self.in_layer(x)
-      elif self.in_layer_mode == 'mod_film':
-        name = f"w_{self.name_prefix}_in"
-        style_ = style_dict[name]
-        if global_cfg.tl_debug:
-          VerboseModel.forward_verbose(self.in_layer,
-                                       inputs_args=(x, style_),
-                                       name_prefix=f'{self.name_prefix}.in_layer.')
-        x = self.in_layer(x, style_)
-      else:
-        raise NotImplementedError
-
-    rgb = None
-    for idx, (name, block) in enumerate(self.blocks.items()):
-
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(block,
-                                     inputs_args=(x, style_dict),
-                                     submodels=['mod1', 'mod2'],
-                                     name_prefix=f'{name}.')
-      x = block(x, style_dict)
-
-      if self.disable_to_rgb:
-        rgb = x
-      else:
-        if global_cfg.tl_debug:
-          VerboseModel.forward_verbose(self.to_rgbs[name],
-                                       inputs_args=(x, rgb),
-                                       name_prefix=f'{name}.to_rgb.')
-        rgb = self.to_rgbs[name](x, skip=rgb)
-
-      if idx + 1 == block_end_index:
-        break
-
-    if self.out_layer is not None:
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.out_layer,
-                                     inputs_args=(rgb, ),
-                                     name_prefix='out_layer.')
-      out = self.out_layer(rgb)
-    else:
-      out= rgb
-
-    return out
-
-
-class SigmaMul(nn.Module):
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               mul=20.):
-    super().__init__()
-
-    self.repr_str = f"mul={mul}"
-    self.mul = mul
-    pass
-
-  def forward(self, sigma):
-    return torch.where(sigma > 0, sigma * self.mul, sigma)
-
-
-class NeRF_Net(nn.Module):
-  """
-
-  """
-  def __repr__(self):
-    return tl2_utils.get_class_repr(self)
-
-  def __init__(self,
-               shape_net_cfg={},
-               app_net_cfg={},
-               name_prefix='nerf',
-               shape_block_end_index=None,
-               app_block_end_index=None,
-               sigma_mul=None,
-               **kwargs):
-    super().__init__()
-
-    self.repr_str = tl2_utils.dict2string(dict_obj={
-      'shape_net_cfg': shape_net_cfg,
-      'app_net_cfg': app_net_cfg,
-      'shape_block_end_index': shape_block_end_index,
-      'app_block_end_index': app_block_end_index,
-      'sigma_mul': sigma_mul,
-    })
-
-    self.shape_net_cfg = shape_net_cfg
-    self.app_net_cfg = app_net_cfg
-    self.name_prefix = name_prefix
-    self.shape_block_end_index = shape_block_end_index
-    self.app_block_end_index = app_block_end_index
-    self.sigma_mul = sigma_mul
-
-    self.module_name_list = []
-
-    # self.style_dim_dict = {}
-
-    self.shape_net = ModSIREN_Skip_Net(**{
-      **shape_net_cfg,
-      'name_prefix': 'shape'
-    })
-    self.style_dim_dict_shape = self.shape_net.style_dim_dict
-    self.module_name_list.append('shape_net')
-
-    # _in_dim = shape_net_cfg['out_dim']
-    _in_dim = self.shape_net.out_dim
-
-    self.sigma_layer = nn.Linear(_in_dim, 1)
-    # self.final_layer.apply(frequency_init(25))
-    self.module_name_list.append('sigma_layer')
-
-    if sigma_mul is not None:
-      self.sigma_mul_layer = SigmaMul(sigma_mul)
-      self.module_name_list.append('sigma_mul_layer')
-    else:
-      self.sigma_mul_layer = None
-
-    from . import cips_net
-
-    self.app_net = cips_net.CIPSNet(**{
-      **app_net_cfg,
-      'input_dim': _in_dim,
-      'name_prefix': 'app'
-    })
-    self.style_dim_dict_app = self.app_net.style_dim_dict
-    self.module_name_list.append('app_net')
-
-    # self.out_dim = app_net_cfg['out_dim']
-    self.out_dim = self.app_net.out_dim
-
-    # self.color_layer_linear = nn.Sequential(
-    #   nn.Linear(_out_dim, rgb_dim),
-    # )
-    # self.color_layer_linear.apply(init_func.kaiming_leaky_init)
-    # self.module_name_list.append('color_layer_linear')
-
-    logger = logging.getLogger('tl')
-    models_dict = {}
-    for name in self.module_name_list:
-      models_dict[name] = getattr(self, name)
-    models_dict[name_prefix] = self
-    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
-    logger.info(self)
-    pass
-
-  def forward(self,
-              x,
-              style_dict,
-              ray_directions=None,
-              **kwargs):
-    """
-
-    :param x: points xyz, (b, num_points, 3)
-    :param style_dict:
-    :param ray_directions: (b, num_points, 3)
-    :param kwargs:
-    :return
-
-    - out: (b, num_points, rgb_dim + 1), rgb(rgb_dim) + sigma(1)
-
-    """
-
-    # scale xyz
-
-    x = self.shape_net(x, style_dict, block_end_index=self.shape_block_end_index)
-
-    if global_cfg.tl_debug:
-      VerboseModel.forward_verbose(self.sigma_layer,
-                                   inputs_args=(x, ),
-                                   name_prefix="sigma_layer")
-    sigma = self.sigma_layer(x)
-
-    if self.sigma_mul_layer is not None:
-      if global_cfg.tl_debug:
-        VerboseModel.forward_verbose(self.sigma_mul_layer,
-                                     inputs_args=(sigma, ),
-                                     name_prefix="sigma_mul_layer")
-      sigma = self.sigma_mul_layer(sigma)
-
-    x = self.app_net(x, style_dict, block_end_index=self.app_block_end_index)
-
-    out = torch.cat([x, sigma], dim=-1)
-    return out
-
-
+import collections
+import math
+from collections import OrderedDict
+import numpy as np
+import logging
+from einops import rearrange
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from tl2 import tl2_utils
+from tl2.proj.pytorch import torch_utils
+from tl2.proj.pytorch import init_func
+from tl2.proj.pytorch.pytorch_hook import VerboseModel
+from tl2.proj.fvcore import global_cfg
+
+
+class Sine(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               w0=20.):
+    super().__init__()
+
+    self.repr_str = f"w0={w0}"
+
+    self.w0 = w0
+    pass
+
+  def forward(self, x):
+    return torch.sin(self.w0 * x)
+
+
+class SirenLayer(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               input_dim,
+               hidden_dim,
+               use_bias=True,
+               w0=1.,
+               is_first=False, # for initialization
+               **kwargs):
+    super().__init__()
+    self.repr_str = f"input_dim={input_dim}, " \
+                    f"hidden_dim={hidden_dim}, " \
+                    f"use_bias={use_bias}, " \
+                    f"w0={w0}, " \
+                    f"is_first={is_first}"
+
+    self.layer = nn.Linear(input_dim, hidden_dim, bias=use_bias)
+    self.activation = Sine(w0)
+    self.is_first = is_first
+    self.input_dim = input_dim
+    self.w0 = w0
+    self.c = 6
+    self.reset_parameters()
+    pass
+
+  def reset_parameters(self):
+    with torch.no_grad():
+      dim = self.input_dim
+      w_std = (1 / dim) if self.is_first else (math.sqrt(self.c / dim) / self.w0)
+      self.layer.weight.uniform_(-w_std, w_std)
+      if self.layer.bias is not None:
+        self.layer.bias.uniform_(-w_std, w_std)
+    pass
+
+  def forward(self, x):
+    out = self.layer(x)
+    out = self.activation(out)
+    return out
+
+
+class ModSirenLayer(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               input_dim,
+               hidden_dim,
+               style_dim,
+               w0=1.,
+               is_first=False, # for initialization
+               **kwargs):
+    super().__init__()
+    self.repr_str = f"input_dim={input_dim}, " \
+                    f"hidden_dim={hidden_dim}, " \
+                    f"style_dim={style_dim}, " \
+                    f"w0={w0}, " \
+                    f"is_first={is_first}"
+
+    from . import cips_net
+    # self.layer = nn.Linear(input_dim, hidden_dim, bias=use_bias)
+    self.layer = cips_net.ModFC(in_channel=input_dim,
+                                out_channel=hidden_dim,
+                                style_dim=style_dim)
+
+    self.activation = Sine(w0)
+    self.is_first = is_first
+    self.input_dim = input_dim
+    self.w0 = w0
+    self.c = 6
+    self.reset_parameters()
+    pass
+
+  def reset_parameters(self):
+    with torch.no_grad():
+      dim = self.input_dim
+      w_std = (1 / dim) if self.is_first else (math.sqrt(self.c / dim) / self.w0)
+      self.layer.weight.uniform_(-w_std, w_std)
+      # if self.layer.bias is not None:
+      #   self.layer.bias.uniform_(-w_std, w_std)
+    pass
+
+  def forward(self,
+              x,
+              style):
+    out = self.layer(x, style)
+    out = self.activation(out)
+    return out
+
+
+class SkipLayer(nn.Module):
+  def __init__(self, ):
+    super(SkipLayer, self).__init__()
+
+  def forward(self, x0, x1):
+    # out = (x0 + x1) / math.pi
+    out = (x0 + x1) / math.sqrt(2)
+    return out
+
+
+class ModSirenBlock(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               in_dim,
+               out_dim,
+               style_dim,
+               name_prefix,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = f"in_dim={in_dim}, " \
+                    f"out_dim={out_dim}, " \
+                    f"style_dim={style_dim})"
+
+    self.in_dim = in_dim
+    self.out_dim = out_dim
+    self.style_dim = style_dim
+    self.name_prefix = name_prefix
+
+    self.style_dim_dict = {}
+
+    self.mod1 = ModSirenLayer(input_dim=in_dim,
+                              hidden_dim=out_dim,
+                              style_dim=style_dim)
+    self.style_dim_dict[f'{name_prefix}_0'] = style_dim
+
+    self.mod2 = ModSirenLayer(input_dim=out_dim,
+                              hidden_dim=out_dim,
+                              style_dim=style_dim)
+    self.style_dim_dict[f'{name_prefix}_1'] = style_dim
+
+    self.skip = SkipLayer()
+    pass
+
+  def forward(self,
+              x,
+              style_dict,
+              skip=True):
+    """
+
+    :param x: (b, in_c, h, w), (b, in_c), (b, n, in_c)
+    :param style_dict:
+    :param skip:
+    :return:
+    """
+    x_orig = x
+
+    style = style_dict[f'{self.name_prefix}_0']
+    x = self.mod1(x, style)
+
+    style = style_dict[f'{self.name_prefix}_1']
+    x = self.mod2(x, style)
+
+    out = x
+
+    if skip and out.shape[-1] == x_orig.shape[-1]:
+      # out = (out + x_orig) / 1.41421
+      out = self.skip(out, x_orig)
+    return out
+
+
+class ToRGB(nn.Module):
+  def __init__(self,
+               in_dim,
+               dim_rgb=3):
+    super().__init__()
+    self.in_dim = in_dim
+    self.dim_rgb = dim_rgb
+
+    self.linear = nn.Linear(in_dim, dim_rgb)
+    pass
+
+  def forward(self,
+              input,
+              skip=None):
+
+    out = self.linear(input)
+
+    if skip is not None:
+      out = out + skip
+    return out
+
+
+def _frequency_init(freq):
+  def init(m):
+    with torch.no_grad():
+      if isinstance(m, nn.Linear):
+        num_input = m.weight.size(-1)
+        m.weight.uniform_(-np.sqrt(6 / num_input) / freq, np.sqrt(6 / num_input) / freq)
+
+  return init
+
+
+class ModSIREN_Skip_Net(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               input_dim,
+               hidden_dim,
+               out_dim,
+               style_dim,
+               num_blocks,
+               device=None,
+               name_prefix='siren_skip',
+               add_out_layer=False,
+               add_in_layer=False,
+               in_layer_mode='siren', # [siren, mod_film]
+               xyz_sine_w=20,
+               disable_to_rgb=False,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'input_dim': input_dim,
+      'hidden_dim': hidden_dim,
+      'out_dim': out_dim,
+      'style_dim': style_dim,
+      'num_blocks': num_blocks,
+      'add_out_layer': add_out_layer,
+      'add_in_layer': add_in_layer,
+      'in_layer_mode': in_layer_mode,
+      'xyz_sine_w': xyz_sine_w,
+      'disable_to_rgb': disable_to_rgb,
+    }, prefix_str=name_prefix)
+
+    self.device = device
+    self.name_prefix = name_prefix
+    self.num_blocks = num_blocks
+    self.in_layer_mode = in_layer_mode
+    self.disable_to_rgb = disable_to_rgb
+
+    if disable_to_rgb:
+      self.out_dim = hidden_dim
+    else:
+      self.out_dim = out_dim
+
+    self.module_name_list = []
+    self.style_dim_dict = {}
+
+    if add_in_layer:
+      _in_dim = input_dim
+      _out_dim = hidden_dim
+
+      if in_layer_mode == 'siren':
+        self.in_layer = SirenLayer(input_dim=_in_dim, hidden_dim=_out_dim,
+                                   w0=xyz_sine_w, is_first=True)
+        self.module_name_list.append('in_layer')
+      elif in_layer_mode == 'mod_film':
+        from .siren_net import ModFiLMLayer, _first_layer_film_sine_init
+
+        self.in_layer = ModFiLMLayer(in_dim=_in_dim, out_dim=_out_dim, style_dim=style_dim)
+        self.in_layer.fc_layer.apply(_first_layer_film_sine_init)
+        name = f"{name_prefix}_in"
+        self.style_dim_dict[f"w_{name}"] = style_dim
+        self.module_name_list.append('in_layer')
+      else:
+        raise NotImplementedError
+    else:
+      self.in_layer = None
+      _out_dim = input_dim
+
+    blocks = OrderedDict()
+    to_rbgs = OrderedDict()
+    for idx in range(num_blocks):
+
+      _in_dim = _out_dim
+      _out_dim = hidden_dim
+
+      name = f"{name_prefix}_b{idx}"
+      _block = ModSirenBlock(in_dim=_in_dim,
+                             out_dim=_out_dim,
+                             style_dim=style_dim,
+                             name_prefix=f'w_{name}')
+      self.style_dim_dict.update(_block.style_dim_dict)
+      blocks[name] = _block
+
+      _to_rgb = ToRGB(in_dim=_out_dim, dim_rgb=out_dim)
+      to_rbgs[name] = _to_rgb
+
+    self.blocks = nn.ModuleDict(blocks)
+    self.module_name_list.append('blocks')
+
+    if disable_to_rgb:
+      self.to_rgbs = None
+    else:
+      self.to_rgbs = nn.ModuleDict(to_rbgs)
+      self.module_name_list.append('to_rgbs')
+
+
+    if add_out_layer:
+      out_layers = []
+      if out_dim > 3:
+        out_layers.append(nn.Linear(out_dim, 3))
+      out_layers.append(nn.Tanh())
+
+      self.out_layer = nn.Sequential(*out_layers)
+      self.out_layer.apply(_frequency_init(100))
+      self.module_name_list.append('out_layer')
+    else:
+      self.out_layer = None
+
+    models_dict = {}
+    for name in self.module_name_list:
+      models_dict[name] = getattr(self, name)
+    models_dict[name_prefix] = self
+    logger = logging.getLogger('tl')
+    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
+    logger.info(self)
+    pass
+
+  def forward(self,
+              input,
+              style_dict,
+              block_end_index=None,
+              **kwargs):
+    """
+
+    :param input: points xyz, (b, num_points, 3)
+    :param style_dict:
+    :param kwargs:
+
+    :return
+
+    - out: (b, num_points, out_dim)
+
+    """
+    if block_end_index is None:
+      block_end_index = self.num_blocks
+
+    x = input
+
+    if self.in_layer is not None:
+      if self.in_layer_mode == 'siren':
+        if global_cfg.tl_debug:
+          VerboseModel.forward_verbose(self.in_layer,
+                                       inputs_args=(x, ),
+                                       name_prefix=f'{self.name_prefix}.in_layer.')
+        x = self.in_layer(x)
+      elif self.in_layer_mode == 'mod_film':
+        name = f"w_{self.name_prefix}_in"
+        style_ = style_dict[name]
+        if global_cfg.tl_debug:
+          VerboseModel.forward_verbose(self.in_layer,
+                                       inputs_args=(x, style_),
+                                       name_prefix=f'{self.name_prefix}.in_layer.')
+        x = self.in_layer(x, style_)
+      else:
+        raise NotImplementedError
+
+    rgb = None
+    for idx, (name, block) in enumerate(self.blocks.items()):
+
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(block,
+                                     inputs_args=(x, style_dict),
+                                     submodels=['mod1', 'mod2'],
+                                     name_prefix=f'{name}.')
+      x = block(x, style_dict)
+
+      if self.disable_to_rgb:
+        rgb = x
+      else:
+        if global_cfg.tl_debug:
+          VerboseModel.forward_verbose(self.to_rgbs[name],
+                                       inputs_args=(x, rgb),
+                                       name_prefix=f'{name}.to_rgb.')
+        rgb = self.to_rgbs[name](x, skip=rgb)
+
+      if idx + 1 == block_end_index:
+        break
+
+    if self.out_layer is not None:
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.out_layer,
+                                     inputs_args=(rgb, ),
+                                     name_prefix='out_layer.')
+      out = self.out_layer(rgb)
+    else:
+      out= rgb
+
+    return out
+
+
+class SigmaMul(nn.Module):
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               mul=20.):
+    super().__init__()
+
+    self.repr_str = f"mul={mul}"
+    self.mul = mul
+    pass
+
+  def forward(self, sigma):
+    return torch.where(sigma > 0, sigma * self.mul, sigma)
+
+
+class NeRF_Net(nn.Module):
+  """
+
+  """
+  def __repr__(self):
+    return tl2_utils.get_class_repr(self)
+
+  def __init__(self,
+               shape_net_cfg={},
+               app_net_cfg={},
+               name_prefix='nerf',
+               shape_block_end_index=None,
+               app_block_end_index=None,
+               sigma_mul=None,
+               **kwargs):
+    super().__init__()
+
+    self.repr_str = tl2_utils.dict2string(dict_obj={
+      'shape_net_cfg': shape_net_cfg,
+      'app_net_cfg': app_net_cfg,
+      'shape_block_end_index': shape_block_end_index,
+      'app_block_end_index': app_block_end_index,
+      'sigma_mul': sigma_mul,
+    })
+
+    self.shape_net_cfg = shape_net_cfg
+    self.app_net_cfg = app_net_cfg
+    self.name_prefix = name_prefix
+    self.shape_block_end_index = shape_block_end_index
+    self.app_block_end_index = app_block_end_index
+    self.sigma_mul = sigma_mul
+
+    self.module_name_list = []
+
+    # self.style_dim_dict = {}
+
+    self.shape_net = ModSIREN_Skip_Net(**{
+      **shape_net_cfg,
+      'name_prefix': 'shape'
+    })
+    self.style_dim_dict_shape = self.shape_net.style_dim_dict
+    self.module_name_list.append('shape_net')
+
+    # _in_dim = shape_net_cfg['out_dim']
+    _in_dim = self.shape_net.out_dim
+
+    self.sigma_layer = nn.Linear(_in_dim, 1)
+    # self.final_layer.apply(frequency_init(25))
+    self.module_name_list.append('sigma_layer')
+
+    if sigma_mul is not None:
+      self.sigma_mul_layer = SigmaMul(sigma_mul)
+      self.module_name_list.append('sigma_mul_layer')
+    else:
+      self.sigma_mul_layer = None
+
+    from . import cips_net
+
+    self.app_net = cips_net.CIPSNet(**{
+      **app_net_cfg,
+      'input_dim': _in_dim,
+      'name_prefix': 'app'
+    })
+    self.style_dim_dict_app = self.app_net.style_dim_dict
+    self.module_name_list.append('app_net')
+
+    # self.out_dim = app_net_cfg['out_dim']
+    self.out_dim = self.app_net.out_dim
+
+    # self.color_layer_linear = nn.Sequential(
+    #   nn.Linear(_out_dim, rgb_dim),
+    # )
+    # self.color_layer_linear.apply(init_func.kaiming_leaky_init)
+    # self.module_name_list.append('color_layer_linear')
+
+    logger = logging.getLogger('tl')
+    models_dict = {}
+    for name in self.module_name_list:
+      models_dict[name] = getattr(self, name)
+    models_dict[name_prefix] = self
+    torch_utils.print_number_params(models_dict=models_dict, logger=logger)
+    logger.info(self)
+    pass
+
+  def forward(self,
+              x,
+              style_dict,
+              ray_directions=None,
+              **kwargs):
+    """
+
+    :param x: points xyz, (b, num_points, 3)
+    :param style_dict:
+    :param ray_directions: (b, num_points, 3)
+    :param kwargs:
+    :return
+
+    - out: (b, num_points, rgb_dim + 1), rgb(rgb_dim) + sigma(1)
+
+    """
+
+    # scale xyz
+
+    x = self.shape_net(x, style_dict, block_end_index=self.shape_block_end_index)
+
+    if global_cfg.tl_debug:
+      VerboseModel.forward_verbose(self.sigma_layer,
+                                   inputs_args=(x, ),
+                                   name_prefix="sigma_layer")
+    sigma = self.sigma_layer(x)
+
+    if self.sigma_mul_layer is not None:
+      if global_cfg.tl_debug:
+        VerboseModel.forward_verbose(self.sigma_mul_layer,
+                                     inputs_args=(sigma, ),
+                                     name_prefix="sigma_mul_layer")
+      sigma = self.sigma_mul_layer(sigma)
+
+    x = self.app_net(x, style_dict, block_end_index=self.app_block_end_index)
+
+    out = torch.cat([x, sigma], dim=-1)
+    return out
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_cips_net.py` & `tl2-0.1.1/tl2/proj/pl/tests/test_pytorch_lightning.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,236 +1,211 @@
 import os
 import sys
 import unittest
 import argparse
 
-import torch
 
-
-class Testing_cips_net(unittest.TestCase):
-
-  def test__build_ModFC(self, debug=True):
+class Testing_lightning_2_steps(unittest.TestCase):
+  
+  def test_train_ae(self, debug=True):
     """
     Usage:
 
         # export CUDA_VISIBLE_DEVICES=$cuda_devices
         # export RUN_NUM=$run_num
 
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export CUDA_VISIBLE_DEVICES=0
         export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
+        export TIME_STR=0
+        export PYTHONPATH=.:tl2_lib:exp
+        python -c "from tl2_lib.tl2.proj.pl.tests.test_pytorch_lightning import Testing_lightning_2_steps;\
+          Testing_lightning_2_steps().test_train_ae(debug=False)" \
+          --tl_opts
 
     :return:
     """
     if 'CUDA_VISIBLE_DEVICES' not in os.environ:
       os.environ['CUDA_VISIBLE_DEVICES'] = '0'
     if 'TIME_STR' not in os.environ:
       os.environ['TIME_STR'] = '0'
     if 'RUN_NUM' not in os.environ:
       os.environ['RUN_NUM'] = '0'
     from tl2 import tl2_utils
     from tl2.launch.launch_utils import \
       (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
+    
     tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
     tl_opts = ' '.join(tl_opts_list)
     print(f'tl_opts:\n {tl_opts}')
-
+    
     if debug:
       # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
       pass
     command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
     resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
              tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
     argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/cips_net.yaml
+                --tl_config_file tl2_lib/tl2/proj/pl/configs/lightning_2_steps.yaml
                 --tl_command {command}
                 --tl_outdir {outdir}
                 {"--tl_resume --tl_resumedir " + outdir if resume else ""}
                 --tl_opts {tl_opts}
                 """
     args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import cips_net
-
-    mod_fc = cips_net.ModFC(use_group_conv=True, **cfg).cuda()
-
-    bs = 4
-    N = 10
-    in_dim = cfg.in_channel
-    style_dim = cfg.style_dim
-
-    x = torch.rand(bs, N, in_dim).cuda()
-    style = torch.rand(bs, style_dim).cuda()
-
-    out_gc = mod_fc(x, style)
-
-    out_bmm = mod_fc(x, style, force_bmm=True)
-
-    err = (out_gc - out_bmm).abs().max()
-
-    pass
-
-  def test__build_ModFCBlock(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
+    
+    if int(os.environ['RUN_NUM']) > 0:
+      run_command = f"""
+                  python -c "from tl2.modelarts.tests.test_run import TestingRun;\
+                        TestingRun().test_run_v2(number={os.environ['RUN_NUM']}, )" \
+                        --tl_opts root_obs {cfg.root_obs}
+                  """
+      p = tl2_utils.Worker(name='Run', args=(run_command,))
+      p.start()
+    
+    os.environ['DNNLIB_CACHE_DIR'] = "cache_dnnlib"
+    os.environ['TORCH_EXTENSIONS_DIR'] = "cache_torch_extensions"
+    os.environ['PATH'] = f"{os.path.dirname(sys.executable)}:{os.environ['PATH']}"
+    os.environ['MAX_JOBS '] = "8"
+    
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    PORT = os.environ.get('PORT', 8888)
+    
+    if n_gpus > 1:
+      python_str = f"python -m torch.distributed.launch --nproc_per_node={n_gpus} --master_port={PORT} "
+    else:
+      python_str = "python "
+    
+    cmd_str = f"""
+        {python_str}
+        tl2_lib/tl2/proj/pl/examples/lightning_2_steps/module.py
+        {get_append_cmd_str(args)}
+        """
     if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/cips_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-    from tl2.proj.pytorch.examples.networks import multi_head_mapping
-    from tl2.proj.pytorch.examples.networks import cips_net
-
-
-    mod_fc_block = cips_net.ModFCBlock(name_prefix='w', **cfg).cuda()
-    head_dim_dict = mod_fc_block.style_dim_dict
-    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg.mapping_cfg).cuda()
-
-    bs = 4
-    N = 10
-    in_dim = cfg.in_dim
-
-    z = torch.randn(bs, cfg.mapping_cfg.z_dim).cuda()
-    style_dict = mapping_net(z)
-
-    x = torch.randn(bs, N, in_dim).cuda()
-
-    out = mod_fc_block(x, style_dict=style_dict)
-
-    VerboseModel.forward_verbose(mapping_net,
-                                 inputs_args=(z, ),
-                                 name_prefix='mapping.',
-                                 )
-
-    VerboseModel.forward_verbose(mod_fc_block,
-                                 inputs_args=(x, style_dict),
-                                 name_prefix='block.',
-                                 )
+      cmd_str += f"""
+                  --tl_debug
+                  --tl_opts
+                  """
+    else:
+      cmd_str += f"""
+                  --tl_opts {tl_opts}
+                  """
+    start_cmd_run(cmd_str)
+    # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
+    # from tl2.modelarts import moxing_utils
+    
+    # update_parser_defaults_from_yaml(parser)
+    # if rank == 0:
+    #   moxing_utils.setup_tl_outdir_obs(global_cfg)
+    #   moxing_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    
+    # moxing_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    
+    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
+    #
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
     pass
 
-  def test__build_CIPSNet(self, debug=True):
+class Testing_step_by_step(unittest.TestCase):
+
+  def test_train_step_by_step(self, debug=True):
     """
     Usage:
 
         # export CUDA_VISIBLE_DEVICES=$cuda_devices
         # export RUN_NUM=$run_num
 
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export CUDA_VISIBLE_DEVICES=0
         export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
+        export TIME_STR=0
+        export PYTHONPATH=.:tl2_lib:exp
+        python -c "from tl2_lib.tl2.proj.pl.tests.test_pytorch_lightning import Testing_step_by_step;\
+          Testing_step_by_step().test_train_step_by_step(debug=False)" \
+          --tl_opts
 
     :return:
     """
     if 'CUDA_VISIBLE_DEVICES' not in os.environ:
       os.environ['CUDA_VISIBLE_DEVICES'] = '0'
     if 'TIME_STR' not in os.environ:
       os.environ['TIME_STR'] = '0'
     if 'RUN_NUM' not in os.environ:
       os.environ['RUN_NUM'] = '0'
     from tl2 import tl2_utils
     from tl2.launch.launch_utils import \
       (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
+  
     tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
     tl_opts = ' '.join(tl_opts_list)
     print(f'tl_opts:\n {tl_opts}')
-
+  
     if debug:
       # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
       pass
     command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
     resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
              tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
     argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/cips_net.yaml
+                --tl_config_file tl2_lib/tl2/proj/pl/configs/step_by_step.yaml
                 --tl_command {command}
                 --tl_outdir {outdir}
                 {"--tl_resume --tl_resumedir " + outdir if resume else ""}
                 --tl_opts {tl_opts}
-                --tl_debug True
                 """
     args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-    from tl2.proj.pytorch.examples.networks import multi_head_mapping
-    from tl2.proj.pytorch.examples.networks import cips_net
-
-    cips_net = cips_net.CIPSNet(**cfg).cuda()
-    head_dim_dict = cips_net.style_dim_dict
-    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg.mapping_cfg).cuda()
-
-    bs = 4
-    N = 10
-    in_dim = cfg.input_dim
-
-    z = torch.randn(bs, cfg.mapping_cfg.z_dim).cuda()
-    style_dict = mapping_net(z)
-
-    x = torch.randn(bs, N, in_dim).cuda()
-
-    out = cips_net(x, style_dict=style_dict, block_end_index=0)
-
-    VerboseModel.forward_verbose(mapping_net,
-                                 inputs_args=(z,),
-                                 name_prefix='mapping.',
-                                 )
-
-    # VerboseModel.forward_verbose(cips_net,
-    #                              inputs_args=(x, style_dict),
-    #                              name_prefix='cips_net.',
-    #                              )
-    pass
-
+  
+    if int(os.environ['RUN_NUM']) > 0:
+      run_command = f"""
+                  python -c "from tl2.modelarts.tests.test_run import TestingRun;\
+                        TestingRun().test_run_v2(number={os.environ['RUN_NUM']}, )" \
+                        --tl_opts root_obs {cfg.root_obs}
+                  """
+      p = tl2_utils.Worker(name='Run', args=(run_command,))
+      p.start()
+  
+    os.environ['DNNLIB_CACHE_DIR'] = "cache_dnnlib"
+    os.environ['TORCH_EXTENSIONS_DIR'] = "cache_torch_extensions"
+    os.environ['PATH'] = f"{os.path.dirname(sys.executable)}:{os.environ['PATH']}"
+    os.environ['MAX_JOBS '] = "8"
+  
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    PORT = os.environ.get('PORT', 8888)
+  
+    if n_gpus > 1:
+      python_str = f"python -m torch.distributed.launch --nproc_per_node={n_gpus} --master_port={PORT} "
+    else:
+      python_str = "python "
+  
+    cmd_str = f"""
+        {python_str}
+        tl2_lib/tl2/proj/pl/examples/step_by_step/module.py
+        {get_append_cmd_str(args)}
+        """
+    if debug:
+      cmd_str += f"""
+                  --tl_debug
+                  --tl_opts
+                  """
+    else:
+      cmd_str += f"""
+                  --tl_opts {tl_opts}
+                  """
+    start_cmd_run(cmd_str)
+    # from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
+    # from tl2.modelarts import moxing_utils
+  
+    # update_parser_defaults_from_yaml(parser)
+    # if rank == 0:
+    #   moxing_utils.setup_tl_outdir_obs(global_cfg)
+    #   moxing_utils.modelarts_sync_results_dir(global_cfg, join=True)
+  
+    # moxing_utils.modelarts_sync_results_dir(global_cfg, join=True)
+  
+    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
+    #
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    pass
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_multi_head_mapping.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/test_multi_head_mapping.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,79 +1,79 @@
-import os
-import sys
-import unittest
-import argparse
-
-import torch
-
-
-class Testing_multi_head_mapping(unittest.TestCase):
-
-  def test__build_MultiHeadMappingNetwork(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/multi_head_mapping.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-    from tl2.proj.pytorch.examples.networks import multi_head_mapping
-
-
-    head_dim_dict = {
-      'w0': 512,
-      'w1': 512
-    }
-    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg).cuda()
-
-    bs = 4
-    in_dim = cfg.z_dim
-
-    x = torch.rand(bs, in_dim).cuda()
-    style = mapping_net(x)
-
-    VerboseModel.forward_verbose(mapping_net,
-                                 inputs_args=(x, ),
-                                 submodels=['base_net'],
-                                 name_prefix='mapping.')
-    pass
+import os
+import sys
+import unittest
+import argparse
+
+import torch
+
+
+class Testing_multi_head_mapping(unittest.TestCase):
+
+  def test__build_MultiHeadMappingNetwork(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/multi_head_mapping.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+    from tl2.proj.pytorch.examples.networks import multi_head_mapping
+
+
+    head_dim_dict = {
+      'w0': 512,
+      'w1': 512
+    }
+    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg).cuda()
+
+    bs = 4
+    in_dim = cfg.z_dim
+
+    x = torch.rand(bs, in_dim).cuda()
+    style = mapping_net(x)
+
+    VerboseModel.forward_verbose(mapping_net,
+                                 inputs_args=(x, ),
+                                 submodels=['base_net'],
+                                 name_prefix='mapping.')
+    pass
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_nerf_net.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/test_nerf_net.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,322 +1,322 @@
-import os
-import sys
-import unittest
-import argparse
-from einops import rearrange
-
-import torch
-
-
-class Testing_nerf_net(unittest.TestCase):
-
-  def test__build_PosEmbedding(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/nerf_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import nerf_net
-
-    net = nerf_net.PosEmbedding(**cfg).cuda()
-
-    bs = 4
-    N = 10
-    in_dim = 3
-
-    x = torch.rand(bs, N, in_dim, requires_grad=True).cuda()
-
-    out = net(x)
-
-    pass
-
-  def test__build_NeRFNetwork_CIPS(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/nerf_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import nerf_net
-    from . import multi_head_mapping
-    from ..nerf import volume_rendering
-    from ..nerf import cam_params
-
-    net = nerf_net.NeRFNetwork_CIPS(**cfg).cuda()
-
-    bs = 4
-    H = 64
-    W = 64
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=1,
-                                                 H0=H,
-                                                 W0=W,
-                                                 so3_repr='axis-angle',
-                                                 intr_repr='square',
-                                                 initial_fov=53.13,
-                                                 freeze_intr=False).cuda()
-
-    intr = cam_param(mode='get_intrinsic')
-
-    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-      device='cuda',
-      bs=bs,
-      intr=intr,
-      h_stddev=0.3,
-      v_stddev=0.155,
-    )
-
-    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
-
-    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                        rays_d=rays_d,
-                                                        near=0.5,
-                                                        far=1.5,
-                                                        N_samples=24,
-                                                        perturb=0)
-
-    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
-    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
-    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
-
-    z_dim = 128
-    style_dim = 128
-
-    mapping_shape = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
-                                                               hidden_dim=z_dim,
-                                                               base_layers=4,
-                                                               head_layers=0,
-                                                               head_dim_dict=net.style_dim_dict_shape,
-                                                               add_norm=True,
-                                                               norm_out=True).cuda()
-    mapping_app = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
-                                                             hidden_dim=z_dim,
-                                                             base_layers=4,
-                                                             head_layers=0,
-                                                             head_dim_dict=net.style_dim_dict_app,
-                                                             add_norm=True,
-                                                             norm_out=True).cuda()
-
-    z_shape = torch.randn(bs, z_dim).cuda()
-    z_app = torch.randn(bs, z_dim).cuda()
-
-    style_dict = {}
-    style_dict.update(mapping_shape(z_shape))
-    style_dict.update(mapping_app(z_app))
-
-    points = rearrange(points, "b Nr Ns c -> b (Nr Ns) c")
-    # points = points[:, 0]
-    out = net(points, style_dict)
-
-    out.mean().backward()
-    pass
-
-  def test__build_NeRFNetwork_SIREN_skip(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/nerf_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import nerf_net
-    from . import multi_head_mapping
-    from ..nerf import volume_rendering
-    from ..nerf import cam_params
-
-    net = nerf_net.NeRFNetwork_SIREN_skip(**cfg).cuda()
-
-    bs = 4
-    H = 64
-    W = 64
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=1,
-                                                 H0=H,
-                                                 W0=W,
-                                                 so3_repr='axis-angle',
-                                                 intr_repr='square',
-                                                 initial_fov=53.13,
-                                                 freeze_intr=False).cuda()
-
-    intr = cam_param(mode='get_intrinsic')
-
-    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-      device='cuda',
-      bs=bs,
-      intr=intr,
-      h_stddev=0.3,
-      v_stddev=0.155,
-    )
-
-    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
-
-    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                        rays_d=rays_d,
-                                                        near=0.5,
-                                                        far=1.5,
-                                                        N_samples=24,
-                                                        perturb=0)
-
-    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
-    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
-    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
-
-    z_dim = 128
-    style_dim = 128
-
-    mapping_shape = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
-                                                               hidden_dim=net.shape_net_cfg.style_dim,
-                                                               base_layers=4,
-                                                               head_layers=0,
-                                                               head_dim_dict=net.style_dim_dict_shape,
-                                                               add_norm=True,
-                                                               norm_out=True).cuda()
-    mapping_app = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
-                                                             hidden_dim=net.app_net_cfg.style_dim,
-                                                             base_layers=4,
-                                                             head_layers=0,
-                                                             head_dim_dict=net.style_dim_dict_app,
-                                                             add_norm=True,
-                                                             norm_out=True).cuda()
-
-    z_shape = torch.randn(bs, z_dim).cuda()
-    z_app = torch.randn(bs, z_dim).cuda()
-
-    style_dict = {}
-    style_dict.update(mapping_shape(z_shape))
-    style_dict.update(mapping_app(z_app))
-
-    points = rearrange(points, "b Nr Ns c -> b (Nr Ns) c")
-    # points = points[:, 0]
-    out = net(points, style_dict)
-
-    out.mean().backward()
-    pass
-
-
+import os
+import sys
+import unittest
+import argparse
+from einops import rearrange
+
+import torch
+
+
+class Testing_nerf_net(unittest.TestCase):
+
+  def test__build_PosEmbedding(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/nerf_net.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.examples.networks import nerf_net
+
+    net = nerf_net.PosEmbedding(**cfg).cuda()
+
+    bs = 4
+    N = 10
+    in_dim = 3
+
+    x = torch.rand(bs, N, in_dim, requires_grad=True).cuda()
+
+    out = net(x)
+
+    pass
+
+  def test__build_NeRFNetwork_CIPS(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/nerf_net.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.examples.networks import nerf_net
+    from . import multi_head_mapping
+    from ..nerf import volume_rendering
+    from ..nerf import cam_params
+
+    net = nerf_net.NeRFNetwork_CIPS(**cfg).cuda()
+
+    bs = 4
+    H = 64
+    W = 64
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=1,
+                                                 H0=H,
+                                                 W0=W,
+                                                 so3_repr='axis-angle',
+                                                 intr_repr='square',
+                                                 initial_fov=53.13,
+                                                 freeze_intr=False).cuda()
+
+    intr = cam_param(mode='get_intrinsic')
+
+    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+      device='cuda',
+      bs=bs,
+      intr=intr,
+      h_stddev=0.3,
+      v_stddev=0.155,
+    )
+
+    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
+    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
+
+    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                        rays_d=rays_d,
+                                                        near=0.5,
+                                                        far=1.5,
+                                                        N_samples=24,
+                                                        perturb=0)
+
+    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
+    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
+    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
+
+    z_dim = 128
+    style_dim = 128
+
+    mapping_shape = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
+                                                               hidden_dim=z_dim,
+                                                               base_layers=4,
+                                                               head_layers=0,
+                                                               head_dim_dict=net.style_dim_dict_shape,
+                                                               add_norm=True,
+                                                               norm_out=True).cuda()
+    mapping_app = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
+                                                             hidden_dim=z_dim,
+                                                             base_layers=4,
+                                                             head_layers=0,
+                                                             head_dim_dict=net.style_dim_dict_app,
+                                                             add_norm=True,
+                                                             norm_out=True).cuda()
+
+    z_shape = torch.randn(bs, z_dim).cuda()
+    z_app = torch.randn(bs, z_dim).cuda()
+
+    style_dict = {}
+    style_dict.update(mapping_shape(z_shape))
+    style_dict.update(mapping_app(z_app))
+
+    points = rearrange(points, "b Nr Ns c -> b (Nr Ns) c")
+    # points = points[:, 0]
+    out = net(points, style_dict)
+
+    out.mean().backward()
+    pass
+
+  def test__build_NeRFNetwork_SIREN_skip(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/nerf_net.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.examples.networks import nerf_net
+    from . import multi_head_mapping
+    from ..nerf import volume_rendering
+    from ..nerf import cam_params
+
+    net = nerf_net.NeRFNetwork_SIREN_skip(**cfg).cuda()
+
+    bs = 4
+    H = 64
+    W = 64
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=1,
+                                                 H0=H,
+                                                 W0=W,
+                                                 so3_repr='axis-angle',
+                                                 intr_repr='square',
+                                                 initial_fov=53.13,
+                                                 freeze_intr=False).cuda()
+
+    intr = cam_param(mode='get_intrinsic')
+
+    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+      device='cuda',
+      bs=bs,
+      intr=intr,
+      h_stddev=0.3,
+      v_stddev=0.155,
+    )
+
+    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
+    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
+
+    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                        rays_d=rays_d,
+                                                        near=0.5,
+                                                        far=1.5,
+                                                        N_samples=24,
+                                                        perturb=0)
+
+    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
+    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
+    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
+
+    z_dim = 128
+    style_dim = 128
+
+    mapping_shape = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
+                                                               hidden_dim=net.shape_net_cfg.style_dim,
+                                                               base_layers=4,
+                                                               head_layers=0,
+                                                               head_dim_dict=net.style_dim_dict_shape,
+                                                               add_norm=True,
+                                                               norm_out=True).cuda()
+    mapping_app = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
+                                                             hidden_dim=net.app_net_cfg.style_dim,
+                                                             base_layers=4,
+                                                             head_layers=0,
+                                                             head_dim_dict=net.style_dim_dict_app,
+                                                             add_norm=True,
+                                                             norm_out=True).cuda()
+
+    z_shape = torch.randn(bs, z_dim).cuda()
+    z_app = torch.randn(bs, z_dim).cuda()
+
+    style_dict = {}
+    style_dict.update(mapping_shape(z_shape))
+    style_dict.update(mapping_app(z_app))
+
+    points = rearrange(points, "b Nr Ns c -> b (Nr Ns) c")
+    # points = points[:, 0]
+    out = net(points, style_dict)
+
+    out.mean().backward()
+    pass
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_pigan_net.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/test_pigan_net.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,556 +1,556 @@
-import itertools
-import os
-import sys
-import unittest
-import argparse
-from einops import rearrange
-
-import torch
-import torch.nn as nn
-
-from tl2.proj.pytorch.pytorch_hook import VerboseModel
-from tl2.proj.pytorch import torch_utils
-
-
-class Testing_pigan_net(unittest.TestCase):
-
-  def test__build_ScaleLinear(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/pigan_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import itertools
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-    from tl2.proj.pytorch.examples.networks import multi_head_mapping
-    from tl2.proj.pytorch.examples.networks import pigan_net
-    from tl2.proj.pytorch import torch_utils
-
-    net = pigan_net.ScaleLinear(**{**cfg,
-                                   'gamma': 10.,
-                                   'scale_gradients': True}).cuda()
-
-    bs = 4
-    N = 10
-    in_dim = cfg.in_features
-
-    x = torch.randn(bs, N, in_dim, requires_grad=True).cuda()
-    x = nn.Parameter(x)
-
-    # scale_w false
-    x.grad = None
-    net.scale_gradients = False
-    net.gamma = 1
-    net.beta = 0
-    net.zero_grad(set_to_none=True)
-    VerboseModel.forward_verbose(net,
-                                 inputs_args=(x,),
-                                 name_prefix='ori.')
-    out = net(x)
-    out.mean().backward()
-
-    grad_str = torch_utils.get_grad_norm_string(itertools.chain(net.named_parameters(), [('x', x)]))
-    print(grad_str)
-
-    # scale_w true
-    x.grad = None
-    net.scale_gradients = False
-    net.gamma = 10
-    net.beta = 30
-    net.zero_grad(set_to_none=True)
-    VerboseModel.forward_verbose(net,
-                                 inputs_args=(x,),
-                                 name_prefix='no_scale.')
-    out = net(x)
-    out.mean().backward()
-
-    grad_str = torch_utils.get_grad_norm_string(itertools.chain(net.named_parameters(), [('x', x)]))
-    print(grad_str)
-
-
-    # scale_gradients true
-    x.grad = None
-    net.scale_gradients = True
-    net.gamma = 10
-    net.beta = 30
-    net.zero_grad(set_to_none=True)
-    VerboseModel.forward_verbose(net,
-                                 inputs_args=(x,),
-                                 name_prefix='scale.')
-    out = net(x)
-    out.mean().backward()
-
-    grad_str = torch_utils.get_grad_norm_string(itertools.chain(net.named_parameters(), [('x', x)]))
-    print(grad_str)
-
-    pass
-
-  def test__build_ModFiLMLayer(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/pigan_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import itertools
-    from tl2.proj.pytorch.examples.networks import pigan_net
-
-    torch_utils.init_seeds()
-
-    net = pigan_net.ModFiLMLayer(**cfg).cuda()
-
-    bs = 4
-    N = 10
-    in_dim = cfg.in_dim
-    style_dim = cfg.style_dim
-
-    x = torch.randn(bs, N, in_dim, requires_grad=True).cuda()
-    x = nn.Parameter(x)
-    style = torch.randn(bs, style_dim).cuda()
-    style = nn.Parameter(style)
-
-    VerboseModel.forward_verbose(net,
-                                 inputs_args=(x, style),
-                                 name_prefix='net3.')
-    out_3 = net(x, style)
-
-    out_3.mean().backward()
-    grad_str = torch_utils.get_grad_norm_string(itertools.chain(net.named_parameters(),
-                                                                [('x', x),
-                                                                 ('style', style)]))
-    print(grad_str)
-
-    x = torch.randn(bs, in_dim, requires_grad=True).cuda()
-    style = torch.randn(bs, style_dim).cuda()
-
-    VerboseModel.forward_verbose(net,
-                                 inputs_args=(x, style),
-                                 name_prefix='net2.')
-    out_2 = net(x, style)
-
-    pass
-
-  def test__build_ModSIREN_Net(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/pigan_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-    from tl2.proj.pytorch.examples.networks import multi_head_mapping
-    from tl2.proj.pytorch.examples.networks import pigan_net
-
-    torch_utils.init_seeds()
-
-    net = pigan_net.ModSIREN_Net(**cfg).cuda()
-    head_dim_dict = net.style_dim_dict
-    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg.mapping_cfg).cuda()
-
-    bs = 4
-    N = 10
-    in_dim = cfg.input_dim
-
-    z = torch.randn(bs, cfg.mapping_cfg.z_dim).cuda()
-    z = nn.Parameter(z)
-    style_dict = mapping_net(z)
-
-    x = torch.randn(bs, N, in_dim).cuda()
-    x = nn.Parameter(x)
-
-    out = net(x, style_dict=style_dict)
-
-    out.mean().backward()
-    grad_str = torch_utils.get_grad_norm_string(itertools.chain(net.named_parameters(),
-                                                                mapping_net.named_parameters(),
-                                                                [('x', x),
-                                                                 ('z', z)]))
-    print(grad_str)
-
-    # VerboseModel.forward_verbose(mapping_net,
-    #                              inputs_args=(z,),
-    #                              name_prefix='mapping.',
-    #                              )
-
-    pass
-
-  def test__build_piGAN_NeRF_Net(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/pigan_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import pigan_net
-    from . import multi_head_mapping
-    from ..nerf import volume_rendering
-    from ..nerf import cam_params
-
-    net = pigan_net.piGAN_NeRF_Net(**cfg).cuda()
-
-    bs = 4
-    H = 64
-    W = 64
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=1,
-                                                 H0=H,
-                                                 W0=W,
-                                                 so3_repr='axis-angle',
-                                                 intr_repr='square',
-                                                 initial_fov=53.13,
-                                                 freeze_intr=False).cuda()
-
-    intr = cam_param(mode='get_intrinsic')
-
-    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-      device='cuda',
-      bs=bs,
-      intr=intr,
-      h_stddev=0.3,
-      v_stddev=0.155,
-    )
-
-    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
-
-    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                        rays_d=rays_d,
-                                                        near=0.5,
-                                                        far=1.5,
-                                                        N_samples=24,
-                                                        perturb=0)
-
-    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
-    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
-    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
-
-    z_dim = 128
-    style_dim = 128
-
-    mapping_shape = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
-                                                               hidden_dim=net.shape_net_cfg.style_dim,
-                                                               base_layers=4,
-                                                               head_layers=0,
-                                                               head_dim_dict=net.style_dim_dict_shape,
-                                                               add_norm=True,
-                                                               norm_out=True).cuda()
-    mapping_app = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
-                                                             hidden_dim=net.app_net_cfg.style_dim,
-                                                             base_layers=4,
-                                                             head_layers=0,
-                                                             head_dim_dict=net.style_dim_dict_app,
-                                                             add_norm=True,
-                                                             norm_out=True).cuda()
-
-    z_shape = torch.randn(bs, z_dim, requires_grad=True).cuda()
-    z_app = torch.randn(bs, z_dim, requires_grad=True).cuda()
-
-    style_dict = {}
-    style_dict.update(mapping_shape(z_shape))
-    style_dict.update(mapping_app(z_app))
-
-    points = rearrange(points, "b Nr Ns c -> b (Nr Ns) c")
-    # points = points[:, 0]
-    out = net(points, style_dict)
-
-    out.mean().backward()
-
-    grad_str = torch_utils.get_grad_norm_string(itertools.chain(net.named_parameters(),
-                                                                mapping_shape.named_parameters(),
-                                                                mapping_app.named_parameters()
-                                                                ))
-    print(grad_str)
-
-    pass
-
-  def test__build_NeRF_Net(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import siren_net
-    from . import multi_head_mapping
-    from ..nerf import volume_rendering
-    from ..nerf import cam_params
-
-    net = siren_net.NeRF_Net(**cfg).cuda()
-
-    bs = 4
-    H = 64
-    W = 64
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=1,
-                                                 H0=H,
-                                                 W0=W,
-                                                 so3_repr='axis-angle',
-                                                 intr_repr='square',
-                                                 initial_fov=53.13,
-                                                 freeze_intr=False).cuda()
-
-    intr = cam_param(mode='get_intrinsic')
-
-    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-      device='cuda',
-      bs=bs,
-      intr=intr,
-      h_stddev=0.3,
-      v_stddev=0.155,
-    )
-
-    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
-
-    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                        rays_d=rays_d,
-                                                        near=0.5,
-                                                        far=1.5,
-                                                        N_samples=24,
-                                                        perturb=0)
-
-    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
-    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
-    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
-
-    z_dim = 128
-    style_dim = 128
-
-    mapping_shape = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
-                                                               hidden_dim=net.shape_net_cfg.style_dim,
-                                                               base_layers=4,
-                                                               head_layers=0,
-                                                               head_dim_dict=net.style_dim_dict_shape,
-                                                               add_norm=True,
-                                                               norm_out=True).cuda()
-    mapping_app = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
-                                                             hidden_dim=net.app_net_cfg.style_dim,
-                                                             base_layers=4,
-                                                             head_layers=0,
-                                                             head_dim_dict=net.style_dim_dict_app,
-                                                             add_norm=True,
-                                                             norm_out=True).cuda()
-
-    z_shape = torch.randn(bs, z_dim).cuda()
-    z_app = torch.randn(bs, z_dim).cuda()
-
-    style_dict = {}
-    style_dict.update(mapping_shape(z_shape))
-    style_dict.update(mapping_app(z_app))
-
-    points = rearrange(points, "b Nr Ns c -> b (Nr Ns) c")
-    # points = points[:, 0]
-    out = net(points, style_dict)
-
-    out.mean().backward()
-    pass
+import itertools
+import os
+import sys
+import unittest
+import argparse
+from einops import rearrange
+
+import torch
+import torch.nn as nn
+
+from tl2.proj.pytorch.pytorch_hook import VerboseModel
+from tl2.proj.pytorch import torch_utils
+
+
+class Testing_pigan_net(unittest.TestCase):
+
+  def test__build_ScaleLinear(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/pigan_net.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import itertools
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+    from tl2.proj.pytorch.examples.networks import multi_head_mapping
+    from tl2.proj.pytorch.examples.networks import pigan_net
+    from tl2.proj.pytorch import torch_utils
+
+    net = pigan_net.ScaleLinear(**{**cfg,
+                                   'gamma': 10.,
+                                   'scale_gradients': True}).cuda()
+
+    bs = 4
+    N = 10
+    in_dim = cfg.in_features
+
+    x = torch.randn(bs, N, in_dim, requires_grad=True).cuda()
+    x = nn.Parameter(x)
+
+    # scale_w false
+    x.grad = None
+    net.scale_gradients = False
+    net.gamma = 1
+    net.beta = 0
+    net.zero_grad(set_to_none=True)
+    VerboseModel.forward_verbose(net,
+                                 inputs_args=(x,),
+                                 name_prefix='ori.')
+    out = net(x)
+    out.mean().backward()
+
+    grad_str = torch_utils.get_grad_norm_string(itertools.chain(net.named_parameters(), [('x', x)]))
+    print(grad_str)
+
+    # scale_w true
+    x.grad = None
+    net.scale_gradients = False
+    net.gamma = 10
+    net.beta = 30
+    net.zero_grad(set_to_none=True)
+    VerboseModel.forward_verbose(net,
+                                 inputs_args=(x,),
+                                 name_prefix='no_scale.')
+    out = net(x)
+    out.mean().backward()
+
+    grad_str = torch_utils.get_grad_norm_string(itertools.chain(net.named_parameters(), [('x', x)]))
+    print(grad_str)
+
+
+    # scale_gradients true
+    x.grad = None
+    net.scale_gradients = True
+    net.gamma = 10
+    net.beta = 30
+    net.zero_grad(set_to_none=True)
+    VerboseModel.forward_verbose(net,
+                                 inputs_args=(x,),
+                                 name_prefix='scale.')
+    out = net(x)
+    out.mean().backward()
+
+    grad_str = torch_utils.get_grad_norm_string(itertools.chain(net.named_parameters(), [('x', x)]))
+    print(grad_str)
+
+    pass
+
+  def test__build_ModFiLMLayer(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/pigan_net.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import itertools
+    from tl2.proj.pytorch.examples.networks import pigan_net
+
+    torch_utils.init_seeds()
+
+    net = pigan_net.ModFiLMLayer(**cfg).cuda()
+
+    bs = 4
+    N = 10
+    in_dim = cfg.in_dim
+    style_dim = cfg.style_dim
+
+    x = torch.randn(bs, N, in_dim, requires_grad=True).cuda()
+    x = nn.Parameter(x)
+    style = torch.randn(bs, style_dim).cuda()
+    style = nn.Parameter(style)
+
+    VerboseModel.forward_verbose(net,
+                                 inputs_args=(x, style),
+                                 name_prefix='net3.')
+    out_3 = net(x, style)
+
+    out_3.mean().backward()
+    grad_str = torch_utils.get_grad_norm_string(itertools.chain(net.named_parameters(),
+                                                                [('x', x),
+                                                                 ('style', style)]))
+    print(grad_str)
+
+    x = torch.randn(bs, in_dim, requires_grad=True).cuda()
+    style = torch.randn(bs, style_dim).cuda()
+
+    VerboseModel.forward_verbose(net,
+                                 inputs_args=(x, style),
+                                 name_prefix='net2.')
+    out_2 = net(x, style)
+
+    pass
+
+  def test__build_ModSIREN_Net(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/pigan_net.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+    from tl2.proj.pytorch.examples.networks import multi_head_mapping
+    from tl2.proj.pytorch.examples.networks import pigan_net
+
+    torch_utils.init_seeds()
+
+    net = pigan_net.ModSIREN_Net(**cfg).cuda()
+    head_dim_dict = net.style_dim_dict
+    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg.mapping_cfg).cuda()
+
+    bs = 4
+    N = 10
+    in_dim = cfg.input_dim
+
+    z = torch.randn(bs, cfg.mapping_cfg.z_dim).cuda()
+    z = nn.Parameter(z)
+    style_dict = mapping_net(z)
+
+    x = torch.randn(bs, N, in_dim).cuda()
+    x = nn.Parameter(x)
+
+    out = net(x, style_dict=style_dict)
+
+    out.mean().backward()
+    grad_str = torch_utils.get_grad_norm_string(itertools.chain(net.named_parameters(),
+                                                                mapping_net.named_parameters(),
+                                                                [('x', x),
+                                                                 ('z', z)]))
+    print(grad_str)
+
+    # VerboseModel.forward_verbose(mapping_net,
+    #                              inputs_args=(z,),
+    #                              name_prefix='mapping.',
+    #                              )
+
+    pass
+
+  def test__build_piGAN_NeRF_Net(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/pigan_net.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.examples.networks import pigan_net
+    from . import multi_head_mapping
+    from ..nerf import volume_rendering
+    from ..nerf import cam_params
+
+    net = pigan_net.piGAN_NeRF_Net(**cfg).cuda()
+
+    bs = 4
+    H = 64
+    W = 64
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=1,
+                                                 H0=H,
+                                                 W0=W,
+                                                 so3_repr='axis-angle',
+                                                 intr_repr='square',
+                                                 initial_fov=53.13,
+                                                 freeze_intr=False).cuda()
+
+    intr = cam_param(mode='get_intrinsic')
+
+    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+      device='cuda',
+      bs=bs,
+      intr=intr,
+      h_stddev=0.3,
+      v_stddev=0.155,
+    )
+
+    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
+    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
+
+    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                        rays_d=rays_d,
+                                                        near=0.5,
+                                                        far=1.5,
+                                                        N_samples=24,
+                                                        perturb=0)
+
+    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
+    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
+    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
+
+    z_dim = 128
+    style_dim = 128
+
+    mapping_shape = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
+                                                               hidden_dim=net.shape_net_cfg.style_dim,
+                                                               base_layers=4,
+                                                               head_layers=0,
+                                                               head_dim_dict=net.style_dim_dict_shape,
+                                                               add_norm=True,
+                                                               norm_out=True).cuda()
+    mapping_app = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
+                                                             hidden_dim=net.app_net_cfg.style_dim,
+                                                             base_layers=4,
+                                                             head_layers=0,
+                                                             head_dim_dict=net.style_dim_dict_app,
+                                                             add_norm=True,
+                                                             norm_out=True).cuda()
+
+    z_shape = torch.randn(bs, z_dim, requires_grad=True).cuda()
+    z_app = torch.randn(bs, z_dim, requires_grad=True).cuda()
+
+    style_dict = {}
+    style_dict.update(mapping_shape(z_shape))
+    style_dict.update(mapping_app(z_app))
+
+    points = rearrange(points, "b Nr Ns c -> b (Nr Ns) c")
+    # points = points[:, 0]
+    out = net(points, style_dict)
+
+    out.mean().backward()
+
+    grad_str = torch_utils.get_grad_norm_string(itertools.chain(net.named_parameters(),
+                                                                mapping_shape.named_parameters(),
+                                                                mapping_app.named_parameters()
+                                                                ))
+    print(grad_str)
+
+    pass
+
+  def test__build_NeRF_Net(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.examples.networks import siren_net
+    from . import multi_head_mapping
+    from ..nerf import volume_rendering
+    from ..nerf import cam_params
+
+    net = siren_net.NeRF_Net(**cfg).cuda()
+
+    bs = 4
+    H = 64
+    W = 64
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=1,
+                                                 H0=H,
+                                                 W0=W,
+                                                 so3_repr='axis-angle',
+                                                 intr_repr='square',
+                                                 initial_fov=53.13,
+                                                 freeze_intr=False).cuda()
+
+    intr = cam_param(mode='get_intrinsic')
+
+    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+      device='cuda',
+      bs=bs,
+      intr=intr,
+      h_stddev=0.3,
+      v_stddev=0.155,
+    )
+
+    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
+    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
+
+    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                        rays_d=rays_d,
+                                                        near=0.5,
+                                                        far=1.5,
+                                                        N_samples=24,
+                                                        perturb=0)
+
+    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
+    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
+    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
+
+    z_dim = 128
+    style_dim = 128
+
+    mapping_shape = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
+                                                               hidden_dim=net.shape_net_cfg.style_dim,
+                                                               base_layers=4,
+                                                               head_layers=0,
+                                                               head_dim_dict=net.style_dim_dict_shape,
+                                                               add_norm=True,
+                                                               norm_out=True).cuda()
+    mapping_app = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
+                                                             hidden_dim=net.app_net_cfg.style_dim,
+                                                             base_layers=4,
+                                                             head_layers=0,
+                                                             head_dim_dict=net.style_dim_dict_app,
+                                                             add_norm=True,
+                                                             norm_out=True).cuda()
+
+    z_shape = torch.randn(bs, z_dim).cuda()
+    z_app = torch.randn(bs, z_dim).cuda()
+
+    style_dict = {}
+    style_dict.update(mapping_shape(z_shape))
+    style_dict.update(mapping_app(z_app))
+
+    points = rearrange(points, "b Nr Ns c -> b (Nr Ns) c")
+    # points = points[:, 0]
+    out = net(points, style_dict)
+
+    out.mean().backward()
+    pass
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_siren_net.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/test_siren_net_v1.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,434 +1,426 @@
-import os
-import sys
-import unittest
-import argparse
-from einops import rearrange
-
-import torch
-
-from tl2.proj.pytorch.pytorch_hook import VerboseModel
-
-
-class Testing_siren_net(unittest.TestCase):
-
-  def test__build_ModFiLMLayer(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import siren_net
-
-    net = siren_net.ModFiLMLayer(**cfg).cuda()
-
-    bs = 4
-    N = 10
-    in_dim = cfg.in_dim
-    style_dim = cfg.style_dim
-
-    x = torch.randn(bs, N, in_dim, requires_grad=True).cuda()
-    style = torch.randn(bs, style_dim).cuda()
-
-    VerboseModel.forward_verbose(net,
-                                 inputs_args=(x, style),
-                                 name_prefix='net3.')
-    out_3 = net(x, style)
-
-    x = torch.randn(bs, in_dim, requires_grad=True).cuda()
-    style = torch.randn(bs, style_dim).cuda()
-
-    VerboseModel.forward_verbose(net,
-                                 inputs_args=(x, style),
-                                 name_prefix='net2.')
-    out_2 = net(x, style)
-
-    pass
-
-  def test__build_ModFiLMBlock(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import siren_net, multi_head_mapping
-
-    net = siren_net.ModFiLMBlock(name_prefix='film', **cfg).cuda()
-
-    head_dim_dict = net.style_dim_dict
-    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg.mapping_cfg).cuda()
-
-
-    bs = 4
-    N = 10
-    in_dim = cfg.in_dim
-
-    x = torch.randn(bs, N, in_dim, requires_grad=True).cuda()
-
-    z = torch.randn(bs, cfg.mapping_cfg.z_dim).cuda()
-    style_dict = mapping_net(z)
-
-    VerboseModel.forward_verbose(net,
-                                 inputs_args=(x, style_dict),
-                                 submodels=['mod1', 'mod2'],
-                                 name_prefix='net.')
-    out_3 = net(x, style_dict)
-
-    pass
-
-  def test__build_SIRENNet_skip(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-    from tl2.proj.pytorch.examples.networks import multi_head_mapping
-    from tl2.proj.pytorch.examples.networks import siren_net
-
-    net = siren_net.SIRENNet_skip(**cfg).cuda()
-    head_dim_dict = net.style_dim_dict
-    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg.mapping_cfg).cuda()
-
-    bs = 4
-    N = 10
-    in_dim = cfg.input_dim
-
-    z = torch.randn(bs, cfg.mapping_cfg.z_dim).cuda()
-    style_dict = mapping_net(z)
-
-    x = torch.randn(bs, N, in_dim).cuda()
-
-    out = net(x, style_dict=style_dict)
-
-    VerboseModel.forward_verbose(mapping_net,
-                                 inputs_args=(z,),
-                                 name_prefix='mapping.',
-                                 )
-
-    pass
-
-  def test__build_ModSIREN_Net(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-    from tl2.proj.pytorch.examples.networks import multi_head_mapping
-    from tl2.proj.pytorch.examples.networks import siren_net
-
-    net = siren_net.ModSIREN_Net(**cfg).cuda()
-    head_dim_dict = net.style_dim_dict
-    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg.mapping_cfg).cuda()
-
-    bs = 4
-    N = 10
-    in_dim = cfg.input_dim
-
-    z = torch.randn(bs, cfg.mapping_cfg.z_dim).cuda()
-    style_dict = mapping_net(z)
-
-    x = torch.randn(bs, N, in_dim).cuda()
-
-    out = net(x, style_dict=style_dict)
-
-    VerboseModel.forward_verbose(mapping_net,
-                                 inputs_args=(z,),
-                                 name_prefix='mapping.',
-                                 )
-
-    pass
-
-  def test__build_NeRF_Net(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import siren_net
-    from . import multi_head_mapping
-    from ..nerf import volume_rendering
-    from ..nerf import cam_params
-
-    net = siren_net.NeRF_Net(**cfg).cuda()
-
-    bs = 4
-    H = 64
-    W = 64
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=1,
-                                                 H0=H,
-                                                 W0=W,
-                                                 so3_repr='axis-angle',
-                                                 intr_repr='square',
-                                                 initial_fov=53.13,
-                                                 freeze_intr=False).cuda()
-
-    intr = cam_param(mode='get_intrinsic')
-
-    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-      device='cuda',
-      bs=bs,
-      intr=intr,
-      h_stddev=0.3,
-      v_stddev=0.155,
-    )
-
-    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
-
-    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                        rays_d=rays_d,
-                                                        near=0.5,
-                                                        far=1.5,
-                                                        N_samples=24,
-                                                        perturb=0)
-
-    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
-    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
-    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
-
-    z_dim = 128
-    style_dim = 128
-
-    mapping_shape = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
-                                                               hidden_dim=net.shape_net_cfg.style_dim,
-                                                               base_layers=4,
-                                                               head_layers=0,
-                                                               head_dim_dict=net.style_dim_dict_shape,
-                                                               add_norm=True,
-                                                               norm_out=True).cuda()
-    mapping_app = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
-                                                             hidden_dim=net.app_net_cfg.style_dim,
-                                                             base_layers=4,
-                                                             head_layers=0,
-                                                             head_dim_dict=net.style_dim_dict_app,
-                                                             add_norm=True,
-                                                             norm_out=True).cuda()
-
-    z_shape = torch.randn(bs, z_dim).cuda()
-    z_app = torch.randn(bs, z_dim).cuda()
-
-    style_dict = {}
-    style_dict.update(mapping_shape(z_shape))
-    style_dict.update(mapping_app(z_app))
-
-    points = rearrange(points, "b Nr Ns c -> b (Nr Ns) c")
-    # points = points[:, 0]
-    out = net(points, style_dict)
-
-    out.mean().backward()
-    pass
+import os
+import sys
+import unittest
+import argparse
+from einops import rearrange
+
+import torch
+
+from tl2.proj.pytorch.pytorch_hook import VerboseModel
+
+
+class Testing_siren_net_v1(unittest.TestCase):
+
+  def test__build_SirenLayer(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_v1.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+    from tl2.proj.pytorch.examples.networks import multi_head_mapping
+    from tl2.proj.pytorch.examples.networks import siren_net_v1
+
+    net = siren_net_v1.SirenLayer(**cfg).cuda()
+
+    bs = 4
+    N = 10
+    in_dim = cfg.input_dim
+
+
+    x = torch.randn(bs, N, in_dim).cuda()
+
+    VerboseModel.forward_verbose(net,
+                                 inputs_args=(x,),
+                                 name_prefix='siren_layer.')
+    out = net(x)
+    pass
+
+  def test__build_ModSirenLayer(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_v1.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+    from tl2.proj.pytorch.examples.networks import multi_head_mapping
+    from tl2.proj.pytorch.examples.networks import siren_net_v1
+
+    net = siren_net_v1.ModSirenLayer(**cfg).cuda()
+
+    bs = 4
+    N = 10
+    in_dim = cfg.input_dim
+
+    x = torch.randn(bs, N, in_dim).cuda()
+    style = torch.randn(bs, cfg.style_dim).cuda()
+
+    VerboseModel.forward_verbose(net,
+                                 inputs_args=(x, style),
+                                 submodels=['layer', ],
+                                 name_prefix='siren_layer.')
+    out = net(x, style)
+
+    pass
+
+  def test__build_ModSirenBlock(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_v1.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+    from tl2.proj.pytorch.examples.networks import multi_head_mapping
+    from tl2.proj.pytorch.examples.networks import siren_net_v1
+
+    net = siren_net_v1.ModSirenBlock(name_prefix='siren', **cfg).cuda()
+
+    bs = 4
+    N = 10
+    in_dim = cfg.in_dim
+
+    x = torch.randn(bs, N, in_dim).cuda()
+
+    z_dim = 256
+    mapping = multi_head_mapping.MultiHeadMappingNetwork(
+      z_dim=z_dim, hidden_dim=cfg.style_dim, base_layers=4, head_layers=0,
+      head_dim_dict=net.style_dim_dict, add_norm=True, norm_out=True).cuda()
+
+    z = torch.randn(bs, z_dim).cuda()
+    style_dict = mapping(z)
+
+    VerboseModel.forward_verbose(net,
+                                 inputs_args=(x, style_dict),
+                                 # submodels=['layer', ],
+                                 name_prefix='siren_layer.')
+    out = net(x, style_dict)
+
+    pass
+
+  def test__build_ModSIREN_Skip_Net(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_v1.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+    from tl2.proj.pytorch.examples.networks import multi_head_mapping
+    from tl2.proj.pytorch.examples.networks import siren_net_v1
+
+    net = siren_net_v1.ModSIREN_Skip_Net(**cfg).cuda()
+    head_dim_dict = net.style_dim_dict
+    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg.mapping_cfg).cuda()
+
+    bs = 4
+    N = 10
+    in_dim = cfg.input_dim
+
+    z = torch.randn(bs, cfg.mapping_cfg.z_dim).cuda()
+    style_dict = mapping_net(z)
+
+    x = torch.randn(bs, N, in_dim).cuda()
+
+    out = net(x, style_dict=style_dict)
+
+    VerboseModel.forward_verbose(mapping_net,
+                                 inputs_args=(z,),
+                                 name_prefix='mapping.',
+                                 )
+
+    pass
+
+  def test__build_NeRF_Net(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_v1.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.examples.networks import siren_net_v1
+    from . import multi_head_mapping
+    from ..nerf import volume_rendering
+    from ..nerf import cam_params
+
+    net = siren_net_v1.NeRF_Net(**cfg).cuda()
+
+    bs = 4
+    H = 64
+    W = 64
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=1,
+                                                 H0=H,
+                                                 W0=W,
+                                                 so3_repr='axis-angle',
+                                                 intr_repr='square',
+                                                 initial_fov=53.13,
+                                                 freeze_intr=False).cuda()
+
+    intr = cam_param(mode='get_intrinsic')
+
+    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+      device='cuda',
+      bs=bs,
+      intr=intr,
+      h_stddev=0.3,
+      v_stddev=0.155,
+    )
+
+    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
+    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
+
+    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                        rays_d=rays_d,
+                                                        near=0.5,
+                                                        far=1.5,
+                                                        N_samples=24,
+                                                        perturb=0)
+
+    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
+    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
+    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
+
+    z_dim = 128
+    style_dim = 128
+
+    mapping_shape = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
+                                                               hidden_dim=net.shape_net_cfg.style_dim,
+                                                               base_layers=4,
+                                                               head_layers=0,
+                                                               head_dim_dict=net.style_dim_dict_shape,
+                                                               add_norm=True,
+                                                               norm_out=True).cuda()
+    mapping_app = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
+                                                             hidden_dim=net.app_net_cfg.style_dim,
+                                                             base_layers=4,
+                                                             head_layers=0,
+                                                             head_dim_dict=net.style_dim_dict_app,
+                                                             add_norm=True,
+                                                             norm_out=True).cuda()
+
+    z_shape = torch.randn(bs, z_dim).cuda()
+    z_app = torch.randn(bs, z_dim).cuda()
+
+    style_dict = {}
+    style_dict.update(mapping_shape(z_shape))
+    style_dict.update(mapping_app(z_app))
+
+    points = rearrange(points, "b Nr Ns c -> b (Nr Ns) c")
+    # points = points[:, 0]
+    out = net(points, style_dict)
+
+    out.mean().backward()
+    pass
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/examples/networks/test_siren_net_pigan.py` & `tl2-0.1.1/tl2/proj/pytorch/examples/networks/test_siren_net_pigan.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,877 +1,877 @@
-import collections
-import math
-import os
-import pprint
-import sys
-import unittest
-import argparse
-from einops import rearrange
-from itertools import chain
-
-import torch
-
-from tl2.proj.pytorch.pytorch_hook import VerboseModel
-from tl2.proj.pytorch import torch_utils
-from tl2.proj.pil import pil_utils
-from tl2.proj.fvcore.checkpoint import Checkpointer
-from tl2.proj.fvcore import TLCfgNode
-
-
-class Testing_siren_net_pigan(unittest.TestCase):
-
-  def test__build_ShapeNet(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-    from tl2.proj.pytorch.examples.networks import multi_head_mapping
-    from tl2.proj.pytorch.examples.networks import siren_net_pigan
-
-    net = siren_net_pigan.ShapeNet(**cfg.shape_cfg).cuda()
-    head_dim_dict = net.style_dim_dict
-    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg.mapping_cfg).cuda()
-
-    bs = 4
-    N = 10
-    in_dim = cfg.shape_cfg.input_dim
-
-    z = torch.randn(bs, cfg.mapping_cfg.z_dim).cuda()
-    style_dict = mapping_net(z)
-
-    x = torch.randn(bs, N, in_dim).cuda()
-
-    out = net(x, style_dict=style_dict)
-
-    VerboseModel.forward_verbose(mapping_net,
-                                 inputs_args=(z,),
-                                 name_prefix='mapping.',
-                                 )
-
-    pass
-
-  def test__build_AppNet(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-    from tl2.proj.pytorch.examples.networks import multi_head_mapping
-    from tl2.proj.pytorch.examples.networks import siren_net_pigan
-
-    net = siren_net_pigan.AppNet(**cfg.shape_cfg).cuda()
-    head_dim_dict = net.style_dim_dict
-    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg.mapping_cfg).cuda()
-
-    bs = 4
-    N = 10
-    in_dim = cfg.shape_cfg.input_dim
-
-    z = torch.randn(bs, cfg.mapping_cfg.z_dim).cuda()
-    style_dict = mapping_net(z)
-
-    x = torch.randn(bs, N, in_dim).cuda()
-
-    out = net(x, style_dict=style_dict)
-
-    VerboseModel.forward_verbose(mapping_net,
-                                 inputs_args=(z,),
-                                 name_prefix='mapping.',
-                                 )
-
-    pass
-
-  def test__build_siren(self, debug=True):
-    """
-    pigan nerf network on celeba;
-    save kwargs and out;
-
-    Usage:
-
-        export CUDA_VISIBLE_DEVICES=0
-        export TIME_STR=1
-        export PYTHONPATH=.:./tl2_lib:./piGAN_lib
-        python -c "from exp.tests.test_pigan import Testing_pretrained;\
-          Testing_pretrained().test_inverse_render_web(debug=False)" \
-          --tl_opts port 8591
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    yaml_file = 'exp/pigan/configs/pretrained.yaml'
-    cfg = TLCfgNode.load_yaml_with_command(yaml_file, command='_build_generator_CelebA')
-
-    import torch
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-    from exp.pigan import pigan_utils
-    from exp.comm.pigan import network
-
-    device = torch.device('cuda')
-
-    model_pkl = 'cache_pretrained/pretrained/CelebA/generator.pth'
-    G_ema = pigan_utils.load_generator_ema(model_pkl)
-    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba.pth"
-    torch.save(G_ema.state_dict(), saved_pkl)
-
-    G_kwargs = cfg.G_kwargs
-    G_kwargs.h_mean = eval(G_kwargs.h_mean)
-    G_kwargs.v_mean = eval(G_kwargs.v_mean)
-
-    del G_ema
-    G_ema = network.ImplicitGenerator3d(device=device, **G_kwargs)
-    Checkpointer(G_ema).load_state_dict_from_file(saved_pkl)
-
-    loaded_kwargs = torch.load("datasets/kwargs/forward_with_frequencies_phase_shifts.pth")
-
-    with torch.no_grad():
-      out = G_ema.siren.forward_with_frequencies_phase_shifts(**loaded_kwargs)
-      rgb = out[:, :, :3]
-      sigma = out[:, :, 3:]
-      saved_data = {
-        'rgb': rgb,
-        'sigma': sigma,
-      }
-      torch.save(saved_data, "datasets/kwargs/forward_with_frequencies_phase_shifts_out.pth")
-
-
-    pass
-
-  def test__build_mapping_pigan(self, debug=True):
-    """
-    Usage:
-
-        export CUDA_VISIBLE_DEVICES=0
-        export TIME_STR=1
-        export PYTHONPATH=.:./tl2_lib:./piGAN_lib
-        python -c "from exp.tests.test_pigan import Testing_pretrained;\
-          Testing_pretrained().test_inverse_render_web(debug=False)" \
-          --tl_opts port 8591
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    yaml_file = 'exp/pigan/configs/pretrained.yaml'
-    cfg = TLCfgNode.load_yaml_with_command(yaml_file, command='_build_generator_CelebA')
-
-    import torch
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-    from exp.pigan import pigan_utils
-    from exp.comm.pigan import network
-
-    device = torch.device('cuda')
-    torch_utils.init_seeds(seed=0)
-
-    model_pkl = 'cache_pretrained/pretrained/CelebA/generator.pth'
-    G_ema = pigan_utils.load_generator_ema(model_pkl)
-    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba.pth"
-    torch.save(G_ema.state_dict(), saved_pkl)
-
-    G_kwargs = cfg.G_kwargs
-    G_kwargs.h_mean = eval(G_kwargs.h_mean)
-    G_kwargs.v_mean = eval(G_kwargs.v_mean)
-
-    del G_ema
-    G_ema = network.ImplicitGenerator3d(device=device, **G_kwargs)
-    Checkpointer(G_ema).load_state_dict_from_file(saved_pkl)
-
-    z = torch.randn((10000, 256), device=device)
-    with torch.no_grad():
-      VerboseModel.forward_verbose(G_ema.siren.mapping_network,
-                                   inputs_args=(z,),
-                                   submodels=['network'],
-                                   name_prefix='mapping.')
-      frequencies, phase_shifts = G_ema.siren.mapping_network(z)
-
-      saved_dict = {
-        'z': z,
-        'frequencies': frequencies,
-        'phase_shifts': phase_shifts,
-      }
-      torch.save(saved_dict, "datasets/kwargs/mapping_network_kwargs_out.pth")
-
-    w_frequencies = frequencies.mean(0, keepdim=True)
-    w_phase_shifts = phase_shifts.mean(0, keepdim=True)
-
-    pass
-
-  def test__build_generator_CelebA(self, debug=True):
-    """
-    Usage:
-
-        export CUDA_VISIBLE_DEVICES=0
-        export TIME_STR=1
-        export PYTHONPATH=.:./tl2_lib:./piGAN_lib
-        python -c "from exp.tests.test_pigan import Testing_pretrained;\
-          Testing_pretrained().test_inverse_render_web(debug=False)" \
-          --tl_opts port 8591
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    yaml_file = 'exp/pigan/configs/pretrained.yaml'
-    cfg = TLCfgNode.load_yaml_with_command(yaml_file, command='_build_generator_CelebA')
-
-    import torch
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-    from exp.pigan import pigan_utils
-    from exp.comm.pigan import network
-
-    device = torch.device('cuda')
-
-    model_pkl = 'cache_pretrained/pretrained/CelebA/generator.pth'
-    G_ema = pigan_utils.load_generator_ema(model_pkl)
-    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba.pth"
-    torch.save(G_ema.state_dict(), saved_pkl)
-
-    G_kwargs = cfg.G_kwargs
-    G_kwargs.h_mean = eval(G_kwargs.h_mean)
-    G_kwargs.v_mean = eval(G_kwargs.v_mean)
-
-    del G_ema
-    G_ema = network.ImplicitGenerator3d(device=device, **G_kwargs)
-    Checkpointer(G_ema).load_state_dict_from_file(saved_pkl)
-
-    z = torch.randn((10000, 256), device=device)
-    with torch.no_grad():
-      VerboseModel.forward_verbose(G_ema.siren.mapping_network,
-                                   inputs_args=(z,),
-                                   submodels=['network'],
-                                   name_prefix='mapping.')
-      frequencies, phase_shifts = G_ema.siren.mapping_network(z)
-
-    w_frequencies = frequencies.mean(0, keepdim=True)
-    w_phase_shifts = phase_shifts.mean(0, keepdim=True)
-    with torch.no_grad():
-      frame, _ = G_ema.forward_with_frequencies(w_frequencies, w_phase_shifts, **G_kwargs)
-      frame_pil = torch_utils.img_tensor_to_pil(frame)
-      pil_utils.imshow_pil(frame_pil, frame.shape)
-
-    pass
-
-  def test__build_NeRF_Net_kwargs(self, debug=True):
-    """
-    Convert pigan nerf weights;
-
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import siren_net_pigan
-    from . import multi_head_mapping
-    from ..nerf import volume_rendering
-    from ..nerf import cam_params
-
-    net = siren_net_pigan.NeRF_Net(**cfg).cuda()
-
-    loaded_kwargs = torch.load("datasets/kwargs/forward_with_frequencies_phase_shifts.pth")
-    # kwargs_saved = {
-    #     'input': input,
-    #     'frequencies': frequencies,
-    #     'phase_shifts': phase_shifts,
-    #     'ray_directions': ray_directions,
-    # }
-    points = loaded_kwargs['input']
-    ray_directions = loaded_kwargs['ray_directions']
-    frequencies = loaded_kwargs['frequencies']
-    phase_shifts = loaded_kwargs['phase_shifts']
-
-    style_dict = net.parse_style_dict(frequencies=frequencies, phase_shifts=phase_shifts)
-
-    # convert weights of pigan
-    model_pkl = 'cache_pretrained/pretrained/CelebA/generator.pth'
-    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba.pth"
-    loaded_state_dict = torch.load(saved_pkl)
-
-    state_dict = collections.OrderedDict()
-    weights_list = list(loaded_state_dict.values())
-    for idx, name in enumerate(net.state_dict().keys()):
-      state_dict[name] = weights_list[idx]
-    Checkpointer(net).load_state_dict(state_dict)
-
-    # save converted weights
-    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba_converted_nerf.pth"
-    torch.save(net.state_dict(), saved_pkl)
-
-    # check out
-    Checkpointer(net).load_state_dict_from_file(saved_pkl)
-    fea, rgb, sigma = net(x=points, style_dict=style_dict, ray_directions=ray_directions)
-
-    loaded_data = torch.load("datasets/kwargs/forward_with_frequencies_phase_shifts_out.pth")
-    loaded_rgb = loaded_data['rgb']
-    loaded_sigma = loaded_data['sigma']
-
-    err_rgb = (loaded_rgb - rgb).abs().sum()
-    err_sigma = (loaded_sigma - sigma).abs().sum()
-
-    # out.mean().backward()
-    pass
-
-  def test__build_MappingNetwork(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import siren_net_pigan
-    from . import multi_head_mapping
-    from ..nerf import volume_rendering
-    from ..nerf import cam_params
-
-    net = siren_net_pigan.MappingNetwork(**cfg).cuda()
-
-    model_pkl = 'cache_pretrained/pretrained/CelebA/generator.pth'
-    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba.pth"
-    loaded_state_dict = torch.load(saved_pkl)
-    weights_list = []
-    for name, weights in loaded_state_dict.items():
-      if 'mapping_network' in name:
-        weights_list.append(weights)
-
-    state_dict = collections.OrderedDict()
-    for idx, name in enumerate(net.state_dict().keys()):
-      state_dict[name] = weights_list[idx]
-    Checkpointer(net).load_state_dict(state_dict)
-
-    # save converted weights
-    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba_converted_mapping.pth"
-    torch.save(net.state_dict(), saved_pkl)
-
-    pass
-
-  def test__build_StyleMappingBaseNet(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import siren_net_pigan
-
-    net = siren_net_pigan.StyleMappingBaseNet(**cfg).cuda()
-
-    bs = 4
-    z_dim = cfg.z_dim
-    z = torch.randn(bs, z_dim).cuda()
-
-    out = net(z)
-
-    pass
-
-  def test__build_StyleMappingShapeApp(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import siren_net_pigan
-    from . import multi_head_mapping
-    from ..nerf import volume_rendering
-    from ..nerf import cam_params
-
-    nerf_net = siren_net_pigan.NeRF_Net(**cfg.nerf_cfg).cuda()
-
-    mapping_net = siren_net_pigan.StyleMappingShapeApp(**cfg.mapping_cfg,
-                                                       style_dim_dict_shape=nerf_net.style_dim_dict_shape,
-                                                       style_dim_dict_app=nerf_net.style_dim_dict_app).cuda()
-
-    bs = 4
-    z_dim = mapping_net.z_dim
-    z_shape = torch.randn(bs, z_dim).cuda()
-    z_app = torch.randn(bs, z_dim).cuda()
-
-    style_dict = mapping_net(z_shape=z_shape, z_app=z_app)
-
-    mapping_net_state_dict = mapping_net.state_dict()
-    pprint.pprint(list(mapping_net_state_dict.keys()))
-
-    # convert pigan weights
-    model_pkl = 'cache_pretrained/pretrained/CelebA/generator.pth'
-    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba.pth"
-    loaded_state_dict = torch.load(saved_pkl)
-    loaded_mapping_weights_list = []
-    for name, weights in loaded_state_dict.items():
-      if 'mapping_network' in name:
-        loaded_mapping_weights_list.append(weights)
-
-    # base_net weights
-    state_list = loaded_mapping_weights_list[:-2]
-
-    # heads
-    heads_weights = loaded_mapping_weights_list[-2]
-    heads_bias = loaded_mapping_weights_list[-1]
-
-    heads_weights_freq, heads_weights_phase = heads_weights.chunk(2, dim=0)
-    heads_bias_freq, heads_bias_phase = heads_bias.chunk(2, dim=0)
-
-    weights_dict = nerf_net.parse_weight_dict(frequencies=heads_weights_freq, phase_shifts=heads_weights_phase)
-    bias_dict = nerf_net.parse_weight_dict(frequencies=heads_bias_freq, phase_shifts=heads_bias_phase)
-    for _weight, _bias in zip(weights_dict.values(), bias_dict.values()):
-      state_list.append(_weight)
-      state_list.append(_bias)
-    assert len(state_list) == len(mapping_net_state_dict)
-
-    state_dict = collections.OrderedDict()
-    for idx, (name, _) in enumerate(mapping_net_state_dict.items()):
-      state_dict[name] = state_list[idx]
-
-    Checkpointer(mapping_net).load_state_dict(state_dict)
-    # save converted weights
-    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba_converted_mapping.pth"
-    torch.save(mapping_net.state_dict(), saved_pkl)
-
-    # check out
-    loaded_kwargs = torch.load("datasets/kwargs/mapping_network_kwargs_out.pth")
-    z_shape = loaded_kwargs['z']
-    frequencies = loaded_kwargs['frequencies']
-    phase_shifts = loaded_kwargs['phase_shifts']
-    loaded_style_dict = nerf_net.parse_style_dict(frequencies=frequencies, phase_shifts=phase_shifts)
-
-    Checkpointer(mapping_net).load_state_dict_from_file(saved_pkl)
-    style_dict = mapping_net(z_shape=z_shape, z_app=z_shape)
-    for name in style_dict.keys():
-      assert (style_dict[name] - loaded_style_dict[name]).abs().sum() == 0, name
-
-    pass
-
-  def test__build_NeRF_Net(self, debug=True):
-    """
-    Usage:
-
-        # export CUDA_VISIBLE_DEVICES=$cuda_devices
-        # export RUN_NUM=$run_num
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export PORT=12345
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
-          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
-          # --tl_outdir results/$resume_dir
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    if 'RUN_NUM' not in os.environ:
-      os.environ['RUN_NUM'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-
-    if debug:
-      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
-      pass
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
-             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
-    argv_str = f"""
-                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
-                --tl_command {command}
-                --tl_outdir {outdir}
-                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
-                --tl_opts {tl_opts}
-                --tl_debug True
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from tl2.proj.pytorch.examples.networks import siren_net_pigan
-    from . import multi_head_mapping
-    from ..nerf import volume_rendering
-    from ..nerf import cam_params
-
-    net = siren_net_pigan.NeRF_Net(**cfg).cuda()
-
-    bs = 4
-    H = 64
-    W = 64
-
-    cam_param = cam_params.CamParams.from_config(num_imgs=1,
-                                                 H0=H,
-                                                 W0=W,
-                                                 so3_repr='axis-angle',
-                                                 intr_repr='square',
-                                                 initial_fov=53.13,
-                                                 freeze_intr=False).cuda()
-
-    intr = cam_param(mode='get_intrinsic')
-
-    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
-      device='cuda',
-      bs=bs,
-      intr=intr,
-      h_stddev=0.3,
-      v_stddev=0.155,
-    )
-
-    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
-    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
-
-    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
-                                                        rays_d=rays_d,
-                                                        near=0.5,
-                                                        far=1.5,
-                                                        N_samples=24,
-                                                        perturb=0)
-
-    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
-    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
-    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
-
-    z_dim = 128
-    style_dim = 128
-
-    mapping_shape = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
-                                                               hidden_dim=net.shape_net_cfg.style_dim,
-                                                               base_layers=4,
-                                                               head_layers=0,
-                                                               head_dim_dict=net.style_dim_dict_shape,
-                                                               add_norm=True,
-                                                               norm_out=True).cuda()
-    mapping_app = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
-                                                             hidden_dim=net.app_net_cfg.style_dim,
-                                                             base_layers=4,
-                                                             head_layers=0,
-                                                             head_dim_dict=net.style_dim_dict_app,
-                                                             add_norm=True,
-                                                             norm_out=True).cuda()
-
-    z_shape = torch.randn(bs, z_dim).cuda()
-    z_app = torch.randn(bs, z_dim).cuda()
-
-    style_dict = {}
-    style_dict.update(mapping_shape(z_shape))
-    style_dict.update(mapping_app(z_app))
-
-    points = rearrange(points, "b Nr Ns c -> b (Nr Ns) c")
-    # points = points[:, 0]
-    out = net(points, style_dict)
-
-    out.mean().backward()
-    pass
+import collections
+import math
+import os
+import pprint
+import sys
+import unittest
+import argparse
+from einops import rearrange
+from itertools import chain
+
+import torch
+
+from tl2.proj.pytorch.pytorch_hook import VerboseModel
+from tl2.proj.pytorch import torch_utils
+from tl2.proj.pil import pil_utils
+from tl2.proj.fvcore.checkpoint import Checkpointer
+from tl2.proj.fvcore import TLCfgNode
+
+
+class Testing_siren_net_pigan(unittest.TestCase):
+
+  def test__build_ShapeNet(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+    from tl2.proj.pytorch.examples.networks import multi_head_mapping
+    from tl2.proj.pytorch.examples.networks import siren_net_pigan
+
+    net = siren_net_pigan.ShapeNet(**cfg.shape_cfg).cuda()
+    head_dim_dict = net.style_dim_dict
+    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg.mapping_cfg).cuda()
+
+    bs = 4
+    N = 10
+    in_dim = cfg.shape_cfg.input_dim
+
+    z = torch.randn(bs, cfg.mapping_cfg.z_dim).cuda()
+    style_dict = mapping_net(z)
+
+    x = torch.randn(bs, N, in_dim).cuda()
+
+    out = net(x, style_dict=style_dict)
+
+    VerboseModel.forward_verbose(mapping_net,
+                                 inputs_args=(z,),
+                                 name_prefix='mapping.',
+                                 )
+
+    pass
+
+  def test__build_AppNet(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+    from tl2.proj.pytorch.examples.networks import multi_head_mapping
+    from tl2.proj.pytorch.examples.networks import siren_net_pigan
+
+    net = siren_net_pigan.AppNet(**cfg.shape_cfg).cuda()
+    head_dim_dict = net.style_dim_dict
+    mapping_net = multi_head_mapping.MultiHeadMappingNetwork(head_dim_dict=head_dim_dict, **cfg.mapping_cfg).cuda()
+
+    bs = 4
+    N = 10
+    in_dim = cfg.shape_cfg.input_dim
+
+    z = torch.randn(bs, cfg.mapping_cfg.z_dim).cuda()
+    style_dict = mapping_net(z)
+
+    x = torch.randn(bs, N, in_dim).cuda()
+
+    out = net(x, style_dict=style_dict)
+
+    VerboseModel.forward_verbose(mapping_net,
+                                 inputs_args=(z,),
+                                 name_prefix='mapping.',
+                                 )
+
+    pass
+
+  def test__build_siren(self, debug=True):
+    """
+    pigan nerf network on celeba;
+    save kwargs and out;
+
+    Usage:
+
+        export CUDA_VISIBLE_DEVICES=0
+        export TIME_STR=1
+        export PYTHONPATH=.:./tl2_lib:./piGAN_lib
+        python -c "from exp.tests.test_pigan import Testing_pretrained;\
+          Testing_pretrained().test_inverse_render_web(debug=False)" \
+          --tl_opts port 8591
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    yaml_file = 'exp/pigan/configs/pretrained.yaml'
+    cfg = TLCfgNode.load_yaml_with_command(yaml_file, command='_build_generator_CelebA')
+
+    import torch
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+    from exp.pigan import pigan_utils
+    from exp.comm.pigan import network
+
+    device = torch.device('cuda')
+
+    model_pkl = 'cache_pretrained/pretrained/CelebA/generator.pth'
+    G_ema = pigan_utils.load_generator_ema(model_pkl)
+    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba.pth"
+    torch.save(G_ema.state_dict(), saved_pkl)
+
+    G_kwargs = cfg.G_kwargs
+    G_kwargs.h_mean = eval(G_kwargs.h_mean)
+    G_kwargs.v_mean = eval(G_kwargs.v_mean)
+
+    del G_ema
+    G_ema = network.ImplicitGenerator3d(device=device, **G_kwargs)
+    Checkpointer(G_ema).load_state_dict_from_file(saved_pkl)
+
+    loaded_kwargs = torch.load("datasets/kwargs/forward_with_frequencies_phase_shifts.pth")
+
+    with torch.no_grad():
+      out = G_ema.siren.forward_with_frequencies_phase_shifts(**loaded_kwargs)
+      rgb = out[:, :, :3]
+      sigma = out[:, :, 3:]
+      saved_data = {
+        'rgb': rgb,
+        'sigma': sigma,
+      }
+      torch.save(saved_data, "datasets/kwargs/forward_with_frequencies_phase_shifts_out.pth")
+
+
+    pass
+
+  def test__build_mapping_pigan(self, debug=True):
+    """
+    Usage:
+
+        export CUDA_VISIBLE_DEVICES=0
+        export TIME_STR=1
+        export PYTHONPATH=.:./tl2_lib:./piGAN_lib
+        python -c "from exp.tests.test_pigan import Testing_pretrained;\
+          Testing_pretrained().test_inverse_render_web(debug=False)" \
+          --tl_opts port 8591
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    yaml_file = 'exp/pigan/configs/pretrained.yaml'
+    cfg = TLCfgNode.load_yaml_with_command(yaml_file, command='_build_generator_CelebA')
+
+    import torch
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+    from exp.pigan import pigan_utils
+    from exp.comm.pigan import network
+
+    device = torch.device('cuda')
+    torch_utils.init_seeds(seed=0)
+
+    model_pkl = 'cache_pretrained/pretrained/CelebA/generator.pth'
+    G_ema = pigan_utils.load_generator_ema(model_pkl)
+    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba.pth"
+    torch.save(G_ema.state_dict(), saved_pkl)
+
+    G_kwargs = cfg.G_kwargs
+    G_kwargs.h_mean = eval(G_kwargs.h_mean)
+    G_kwargs.v_mean = eval(G_kwargs.v_mean)
+
+    del G_ema
+    G_ema = network.ImplicitGenerator3d(device=device, **G_kwargs)
+    Checkpointer(G_ema).load_state_dict_from_file(saved_pkl)
+
+    z = torch.randn((10000, 256), device=device)
+    with torch.no_grad():
+      VerboseModel.forward_verbose(G_ema.siren.mapping_network,
+                                   inputs_args=(z,),
+                                   submodels=['network'],
+                                   name_prefix='mapping.')
+      frequencies, phase_shifts = G_ema.siren.mapping_network(z)
+
+      saved_dict = {
+        'z': z,
+        'frequencies': frequencies,
+        'phase_shifts': phase_shifts,
+      }
+      torch.save(saved_dict, "datasets/kwargs/mapping_network_kwargs_out.pth")
+
+    w_frequencies = frequencies.mean(0, keepdim=True)
+    w_phase_shifts = phase_shifts.mean(0, keepdim=True)
+
+    pass
+
+  def test__build_generator_CelebA(self, debug=True):
+    """
+    Usage:
+
+        export CUDA_VISIBLE_DEVICES=0
+        export TIME_STR=1
+        export PYTHONPATH=.:./tl2_lib:./piGAN_lib
+        python -c "from exp.tests.test_pigan import Testing_pretrained;\
+          Testing_pretrained().test_inverse_render_web(debug=False)" \
+          --tl_opts port 8591
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    yaml_file = 'exp/pigan/configs/pretrained.yaml'
+    cfg = TLCfgNode.load_yaml_with_command(yaml_file, command='_build_generator_CelebA')
+
+    import torch
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+    from exp.pigan import pigan_utils
+    from exp.comm.pigan import network
+
+    device = torch.device('cuda')
+
+    model_pkl = 'cache_pretrained/pretrained/CelebA/generator.pth'
+    G_ema = pigan_utils.load_generator_ema(model_pkl)
+    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba.pth"
+    torch.save(G_ema.state_dict(), saved_pkl)
+
+    G_kwargs = cfg.G_kwargs
+    G_kwargs.h_mean = eval(G_kwargs.h_mean)
+    G_kwargs.v_mean = eval(G_kwargs.v_mean)
+
+    del G_ema
+    G_ema = network.ImplicitGenerator3d(device=device, **G_kwargs)
+    Checkpointer(G_ema).load_state_dict_from_file(saved_pkl)
+
+    z = torch.randn((10000, 256), device=device)
+    with torch.no_grad():
+      VerboseModel.forward_verbose(G_ema.siren.mapping_network,
+                                   inputs_args=(z,),
+                                   submodels=['network'],
+                                   name_prefix='mapping.')
+      frequencies, phase_shifts = G_ema.siren.mapping_network(z)
+
+    w_frequencies = frequencies.mean(0, keepdim=True)
+    w_phase_shifts = phase_shifts.mean(0, keepdim=True)
+    with torch.no_grad():
+      frame, _ = G_ema.forward_with_frequencies(w_frequencies, w_phase_shifts, **G_kwargs)
+      frame_pil = torch_utils.img_tensor_to_pil(frame)
+      pil_utils.imshow_pil(frame_pil, frame.shape)
+
+    pass
+
+  def test__build_NeRF_Net_kwargs(self, debug=True):
+    """
+    Convert pigan nerf weights;
+
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.examples.networks import siren_net_pigan
+    from . import multi_head_mapping
+    from ..nerf import volume_rendering
+    from ..nerf import cam_params
+
+    net = siren_net_pigan.NeRF_Net(**cfg).cuda()
+
+    loaded_kwargs = torch.load("datasets/kwargs/forward_with_frequencies_phase_shifts.pth")
+    # kwargs_saved = {
+    #     'input': input,
+    #     'frequencies': frequencies,
+    #     'phase_shifts': phase_shifts,
+    #     'ray_directions': ray_directions,
+    # }
+    points = loaded_kwargs['input']
+    ray_directions = loaded_kwargs['ray_directions']
+    frequencies = loaded_kwargs['frequencies']
+    phase_shifts = loaded_kwargs['phase_shifts']
+
+    style_dict = net.parse_style_dict(frequencies=frequencies, phase_shifts=phase_shifts)
+
+    # convert weights of pigan
+    model_pkl = 'cache_pretrained/pretrained/CelebA/generator.pth'
+    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba.pth"
+    loaded_state_dict = torch.load(saved_pkl)
+
+    state_dict = collections.OrderedDict()
+    weights_list = list(loaded_state_dict.values())
+    for idx, name in enumerate(net.state_dict().keys()):
+      state_dict[name] = weights_list[idx]
+    Checkpointer(net).load_state_dict(state_dict)
+
+    # save converted weights
+    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba_converted_nerf.pth"
+    torch.save(net.state_dict(), saved_pkl)
+
+    # check out
+    Checkpointer(net).load_state_dict_from_file(saved_pkl)
+    fea, rgb, sigma = net(x=points, style_dict=style_dict, ray_directions=ray_directions)
+
+    loaded_data = torch.load("datasets/kwargs/forward_with_frequencies_phase_shifts_out.pth")
+    loaded_rgb = loaded_data['rgb']
+    loaded_sigma = loaded_data['sigma']
+
+    err_rgb = (loaded_rgb - rgb).abs().sum()
+    err_sigma = (loaded_sigma - sigma).abs().sum()
+
+    # out.mean().backward()
+    pass
+
+  def test__build_MappingNetwork(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.examples.networks import siren_net_pigan
+    from . import multi_head_mapping
+    from ..nerf import volume_rendering
+    from ..nerf import cam_params
+
+    net = siren_net_pigan.MappingNetwork(**cfg).cuda()
+
+    model_pkl = 'cache_pretrained/pretrained/CelebA/generator.pth'
+    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba.pth"
+    loaded_state_dict = torch.load(saved_pkl)
+    weights_list = []
+    for name, weights in loaded_state_dict.items():
+      if 'mapping_network' in name:
+        weights_list.append(weights)
+
+    state_dict = collections.OrderedDict()
+    for idx, name in enumerate(net.state_dict().keys()):
+      state_dict[name] = weights_list[idx]
+    Checkpointer(net).load_state_dict(state_dict)
+
+    # save converted weights
+    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba_converted_mapping.pth"
+    torch.save(net.state_dict(), saved_pkl)
+
+    pass
+
+  def test__build_StyleMappingBaseNet(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.examples.networks import siren_net_pigan
+
+    net = siren_net_pigan.StyleMappingBaseNet(**cfg).cuda()
+
+    bs = 4
+    z_dim = cfg.z_dim
+    z = torch.randn(bs, z_dim).cuda()
+
+    out = net(z)
+
+    pass
+
+  def test__build_StyleMappingShapeApp(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.examples.networks import siren_net_pigan
+    from . import multi_head_mapping
+    from ..nerf import volume_rendering
+    from ..nerf import cam_params
+
+    nerf_net = siren_net_pigan.NeRF_Net(**cfg.nerf_cfg).cuda()
+
+    mapping_net = siren_net_pigan.StyleMappingShapeApp(**cfg.mapping_cfg,
+                                                       style_dim_dict_shape=nerf_net.style_dim_dict_shape,
+                                                       style_dim_dict_app=nerf_net.style_dim_dict_app).cuda()
+
+    bs = 4
+    z_dim = mapping_net.z_dim
+    z_shape = torch.randn(bs, z_dim).cuda()
+    z_app = torch.randn(bs, z_dim).cuda()
+
+    style_dict = mapping_net(z_shape=z_shape, z_app=z_app)
+
+    mapping_net_state_dict = mapping_net.state_dict()
+    pprint.pprint(list(mapping_net_state_dict.keys()))
+
+    # convert pigan weights
+    model_pkl = 'cache_pretrained/pretrained/CelebA/generator.pth'
+    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba.pth"
+    loaded_state_dict = torch.load(saved_pkl)
+    loaded_mapping_weights_list = []
+    for name, weights in loaded_state_dict.items():
+      if 'mapping_network' in name:
+        loaded_mapping_weights_list.append(weights)
+
+    # base_net weights
+    state_list = loaded_mapping_weights_list[:-2]
+
+    # heads
+    heads_weights = loaded_mapping_weights_list[-2]
+    heads_bias = loaded_mapping_weights_list[-1]
+
+    heads_weights_freq, heads_weights_phase = heads_weights.chunk(2, dim=0)
+    heads_bias_freq, heads_bias_phase = heads_bias.chunk(2, dim=0)
+
+    weights_dict = nerf_net.parse_weight_dict(frequencies=heads_weights_freq, phase_shifts=heads_weights_phase)
+    bias_dict = nerf_net.parse_weight_dict(frequencies=heads_bias_freq, phase_shifts=heads_bias_phase)
+    for _weight, _bias in zip(weights_dict.values(), bias_dict.values()):
+      state_list.append(_weight)
+      state_list.append(_bias)
+    assert len(state_list) == len(mapping_net_state_dict)
+
+    state_dict = collections.OrderedDict()
+    for idx, (name, _) in enumerate(mapping_net_state_dict.items()):
+      state_dict[name] = state_list[idx]
+
+    Checkpointer(mapping_net).load_state_dict(state_dict)
+    # save converted weights
+    saved_pkl = f"{os.path.dirname(model_pkl)}/G_ema_celeba_converted_mapping.pth"
+    torch.save(mapping_net.state_dict(), saved_pkl)
+
+    # check out
+    loaded_kwargs = torch.load("datasets/kwargs/mapping_network_kwargs_out.pth")
+    z_shape = loaded_kwargs['z']
+    frequencies = loaded_kwargs['frequencies']
+    phase_shifts = loaded_kwargs['phase_shifts']
+    loaded_style_dict = nerf_net.parse_style_dict(frequencies=frequencies, phase_shifts=phase_shifts)
+
+    Checkpointer(mapping_net).load_state_dict_from_file(saved_pkl)
+    style_dict = mapping_net(z_shape=z_shape, z_app=z_shape)
+    for name in style_dict.keys():
+      assert (style_dict[name] - loaded_style_dict[name]).abs().sum() == 0, name
+
+    pass
+
+  def test__build_NeRF_Net(self, debug=True):
+    """
+    Usage:
+
+        # export CUDA_VISIBLE_DEVICES=$cuda_devices
+        # export RUN_NUM=$run_num
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export PORT=12345
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts root_obs s3://$bucket/ZhouPeng/ \
+          --tl_outdir results/train_ffhq_256/train_ffhq_256-20210726_202423_412
+          # --tl_outdir results/$resume_dir
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '1'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    if 'RUN_NUM' not in os.environ:
+      os.environ['RUN_NUM'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+
+    if debug:
+      # sys.argv.extend(['--tl_outdir', 'results/train_ffhq_256/train_ffhq_256-test'])
+      pass
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    resume = os.path.isdir(f"{outdir}/ckptdir/resume") and \
+             tl2_utils.parser_args_from_list(name="--tl_outdir", argv_list=sys.argv, type='str') is not None
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/pytorch/examples/networks/siren_net_pigan.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                {"--tl_resume --tl_resumedir " + outdir if resume else ""}
+                --tl_opts {tl_opts}
+                --tl_debug True
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from tl2.proj.pytorch.examples.networks import siren_net_pigan
+    from . import multi_head_mapping
+    from ..nerf import volume_rendering
+    from ..nerf import cam_params
+
+    net = siren_net_pigan.NeRF_Net(**cfg).cuda()
+
+    bs = 4
+    H = 64
+    W = 64
+
+    cam_param = cam_params.CamParams.from_config(num_imgs=1,
+                                                 H0=H,
+                                                 W0=W,
+                                                 so3_repr='axis-angle',
+                                                 intr_repr='square',
+                                                 initial_fov=53.13,
+                                                 freeze_intr=False).cuda()
+
+    intr = cam_param(mode='get_intrinsic')
+
+    rays_o, rays_d, select_inds = cam_param.get_rays_random_pose(
+      device='cuda',
+      bs=bs,
+      intr=intr,
+      h_stddev=0.3,
+      v_stddev=0.155,
+    )
+
+    rays_o = rearrange(rays_o, "b h w c -> b (h w) c", h=H, w=W)
+    rays_d = rearrange(rays_d, "b h w c -> b (h w) c", h=H, w=W)
+
+    z_vals, points = volume_rendering.ray_sample_points(rays_o=rays_o,
+                                                        rays_d=rays_d,
+                                                        near=0.5,
+                                                        far=1.5,
+                                                        N_samples=24,
+                                                        perturb=0)
+
+    print(f"x range: [{points[..., 0].min()}, {points[..., 0].max()}]")
+    print(f"y range: [{points[..., 1].min()}, {points[..., 1].max()}]")
+    print(f"z range: [{points[..., 2].min()}, {points[..., 2].max()}]")
+
+    z_dim = 128
+    style_dim = 128
+
+    mapping_shape = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
+                                                               hidden_dim=net.shape_net_cfg.style_dim,
+                                                               base_layers=4,
+                                                               head_layers=0,
+                                                               head_dim_dict=net.style_dim_dict_shape,
+                                                               add_norm=True,
+                                                               norm_out=True).cuda()
+    mapping_app = multi_head_mapping.MultiHeadMappingNetwork(z_dim=z_dim,
+                                                             hidden_dim=net.app_net_cfg.style_dim,
+                                                             base_layers=4,
+                                                             head_layers=0,
+                                                             head_dim_dict=net.style_dim_dict_app,
+                                                             add_norm=True,
+                                                             norm_out=True).cuda()
+
+    z_shape = torch.randn(bs, z_dim).cuda()
+    z_app = torch.randn(bs, z_dim).cuda()
+
+    style_dict = {}
+    style_dict.update(mapping_shape(z_shape))
+    style_dict.update(mapping_app(z_app))
+
+    points = rearrange(points, "b Nr Ns c -> b (Nr Ns) c")
+    # points = points[:, 0]
+    out = net(points, style_dict)
+
+    out.mean().backward()
+    pass
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/ops/grid_sample.py` & `tl2-0.1.1/tl2/proj/pytorch/ops/grid_sample.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/pytorch/ops/grid_sample_gradfix.py` & `tl2-0.1.1/tl2/proj/pytorch/ops/grid_sample_gradfix.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/pytorch/optim.py` & `tl2-0.1.1/tl2/proj/pytorch/optim.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,827 +1,827 @@
-import os
-import subprocess
-import sys
-import unittest
-import argparse
-
-from torch.optim import SGD
-import torch
-from torch import nn
-
-
-class Testing_optim(unittest.TestCase):
-
-  def test_base_usage(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=./exp:./stylegan2-pytorch:./
-        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
-          Testing_stylegan2().test_train_ffhq_128()"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
-    from template_lib.v2.config_cfgnode.argparser import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                """
-    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import torch
-    import numpy as np
-    import warnings
-    warnings.filterwarnings('ignore')  # ignore warnings
-
-    x = torch.linspace(-np.pi, np.pi, 2000)
-    y = torch.sin(x)
-
-    p = torch.tensor([1, 2, 3])
-    xx = x.unsqueeze(-1).pow(p)
-
-    model = torch.nn.Sequential(
-      torch.nn.Linear(3, 1),
-      torch.nn.Flatten(0, 1)
-    )
-    loss_fn = torch.nn.MSELoss(reduction='sum')
-
-    learning_rate = 1e-3
-    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)
-    for t in range(1, 1001):
-      y_pred = model(xx)
-      loss = loss_fn(y_pred, y)
-      if t % 100 == 0:
-        print('No.{: 5d}, loss: {:.6f}'.format(t, loss.item()))
-      optimizer.zero_grad()  # 梯度清零
-      loss.backward()  # 反向传播计算梯度
-      optimizer.step()  # 梯度下降法更新参数
-
-    pass
-
-  def test_diff_lr(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=./exp:./stylegan2-pytorch:./
-        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
-          Testing_stylegan2().test_train_ffhq_128()"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
-    from template_lib.v2.config_cfgnode.argparser import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                """
-    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from torch.optim import SGD
-    from torch import nn
-
-    class DummyModel(nn.Module):
-      def __init__(self, class_num=10):
-        super(DummyModel, self).__init__()
-        self.base = nn.Sequential(
-          nn.Conv2d(3, 64, kernel_size=3, padding=1),
-          nn.ReLU(),
-          nn.Conv2d(64, 128, kernel_size=3, padding=1),
-          nn.ReLU(),
-        )
-        self.gap = nn.AdaptiveAvgPool2d(1)
-        self.fc = nn.Linear(128, class_num)
-
-      def forward(self, x):
-        x = self.base(x)
-        x = self.gap(x)
-        x = x.view(x.shape[0], -1)
-        x = self.fc(x)
-        return x
-
-    model = DummyModel().cuda()
-
-    optimizer = SGD([
-      {'params': model.base.parameters()},
-      {'params': model.fc.parameters(), 'lr': 1e-3}  # 对 fc的参数设置不同的学习率
-    ], lr=1e-2, momentum=0.9)
-
-    pass
-
-  def test_step_closure(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=./exp:./stylegan2-pytorch:./
-        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
-          Testing_stylegan2().test_train_ffhq_128()"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
-    from template_lib.v2.config_cfgnode.argparser import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                """
-    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    from torch.nn import CrossEntropyLoss
-
-    class DummyModel(nn.Module):
-      def __init__(self, class_num=10):
-        super(DummyModel, self).__init__()
-        self.base = nn.Sequential(
-          nn.Conv2d(3, 64, kernel_size=3, padding=1),
-          nn.ReLU(),
-          nn.Conv2d(64, 128, kernel_size=3, padding=1),
-          nn.ReLU(),
-        )
-        self.gap = nn.AdaptiveAvgPool2d(1)
-        self.fc = nn.Linear(128, class_num)
-
-      def forward(self, x):
-        x = self.base(x)
-        x = self.gap(x)
-        x = x.view(x.shape[0], -1)
-        x = self.fc(x)
-        return x
-
-    dummy_model = DummyModel().cuda()
-
-    optimizer = SGD(dummy_model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)
-    # 定义loss
-    loss_fn = CrossEntropyLoss()
-    # 定义数据
-    batch_size = 2
-    data = torch.randn(64, 3, 64, 128).cuda()  # 制造假数据shape=64 * 3 * 64 * 128
-    data_label = torch.randint(0, 10, size=(64,), dtype=torch.long).cuda()  # 制造假的label
-
-    for batch_index in range(10):
-      batch_data = data[batch_index * batch_size: batch_index * batch_size + batch_size]
-      batch_label = data_label[batch_index * batch_size: batch_index * batch_size + batch_size]
-
-      def closure():
-        optimizer.zero_grad()  # 清空梯度
-        output = dummy_model(batch_data)  # forward
-        loss = loss_fn(output, batch_label)  # 计算loss
-        loss.backward()  # backward
-        print('No.{: 2d} loss: {:.6f}'.format(batch_index, loss.item()))
-        return loss
-
-      optimizer.step(closure=closure)  # 更新参数
-
-    pass
-
-
-class Testing_scheduler(unittest.TestCase):
-
-  def test_StepLR(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=./exp:./stylegan2-pytorch:./
-        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
-          Testing_stylegan2().test_train_ffhq_128()"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
-    from template_lib.v2.config_cfgnode.argparser import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                """
-    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import torch.nn as nn
-    from torch.optim import lr_scheduler
-    from matplotlib import pyplot as plt
-
-    model = nn.Linear(3, 64)
-    def create_optimizer():
-      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
-
-    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
-      lr_li = [[] for _ in range(len(labels))]
-      epoch_li = list(range(nrof_epoch))
-      for epoch in epoch_li:
-        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
-        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
-        for i in range(len(labels)):
-          lr_li[i].append(lr[i])
-      for lr, label in zip(lr_li, labels):
-        plt.plot(epoch_li, lr, label=label)
-      plt.grid()
-      plt.xlabel('epoch')
-      plt.ylabel('lr')
-      plt.title(title)
-      plt.legend()
-      plt.show()
-
-    ## StepLR 可视化学习率
-    optimizer = create_optimizer()
-    scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
-    plot_lr(scheduler, title='StepLR')
-    pass
-
-  def test_MultiplicativeLR(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=./exp:./stylegan2-pytorch:./
-        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
-          Testing_stylegan2().test_train_ffhq_128()"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
-    from template_lib.v2.config_cfgnode.argparser import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                """
-    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import torch.nn as nn
-    from torch.optim import lr_scheduler
-    from matplotlib import pyplot as plt
-
-    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
-      lr_li = [[] for _ in range(len(labels))]
-      epoch_li = list(range(nrof_epoch))
-      for epoch in epoch_li:
-        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
-        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
-        for i in range(len(labels)):
-          lr_li[i].append(lr[i])
-      for lr, label in zip(lr_li, labels):
-        plt.plot(epoch_li, lr, label=label)
-      plt.grid()
-      plt.xlabel('epoch')
-      plt.ylabel('lr')
-      plt.title(title)
-      plt.legend()
-      plt.show()
-
-    ## StepLR 可视化学习率
-
-    base = nn.Linear(3, 32)
-    fc = nn.Linear(32, 10)
-    optimizer = SGD([
-      {'params': base.parameters()},
-      {'params': fc.parameters(), 'lr': 0.05}  # 对 fc的参数设置不同的学习率
-    ], lr=0.1, momentum=0.9)
-    lambda_base = lambda epoch: 0.5 if epoch % 10 == 0 else 1
-    lambda_fc = lambda epoch: 0.8 if epoch % 10 == 0 else 1
-    scheduler = lr_scheduler.MultiplicativeLR(optimizer, [lambda_base, lambda_fc])
-    plot_lr(scheduler, title='MultiplicativeLR', labels=['base', 'fc'])
-    pass
-
-  def test_LambdaLR(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=./exp:./stylegan2-pytorch:./
-        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
-          Testing_stylegan2().test_train_ffhq_128()"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                """
-    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import torch.nn as nn
-    from torch.optim import lr_scheduler
-    from matplotlib import pyplot as plt
-
-    model = nn.Linear(3, 64)
-
-    def create_optimizer():
-      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
-
-    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
-      lr_li = [[] for _ in range(len(labels))]
-      epoch_li = list(range(nrof_epoch))
-      for epoch in epoch_li:
-        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
-        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
-        for i in range(len(labels)):
-          lr_li[i].append(lr[i])
-      for lr, label in zip(lr_li, labels):
-        plt.plot(epoch_li, lr, label=label)
-      plt.grid()
-      plt.xlabel('epoch')
-      plt.ylabel('lr')
-      plt.title(title)
-      plt.legend()
-      plt.show()
-
-    def lambda_foo(epoch):
-      if epoch < 10:
-        return (epoch + 1) * 1e-3
-      elif epoch < 40:
-        return 1e-2
-      else:
-        return 1e-3
-
-    optimizer = create_optimizer()
-    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_foo)
-    plot_lr(scheduler, title='LambdaLR')
-    pass
-
-  def test_ExponentialLR(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=./exp:./stylegan2-pytorch:./
-        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
-          Testing_stylegan2().test_train_ffhq_128()"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
-    from template_lib.v2.config_cfgnode.argparser import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                """
-    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import torch.nn as nn
-    from torch.optim import lr_scheduler
-    from matplotlib import pyplot as plt
-
-    model = nn.Linear(3, 64)
-
-    def create_optimizer():
-      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
-
-    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
-      lr_li = [[] for _ in range(len(labels))]
-      epoch_li = list(range(nrof_epoch))
-      for epoch in epoch_li:
-        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
-        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
-        for i in range(len(labels)):
-          lr_li[i].append(lr[i])
-      for lr, label in zip(lr_li, labels):
-        plt.plot(epoch_li, lr, label=label)
-      plt.grid()
-      plt.xlabel('epoch')
-      plt.ylabel('lr')
-      plt.title(title)
-      plt.legend()
-      plt.show()
-
-    optimizer = create_optimizer()
-    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
-    plot_lr(scheduler, title='ExponentialLR')
-    pass
-
-  def test_CosineAnnealingLR(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=./exp:./stylegan2-pytorch:./
-        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
-          Testing_stylegan2().test_train_ffhq_128()"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
-    from template_lib.v2.config_cfgnode.argparser import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                """
-    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import torch.nn as nn
-    from torch.optim import lr_scheduler
-    from matplotlib import pyplot as plt
-
-    model = nn.Linear(3, 64)
-
-    def create_optimizer():
-      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
-
-    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
-      lr_li = [[] for _ in range(len(labels))]
-      epoch_li = list(range(nrof_epoch))
-      for epoch in epoch_li:
-        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
-        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
-        for i in range(len(labels)):
-          lr_li[i].append(lr[i])
-      for lr, label in zip(lr_li, labels):
-        plt.plot(epoch_li, lr, label=label)
-      plt.grid()
-      plt.xlabel('epoch')
-      plt.ylabel('lr')
-      plt.title(title)
-      plt.legend()
-      plt.show()
-
-    optimizer = create_optimizer()
-    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5)
-    plot_lr(scheduler, title='CosineAnnealingLR')
-
-    pass
-
-  def test_CosineAnnealingWarmRestarts(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=./exp:./stylegan2-pytorch:./
-        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
-          Testing_stylegan2().test_train_ffhq_128()"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                """
-    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import torch.nn as nn
-    from torch.optim import lr_scheduler
-    from matplotlib import pyplot as plt
-
-    model = nn.Linear(3, 64)
-
-    def create_optimizer():
-      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
-
-    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
-      lr_li = [[] for _ in range(len(labels))]
-      epoch_li = list(range(nrof_epoch))
-      for epoch in epoch_li:
-        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
-        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
-        for i in range(len(labels)):
-          lr_li[i].append(lr[i])
-      for lr, label in zip(lr_li, labels):
-        plt.plot(epoch_li, lr, label=label)
-      plt.grid()
-      plt.xlabel('epoch')
-      plt.ylabel('lr')
-      plt.title(title)
-      plt.legend()
-      plt.show()
-
-    optimizer = create_optimizer()
-    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)
-    plot_lr(scheduler, title='CosineAnnealingWarmRestarts')
-
-    pass
-
-  def test_CyclicLR(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=./exp:./stylegan2-pytorch:./
-        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
-          Testing_stylegan2().test_train_ffhq_128()"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
-    from template_lib.v2.config_cfgnode.argparser import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                """
-    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import torch.nn as nn
-    from torch.optim import lr_scheduler
-    from matplotlib import pyplot as plt
-
-    model = nn.Linear(3, 64)
-
-    def create_optimizer():
-      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
-
-    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
-      lr_li = [[] for _ in range(len(labels))]
-      epoch_li = list(range(nrof_epoch))
-      for epoch in epoch_li:
-        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
-        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
-        for i in range(len(labels)):
-          lr_li[i].append(lr[i])
-      for lr, label in zip(lr_li, labels):
-        plt.plot(epoch_li, lr, label=label)
-      plt.grid()
-      plt.xlabel('epoch')
-      plt.ylabel('lr')
-      plt.title(title)
-      plt.legend()
-      plt.show()
-
-    optimizer = create_optimizer()
-    scheduler = lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1, step_size_up=25, step_size_down=10)
-    plot_lr(scheduler, title='CyclicLR')
-
-    pass
-  
-  def test_OneCycleLR(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=./exp:./stylegan2-pytorch:./
-        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
-          Testing_stylegan2().test_train_ffhq_128()"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                """
-    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    import torch.nn as nn
-    from torch.optim import lr_scheduler
-    from matplotlib import pyplot as plt
-
-    model = nn.Linear(3, 64)
-
-    def create_optimizer():
-      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
-
-    def plot_lr(scheduler, optimizer, title='', labels=['base'], nrof_epoch=100):
-      lr_li = [[] for _ in range(len(labels))]
-      epoch_li = list(range(nrof_epoch))
-      for epoch in epoch_li:
-        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
-        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
-        lr = [optimizer.param_groups[0]['lr']]
-        for i in range(len(labels)):
-          lr_li[i].append(lr[i])
-      for lr, label in zip(lr_li, labels):
-        plt.plot(epoch_li, lr, label=label)
-      plt.grid()
-      plt.xlabel('epoch')
-      plt.ylabel('lr')
-      plt.title(title)
-      plt.legend()
-      plt.show()
-
-    optimizer = create_optimizer()
-    scheduler = lr_scheduler.OneCycleLR(optimizer, 0.1, total_steps=100)
-    plot_lr(scheduler, optimizer, title='OneCycleLR')
+import os
+import subprocess
+import sys
+import unittest
+import argparse
+
+from torch.optim import SGD
+import torch
+from torch import nn
+
+
+class Testing_optim(unittest.TestCase):
+
+  def test_base_usage(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=./exp:./stylegan2-pytorch:./
+        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
+          Testing_stylegan2().test_train_ffhq_128()"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
+    from template_lib.v2.config_cfgnode.argparser import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                """
+    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import torch
+    import numpy as np
+    import warnings
+    warnings.filterwarnings('ignore')  # ignore warnings
+
+    x = torch.linspace(-np.pi, np.pi, 2000)
+    y = torch.sin(x)
+
+    p = torch.tensor([1, 2, 3])
+    xx = x.unsqueeze(-1).pow(p)
+
+    model = torch.nn.Sequential(
+      torch.nn.Linear(3, 1),
+      torch.nn.Flatten(0, 1)
+    )
+    loss_fn = torch.nn.MSELoss(reduction='sum')
+
+    learning_rate = 1e-3
+    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)
+    for t in range(1, 1001):
+      y_pred = model(xx)
+      loss = loss_fn(y_pred, y)
+      if t % 100 == 0:
+        print('No.{: 5d}, loss: {:.6f}'.format(t, loss.item()))
+      optimizer.zero_grad()  # 梯度清零
+      loss.backward()  # 反向传播计算梯度
+      optimizer.step()  # 梯度下降法更新参数
+
+    pass
+
+  def test_diff_lr(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=./exp:./stylegan2-pytorch:./
+        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
+          Testing_stylegan2().test_train_ffhq_128()"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
+    from template_lib.v2.config_cfgnode.argparser import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                """
+    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from torch.optim import SGD
+    from torch import nn
+
+    class DummyModel(nn.Module):
+      def __init__(self, class_num=10):
+        super(DummyModel, self).__init__()
+        self.base = nn.Sequential(
+          nn.Conv2d(3, 64, kernel_size=3, padding=1),
+          nn.ReLU(),
+          nn.Conv2d(64, 128, kernel_size=3, padding=1),
+          nn.ReLU(),
+        )
+        self.gap = nn.AdaptiveAvgPool2d(1)
+        self.fc = nn.Linear(128, class_num)
+
+      def forward(self, x):
+        x = self.base(x)
+        x = self.gap(x)
+        x = x.view(x.shape[0], -1)
+        x = self.fc(x)
+        return x
+
+    model = DummyModel().cuda()
+
+    optimizer = SGD([
+      {'params': model.base.parameters()},
+      {'params': model.fc.parameters(), 'lr': 1e-3}  # 对 fc的参数设置不同的学习率
+    ], lr=1e-2, momentum=0.9)
+
+    pass
+
+  def test_step_closure(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=./exp:./stylegan2-pytorch:./
+        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
+          Testing_stylegan2().test_train_ffhq_128()"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
+    from template_lib.v2.config_cfgnode.argparser import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                """
+    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    from torch.nn import CrossEntropyLoss
+
+    class DummyModel(nn.Module):
+      def __init__(self, class_num=10):
+        super(DummyModel, self).__init__()
+        self.base = nn.Sequential(
+          nn.Conv2d(3, 64, kernel_size=3, padding=1),
+          nn.ReLU(),
+          nn.Conv2d(64, 128, kernel_size=3, padding=1),
+          nn.ReLU(),
+        )
+        self.gap = nn.AdaptiveAvgPool2d(1)
+        self.fc = nn.Linear(128, class_num)
+
+      def forward(self, x):
+        x = self.base(x)
+        x = self.gap(x)
+        x = x.view(x.shape[0], -1)
+        x = self.fc(x)
+        return x
+
+    dummy_model = DummyModel().cuda()
+
+    optimizer = SGD(dummy_model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)
+    # 定义loss
+    loss_fn = CrossEntropyLoss()
+    # 定义数据
+    batch_size = 2
+    data = torch.randn(64, 3, 64, 128).cuda()  # 制造假数据shape=64 * 3 * 64 * 128
+    data_label = torch.randint(0, 10, size=(64,), dtype=torch.long).cuda()  # 制造假的label
+
+    for batch_index in range(10):
+      batch_data = data[batch_index * batch_size: batch_index * batch_size + batch_size]
+      batch_label = data_label[batch_index * batch_size: batch_index * batch_size + batch_size]
+
+      def closure():
+        optimizer.zero_grad()  # 清空梯度
+        output = dummy_model(batch_data)  # forward
+        loss = loss_fn(output, batch_label)  # 计算loss
+        loss.backward()  # backward
+        print('No.{: 2d} loss: {:.6f}'.format(batch_index, loss.item()))
+        return loss
+
+      optimizer.step(closure=closure)  # 更新参数
+
+    pass
+
+
+class Testing_scheduler(unittest.TestCase):
+
+  def test_StepLR(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=./exp:./stylegan2-pytorch:./
+        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
+          Testing_stylegan2().test_train_ffhq_128()"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
+    from template_lib.v2.config_cfgnode.argparser import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                """
+    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import torch.nn as nn
+    from torch.optim import lr_scheduler
+    from matplotlib import pyplot as plt
+
+    model = nn.Linear(3, 64)
+    def create_optimizer():
+      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
+
+    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
+      lr_li = [[] for _ in range(len(labels))]
+      epoch_li = list(range(nrof_epoch))
+      for epoch in epoch_li:
+        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
+        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
+        for i in range(len(labels)):
+          lr_li[i].append(lr[i])
+      for lr, label in zip(lr_li, labels):
+        plt.plot(epoch_li, lr, label=label)
+      plt.grid()
+      plt.xlabel('epoch')
+      plt.ylabel('lr')
+      plt.title(title)
+      plt.legend()
+      plt.show()
+
+    ## StepLR 可视化学习率
+    optimizer = create_optimizer()
+    scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
+    plot_lr(scheduler, title='StepLR')
+    pass
+
+  def test_MultiplicativeLR(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=./exp:./stylegan2-pytorch:./
+        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
+          Testing_stylegan2().test_train_ffhq_128()"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
+    from template_lib.v2.config_cfgnode.argparser import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                """
+    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import torch.nn as nn
+    from torch.optim import lr_scheduler
+    from matplotlib import pyplot as plt
+
+    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
+      lr_li = [[] for _ in range(len(labels))]
+      epoch_li = list(range(nrof_epoch))
+      for epoch in epoch_li:
+        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
+        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
+        for i in range(len(labels)):
+          lr_li[i].append(lr[i])
+      for lr, label in zip(lr_li, labels):
+        plt.plot(epoch_li, lr, label=label)
+      plt.grid()
+      plt.xlabel('epoch')
+      plt.ylabel('lr')
+      plt.title(title)
+      plt.legend()
+      plt.show()
+
+    ## StepLR 可视化学习率
+
+    base = nn.Linear(3, 32)
+    fc = nn.Linear(32, 10)
+    optimizer = SGD([
+      {'params': base.parameters()},
+      {'params': fc.parameters(), 'lr': 0.05}  # 对 fc的参数设置不同的学习率
+    ], lr=0.1, momentum=0.9)
+    lambda_base = lambda epoch: 0.5 if epoch % 10 == 0 else 1
+    lambda_fc = lambda epoch: 0.8 if epoch % 10 == 0 else 1
+    scheduler = lr_scheduler.MultiplicativeLR(optimizer, [lambda_base, lambda_fc])
+    plot_lr(scheduler, title='MultiplicativeLR', labels=['base', 'fc'])
+    pass
+
+  def test_LambdaLR(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=./exp:./stylegan2-pytorch:./
+        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
+          Testing_stylegan2().test_train_ffhq_128()"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                """
+    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import torch.nn as nn
+    from torch.optim import lr_scheduler
+    from matplotlib import pyplot as plt
+
+    model = nn.Linear(3, 64)
+
+    def create_optimizer():
+      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
+
+    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
+      lr_li = [[] for _ in range(len(labels))]
+      epoch_li = list(range(nrof_epoch))
+      for epoch in epoch_li:
+        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
+        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
+        for i in range(len(labels)):
+          lr_li[i].append(lr[i])
+      for lr, label in zip(lr_li, labels):
+        plt.plot(epoch_li, lr, label=label)
+      plt.grid()
+      plt.xlabel('epoch')
+      plt.ylabel('lr')
+      plt.title(title)
+      plt.legend()
+      plt.show()
+
+    def lambda_foo(epoch):
+      if epoch < 10:
+        return (epoch + 1) * 1e-3
+      elif epoch < 40:
+        return 1e-2
+      else:
+        return 1e-3
+
+    optimizer = create_optimizer()
+    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_foo)
+    plot_lr(scheduler, title='LambdaLR')
+    pass
+
+  def test_ExponentialLR(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=./exp:./stylegan2-pytorch:./
+        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
+          Testing_stylegan2().test_train_ffhq_128()"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
+    from template_lib.v2.config_cfgnode.argparser import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                """
+    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import torch.nn as nn
+    from torch.optim import lr_scheduler
+    from matplotlib import pyplot as plt
+
+    model = nn.Linear(3, 64)
+
+    def create_optimizer():
+      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
+
+    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
+      lr_li = [[] for _ in range(len(labels))]
+      epoch_li = list(range(nrof_epoch))
+      for epoch in epoch_li:
+        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
+        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
+        for i in range(len(labels)):
+          lr_li[i].append(lr[i])
+      for lr, label in zip(lr_li, labels):
+        plt.plot(epoch_li, lr, label=label)
+      plt.grid()
+      plt.xlabel('epoch')
+      plt.ylabel('lr')
+      plt.title(title)
+      plt.legend()
+      plt.show()
+
+    optimizer = create_optimizer()
+    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
+    plot_lr(scheduler, title='ExponentialLR')
+    pass
+
+  def test_CosineAnnealingLR(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=./exp:./stylegan2-pytorch:./
+        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
+          Testing_stylegan2().test_train_ffhq_128()"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
+    from template_lib.v2.config_cfgnode.argparser import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                """
+    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import torch.nn as nn
+    from torch.optim import lr_scheduler
+    from matplotlib import pyplot as plt
+
+    model = nn.Linear(3, 64)
+
+    def create_optimizer():
+      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
+
+    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
+      lr_li = [[] for _ in range(len(labels))]
+      epoch_li = list(range(nrof_epoch))
+      for epoch in epoch_li:
+        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
+        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
+        for i in range(len(labels)):
+          lr_li[i].append(lr[i])
+      for lr, label in zip(lr_li, labels):
+        plt.plot(epoch_li, lr, label=label)
+      plt.grid()
+      plt.xlabel('epoch')
+      plt.ylabel('lr')
+      plt.title(title)
+      plt.legend()
+      plt.show()
+
+    optimizer = create_optimizer()
+    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5)
+    plot_lr(scheduler, title='CosineAnnealingLR')
+
+    pass
+
+  def test_CosineAnnealingWarmRestarts(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=./exp:./stylegan2-pytorch:./
+        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
+          Testing_stylegan2().test_train_ffhq_128()"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                """
+    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import torch.nn as nn
+    from torch.optim import lr_scheduler
+    from matplotlib import pyplot as plt
+
+    model = nn.Linear(3, 64)
+
+    def create_optimizer():
+      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
+
+    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
+      lr_li = [[] for _ in range(len(labels))]
+      epoch_li = list(range(nrof_epoch))
+      for epoch in epoch_li:
+        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
+        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
+        for i in range(len(labels)):
+          lr_li[i].append(lr[i])
+      for lr, label in zip(lr_li, labels):
+        plt.plot(epoch_li, lr, label=label)
+      plt.grid()
+      plt.xlabel('epoch')
+      plt.ylabel('lr')
+      plt.title(title)
+      plt.legend()
+      plt.show()
+
+    optimizer = create_optimizer()
+    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)
+    plot_lr(scheduler, title='CosineAnnealingWarmRestarts')
+
+    pass
+
+  def test_CyclicLR(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=./exp:./stylegan2-pytorch:./
+        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
+          Testing_stylegan2().test_train_ffhq_128()"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0' if utils.is_debugging() else '0'
+    from template_lib.v2.config_cfgnode.argparser import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                """
+    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import torch.nn as nn
+    from torch.optim import lr_scheduler
+    from matplotlib import pyplot as plt
+
+    model = nn.Linear(3, 64)
+
+    def create_optimizer():
+      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
+
+    def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
+      lr_li = [[] for _ in range(len(labels))]
+      epoch_li = list(range(nrof_epoch))
+      for epoch in epoch_li:
+        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
+        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
+        for i in range(len(labels)):
+          lr_li[i].append(lr[i])
+      for lr, label in zip(lr_li, labels):
+        plt.plot(epoch_li, lr, label=label)
+      plt.grid()
+      plt.xlabel('epoch')
+      plt.ylabel('lr')
+      plt.title(title)
+      plt.legend()
+      plt.show()
+
+    optimizer = create_optimizer()
+    scheduler = lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1, step_size_up=25, step_size_down=10)
+    plot_lr(scheduler, title='CyclicLR')
+
+    pass
+  
+  def test_OneCycleLR(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=./exp:./stylegan2-pytorch:./
+        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
+          Testing_stylegan2().test_train_ffhq_128()"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                """
+    args = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    import torch.nn as nn
+    from torch.optim import lr_scheduler
+    from matplotlib import pyplot as plt
+
+    model = nn.Linear(3, 64)
+
+    def create_optimizer():
+      return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
+
+    def plot_lr(scheduler, optimizer, title='', labels=['base'], nrof_epoch=100):
+      lr_li = [[] for _ in range(len(labels))]
+      epoch_li = list(range(nrof_epoch))
+      for epoch in epoch_li:
+        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
+        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
+        lr = [optimizer.param_groups[0]['lr']]
+        for i in range(len(labels)):
+          lr_li[i].append(lr[i])
+      for lr, label in zip(lr_li, labels):
+        plt.plot(epoch_li, lr, label=label)
+      plt.grid()
+      plt.xlabel('epoch')
+      plt.ylabel('lr')
+      plt.title(title)
+      plt.legend()
+      plt.show()
+
+    optimizer = create_optimizer()
+    scheduler = lr_scheduler.OneCycleLR(optimizer, 0.1, total_steps=100)
+    plot_lr(scheduler, optimizer, title='OneCycleLR')
     pass
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/pytorch_hook.py` & `tl2-0.1.1/tl2/proj/pytorch/pytorch_hook.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,534 +1,534 @@
-import time
-from itertools import chain
-import os
-import sys
-from operator import attrgetter
-import pickle
-from typing import Dict, Iterable, Callable, Tuple
-import copy
-import unittest
-from termcolor import cprint
-import logging
-from functools import partial
-
-import torch
-from torch import nn, Tensor
-
-from tl2.tl2_utils import TermColor, attrgetter_default
-from tl2.proj.pytorch import torch_utils
-from tl2 import tl2_utils
-
-__all__ = ["VerboseModel", "FeatureExtractor", "GradExtractor", ]
-
-
-class VerboseModel(nn.Module):
-  def __init__(self,
-               model: nn.Module,
-               submodels=None,
-               name_prefix="",
-               register_itself=False,
-               register_children=True,
-               add_newline=True,
-               input_padding=36,
-               **kwargs):
-    super().__init__()
-    self.add_newline = add_newline
-    self.input_padding = input_padding
-
-    try:
-      self.model = copy.deepcopy(model)
-    except:
-      self.model = pickle.loads(pickle.dumps(model))
-
-    term_color = TermColor()
-
-    # Register a hook for each layer
-    if register_children:
-      modules = self.model.named_children()
-    else:
-      modules = []
-    if register_itself:
-      modules = chain([("", self.model)], modules)
-      self.model.__term_color__ = term_color.green
-
-    for name, layer in modules:
-      class_name = attrgetter_default(layer, "__class__.__name__", default='')
-      layer.__name__ = f"{name_prefix}{name} ({class_name})"
-      if not hasattr(layer, '__term_color__'):
-        layer.__term_color__ = term_color.black
-      layer.register_forward_hook(self._hook())
-
-    if submodels is not None:
-      if not isinstance(submodels, (list, tuple)):
-        submodels = list(submodels)
-      for submodel in submodels:
-        color = term_color.get_a_color()
-        sub_module = attrgetter(submodel)(self.model)
-        for name, layer in sub_module.named_children():
-          class_name = attrgetter_default(layer, "__class__.__name__", default='')
-          layer.__name__ = f"{name_prefix}{submodel}.{name} ({class_name})"
-          layer.__term_color__ = color
-          layer.register_forward_hook(self._hook())
-    pass
-
-  def _hook(self, ) -> Callable:
-    def fn(layer, input, output):
-      input_shape = f"in({len(input)}): "
-      for elem in input:
-        if hasattr(elem, 'shape') and elem.nelement() > 0:
-          input_shape += f"{str(list(elem.shape))}"
-          min_max = f"({elem.min():.2f}, {elem.max():.2f}, m{elem.mean():.2f}, s{elem.std():.2f}), "
-          input_shape += min_max
-        else:
-          input_shape += f"{type(elem)}, "
-
-      input_shape = input_shape.strip(" ,")
-      # if self.add_newline:
-      #   input_shape += "\n"
-
-      if not isinstance(output, tuple):
-        output = (output, )
-      output_shape = f"out({len(output)}): "
-      for elem in output:
-        if hasattr(elem, 'shape'):
-          output_shape += f"{str(list(elem.shape))}"
-          min_max = f"({elem.min():.2f}, {elem.max():.2f}, m{elem.mean():.2f}, s{elem.std():.2f}), "
-          output_shape += min_max
-        else:
-          output_shape += f"{type(elem)}, "
-      output_shape = output_shape.strip(" ,")
-
-      num_params = sum([p.data.nelement() for p in layer.parameters()])/1e6
-      num_bufs = sum([p.data.nelement() for p in layer.buffers()])
-      param_str = f"{'':<{1}} paras: {str(num_params)}M "
-      param_str = f"{param_str:<{19}}bufs: {str(num_bufs)}M"
-
-      layer_str = str(layer)
-      if '\n' in layer_str:
-        layer_str = ''
-      min_max_v = self._get_parameters_min_max_values(layer=layer, attr_name='weight')
-      if min_max_v:
-        layer_str += f", {min_max_v}"
-      min_max_v = self._get_parameters_min_max_values(layer=layer, attr_name='bias')
-      if min_max_v:
-        layer_str += f", {min_max_v}"
-
-      cprint(f"{(layer.__name__ + ': '):<{self.input_padding}}{input_shape}"             
-             f"\n"
-             f"{param_str:<{self.input_padding - 3}}=>{output_shape}"
-             f"{'':<{3}} {layer_str}"
-             , color=layer.__term_color__)
-
-    return fn
-
-  def _get_parameters_min_max_values(self, layer, attr_name):
-    min_max_str = ""
-    if hasattr(layer, attr_name):
-      with torch.no_grad():
-        p = getattr(layer, attr_name)
-        if p is None:
-          return f"{attr_name}({p})"
-        min_v = p.min()
-        max_v = p.max()
-        min_max_str = f"{attr_name}({min_v:.2f}, {max_v:.2f}, m{p.mean():.2f})"
-    return min_max_str
-
-
-  def forward(self, *args, **kwargs) -> Tensor:
-    return self.model(*args, **kwargs)
-
-  @staticmethod
-  def forward_verbose(model: nn.Module,
-                      inputs_args: Tuple = None,
-                      inputs_kwargs: Dict = None,
-                      submodels=None,
-                      name_prefix="",
-                      register_itself=True,
-                      register_children=True,
-                      add_newline=True,
-                      input_padding=36,
-                      verbose=True,
-                      **kwargs
-                      ):
-    with torch.no_grad():
-      model_ver = VerboseModel(model=model,
-                               submodels=submodels,
-                               name_prefix=name_prefix,
-                               register_itself=register_itself,
-                               register_children=register_children,
-                               add_newline=add_newline,
-                               input_padding=input_padding,
-                               )
-      if inputs_args is None:
-        inputs_args = ()
-      if inputs_kwargs is None:
-        inputs_kwargs = {}
-      # inputs_args = copy.deepcopy(inputs_args)
-      # inputs_kwargs = copy.deepcopy(inputs_kwargs)
-      time_start = time.time()
-      out = model_ver(*inputs_args, **inputs_kwargs)
-      elapsed = time.time() - time_start
-      del model_ver, out
-
-      if verbose:
-        torch_utils.print_number_params({name_prefix: model}, add_info=f"time: {elapsed:.4f}")
-    return
-
-class FeatureExtractor(nn.Module):
-  def __init__(self, model: nn.Module, layers: Iterable[str]):
-    super().__init__()
-    self.model = model
-    self.layers = layers
-    self._features = {layer: torch.empty(0) for layer in layers}
-
-    for layer_id in layers:
-      layer = dict([*self.model.named_modules()])[layer_id]
-      layer.register_forward_hook(self.save_outputs_hook(layer_id))
-    pass
-
-  def save_outputs_hook(self, layer_id: str) -> Callable:
-    def fn(_, __, output):
-      self._features[layer_id] = output
-    return fn
-
-  def forward(self, *args, **kwargs) -> Dict[str, Tensor]:
-    self._features.clear()
-    _ = self.model(*args, **kwargs)
-    return self._features
-
-
-class FeatureExtractor_v1(nn.Module):
-  def __init__(self, model: nn.Module, layers: Iterable[str]):
-    super().__init__()
-    self.model = model
-    self.layers = layers
-    self._features = {layer: torch.empty(0) for layer in layers}
-
-    for layer_id in layers:
-      layer = dict([*self.model.named_modules()])[layer_id]
-      layer.register_forward_hook(self.save_outputs_hook(layer_id))
-    pass
-
-  def save_outputs_hook(self, layer_id: str) -> Callable:
-    hook = partial(self.fn, _features=self._features, layer_id=layer_id)
-    return hook
-
-  @staticmethod
-  def fn(_, __, output, _features, layer_id):
-    _features[layer_id] = output
-
-  def forward(self, *args, **kwargs):
-    self._features.clear()
-    _ = self.model(*args, **kwargs)
-
-    out = {k: v for k, v in self._features.items()}
-    self._features.clear()
-
-    return out
-
-
-
-class GradExtractor(nn.Module):
-  def __init__(self, model: nn.Module, param_names: Iterable[str]=[]):
-    super().__init__()
-    self.model = model
-    self.param_names = param_names
-    self._grads = {}
-
-    for name, param in model.named_parameters():
-      print(f"{name:<30}: {list(param.shape)}")
-      if name in self.param_names:
-        param.register_hook(hook=self._hook(name))
-    pass
-
-  def _hook(self, name: str) -> Callable:
-    def fn(grad):
-      self._grads[name] = grad.clone()
-      return grad
-    return fn
-
-  def forward(self, x: Tensor) -> Dict[str, Tensor]:
-    self._grads.clear()
-    out = self.model(x)
-    return out
-
-
-
-class PytorchHook(unittest.TestCase):
-
-  def test_register_forward_hook(self):
-
-    import torch
-    import torch.nn as nn
-
-    class TestForHook(nn.Module):
-      def __init__(self):
-        super().__init__()
-
-        self.linear_1 = nn.Linear(in_features=2, out_features=2)
-        self.linear_2 = nn.Linear(in_features=2, out_features=1)
-        self.relu = nn.ReLU()
-        self.relu6 = nn.ReLU6()
-        self.initialize()
-
-      def forward(self, x):
-        linear_1 = self.linear_1(x)
-        linear_2 = self.linear_2(linear_1)
-        relu = self.relu(linear_2)
-        relu_6 = self.relu6(relu)
-        layers_in = (x, linear_1, linear_2)
-        layers_out = (linear_1, linear_2, relu)
-        return relu_6, layers_in, layers_out
-
-      def initialize(self):
-        """ 定义特殊的初始化，用于验证是不是获取了权重"""
-        self.linear_1.weight = torch.nn.Parameter(torch.FloatTensor([[1, 1], [1, 1]]))
-        self.linear_1.bias = torch.nn.Parameter(torch.FloatTensor([1, 1]))
-        self.linear_2.weight = torch.nn.Parameter(torch.FloatTensor([[1, 1]]))
-        self.linear_2.bias = torch.nn.Parameter(torch.FloatTensor([1]))
-        return True
-
-    # 1：定义用于获取网络各层输入输出tensor的容器
-    # 并定义module_name用于记录相应的module名字
-    module_name = []
-    features_in_hook = []
-    features_out_hook = []
-
-    # 2：hook函数负责将获取的输入输出添加到feature列表中
-    # 并提供相应的module名字
-    def hook(module, fea_in, fea_out):
-      print("hooker working")
-      module_name.append(module.__class__)
-      features_in_hook.append(fea_in)
-      features_out_hook.append(fea_out)
-      return None
-
-    # 3：定义全部是1的输入
-    x = torch.FloatTensor([[0.1, 0.1], [0.1, 0.1]])
-
-    # 4:注册钩子可以对某些层单独进行
-    net = TestForHook()
-    net_chilren = net.children()
-    for child in net_chilren:
-      if not isinstance(child, nn.ReLU6):
-        child.register_forward_hook(hook=hook)
-
-    # 5:测试网络输出
-    out, features_in_forward, features_out_forward = net(x)
-    print("*" * 5 + "forward return features" + "*" * 5)
-    print(features_in_forward)
-    print(features_out_forward)
-    print("*" * 5 + "forward return features" + "*" * 5)
-
-    # 6:测试features_in是不是存储了输入
-    print("*" * 5 + "hook record features" + "*" * 5)
-    print(features_in_hook)
-    print(features_out_hook)
-    print(module_name)
-    print("*" * 5 + "hook record features" + "*" * 5)
-
-    # 7：测试forward返回的feautes_in是不是和hook记录的一致
-    print("sub result")
-    for forward_return, hook_record in zip(features_in_forward, features_in_hook):
-      print(forward_return - hook_record[0])
-
-    pass
-
-  def test_VerboseModel(self):
-
-    from torchvision.models.segmentation import deeplabv3_resnet101
-
-    verbose_resnet = VerboseModel(deeplabv3_resnet101(pretrained=True), submodels=['backbone', 'classifier', ])
-    dummy_input = torch.ones(10, 3, 256, 256)
-    _ = verbose_resnet(dummy_input)
-
-    pass
-
-  def test_VerboseModel_register_itself(self):
-
-    import torch
-
-    net = torch.nn.Linear(256, 512)
-    net_ver = VerboseModel(net, register_itself=True)
-    dummy_input = torch.ones(10, 256)
-    out = net_ver(dummy_input)
-
-    net = torch.nn.Sequential(
-      torch.nn.Linear(256, 512)
-    )
-    net_ver = VerboseModel(net, register_itself=True, name_prefix='linear.')
-    dummy_input = torch.ones(10, 256)
-    out = net_ver(dummy_input)
-
-    pass
-
-  def test_VerboseModel_multi_input(self):
-
-    import torch
-
-    class FiLMLayer(nn.Module):
-      def __init__(self, input_dim, hidden_dim):
-        super().__init__()
-        self.layer = nn.Linear(input_dim, hidden_dim)
-
-      def forward(self, x, freq, phase_shift):
-        x = self.layer(x)
-        freq = freq.unsqueeze(1).expand_as(x)
-        phase_shift = phase_shift.unsqueeze(1).expand_as(x)
-        out = torch.sin(freq * x + phase_shift)
-        return out, out
-
-    net = FiLMLayer(256, 512)
-    net_ver = VerboseModel(net, name_prefix="FiLM.", register_itself=True, register_children=False)
-    x = torch.randn(2, 12, 256)
-    freq = torch.randn(2, 512)
-    phase = torch.randn(2, 512)
-    out = net_ver(x, freq, phase)
-
-    pass
-
-  def test_FeatureExtractor(self):
-
-    from torchvision.models import resnet50
-
-    verbose_resnet = VerboseModel(resnet50(), submodels=['layer1', 'layer4', 'layer4.2'])
-    dummy_input = torch.ones(10, 3, 224, 224)
-    verbose_resnet(dummy_input)
-
-    resnet_features = FeatureExtractor(resnet50(), layers=["layer4.2.relu", "avgpool", ])
-    features = resnet_features(dummy_input)
-
-    print({name: output.shape for name, output in features.items()})
-
-    pass
-
-  def test_GradExtractor(self):
-
-    from torchvision.models import resnet50
-
-    resnet_grad = GradExtractor(resnet50(), param_names=['fc.weight', 'conv1.weight'])
-    dummy_input = torch.ones(10, 3, 224, 224)
-
-    out = resnet_grad(dummy_input)
-    loss = out.mean()
-    loss.backward()
-    grads = resnet_grad._grads
-    print({name: output.mean() for name, output in grads.items()})
-
-    pass
-
-  def test_register_hook_grad(self):
-
-    import torch
-
-    y_grad = list()
-    def grad_hook(grad):
-      y_grad.append(grad)
-
-    x = torch.tensor([2., 2., 2., 2.], requires_grad=True)
-    y = torch.pow(x, 2)
-    z = torch.mean(y)
-
-    h = y.register_hook(grad_hook)
-
-    z.backward()
-    print("y.grad: ", y.grad)
-    print("y_grad[0]: ", y_grad[0])
-    h.remove()  # removes the hook
-    pass
-
-  def test_register_hook_scale_grad(self):
-
-    import torch
-
-    def grad_hook(grad):
-      grad *= 2
-
-    x = torch.tensor([2., 2., 2., 2.], requires_grad=True)
-    y = torch.pow(x, 2)
-    z = torch.mean(y)
-    h = x.register_hook(grad_hook)
-    z.backward()
-    print(x.grad)
-    h.remove()  # removes the hook
-
-    pass
-
-  def test_hook_for_grad_cam(self, debug=True):
-
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=./exp:./stylegan2-pytorch:./
-        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
-          Testing_stylegan2().test_train_ffhq_128()"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-    cmd_str = f"""
-        python 
-        template_lib/proj/pytorch/examples/hook_for_grad_cam.py
-        {get_append_cmd_str(args)}
-        """
-    if debug:
-      cmd_str += f"""
-                  --tl_debug
-                  --tl_opts 
-                  """
-    else:
-      cmd_str += f"""
-                  --tl_opts {tl_opts}
-                  """
-    start_cmd_run(cmd_str)
-    # from template_lib.v2.config_cfgnode import update_parser_defaults_from_yaml, global_cfg
-    # from template_lib.modelarts import modelarts_utils
-    # update_parser_defaults_from_yaml(parser)
-
-    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
-    #
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-
-    pass
-
-
-
-
-
-
-
-
+import time
+from itertools import chain
+import os
+import sys
+from operator import attrgetter
+import pickle
+from typing import Dict, Iterable, Callable, Tuple
+import copy
+import unittest
+from termcolor import cprint
+import logging
+from functools import partial
+
+import torch
+from torch import nn, Tensor
+
+from tl2.tl2_utils import TermColor, attrgetter_default
+from tl2.proj.pytorch import torch_utils
+from tl2 import tl2_utils
+
+__all__ = ["VerboseModel", "FeatureExtractor", "GradExtractor", ]
+
+
+class VerboseModel(nn.Module):
+  def __init__(self,
+               model: nn.Module,
+               submodels=None,
+               name_prefix="",
+               register_itself=False,
+               register_children=True,
+               add_newline=True,
+               input_padding=36,
+               **kwargs):
+    super().__init__()
+    self.add_newline = add_newline
+    self.input_padding = input_padding
+
+    try:
+      self.model = copy.deepcopy(model)
+    except:
+      self.model = pickle.loads(pickle.dumps(model))
+
+    term_color = TermColor()
+
+    # Register a hook for each layer
+    if register_children:
+      modules = self.model.named_children()
+    else:
+      modules = []
+    if register_itself:
+      modules = chain([("", self.model)], modules)
+      self.model.__term_color__ = term_color.green
+
+    for name, layer in modules:
+      class_name = attrgetter_default(layer, "__class__.__name__", default='')
+      layer.__name__ = f"{name_prefix}{name} ({class_name})"
+      if not hasattr(layer, '__term_color__'):
+        layer.__term_color__ = term_color.black
+      layer.register_forward_hook(self._hook())
+
+    if submodels is not None:
+      if not isinstance(submodels, (list, tuple)):
+        submodels = list(submodels)
+      for submodel in submodels:
+        color = term_color.get_a_color()
+        sub_module = attrgetter(submodel)(self.model)
+        for name, layer in sub_module.named_children():
+          class_name = attrgetter_default(layer, "__class__.__name__", default='')
+          layer.__name__ = f"{name_prefix}{submodel}.{name} ({class_name})"
+          layer.__term_color__ = color
+          layer.register_forward_hook(self._hook())
+    pass
+
+  def _hook(self, ) -> Callable:
+    def fn(layer, input, output):
+      input_shape = f"in({len(input)}): "
+      for elem in input:
+        if hasattr(elem, 'shape') and elem.nelement() > 0:
+          input_shape += f"{str(list(elem.shape))}"
+          min_max = f"({elem.min():.2f}, {elem.max():.2f}, m{elem.mean():.2f}, s{elem.std():.2f}), "
+          input_shape += min_max
+        else:
+          input_shape += f"{type(elem)}, "
+
+      input_shape = input_shape.strip(" ,")
+      # if self.add_newline:
+      #   input_shape += "\n"
+
+      if not isinstance(output, tuple):
+        output = (output, )
+      output_shape = f"out({len(output)}): "
+      for elem in output:
+        if hasattr(elem, 'shape'):
+          output_shape += f"{str(list(elem.shape))}"
+          min_max = f"({elem.min():.2f}, {elem.max():.2f}, m{elem.mean():.2f}, s{elem.std():.2f}), "
+          output_shape += min_max
+        else:
+          output_shape += f"{type(elem)}, "
+      output_shape = output_shape.strip(" ,")
+
+      num_params = sum([p.data.nelement() for p in layer.parameters()])/1e6
+      num_bufs = sum([p.data.nelement() for p in layer.buffers()])
+      param_str = f"{'':<{1}} paras: {str(num_params)}M "
+      param_str = f"{param_str:<{19}}bufs: {str(num_bufs)}M"
+
+      layer_str = str(layer)
+      if '\n' in layer_str:
+        layer_str = ''
+      min_max_v = self._get_parameters_min_max_values(layer=layer, attr_name='weight')
+      if min_max_v:
+        layer_str += f", {min_max_v}"
+      min_max_v = self._get_parameters_min_max_values(layer=layer, attr_name='bias')
+      if min_max_v:
+        layer_str += f", {min_max_v}"
+
+      cprint(f"{(layer.__name__ + ': '):<{self.input_padding}}{input_shape}"             
+             f"\n"
+             f"{param_str:<{self.input_padding - 3}}=>{output_shape}"
+             f"{'':<{3}} {layer_str}"
+             , color=layer.__term_color__)
+
+    return fn
+
+  def _get_parameters_min_max_values(self, layer, attr_name):
+    min_max_str = ""
+    if hasattr(layer, attr_name):
+      with torch.no_grad():
+        p = getattr(layer, attr_name)
+        if p is None:
+          return f"{attr_name}({p})"
+        min_v = p.min()
+        max_v = p.max()
+        min_max_str = f"{attr_name}({min_v:.2f}, {max_v:.2f}, m{p.mean():.2f})"
+    return min_max_str
+
+
+  def forward(self, *args, **kwargs) -> Tensor:
+    return self.model(*args, **kwargs)
+
+  @staticmethod
+  def forward_verbose(model: nn.Module,
+                      inputs_args: Tuple = None,
+                      inputs_kwargs: Dict = None,
+                      submodels=None,
+                      name_prefix="",
+                      register_itself=True,
+                      register_children=True,
+                      add_newline=True,
+                      input_padding=36,
+                      verbose=True,
+                      **kwargs
+                      ):
+    with torch.no_grad():
+      model_ver = VerboseModel(model=model,
+                               submodels=submodels,
+                               name_prefix=name_prefix,
+                               register_itself=register_itself,
+                               register_children=register_children,
+                               add_newline=add_newline,
+                               input_padding=input_padding,
+                               )
+      if inputs_args is None:
+        inputs_args = ()
+      if inputs_kwargs is None:
+        inputs_kwargs = {}
+      # inputs_args = copy.deepcopy(inputs_args)
+      # inputs_kwargs = copy.deepcopy(inputs_kwargs)
+      time_start = time.time()
+      out = model_ver(*inputs_args, **inputs_kwargs)
+      elapsed = time.time() - time_start
+      del model_ver, out
+
+      if verbose:
+        torch_utils.print_number_params({name_prefix: model}, add_info=f"time: {elapsed:.4f}")
+    return
+
+class FeatureExtractor(nn.Module):
+  def __init__(self, model: nn.Module, layers: Iterable[str]):
+    super().__init__()
+    self.model = model
+    self.layers = layers
+    self._features = {layer: torch.empty(0) for layer in layers}
+
+    for layer_id in layers:
+      layer = dict([*self.model.named_modules()])[layer_id]
+      layer.register_forward_hook(self.save_outputs_hook(layer_id))
+    pass
+
+  def save_outputs_hook(self, layer_id: str) -> Callable:
+    def fn(_, __, output):
+      self._features[layer_id] = output
+    return fn
+
+  def forward(self, *args, **kwargs) -> Dict[str, Tensor]:
+    self._features.clear()
+    _ = self.model(*args, **kwargs)
+    return self._features
+
+
+class FeatureExtractor_v1(nn.Module):
+  def __init__(self, model: nn.Module, layers: Iterable[str]):
+    super().__init__()
+    self.model = model
+    self.layers = layers
+    self._features = {layer: torch.empty(0) for layer in layers}
+
+    for layer_id in layers:
+      layer = dict([*self.model.named_modules()])[layer_id]
+      layer.register_forward_hook(self.save_outputs_hook(layer_id))
+    pass
+
+  def save_outputs_hook(self, layer_id: str) -> Callable:
+    hook = partial(self.fn, _features=self._features, layer_id=layer_id)
+    return hook
+
+  @staticmethod
+  def fn(_, __, output, _features, layer_id):
+    _features[layer_id] = output
+
+  def forward(self, *args, **kwargs):
+    self._features.clear()
+    _ = self.model(*args, **kwargs)
+
+    out = {k: v for k, v in self._features.items()}
+    self._features.clear()
+
+    return out
+
+
+
+class GradExtractor(nn.Module):
+  def __init__(self, model: nn.Module, param_names: Iterable[str]=[]):
+    super().__init__()
+    self.model = model
+    self.param_names = param_names
+    self._grads = {}
+
+    for name, param in model.named_parameters():
+      print(f"{name:<30}: {list(param.shape)}")
+      if name in self.param_names:
+        param.register_hook(hook=self._hook(name))
+    pass
+
+  def _hook(self, name: str) -> Callable:
+    def fn(grad):
+      self._grads[name] = grad.clone()
+      return grad
+    return fn
+
+  def forward(self, x: Tensor) -> Dict[str, Tensor]:
+    self._grads.clear()
+    out = self.model(x)
+    return out
+
+
+
+class PytorchHook(unittest.TestCase):
+
+  def test_register_forward_hook(self):
+
+    import torch
+    import torch.nn as nn
+
+    class TestForHook(nn.Module):
+      def __init__(self):
+        super().__init__()
+
+        self.linear_1 = nn.Linear(in_features=2, out_features=2)
+        self.linear_2 = nn.Linear(in_features=2, out_features=1)
+        self.relu = nn.ReLU()
+        self.relu6 = nn.ReLU6()
+        self.initialize()
+
+      def forward(self, x):
+        linear_1 = self.linear_1(x)
+        linear_2 = self.linear_2(linear_1)
+        relu = self.relu(linear_2)
+        relu_6 = self.relu6(relu)
+        layers_in = (x, linear_1, linear_2)
+        layers_out = (linear_1, linear_2, relu)
+        return relu_6, layers_in, layers_out
+
+      def initialize(self):
+        """ 定义特殊的初始化，用于验证是不是获取了权重"""
+        self.linear_1.weight = torch.nn.Parameter(torch.FloatTensor([[1, 1], [1, 1]]))
+        self.linear_1.bias = torch.nn.Parameter(torch.FloatTensor([1, 1]))
+        self.linear_2.weight = torch.nn.Parameter(torch.FloatTensor([[1, 1]]))
+        self.linear_2.bias = torch.nn.Parameter(torch.FloatTensor([1]))
+        return True
+
+    # 1：定义用于获取网络各层输入输出tensor的容器
+    # 并定义module_name用于记录相应的module名字
+    module_name = []
+    features_in_hook = []
+    features_out_hook = []
+
+    # 2：hook函数负责将获取的输入输出添加到feature列表中
+    # 并提供相应的module名字
+    def hook(module, fea_in, fea_out):
+      print("hooker working")
+      module_name.append(module.__class__)
+      features_in_hook.append(fea_in)
+      features_out_hook.append(fea_out)
+      return None
+
+    # 3：定义全部是1的输入
+    x = torch.FloatTensor([[0.1, 0.1], [0.1, 0.1]])
+
+    # 4:注册钩子可以对某些层单独进行
+    net = TestForHook()
+    net_chilren = net.children()
+    for child in net_chilren:
+      if not isinstance(child, nn.ReLU6):
+        child.register_forward_hook(hook=hook)
+
+    # 5:测试网络输出
+    out, features_in_forward, features_out_forward = net(x)
+    print("*" * 5 + "forward return features" + "*" * 5)
+    print(features_in_forward)
+    print(features_out_forward)
+    print("*" * 5 + "forward return features" + "*" * 5)
+
+    # 6:测试features_in是不是存储了输入
+    print("*" * 5 + "hook record features" + "*" * 5)
+    print(features_in_hook)
+    print(features_out_hook)
+    print(module_name)
+    print("*" * 5 + "hook record features" + "*" * 5)
+
+    # 7：测试forward返回的feautes_in是不是和hook记录的一致
+    print("sub result")
+    for forward_return, hook_record in zip(features_in_forward, features_in_hook):
+      print(forward_return - hook_record[0])
+
+    pass
+
+  def test_VerboseModel(self):
+
+    from torchvision.models.segmentation import deeplabv3_resnet101
+
+    verbose_resnet = VerboseModel(deeplabv3_resnet101(pretrained=True), submodels=['backbone', 'classifier', ])
+    dummy_input = torch.ones(10, 3, 256, 256)
+    _ = verbose_resnet(dummy_input)
+
+    pass
+
+  def test_VerboseModel_register_itself(self):
+
+    import torch
+
+    net = torch.nn.Linear(256, 512)
+    net_ver = VerboseModel(net, register_itself=True)
+    dummy_input = torch.ones(10, 256)
+    out = net_ver(dummy_input)
+
+    net = torch.nn.Sequential(
+      torch.nn.Linear(256, 512)
+    )
+    net_ver = VerboseModel(net, register_itself=True, name_prefix='linear.')
+    dummy_input = torch.ones(10, 256)
+    out = net_ver(dummy_input)
+
+    pass
+
+  def test_VerboseModel_multi_input(self):
+
+    import torch
+
+    class FiLMLayer(nn.Module):
+      def __init__(self, input_dim, hidden_dim):
+        super().__init__()
+        self.layer = nn.Linear(input_dim, hidden_dim)
+
+      def forward(self, x, freq, phase_shift):
+        x = self.layer(x)
+        freq = freq.unsqueeze(1).expand_as(x)
+        phase_shift = phase_shift.unsqueeze(1).expand_as(x)
+        out = torch.sin(freq * x + phase_shift)
+        return out, out
+
+    net = FiLMLayer(256, 512)
+    net_ver = VerboseModel(net, name_prefix="FiLM.", register_itself=True, register_children=False)
+    x = torch.randn(2, 12, 256)
+    freq = torch.randn(2, 512)
+    phase = torch.randn(2, 512)
+    out = net_ver(x, freq, phase)
+
+    pass
+
+  def test_FeatureExtractor(self):
+
+    from torchvision.models import resnet50
+
+    verbose_resnet = VerboseModel(resnet50(), submodels=['layer1', 'layer4', 'layer4.2'])
+    dummy_input = torch.ones(10, 3, 224, 224)
+    verbose_resnet(dummy_input)
+
+    resnet_features = FeatureExtractor(resnet50(), layers=["layer4.2.relu", "avgpool", ])
+    features = resnet_features(dummy_input)
+
+    print({name: output.shape for name, output in features.items()})
+
+    pass
+
+  def test_GradExtractor(self):
+
+    from torchvision.models import resnet50
+
+    resnet_grad = GradExtractor(resnet50(), param_names=['fc.weight', 'conv1.weight'])
+    dummy_input = torch.ones(10, 3, 224, 224)
+
+    out = resnet_grad(dummy_input)
+    loss = out.mean()
+    loss.backward()
+    grads = resnet_grad._grads
+    print({name: output.mean() for name, output in grads.items()})
+
+    pass
+
+  def test_register_hook_grad(self):
+
+    import torch
+
+    y_grad = list()
+    def grad_hook(grad):
+      y_grad.append(grad)
+
+    x = torch.tensor([2., 2., 2., 2.], requires_grad=True)
+    y = torch.pow(x, 2)
+    z = torch.mean(y)
+
+    h = y.register_hook(grad_hook)
+
+    z.backward()
+    print("y.grad: ", y.grad)
+    print("y_grad[0]: ", y_grad[0])
+    h.remove()  # removes the hook
+    pass
+
+  def test_register_hook_scale_grad(self):
+
+    import torch
+
+    def grad_hook(grad):
+      grad *= 2
+
+    x = torch.tensor([2., 2., 2., 2.], requires_grad=True)
+    y = torch.pow(x, 2)
+    z = torch.mean(y)
+    h = x.register_hook(grad_hook)
+    z.backward()
+    print(x.grad)
+    h.remove()  # removes the hook
+
+    pass
+
+  def test_hook_for_grad_cam(self, debug=True):
+
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=./exp:./stylegan2-pytorch:./
+        python 	-c "from exp.tests.test_styleganv2 import Testing_stylegan2;\
+          Testing_stylegan2().test_train_ffhq_128()"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    cmd_str = f"""
+        python 
+        template_lib/proj/pytorch/examples/hook_for_grad_cam.py
+        {get_append_cmd_str(args)}
+        """
+    if debug:
+      cmd_str += f"""
+                  --tl_debug
+                  --tl_opts 
+                  """
+    else:
+      cmd_str += f"""
+                  --tl_opts {tl_opts}
+                  """
+    start_cmd_run(cmd_str)
+    # from template_lib.v2.config_cfgnode import update_parser_defaults_from_yaml, global_cfg
+    # from template_lib.modelarts import modelarts_utils
+    # update_parser_defaults_from_yaml(parser)
+
+    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
+    #
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+
+    pass
+
+
+
+
+
+
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/pytorch/scripts/resize_antialias.py` & `tl2-0.1.1/tl2/proj/pytorch/scripts/resize_antialias.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/skimage/skimage_utils.py` & `tl2-0.1.1/tl2/proj/skimage/skimage_utils.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/streamlit/SessionState.py` & `tl2-0.1.1/tl2/proj/streamlit/SessionState.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/streamlit/configs/Streamlit.yaml` & `tl2-0.1.1/tl2/proj/streamlit/configs/Streamlit.yaml`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/streamlit/scripts/run_web.py` & `tl2-0.1.1/tl2/proj/streamlit/scripts/run_web.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/streamlit/st_utils.py` & `tl2-0.1.1/tl2/proj/streamlit/st_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,18 @@
 """
 @st.cache(allow_output_mutation=True, suppress_st_warning=True)
 
+@st.cache_data
+def long_running_function(param1, param2):
+    return …
+cache computations that return data
+    
+@st.cache_resource
+cache global resources like ML models
+
 """
 import numpy as np
 from PIL import Image
 from pathlib import Path
 import logging
 import json
 import ast
@@ -265,17 +273,17 @@
   if not isinstance(image_list_file, (list, tuple)):
     image_list_file = [image_list_file, ]
 
   if not header:
     header = "Image list file: "
 
   if len(image_list_file) > 0 and image_list_file[0]:
-    st.header(header)
+    st.sidebar.header(header)
   for image_file in image_list_file:
-    st.write(image_file)
+    st.sidebar.write(image_file)
 
   all_image_list = read_image_list_from_files(image_list_file)
   if show_dataframe:
     image_list_df = pd.DataFrame(all_image_list, columns=columns)
     st.dataframe(image_list_df)
   return all_image_list
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `tl2-0.1.0/tl2/proj/streamlit/tests/test_streamlit.py` & `tl2-0.1.1/tl2/proj/streamlit/tests/test_streamlit.py`

 * *Files 15% similar despite different names*

```diff
@@ -185,7 +185,72 @@
             {get_append_cmd_str(args)}
             --tl_opts {tl_opts}
         """
     start_cmd_run(cmd_str)
     pass
 
 
+class Testing_Streamlit_v2(unittest.TestCase):
+  
+  def test_st_two_column(self, debug=True):
+    """
+    Usage:
+
+        export CUDA_VISIBLE_DEVICES=0
+        export TIME_STR=0
+        export PYTHONPATH=.:./tl2_lib:exp
+        python -c "from tl2_lib.tl2.proj.streamlit.tests.test_streamlit import Testing_Streamlit_v2;\
+          Testing_Streamlit_v2().test_st_two_column(debug=False)" \
+          --tl_opts port 8501
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+  
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+  
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file tl2_lib/tl2/proj/streamlit/configs/Streamlit_v2.yaml
+                --tl_command {command}
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+  
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    PORT = cfg.get('port', 8501)
+  
+    # import importlib
+    # script = importlib.import_module('tl2.proj.streamlit.scripts.run_web').__file__
+    script = "tl2_lib/tl2/proj/streamlit/scripts/st_two_column.py"
+    if debug:
+      cmd_str = f"""
+          python
+            {script}
+            {get_append_cmd_str(args)}
+            --tl_debug
+            --tl_opts
+          """
+  
+    else:
+      cmd_str = f"""
+          python -m streamlit run
+              --server.port {PORT}
+              --server.headless true
+              {script}
+              --
+              {get_append_cmd_str(args)}
+              --tl_opts {tl_opts}
+          """
+  
+    start_cmd_run(cmd_str)
+    pass
+
```

### Comparing `tl2-0.1.0/tl2/proj/stylegan2_ada/persistence.py` & `tl2-0.1.1/tl2/proj/stylegan2_ada/persistence.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,270 +1,270 @@
-# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
-#
-# NVIDIA CORPORATION and its licensors retain all intellectual property
-# and proprietary rights in and to this software, related documentation
-# and any modifications thereto.  Any use, reproduction, disclosure or
-# distribution of this software and related documentation without an express
-# license agreement from NVIDIA CORPORATION is strictly prohibited.
-
-"""Facilities for pickling Python code alongside other data.
-
-The pickled code is automatically imported into a separate Python module
-during unpickling. This way, any previously exported pickles will remain
-usable even if the original code is no longer available, or if the current
-version of the code is not consistent with what was originally pickled."""
-
-import sys
-import pickle
-import io
-import inspect
-import copy
-import uuid
-import types
-# import dnnlib
-from typing import Any
-
-#----------------------------------------------------------------------------
-
-_version            = 6         # internal version number
-_decorators         = set()     # {decorator_class, ...}
-_import_hooks       = []        # [hook_function, ...]
-_module_to_src_dict = dict()    # {module: src, ...}
-_src_to_module_dict = dict()    # {src: module, ...}
-
-#----------------------------------------------------------------------------
-
-def persistent_class(orig_class):
-    r"""Class decorator that extends a given class to save its source code
-    when pickled.
-
-    Example:
-
-        from torch_utils import persistence
-
-        @persistence.persistent_class
-        class MyNetwork(torch.nn.Module):
-            def __init__(self, num_inputs, num_outputs):
-                super().__init__()
-                self.fc = MyLayer(num_inputs, num_outputs)
-                ...
-
-        @persistence.persistent_class
-        class MyLayer(torch.nn.Module):
-            ...
-
-    When pickled, any instance of `MyNetwork` and `MyLayer` will save its
-    source code alongside other internal state (e.g., parameters, buffers,
-    and submodules). This way, any previously exported pickle will remain
-    usable even if the class definitions have been modified or are no
-    longer available.
-
-    The decorator saves the source code of the entire Python module
-    containing the decorated class. It does *not* save the source code of
-    any imported modules. Thus, the imported modules must be available
-    during unpickling, also including `torch_utils.persistence` itself.
-
-    It is ok to call functions defined in the same module from the
-    decorated class. However, if the decorated class depends on other
-    classes defined in the same module, they must be decorated as well.
-    This is illustrated in the above example in the case of `MyLayer`.
-
-    It is also possible to employ the decorator just-in-time before
-    calling the constructor. For example:
-
-        cls = MyLayer
-        if want_to_make_it_persistent:
-            cls = persistence.persistent_class(cls)
-        layer = cls(num_inputs, num_outputs)
-
-    As an additional feature, the decorator also keeps track of the
-    arguments that were used to construct each instance of the decorated
-    class. The arguments can be queried via `obj.init_args` and
-    `obj.init_kwargs`, and they are automatically pickled alongside other
-    object state. A typical use case is to first unpickle a previous
-    instance of a persistent class, and then upgrade it to use the latest
-    version of the source code:
-
-        with open('old_pickle.pkl', 'rb') as f:
-            old_net = pickle.load(f)
-        new_net = MyNetwork(*old_obj.init_args, **old_obj.init_kwargs)
-        misc.copy_params_and_buffers(old_net, new_net, require_all=True)
-    """
-    assert isinstance(orig_class, type)
-    if is_persistent(orig_class):
-        return orig_class
-
-    assert orig_class.__module__ in sys.modules
-    orig_module = sys.modules[orig_class.__module__]
-    orig_module_src = _module_to_src(orig_module)
-
-    class Decorator(orig_class):
-        _orig_module_src = orig_module_src
-        _orig_class_name = orig_class.__name__
-
-        def __init__(self, *args, **kwargs):
-            super().__init__(*args, **kwargs)
-            self._init_args = copy.deepcopy(args)
-            self._init_kwargs = copy.deepcopy(kwargs)
-            assert orig_class.__name__ in orig_module.__dict__
-            _check_pickleable(self.__reduce__())
-
-        @property
-        def init_args(self):
-            return copy.deepcopy(self._init_args)
-
-        @property
-        def init_kwargs(self):
-            # return dnnlib.EasyDict(copy.deepcopy(self._init_kwargs))
-            return EasyDict(copy.deepcopy(self._init_kwargs))
-
-        def __reduce__(self):
-            fields = list(super().__reduce__())
-            fields += [None] * max(3 - len(fields), 0)
-            if fields[0] is not _reconstruct_persistent_obj:
-                meta = dict(type='class', version=_version, module_src=self._orig_module_src, class_name=self._orig_class_name, state=fields[2])
-                fields[0] = _reconstruct_persistent_obj # reconstruct func
-                fields[1] = (meta,) # reconstruct args
-                fields[2] = None # state dict
-            return tuple(fields)
-
-    Decorator.__name__ = orig_class.__name__
-    _decorators.add(Decorator)
-    return Decorator
-
-#----------------------------------------------------------------------------
-
-def is_persistent(obj):
-    r"""Test whether the given object or class is persistent, i.e.,
-    whether it will save its source code when pickled.
-    """
-    try:
-        if obj in _decorators:
-            return True
-    except TypeError:
-        pass
-    return type(obj) in _decorators # pylint: disable=unidiomatic-typecheck
-
-#----------------------------------------------------------------------------
-
-def import_hook(hook):
-    r"""Register an import hook that is called whenever a persistent object
-    is being unpickled. A typical use case is to patch the pickled source
-    code to avoid errors and inconsistencies when the API of some imported
-    module has changed.
-
-    The hook should have the following signature:
-
-        hook(meta) -> modified meta
-
-    `meta` is an instance of `dnnlib.EasyDict` with the following fields:
-
-        type:       Type of the persistent object, e.g. `'class'`.
-        version:    Internal version number of `torch_utils.persistence`.
-        module_src  Original source code of the Python module.
-        class_name: Class name in the original Python module.
-        state:      Internal state of the object.
-
-    Example:
-
-        @persistence.import_hook
-        def wreck_my_network(meta):
-            if meta.class_name == 'MyNetwork':
-                print('MyNetwork is being imported. I will wreck it!')
-                meta.module_src = meta.module_src.replace("True", "False")
-            return meta
-    """
-    assert callable(hook)
-    _import_hooks.append(hook)
-
-#----------------------------------------------------------------------------
-
-def _reconstruct_persistent_obj(meta):
-    r"""Hook that is called internally by the `pickle` module to unpickle
-    a persistent object.
-    """
-    # meta = dnnlib.EasyDict(meta)
-    # meta.state = dnnlib.EasyDict(meta.state)
-    meta = EasyDict(meta)
-    meta.state = EasyDict(meta.state)
-    for hook in _import_hooks:
-        meta = hook(meta)
-        assert meta is not None
-
-    assert meta.version == _version
-    module = _src_to_module(meta.module_src)
-
-    assert meta.type == 'class'
-    orig_class = module.__dict__[meta.class_name]
-    decorator_class = persistent_class(orig_class)
-    obj = decorator_class.__new__(decorator_class)
-
-    setstate = getattr(obj, '__setstate__', None)
-    if callable(setstate):
-        setstate(meta.state) # pylint: disable=not-callable
-    else:
-        obj.__dict__.update(meta.state)
-    return obj
-
-#----------------------------------------------------------------------------
-
-def _module_to_src(module):
-    r"""Query the source code of a given Python module.
-    """
-    src = _module_to_src_dict.get(module, None)
-    if src is None:
-        src = inspect.getsource(module)
-        _module_to_src_dict[module] = src
-        _src_to_module_dict[src] = module
-    return src
-
-def _src_to_module(src):
-    r"""Get or create a Python module for the given source code.
-    """
-    module = _src_to_module_dict.get(src, None)
-    if module is None:
-        module_name = "_imported_module_" + uuid.uuid4().hex
-        module = types.ModuleType(module_name)
-        sys.modules[module_name] = module
-        _module_to_src_dict[module] = src
-        _src_to_module_dict[src] = module
-        exec(src, module.__dict__) # pylint: disable=exec-used
-    return module
-
-#----------------------------------------------------------------------------
-
-def _check_pickleable(obj):
-    r"""Check that the given object is pickleable, raising an exception if
-    it is not. This function is expected to be considerably more efficient
-    than actually pickling the object.
-    """
-    def recurse(obj):
-        if isinstance(obj, (list, tuple, set)):
-            return [recurse(x) for x in obj]
-        if isinstance(obj, dict):
-            return [[recurse(x), recurse(y)] for x, y in obj.items()]
-        if isinstance(obj, (str, int, float, bool, bytes, bytearray)):
-            return None # Python primitive types are pickleable.
-        if f'{type(obj).__module__}.{type(obj).__name__}' in ['numpy.ndarray', 'torch.Tensor']:
-            return None # NumPy arrays and PyTorch tensors are pickleable.
-        if is_persistent(obj):
-            return None # Persistent objects are pickleable, by virtue of the constructor check.
-        return obj
-    with io.BytesIO() as f:
-        pickle.dump(recurse(obj), f)
-
-#----------------------------------------------------------------------------
-
-class EasyDict(dict):
-  """Convenience class that behaves like a dict but allows access with the attribute syntax."""
-
-  def __getattr__(self, name: str) -> Any:
-    try:
-      return self[name]
-    except KeyError:
-      raise AttributeError(name)
-
-  def __setattr__(self, name: str, value: Any) -> None:
-    self[name] = value
-
-  def __delattr__(self, name: str) -> None:
-    del self[name]
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+#
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+"""Facilities for pickling Python code alongside other data.
+
+The pickled code is automatically imported into a separate Python module
+during unpickling. This way, any previously exported pickles will remain
+usable even if the original code is no longer available, or if the current
+version of the code is not consistent with what was originally pickled."""
+
+import sys
+import pickle
+import io
+import inspect
+import copy
+import uuid
+import types
+# import dnnlib
+from typing import Any
+
+#----------------------------------------------------------------------------
+
+_version            = 6         # internal version number
+_decorators         = set()     # {decorator_class, ...}
+_import_hooks       = []        # [hook_function, ...]
+_module_to_src_dict = dict()    # {module: src, ...}
+_src_to_module_dict = dict()    # {src: module, ...}
+
+#----------------------------------------------------------------------------
+
+def persistent_class(orig_class):
+    r"""Class decorator that extends a given class to save its source code
+    when pickled.
+
+    Example:
+
+        from torch_utils import persistence
+
+        @persistence.persistent_class
+        class MyNetwork(torch.nn.Module):
+            def __init__(self, num_inputs, num_outputs):
+                super().__init__()
+                self.fc = MyLayer(num_inputs, num_outputs)
+                ...
+
+        @persistence.persistent_class
+        class MyLayer(torch.nn.Module):
+            ...
+
+    When pickled, any instance of `MyNetwork` and `MyLayer` will save its
+    source code alongside other internal state (e.g., parameters, buffers,
+    and submodules). This way, any previously exported pickle will remain
+    usable even if the class definitions have been modified or are no
+    longer available.
+
+    The decorator saves the source code of the entire Python module
+    containing the decorated class. It does *not* save the source code of
+    any imported modules. Thus, the imported modules must be available
+    during unpickling, also including `torch_utils.persistence` itself.
+
+    It is ok to call functions defined in the same module from the
+    decorated class. However, if the decorated class depends on other
+    classes defined in the same module, they must be decorated as well.
+    This is illustrated in the above example in the case of `MyLayer`.
+
+    It is also possible to employ the decorator just-in-time before
+    calling the constructor. For example:
+
+        cls = MyLayer
+        if want_to_make_it_persistent:
+            cls = persistence.persistent_class(cls)
+        layer = cls(num_inputs, num_outputs)
+
+    As an additional feature, the decorator also keeps track of the
+    arguments that were used to construct each instance of the decorated
+    class. The arguments can be queried via `obj.init_args` and
+    `obj.init_kwargs`, and they are automatically pickled alongside other
+    object state. A typical use case is to first unpickle a previous
+    instance of a persistent class, and then upgrade it to use the latest
+    version of the source code:
+
+        with open('old_pickle.pkl', 'rb') as f:
+            old_net = pickle.load(f)
+        new_net = MyNetwork(*old_obj.init_args, **old_obj.init_kwargs)
+        misc.copy_params_and_buffers(old_net, new_net, require_all=True)
+    """
+    assert isinstance(orig_class, type)
+    if is_persistent(orig_class):
+        return orig_class
+
+    assert orig_class.__module__ in sys.modules
+    orig_module = sys.modules[orig_class.__module__]
+    orig_module_src = _module_to_src(orig_module)
+
+    class Decorator(orig_class):
+        _orig_module_src = orig_module_src
+        _orig_class_name = orig_class.__name__
+
+        def __init__(self, *args, **kwargs):
+            super().__init__(*args, **kwargs)
+            self._init_args = copy.deepcopy(args)
+            self._init_kwargs = copy.deepcopy(kwargs)
+            assert orig_class.__name__ in orig_module.__dict__
+            _check_pickleable(self.__reduce__())
+
+        @property
+        def init_args(self):
+            return copy.deepcopy(self._init_args)
+
+        @property
+        def init_kwargs(self):
+            # return dnnlib.EasyDict(copy.deepcopy(self._init_kwargs))
+            return EasyDict(copy.deepcopy(self._init_kwargs))
+
+        def __reduce__(self):
+            fields = list(super().__reduce__())
+            fields += [None] * max(3 - len(fields), 0)
+            if fields[0] is not _reconstruct_persistent_obj:
+                meta = dict(type='class', version=_version, module_src=self._orig_module_src, class_name=self._orig_class_name, state=fields[2])
+                fields[0] = _reconstruct_persistent_obj # reconstruct func
+                fields[1] = (meta,) # reconstruct args
+                fields[2] = None # state dict
+            return tuple(fields)
+
+    Decorator.__name__ = orig_class.__name__
+    _decorators.add(Decorator)
+    return Decorator
+
+#----------------------------------------------------------------------------
+
+def is_persistent(obj):
+    r"""Test whether the given object or class is persistent, i.e.,
+    whether it will save its source code when pickled.
+    """
+    try:
+        if obj in _decorators:
+            return True
+    except TypeError:
+        pass
+    return type(obj) in _decorators # pylint: disable=unidiomatic-typecheck
+
+#----------------------------------------------------------------------------
+
+def import_hook(hook):
+    r"""Register an import hook that is called whenever a persistent object
+    is being unpickled. A typical use case is to patch the pickled source
+    code to avoid errors and inconsistencies when the API of some imported
+    module has changed.
+
+    The hook should have the following signature:
+
+        hook(meta) -> modified meta
+
+    `meta` is an instance of `dnnlib.EasyDict` with the following fields:
+
+        type:       Type of the persistent object, e.g. `'class'`.
+        version:    Internal version number of `torch_utils.persistence`.
+        module_src  Original source code of the Python module.
+        class_name: Class name in the original Python module.
+        state:      Internal state of the object.
+
+    Example:
+
+        @persistence.import_hook
+        def wreck_my_network(meta):
+            if meta.class_name == 'MyNetwork':
+                print('MyNetwork is being imported. I will wreck it!')
+                meta.module_src = meta.module_src.replace("True", "False")
+            return meta
+    """
+    assert callable(hook)
+    _import_hooks.append(hook)
+
+#----------------------------------------------------------------------------
+
+def _reconstruct_persistent_obj(meta):
+    r"""Hook that is called internally by the `pickle` module to unpickle
+    a persistent object.
+    """
+    # meta = dnnlib.EasyDict(meta)
+    # meta.state = dnnlib.EasyDict(meta.state)
+    meta = EasyDict(meta)
+    meta.state = EasyDict(meta.state)
+    for hook in _import_hooks:
+        meta = hook(meta)
+        assert meta is not None
+
+    assert meta.version == _version
+    module = _src_to_module(meta.module_src)
+
+    assert meta.type == 'class'
+    orig_class = module.__dict__[meta.class_name]
+    decorator_class = persistent_class(orig_class)
+    obj = decorator_class.__new__(decorator_class)
+
+    setstate = getattr(obj, '__setstate__', None)
+    if callable(setstate):
+        setstate(meta.state) # pylint: disable=not-callable
+    else:
+        obj.__dict__.update(meta.state)
+    return obj
+
+#----------------------------------------------------------------------------
+
+def _module_to_src(module):
+    r"""Query the source code of a given Python module.
+    """
+    src = _module_to_src_dict.get(module, None)
+    if src is None:
+        src = inspect.getsource(module)
+        _module_to_src_dict[module] = src
+        _src_to_module_dict[src] = module
+    return src
+
+def _src_to_module(src):
+    r"""Get or create a Python module for the given source code.
+    """
+    module = _src_to_module_dict.get(src, None)
+    if module is None:
+        module_name = "_imported_module_" + uuid.uuid4().hex
+        module = types.ModuleType(module_name)
+        sys.modules[module_name] = module
+        _module_to_src_dict[module] = src
+        _src_to_module_dict[src] = module
+        exec(src, module.__dict__) # pylint: disable=exec-used
+    return module
+
+#----------------------------------------------------------------------------
+
+def _check_pickleable(obj):
+    r"""Check that the given object is pickleable, raising an exception if
+    it is not. This function is expected to be considerably more efficient
+    than actually pickling the object.
+    """
+    def recurse(obj):
+        if isinstance(obj, (list, tuple, set)):
+            return [recurse(x) for x in obj]
+        if isinstance(obj, dict):
+            return [[recurse(x), recurse(y)] for x, y in obj.items()]
+        if isinstance(obj, (str, int, float, bool, bytes, bytearray)):
+            return None # Python primitive types are pickleable.
+        if f'{type(obj).__module__}.{type(obj).__name__}' in ['numpy.ndarray', 'torch.Tensor']:
+            return None # NumPy arrays and PyTorch tensors are pickleable.
+        if is_persistent(obj):
+            return None # Persistent objects are pickleable, by virtue of the constructor check.
+        return obj
+    with io.BytesIO() as f:
+        pickle.dump(recurse(obj), f)
+
+#----------------------------------------------------------------------------
+
+class EasyDict(dict):
+  """Convenience class that behaves like a dict but allows access with the attribute syntax."""
+
+  def __getattr__(self, name: str) -> Any:
+    try:
+      return self[name]
+    except KeyError:
+      raise AttributeError(name)
+
+  def __setattr__(self, name: str, value: Any) -> None:
+    self[name] = value
+
+  def __delattr__(self, name: str) -> None:
+    del self[name]
```

### Comparing `tl2-0.1.0/tl2/proj/stylegan2_ada/test/test_ada.py` & `tl2-0.1.1/tl2/proj/stylegan2_ada/test/test_ada.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,152 +1,152 @@
-import os
-import sys
-import unittest
-
-import torch
-
-from tl2.proj.stylegan2_ada import persistence
-
-
-# @MODEL_REGISTRY.register(name_prefix=__name__)
-@persistence.persistent_class
-class PersistenceNetwork(torch.nn.Module):
-  def __init__(self, num_inputs, num_outputs):
-    super().__init__()
-    self.fc = torch.nn.Linear(num_inputs, num_outputs)
-    pass
-  def forward(self, x):
-    out = self.fc(x)
-    return out
-
-
-class Testing_persistence(unittest.TestCase):
-
-  def test_persistence_usage(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts test0 10 test1 11 --test 1
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-    PORT = os.environ.get('PORT', 8888)
-
-    from tl2.proj.stylegan2_ada import ada_utils
-
-    net = PersistenceNetwork(3, 6)
-
-    snapshot_pkl = f"{args.tl_outdir}/net.pkl"
-    ada_utils.save_model(net, snapshot_pkl=snapshot_pkl)
-
-    pass
-
-  def test_load_persistence_model(self, debug=True):
-    """
-    Usage:
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
-        for filename in /cache/pypi/*.whl; do
-            pip install $filename
-        done
-        proj_root=moco-exp
-        python template_lib/modelarts/scripts/copy_tool.py \
-          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
-        cd /cache/$proj_root
-        pip install -r requirements.txt
-
-        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
-        export TIME_STR=1
-        export PYTHONPATH=.
-        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
-          Testing_Launch_v1().test_launch_ddp(debug=False)" \
-          --tl_opts test0 10 test1 11 --test 1
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
-    tl_opts = ' '.join(tl_opts_list)
-    print(f'tl_opts:\n {tl_opts}')
-    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    # print(f'tl_opts:\n {tl_opts}')
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                --tl_opts {tl_opts}
-                """
-    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-    PORT = os.environ.get('PORT', 8888)
-
-    import pickle
-    from tl2.proj.stylegan2_ada import ada_utils
-    from tl2.proj.pytorch.pytorch_hook import VerboseModel
-
-    snapshot_pkl = f"results/persistence/persistence_usage/net.pkl"
-
-    net = ada_utils.load_model(snapshot_pkl)
-
-    net_verbose = VerboseModel(net).cuda()
-    x = torch.rand((2, 3)).cuda()
-    out = net_verbose(x)
-    pass
-
-
-
-
-
-
-
-
-
-
+import os
+import sys
+import unittest
+
+import torch
+
+from tl2.proj.stylegan2_ada import persistence
+
+
+# @MODEL_REGISTRY.register(name_prefix=__name__)
+@persistence.persistent_class
+class PersistenceNetwork(torch.nn.Module):
+  def __init__(self, num_inputs, num_outputs):
+    super().__init__()
+    self.fc = torch.nn.Linear(num_inputs, num_outputs)
+    pass
+  def forward(self, x):
+    out = self.fc(x)
+    return out
+
+
+class Testing_persistence(unittest.TestCase):
+
+  def test_persistence_usage(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts test0 10 test1 11 --test 1
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    PORT = os.environ.get('PORT', 8888)
+
+    from tl2.proj.stylegan2_ada import ada_utils
+
+    net = PersistenceNetwork(3, 6)
+
+    snapshot_pkl = f"{args.tl_outdir}/net.pkl"
+    ada_utils.save_model(net, snapshot_pkl=snapshot_pkl)
+
+    pass
+
+  def test_load_persistence_model(self, debug=True):
+    """
+    Usage:
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/pypi/torch1_7_0 -d /cache/pypi -t copytree
+        for filename in /cache/pypi/*.whl; do
+            pip install $filename
+        done
+        proj_root=moco-exp
+        python template_lib/modelarts/scripts/copy_tool.py \
+          -s s3://bucket-7001/ZhouPeng/codes/$proj_root -d /cache/$proj_root -t copytree -b /cache/$proj_root/code.zip
+        cd /cache/$proj_root
+        pip install -r requirements.txt
+
+        export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+        export TIME_STR=1
+        export PYTHONPATH=.
+        python -c "from tl2.launch.tests.test_launch import Testing_Launch_v1;\
+          Testing_Launch_v1().test_launch_ddp(debug=False)" \
+          --tl_opts test0 10 test1 11 --test 1
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    tl_opts_list = tl2_utils.parser_args_from_list(name="--tl_opts", argv_list=sys.argv, type='list')
+    tl_opts = ' '.join(tl_opts_list)
+    print(f'tl_opts:\n {tl_opts}')
+    # tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    # print(f'tl_opts:\n {tl_opts}')
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                --tl_opts {tl_opts}
+                """
+    args, cfg = setup_outdir_and_yaml(argv_str, return_cfg=True)
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    PORT = os.environ.get('PORT', 8888)
+
+    import pickle
+    from tl2.proj.stylegan2_ada import ada_utils
+    from tl2.proj.pytorch.pytorch_hook import VerboseModel
+
+    snapshot_pkl = f"results/persistence/persistence_usage/net.pkl"
+
+    net = ada_utils.load_model(snapshot_pkl)
+
+    net_verbose = VerboseModel(net).cuda()
+    x = torch.rand((2, 3)).cuda()
+    out = net_verbose(x)
+    pass
+
+
+
+
+
+
+
+
+
+
```

### Comparing `tl2-0.1.0/tl2/proj/tools3d/camera_pose_visualizer.py` & `tl2-0.1.1/tl2/proj/tools3d/camera_pose_visualizer.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/tools3d/compute_normal_from_depth.py` & `tl2-0.1.1/tl2/proj/tools3d/compute_normal_from_depth.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/tools3d/depth.npy` & `tl2-0.1.1/tl2/proj/tools3d/depth.npy`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/tools3d/img_0.png` & `tl2-0.1.1/tl2/proj/tools3d/img_0.png`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/trimesh/__init__.py` & `tl2-0.1.1/tl2/proj/trimesh/__init__.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/trimesh/aitvconfig.yaml` & `tl2-0.1.1/tl2/proj/trimesh/aitvconfig.yaml`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/trimesh/examples/aitviewer/quick_start.py` & `tl2-0.1.1/tl2/proj/trimesh/examples/aitviewer/quick_start.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/trimesh/examples/open3d/open3d_tensorboard.py` & `tl2-0.1.1/tl2/proj/trimesh/examples/open3d/open3d_tensorboard.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/trimesh/examples/open3d/test_open3d_tensorboard.py` & `tl2-0.1.1/tl2/proj/trimesh/examples/open3d/test_open3d_tensorboard.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/trimesh/examples/pyrender/example.py` & `tl2-0.1.1/tl2/proj/trimesh/examples/pyrender/example.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/trimesh/open3d_utils.py` & `tl2-0.1.1/tl2/proj/trimesh/open3d_utils.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/proj/trimesh/pyrender_utils.py` & `tl2-0.1.1/tl2/proj/trimesh/pyrender_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -304,14 +304,15 @@
                img=None,
                camera_rotation=np.eye(3),
                camera_translation=np.zeros(3),
                color_type='purple',
                color=[1.0, 1.0, 0.9],
                rgba_mode=False,
                mesh_filename=None,
+               vertex_colors=None,
                ):
     """
     sx, tx, ty = cam
     sy = sx
     camera_translation = np.array([- tx, ty, 2 * focal_length[0] / (resolution[0] * sy + 1e-9)])
     
     :param verts:
@@ -330,26 +331,34 @@
     """
     
     if img is None:
       img = np.zeros((self.height, self.width, 3)),
     assert img.shape[:2] == (self.height, self.width)
     
     # add mesh
-    mesh = trimesh.Trimesh(vertices=verts, faces=faces, process=False)
+    mesh = trimesh.Trimesh(vertices=verts, faces=faces, process=False,
+                           vertex_colors=vertex_colors)
+      
     if mesh_filename is not None:
       mesh.export(mesh_filename)
-    if color_type != None:
-      color = self.colors_dict[color_type]
-    material = pyrender.MetallicRoughnessMaterial(
-      metallicFactor=0.2,
-      roughnessFactor=0.6,
-      alphaMode='OPAQUE',
-      baseColorFactor=(color[0], color[1], color[2], 1.0)
-    )
-    mesh = pyrender.Mesh.from_trimesh(mesh, material=material)
+    
+    if vertex_colors is None:
+      if color_type != None:
+        color = self.colors_dict[color_type]
+      material = pyrender.MetallicRoughnessMaterial(
+        metallicFactor=0.2,
+        roughnessFactor=0.6,
+        alphaMode='OPAQUE',
+        baseColorFactor=(color[0], color[1], color[2], 1.0)
+      )
+      mesh = pyrender.Mesh.from_trimesh(mesh, material=material)
+    else:
+      mesh.vertex_colors = vertex_colors
+      mesh = pyrender.Mesh.from_trimesh(mesh, smooth=False, wireframe=False)
+    
     mesh_node = self.scene.add(mesh, 'mesh')
     
     # add camera
     fx, fy = self.intrinsic.get_focal_length()
     cx, cy = self.intrinsic.get_principal_point()
     camera = pyrender.IntrinsicsCamera(fx=fx,
                                        fy=fy,
```

### Comparing `tl2-0.1.0/tl2/proj/trimesh/trimesh_utils.py` & `tl2-0.1.1/tl2/proj/trimesh/trimesh_utils.py`

 * *Files identical despite different names*

### Comparing `tl2-0.1.0/tl2/tl2_utils.py` & `tl2-0.1.1/tl2/tl2_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -463,14 +463,22 @@
   fz = zipfile.ZipFile(zip_file, 'r')
   for file in fz.namelist():
     fz.extract(file, dst_dir)
   fz.close()
   print(f'Unzip {zip_file} to {dst_dir}')
 
 
+def read_file_from_zip(zip_file,
+                       file_name):
+  with zipfile.ZipFile(zip_file, 'r') as fz:
+    data_bytes = fz.read(file_name)
+    data = pickle.loads(data_bytes)
+  return data
+
+
 def get_filelist_recursive(directory,
                            ext=('*.jpg', '*.png'),
                            sort=True,
                            to_str=False,
                            recursive=True):
 
   if not isinstance(ext, (list, tuple)):
```

### Comparing `tl2-0.1.0/tl2/tools/dataset_tool.py` & `tl2-0.1.1/tl2/tools/dataset_tool.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,515 +1,515 @@
-import functools
-import io
-import json
-import os
-import pickle
-import sys
-import tarfile
-import gzip
-import zipfile
-from pathlib import Path
-from typing import Callable, Optional, Tuple, Union
-
-import click
-import numpy as np
-import PIL.Image
-from tqdm import tqdm
-
-#----------------------------------------------------------------------------
-
-def error(msg):
-    print('Error: ' + msg)
-    sys.exit(1)
-
-#----------------------------------------------------------------------------
-
-def maybe_min(a: int, b: Optional[int]) -> int:
-    if b is not None:
-        return min(a, b)
-    return a
-
-#----------------------------------------------------------------------------
-
-def file_ext(name: Union[str, Path]) -> str:
-    return str(name).split('.')[-1]
-
-#----------------------------------------------------------------------------
-
-def is_image_ext(fname: Union[str, Path]) -> bool:
-    ext = file_ext(fname).lower()
-    return f'.{ext}' in PIL.Image.EXTENSION # type: ignore
-
-#----------------------------------------------------------------------------
-
-def open_image_folder(source_dir, *, max_images: Optional[int]):
-    input_images = [str(f) for f in sorted(Path(source_dir).rglob('*')) if is_image_ext(f) and os.path.isfile(f)]
-
-    # Load labels.
-    labels = {}
-    meta_fname = os.path.join(source_dir, 'dataset.json')
-    if os.path.isfile(meta_fname):
-        with open(meta_fname, 'r') as file:
-            labels = json.load(file)['labels']
-            if labels is not None:
-                labels = { x[0]: x[1] for x in labels }
-            else:
-                labels = {}
-
-    max_idx = maybe_min(len(input_images), max_images)
-
-    def iterate_images():
-        for idx, fname in enumerate(input_images):
-            arch_fname = os.path.relpath(fname, source_dir)
-            arch_fname = arch_fname.replace('\\', '/')
-            img = np.array(PIL.Image.open(fname))
-            yield dict(img=img, label=labels.get(arch_fname))
-            if idx >= max_idx-1:
-                break
-    return max_idx, iterate_images()
-
-#----------------------------------------------------------------------------
-
-def open_image_zip(source, *, max_images: Optional[int]):
-    with zipfile.ZipFile(source, mode='r') as z:
-        input_images = [str(f) for f in sorted(z.namelist()) if is_image_ext(f)]
-
-        # Load labels.
-        labels = {}
-        if 'dataset.json' in z.namelist():
-            with z.open('dataset.json', 'r') as file:
-                labels = json.load(file)['labels']
-                if labels is not None:
-                    labels = { x[0]: x[1] for x in labels }
-                else:
-                    labels = {}
-
-    max_idx = maybe_min(len(input_images), max_images)
-
-    def iterate_images():
-        with zipfile.ZipFile(source, mode='r') as z:
-            for idx, fname in enumerate(input_images):
-                with z.open(fname, 'r') as file:
-                    img = PIL.Image.open(file) # type: ignore
-                    img = np.array(img)
-                yield dict(img=img, label=labels.get(fname))
-                if idx >= max_idx-1:
-                    break
-    return max_idx, iterate_images()
-
-#----------------------------------------------------------------------------
-
-def open_lmdb(lmdb_dir: str, *, max_images: Optional[int]):
-    import cv2  # pip install opencv-python
-    import lmdb  # pip install lmdb # pylint: disable=import-error
-
-    with lmdb.open(lmdb_dir, readonly=True, lock=False).begin(write=False) as txn:
-        max_idx = maybe_min(txn.stat()['entries'], max_images)
-
-    def iterate_images():
-        with lmdb.open(lmdb_dir, readonly=True, lock=False).begin(write=False) as txn:
-            for idx, (_key, value) in enumerate(txn.cursor()):
-                try:
-                    try:
-                        img = cv2.imdecode(np.frombuffer(value, dtype=np.uint8), 1)
-                        if img is None:
-                            raise IOError('cv2.imdecode failed')
-                        img = img[:, :, ::-1] # BGR => RGB
-                    except IOError:
-                        img = np.array(PIL.Image.open(io.BytesIO(value)))
-                    yield dict(img=img, label=None)
-                    if idx >= max_idx-1:
-                        break
-                except:
-                    print(sys.exc_info()[1])
-
-    return max_idx, iterate_images()
-
-#----------------------------------------------------------------------------
-
-def open_cifar10(tarball: str, *, max_images: Optional[int]):
-    images = []
-    labels = []
-
-    with tarfile.open(tarball, 'r:gz') as tar:
-        for batch in range(1, 6):
-            member = tar.getmember(f'cifar-10-batches-py/data_batch_{batch}')
-            with tar.extractfile(member) as file:
-                data = pickle.load(file, encoding='latin1')
-            images.append(data['data'].reshape(-1, 3, 32, 32))
-            labels.append(data['labels'])
-
-    images = np.concatenate(images)
-    labels = np.concatenate(labels)
-    images = images.transpose([0, 2, 3, 1]) # NCHW -> NHWC
-    assert images.shape == (50000, 32, 32, 3) and images.dtype == np.uint8
-    assert labels.shape == (50000,) and labels.dtype in [np.int32, np.int64]
-    assert np.min(images) == 0 and np.max(images) == 255
-    assert np.min(labels) == 0 and np.max(labels) == 9
-
-    max_idx = maybe_min(len(images), max_images)
-
-    def iterate_images():
-        for idx, img in enumerate(images):
-            yield dict(img=img, label=int(labels[idx]))
-            if idx >= max_idx-1:
-                break
-
-    return max_idx, iterate_images()
-
-#----------------------------------------------------------------------------
-
-def open_mnist(images_gz: str, *, max_images: Optional[int]):
-    labels_gz = images_gz.replace('-images-idx3-ubyte.gz', '-labels-idx1-ubyte.gz')
-    assert labels_gz != images_gz
-    images = []
-    labels = []
-
-    with gzip.open(images_gz, 'rb') as f:
-        images = np.frombuffer(f.read(), np.uint8, offset=16)
-    with gzip.open(labels_gz, 'rb') as f:
-        labels = np.frombuffer(f.read(), np.uint8, offset=8)
-
-    images = images.reshape(-1, 28, 28)
-    images = np.pad(images, [(0,0), (2,2), (2,2)], 'constant', constant_values=0)
-    assert images.shape == (60000, 32, 32) and images.dtype == np.uint8
-    assert labels.shape == (60000,) and labels.dtype == np.uint8
-    assert np.min(images) == 0 and np.max(images) == 255
-    assert np.min(labels) == 0 and np.max(labels) == 9
-
-    max_idx = maybe_min(len(images), max_images)
-
-    def iterate_images():
-        for idx, img in enumerate(images):
-            yield dict(img=img, label=int(labels[idx]))
-            if idx >= max_idx-1:
-                break
-
-    return max_idx, iterate_images()
-
-def open_image_txt(source, *, max_images: Optional[int]):
-    images = []
-    labels = []
-
-    with open(source) as f:
-        for line in f:
-            image_path, label = line.strip().split(' ')
-            images.append(image_path)
-            labels.append(label)
-
-    max_idx = maybe_min(len(images), max_images)
-
-    def iterate_images():
-        for idx, fname in enumerate(images):
-            img = np.array(PIL.Image.open(fname))
-            yield dict(img=img, label=int(labels[idx]))
-            if idx >= max_idx - 1:
-                break
-    return max_idx, iterate_images()
-#----------------------------------------------------------------------------
-
-def make_transform(
-    transform: Optional[str],
-    output_width: Optional[int],
-    output_height: Optional[int],
-    resize_filter: str
-) -> Callable[[np.ndarray], Optional[np.ndarray]]:
-    resample = { 'box': PIL.Image.BOX, 'lanczos': PIL.Image.LANCZOS }[resize_filter]
-    def scale(width, height, img):
-        w = img.shape[1]
-        h = img.shape[0]
-        if width == w and height == h:
-            return img
-        img = PIL.Image.fromarray(img)
-        ww = width if width is not None else w
-        hh = height if height is not None else h
-        img = img.resize((ww, hh), resample)
-        return np.array(img)
-
-    def center_crop(width, height, img):
-        crop = np.min(img.shape[:2])
-        img = img[(img.shape[0] - crop) // 2 : (img.shape[0] + crop) // 2, (img.shape[1] - crop) // 2 : (img.shape[1] + crop) // 2]
-        img = PIL.Image.fromarray(img, 'RGB')
-        img = img.resize((width, height), resample)
-        return np.array(img)
-
-    def center_crop_wide(width, height, img):
-        ch = int(np.round(width * img.shape[0] / img.shape[1]))
-        if img.shape[1] < width or ch < height:
-            return None
-
-        img = img[(img.shape[0] - ch) // 2 : (img.shape[0] + ch) // 2]
-        img = PIL.Image.fromarray(img, 'RGB')
-        img = img.resize((width, height), resample)
-        img = np.array(img)
-
-        canvas = np.zeros([width, width, 3], dtype=np.uint8)
-        canvas[(width - height) // 2 : (width + height) // 2, :] = img
-        return canvas
-
-    def pad_zero(width, height, img):
-        h, w, c = img.shape
-        start_h = (height - h) // 2
-        end_h = start_h + h
-        start_w = (width - w) // 2
-        end_w = start_w + w
-
-        canvas = np.zeros([height, width, 3], dtype=np.uint8)
-        canvas[start_h:end_h, start_w:end_w, :] = img
-
-        return canvas
-
-    def pad_mirror(width, height, img):
-        import cv2
-
-        h, w, c = img.shape
-        start_h = (height - h) // 2
-        end_h = start_h + h
-        start_w = (width - w) // 2
-        end_w = start_w + w
-
-        border_t = start_h
-        border_b = height - end_h
-        border_l = start_w
-        border_r = width - end_w
-
-        # canvas = np.zeros([height, width, 3], dtype=np.uint8)
-        # canvas[start_h:end_h, start_w:end_w, :] = img
-        canvas = cv2.copyMakeBorder(img.copy(), border_t, border_b, border_l, border_r, cv2.BORDER_REFLECT_101)
-        return canvas
-
-    if transform is None:
-        return functools.partial(scale, output_width, output_height)
-    if transform == 'center-crop':
-        if (output_width is None) or (output_height is None):
-            error ('must specify --width and --height when using ' + transform + 'transform')
-        return functools.partial(center_crop, output_width, output_height)
-    if transform == 'center-crop-wide':
-        if (output_width is None) or (output_height is None):
-            error ('must specify --width and --height when using ' + transform + ' transform')
-        return functools.partial(center_crop_wide, output_width, output_height)
-    if transform == 'pad_zero':
-        if (output_width is None) or (output_height is None):
-            error('must specify --width and --height when using ' + transform + ' transform')
-        return functools.partial(pad_zero, output_width, output_height)
-    if transform == 'pad_mirror':
-        if (output_width is None) or (output_height is None):
-            error('must specify --width and --height when using ' + transform + ' transform')
-        return functools.partial(pad_mirror, output_width, output_height)
-    assert False, 'unknown transform'
-
-#----------------------------------------------------------------------------
-
-def open_dataset(source, *, max_images: Optional[int]):
-    if os.path.isdir(source):
-        if source.rstrip('/').endswith('_lmdb'):
-            return open_lmdb(source, max_images=max_images)
-        else:
-            return open_image_folder(source, max_images=max_images)
-    elif os.path.isfile(source):
-        if os.path.basename(source) == 'cifar-10-python.tar.gz':
-            return open_cifar10(source, max_images=max_images)
-        elif os.path.basename(source) == 'train-images-idx3-ubyte.gz':
-            return open_mnist(source, max_images=max_images)
-        elif file_ext(source) == 'zip':
-            return open_image_zip(source, max_images=max_images)
-        elif file_ext(source) == 'txt':
-            return open_image_txt(source, max_images=max_images)
-        else:
-            assert False, 'unknown archive type'
-    else:
-        error(f'Missing input file or directory: {source}')
-
-#----------------------------------------------------------------------------
-
-def open_dest(dest: str, create_root_dir_in_zip=True) -> Tuple[str, Callable[[str, Union[bytes, str]], None], Callable[[], None]]:
-    dest_ext = file_ext(dest)
-    root_dir = Path(dest).stem
-
-    if dest_ext == 'zip':
-        if os.path.dirname(dest) != '':
-            os.makedirs(os.path.dirname(dest), exist_ok=True)
-        zf = zipfile.ZipFile(file=dest, mode='w', compression=zipfile.ZIP_STORED)
-        def zip_write_bytes(fname: str, data: Union[bytes, str]):
-            if create_root_dir_in_zip:
-                fname = f"{root_dir}/{fname}"
-            zf.writestr(fname, data)
-        return '', zip_write_bytes, zf.close
-    else:
-        # If the output folder already exists, check that is is
-        # empty.
-        #
-        # Note: creating the output directory is not strictly
-        # necessary as folder_write_bytes() also mkdirs, but it's better
-        # to give an error message earlier in case the dest folder
-        # somehow cannot be created.
-        if os.path.isdir(dest) and len(os.listdir(dest)) != 0:
-            error('--dest folder must be empty')
-        os.makedirs(dest, exist_ok=True)
-
-        def folder_write_bytes(fname: str, data: Union[bytes, str]):
-            os.makedirs(os.path.dirname(fname), exist_ok=True)
-            with open(fname, 'wb') as fout:
-                if isinstance(data, str):
-                    data = data.encode('utf8')
-                fout.write(data)
-        return dest, folder_write_bytes, lambda: None
-
-#----------------------------------------------------------------------------
-
-@click.command(context_settings=dict(ignore_unknown_options=True, help_option_names=[]))
-@click.pass_context
-@click.option('--source', help='Directory or archive name for input dataset', required=True, metavar='PATH')
-@click.option('--dest', help='Output directory or archive name for output dataset', required=True, metavar='PATH')
-@click.option('--max-images', help='Output only up to `max-images` images', type=int, default=None)
-@click.option('--resize-filter', help='Filter to use when resizing images for output resolution', type=click.Choice(['box', 'lanczos']), default='lanczos', show_default=True)
-@click.option('--transform', help='Input crop/resize mode', type=click.Choice(['center-crop', 'center-crop-wide', 'pad_zero', 'pad_mirror']))
-@click.option('--width', help='Output width', type=int)
-@click.option('--height', help='Output height', type=int)
-@click.argument('argv', nargs=-1, type=click.UNPROCESSED)
-def convert_dataset(
-    ctx: click.Context,
-    source: str,
-    dest: str,
-    max_images: Optional[int],
-    transform: Optional[str],
-    resize_filter: str,
-    width: Optional[int],
-    height: Optional[int],
-    argv=None
-):
-    """Convert an image dataset into a dataset archive usable with StyleGAN2 ADA PyTorch.
-
-    The input dataset format is guessed from the --source argument:
-
-    \b
-    --source *_lmdb/                    Load LSUN dataset
-    --source cifar-10-python.tar.gz     Load CIFAR-10 dataset
-    --source train-images-idx3-ubyte.gz Load MNIST dataset
-    --source path/                      Recursively load all images from path/
-    --source dataset.zip                Recursively load all images from dataset.zip
-
-    Specifying the output format and path:
-
-    \b
-    --dest /path/to/dir                 Save output files under /path/to/dir
-    --dest /path/to/dataset.zip         Save output files into /path/to/dataset.zip
-
-    The output dataset format can be either an image folder or an uncompressed zip archive.
-    Zip archives makes it easier to move datasets around file servers and clusters, and may
-    offer better training performance on network file systems.
-
-    Images within the dataset archive will be stored as uncompressed PNG.
-    Uncompresed PNGs can be efficiently decoded in the training loop.
-
-    Class labels are stored in a file called 'dataset.json' that is stored at the
-    dataset root folder.  This file has the following structure:
-
-    \b
-    {
-        "labels": [
-            ["00000/img00000000.png",6],
-            ["00000/img00000001.png",9],
-            ... repeated for every image in the datase
-            ["00049/img00049999.png",1]
-        ]
-    }
-
-    If the 'dataset.json' file cannot be found, the dataset is interpreted as
-    not containing class labels.
-
-    Image scale/crop and resolution requirements:
-
-    Output images must be square-shaped and they must all have the same power-of-two
-    dimensions.
-
-    To scale arbitrary input image size to a specific width and height, use the
-    --width and --height options.  Output resolution will be either the original
-    input resolution (if --width/--height was not specified) or the one specified with
-    --width/height.
-
-    Use the --transform=center-crop or --transform=center-crop-wide options to apply a
-    center crop transform on the input image.  These options should be used with the
-    --width and --height options.  For example:
-
-    \b
-    python dataset_tool.py --source LSUN/raw/cat_lmdb --dest /tmp/lsun_cat \\
-        --transform=center-crop-wide --width 512 --height=384
-    """
-
-    PIL.Image.init() # type: ignore
-
-    if dest == '':
-        ctx.fail('--dest output filename or directory must not be an empty string')
-
-    print(f"Convert {source} to {dest}")
-    num_files, input_iter = open_dataset(source, max_images=max_images)
-    archive_root_dir, save_bytes, close_dest = open_dest(dest)
-
-    transform_image = make_transform(transform, width, height, resize_filter)
-
-    dataset_attrs = None
-
-    labels = []
-    for idx, image in tqdm(enumerate(input_iter), total=num_files):
-        idx_str = f'{idx:08d}'
-        archive_fname = f'{idx_str[:5]}/img{idx_str}.png'
-
-        # Apply crop and resize.
-        img = transform_image(image['img'])
-
-        # Transform may drop images.
-        if img is None:
-            continue
-
-        # Error check to require uniform image attributes across
-        # the whole dataset.
-        channels = img.shape[2] if img.ndim == 3 else 1
-        cur_image_attrs = {
-            'width': img.shape[1],
-            'height': img.shape[0],
-            'channels': channels
-        }
-        if dataset_attrs is None:
-            dataset_attrs = cur_image_attrs
-            width = dataset_attrs['width']
-            height = dataset_attrs['height']
-            if width != height:
-                error(f'Image dimensions after scale and crop are required to be square.  Got {width}x{height}')
-            if dataset_attrs['channels'] not in [1, 3]:
-                error('Input images must be stored as RGB or grayscale')
-            # if width != 2 ** int(np.floor(np.log2(width))):
-            #     error('Image width/height after scale and crop are required to be power-of-two')
-        elif dataset_attrs != cur_image_attrs:
-            err = [f'  dataset {k}/cur image {k}: {dataset_attrs[k]}/{cur_image_attrs[k]}' for k in dataset_attrs.keys()]
-            error(f'Image {archive_fname} attributes must be equal across all images of the dataset.  Got:\n' + '\n'.join(err))
-
-        # Save the image as an uncompressed PNG.
-        img = PIL.Image.fromarray(img, { 1: 'L', 3: 'RGB' }[channels])
-        image_bits = io.BytesIO()
-        img.save(image_bits, format='png', compress_level=0, optimize=False)
-        save_bytes(os.path.join(archive_root_dir, archive_fname), image_bits.getbuffer())
-        labels.append([archive_fname, image['label']] if image['label'] is not None else None)
-        if global_cfg.tl_debug:
-            break
-
-    metadata = {
-        'labels': labels if all(x is not None for x in labels) else None
-    }
-    save_bytes(os.path.join(archive_root_dir, 'dataset.json'), json.dumps(metadata))
-    close_dest()
-
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    pass
-#----------------------------------------------------------------------------
-
-if __name__ == "__main__":
-    from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
-    update_parser_defaults_from_yaml(parser=None)
-
-    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
-    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
-    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
-
-    convert_dataset() # pylint: disable=no-value-for-parameter
-
+import functools
+import io
+import json
+import os
+import pickle
+import sys
+import tarfile
+import gzip
+import zipfile
+from pathlib import Path
+from typing import Callable, Optional, Tuple, Union
+
+import click
+import numpy as np
+import PIL.Image
+from tqdm import tqdm
+
+#----------------------------------------------------------------------------
+
+def error(msg):
+    print('Error: ' + msg)
+    sys.exit(1)
+
+#----------------------------------------------------------------------------
+
+def maybe_min(a: int, b: Optional[int]) -> int:
+    if b is not None:
+        return min(a, b)
+    return a
+
+#----------------------------------------------------------------------------
+
+def file_ext(name: Union[str, Path]) -> str:
+    return str(name).split('.')[-1]
+
+#----------------------------------------------------------------------------
+
+def is_image_ext(fname: Union[str, Path]) -> bool:
+    ext = file_ext(fname).lower()
+    return f'.{ext}' in PIL.Image.EXTENSION # type: ignore
+
+#----------------------------------------------------------------------------
+
+def open_image_folder(source_dir, *, max_images: Optional[int]):
+    input_images = [str(f) for f in sorted(Path(source_dir).rglob('*')) if is_image_ext(f) and os.path.isfile(f)]
+
+    # Load labels.
+    labels = {}
+    meta_fname = os.path.join(source_dir, 'dataset.json')
+    if os.path.isfile(meta_fname):
+        with open(meta_fname, 'r') as file:
+            labels = json.load(file)['labels']
+            if labels is not None:
+                labels = { x[0]: x[1] for x in labels }
+            else:
+                labels = {}
+
+    max_idx = maybe_min(len(input_images), max_images)
+
+    def iterate_images():
+        for idx, fname in enumerate(input_images):
+            arch_fname = os.path.relpath(fname, source_dir)
+            arch_fname = arch_fname.replace('\\', '/')
+            img = np.array(PIL.Image.open(fname))
+            yield dict(img=img, label=labels.get(arch_fname))
+            if idx >= max_idx-1:
+                break
+    return max_idx, iterate_images()
+
+#----------------------------------------------------------------------------
+
+def open_image_zip(source, *, max_images: Optional[int]):
+    with zipfile.ZipFile(source, mode='r') as z:
+        input_images = [str(f) for f in sorted(z.namelist()) if is_image_ext(f)]
+
+        # Load labels.
+        labels = {}
+        if 'dataset.json' in z.namelist():
+            with z.open('dataset.json', 'r') as file:
+                labels = json.load(file)['labels']
+                if labels is not None:
+                    labels = { x[0]: x[1] for x in labels }
+                else:
+                    labels = {}
+
+    max_idx = maybe_min(len(input_images), max_images)
+
+    def iterate_images():
+        with zipfile.ZipFile(source, mode='r') as z:
+            for idx, fname in enumerate(input_images):
+                with z.open(fname, 'r') as file:
+                    img = PIL.Image.open(file) # type: ignore
+                    img = np.array(img)
+                yield dict(img=img, label=labels.get(fname))
+                if idx >= max_idx-1:
+                    break
+    return max_idx, iterate_images()
+
+#----------------------------------------------------------------------------
+
+def open_lmdb(lmdb_dir: str, *, max_images: Optional[int]):
+    import cv2  # pip install opencv-python
+    import lmdb  # pip install lmdb # pylint: disable=import-error
+
+    with lmdb.open(lmdb_dir, readonly=True, lock=False).begin(write=False) as txn:
+        max_idx = maybe_min(txn.stat()['entries'], max_images)
+
+    def iterate_images():
+        with lmdb.open(lmdb_dir, readonly=True, lock=False).begin(write=False) as txn:
+            for idx, (_key, value) in enumerate(txn.cursor()):
+                try:
+                    try:
+                        img = cv2.imdecode(np.frombuffer(value, dtype=np.uint8), 1)
+                        if img is None:
+                            raise IOError('cv2.imdecode failed')
+                        img = img[:, :, ::-1] # BGR => RGB
+                    except IOError:
+                        img = np.array(PIL.Image.open(io.BytesIO(value)))
+                    yield dict(img=img, label=None)
+                    if idx >= max_idx-1:
+                        break
+                except:
+                    print(sys.exc_info()[1])
+
+    return max_idx, iterate_images()
+
+#----------------------------------------------------------------------------
+
+def open_cifar10(tarball: str, *, max_images: Optional[int]):
+    images = []
+    labels = []
+
+    with tarfile.open(tarball, 'r:gz') as tar:
+        for batch in range(1, 6):
+            member = tar.getmember(f'cifar-10-batches-py/data_batch_{batch}')
+            with tar.extractfile(member) as file:
+                data = pickle.load(file, encoding='latin1')
+            images.append(data['data'].reshape(-1, 3, 32, 32))
+            labels.append(data['labels'])
+
+    images = np.concatenate(images)
+    labels = np.concatenate(labels)
+    images = images.transpose([0, 2, 3, 1]) # NCHW -> NHWC
+    assert images.shape == (50000, 32, 32, 3) and images.dtype == np.uint8
+    assert labels.shape == (50000,) and labels.dtype in [np.int32, np.int64]
+    assert np.min(images) == 0 and np.max(images) == 255
+    assert np.min(labels) == 0 and np.max(labels) == 9
+
+    max_idx = maybe_min(len(images), max_images)
+
+    def iterate_images():
+        for idx, img in enumerate(images):
+            yield dict(img=img, label=int(labels[idx]))
+            if idx >= max_idx-1:
+                break
+
+    return max_idx, iterate_images()
+
+#----------------------------------------------------------------------------
+
+def open_mnist(images_gz: str, *, max_images: Optional[int]):
+    labels_gz = images_gz.replace('-images-idx3-ubyte.gz', '-labels-idx1-ubyte.gz')
+    assert labels_gz != images_gz
+    images = []
+    labels = []
+
+    with gzip.open(images_gz, 'rb') as f:
+        images = np.frombuffer(f.read(), np.uint8, offset=16)
+    with gzip.open(labels_gz, 'rb') as f:
+        labels = np.frombuffer(f.read(), np.uint8, offset=8)
+
+    images = images.reshape(-1, 28, 28)
+    images = np.pad(images, [(0,0), (2,2), (2,2)], 'constant', constant_values=0)
+    assert images.shape == (60000, 32, 32) and images.dtype == np.uint8
+    assert labels.shape == (60000,) and labels.dtype == np.uint8
+    assert np.min(images) == 0 and np.max(images) == 255
+    assert np.min(labels) == 0 and np.max(labels) == 9
+
+    max_idx = maybe_min(len(images), max_images)
+
+    def iterate_images():
+        for idx, img in enumerate(images):
+            yield dict(img=img, label=int(labels[idx]))
+            if idx >= max_idx-1:
+                break
+
+    return max_idx, iterate_images()
+
+def open_image_txt(source, *, max_images: Optional[int]):
+    images = []
+    labels = []
+
+    with open(source) as f:
+        for line in f:
+            image_path, label = line.strip().split(' ')
+            images.append(image_path)
+            labels.append(label)
+
+    max_idx = maybe_min(len(images), max_images)
+
+    def iterate_images():
+        for idx, fname in enumerate(images):
+            img = np.array(PIL.Image.open(fname))
+            yield dict(img=img, label=int(labels[idx]))
+            if idx >= max_idx - 1:
+                break
+    return max_idx, iterate_images()
+#----------------------------------------------------------------------------
+
+def make_transform(
+    transform: Optional[str],
+    output_width: Optional[int],
+    output_height: Optional[int],
+    resize_filter: str
+) -> Callable[[np.ndarray], Optional[np.ndarray]]:
+    resample = { 'box': PIL.Image.BOX, 'lanczos': PIL.Image.LANCZOS }[resize_filter]
+    def scale(width, height, img):
+        w = img.shape[1]
+        h = img.shape[0]
+        if width == w and height == h:
+            return img
+        img = PIL.Image.fromarray(img)
+        ww = width if width is not None else w
+        hh = height if height is not None else h
+        img = img.resize((ww, hh), resample)
+        return np.array(img)
+
+    def center_crop(width, height, img):
+        crop = np.min(img.shape[:2])
+        img = img[(img.shape[0] - crop) // 2 : (img.shape[0] + crop) // 2, (img.shape[1] - crop) // 2 : (img.shape[1] + crop) // 2]
+        img = PIL.Image.fromarray(img, 'RGB')
+        img = img.resize((width, height), resample)
+        return np.array(img)
+
+    def center_crop_wide(width, height, img):
+        ch = int(np.round(width * img.shape[0] / img.shape[1]))
+        if img.shape[1] < width or ch < height:
+            return None
+
+        img = img[(img.shape[0] - ch) // 2 : (img.shape[0] + ch) // 2]
+        img = PIL.Image.fromarray(img, 'RGB')
+        img = img.resize((width, height), resample)
+        img = np.array(img)
+
+        canvas = np.zeros([width, width, 3], dtype=np.uint8)
+        canvas[(width - height) // 2 : (width + height) // 2, :] = img
+        return canvas
+
+    def pad_zero(width, height, img):
+        h, w, c = img.shape
+        start_h = (height - h) // 2
+        end_h = start_h + h
+        start_w = (width - w) // 2
+        end_w = start_w + w
+
+        canvas = np.zeros([height, width, 3], dtype=np.uint8)
+        canvas[start_h:end_h, start_w:end_w, :] = img
+
+        return canvas
+
+    def pad_mirror(width, height, img):
+        import cv2
+
+        h, w, c = img.shape
+        start_h = (height - h) // 2
+        end_h = start_h + h
+        start_w = (width - w) // 2
+        end_w = start_w + w
+
+        border_t = start_h
+        border_b = height - end_h
+        border_l = start_w
+        border_r = width - end_w
+
+        # canvas = np.zeros([height, width, 3], dtype=np.uint8)
+        # canvas[start_h:end_h, start_w:end_w, :] = img
+        canvas = cv2.copyMakeBorder(img.copy(), border_t, border_b, border_l, border_r, cv2.BORDER_REFLECT_101)
+        return canvas
+
+    if transform is None:
+        return functools.partial(scale, output_width, output_height)
+    if transform == 'center-crop':
+        if (output_width is None) or (output_height is None):
+            error ('must specify --width and --height when using ' + transform + 'transform')
+        return functools.partial(center_crop, output_width, output_height)
+    if transform == 'center-crop-wide':
+        if (output_width is None) or (output_height is None):
+            error ('must specify --width and --height when using ' + transform + ' transform')
+        return functools.partial(center_crop_wide, output_width, output_height)
+    if transform == 'pad_zero':
+        if (output_width is None) or (output_height is None):
+            error('must specify --width and --height when using ' + transform + ' transform')
+        return functools.partial(pad_zero, output_width, output_height)
+    if transform == 'pad_mirror':
+        if (output_width is None) or (output_height is None):
+            error('must specify --width and --height when using ' + transform + ' transform')
+        return functools.partial(pad_mirror, output_width, output_height)
+    assert False, 'unknown transform'
+
+#----------------------------------------------------------------------------
+
+def open_dataset(source, *, max_images: Optional[int]):
+    if os.path.isdir(source):
+        if source.rstrip('/').endswith('_lmdb'):
+            return open_lmdb(source, max_images=max_images)
+        else:
+            return open_image_folder(source, max_images=max_images)
+    elif os.path.isfile(source):
+        if os.path.basename(source) == 'cifar-10-python.tar.gz':
+            return open_cifar10(source, max_images=max_images)
+        elif os.path.basename(source) == 'train-images-idx3-ubyte.gz':
+            return open_mnist(source, max_images=max_images)
+        elif file_ext(source) == 'zip':
+            return open_image_zip(source, max_images=max_images)
+        elif file_ext(source) == 'txt':
+            return open_image_txt(source, max_images=max_images)
+        else:
+            assert False, 'unknown archive type'
+    else:
+        error(f'Missing input file or directory: {source}')
+
+#----------------------------------------------------------------------------
+
+def open_dest(dest: str, create_root_dir_in_zip=True) -> Tuple[str, Callable[[str, Union[bytes, str]], None], Callable[[], None]]:
+    dest_ext = file_ext(dest)
+    root_dir = Path(dest).stem
+
+    if dest_ext == 'zip':
+        if os.path.dirname(dest) != '':
+            os.makedirs(os.path.dirname(dest), exist_ok=True)
+        zf = zipfile.ZipFile(file=dest, mode='w', compression=zipfile.ZIP_STORED)
+        def zip_write_bytes(fname: str, data: Union[bytes, str]):
+            if create_root_dir_in_zip:
+                fname = f"{root_dir}/{fname}"
+            zf.writestr(fname, data)
+        return '', zip_write_bytes, zf.close
+    else:
+        # If the output folder already exists, check that is is
+        # empty.
+        #
+        # Note: creating the output directory is not strictly
+        # necessary as folder_write_bytes() also mkdirs, but it's better
+        # to give an error message earlier in case the dest folder
+        # somehow cannot be created.
+        if os.path.isdir(dest) and len(os.listdir(dest)) != 0:
+            error('--dest folder must be empty')
+        os.makedirs(dest, exist_ok=True)
+
+        def folder_write_bytes(fname: str, data: Union[bytes, str]):
+            os.makedirs(os.path.dirname(fname), exist_ok=True)
+            with open(fname, 'wb') as fout:
+                if isinstance(data, str):
+                    data = data.encode('utf8')
+                fout.write(data)
+        return dest, folder_write_bytes, lambda: None
+
+#----------------------------------------------------------------------------
+
+@click.command(context_settings=dict(ignore_unknown_options=True, help_option_names=[]))
+@click.pass_context
+@click.option('--source', help='Directory or archive name for input dataset', required=True, metavar='PATH')
+@click.option('--dest', help='Output directory or archive name for output dataset', required=True, metavar='PATH')
+@click.option('--max-images', help='Output only up to `max-images` images', type=int, default=None)
+@click.option('--resize-filter', help='Filter to use when resizing images for output resolution', type=click.Choice(['box', 'lanczos']), default='lanczos', show_default=True)
+@click.option('--transform', help='Input crop/resize mode', type=click.Choice(['center-crop', 'center-crop-wide', 'pad_zero', 'pad_mirror']))
+@click.option('--width', help='Output width', type=int)
+@click.option('--height', help='Output height', type=int)
+@click.argument('argv', nargs=-1, type=click.UNPROCESSED)
+def convert_dataset(
+    ctx: click.Context,
+    source: str,
+    dest: str,
+    max_images: Optional[int],
+    transform: Optional[str],
+    resize_filter: str,
+    width: Optional[int],
+    height: Optional[int],
+    argv=None
+):
+    """Convert an image dataset into a dataset archive usable with StyleGAN2 ADA PyTorch.
+
+    The input dataset format is guessed from the --source argument:
+
+    \b
+    --source *_lmdb/                    Load LSUN dataset
+    --source cifar-10-python.tar.gz     Load CIFAR-10 dataset
+    --source train-images-idx3-ubyte.gz Load MNIST dataset
+    --source path/                      Recursively load all images from path/
+    --source dataset.zip                Recursively load all images from dataset.zip
+
+    Specifying the output format and path:
+
+    \b
+    --dest /path/to/dir                 Save output files under /path/to/dir
+    --dest /path/to/dataset.zip         Save output files into /path/to/dataset.zip
+
+    The output dataset format can be either an image folder or an uncompressed zip archive.
+    Zip archives makes it easier to move datasets around file servers and clusters, and may
+    offer better training performance on network file systems.
+
+    Images within the dataset archive will be stored as uncompressed PNG.
+    Uncompresed PNGs can be efficiently decoded in the training loop.
+
+    Class labels are stored in a file called 'dataset.json' that is stored at the
+    dataset root folder.  This file has the following structure:
+
+    \b
+    {
+        "labels": [
+            ["00000/img00000000.png",6],
+            ["00000/img00000001.png",9],
+            ... repeated for every image in the datase
+            ["00049/img00049999.png",1]
+        ]
+    }
+
+    If the 'dataset.json' file cannot be found, the dataset is interpreted as
+    not containing class labels.
+
+    Image scale/crop and resolution requirements:
+
+    Output images must be square-shaped and they must all have the same power-of-two
+    dimensions.
+
+    To scale arbitrary input image size to a specific width and height, use the
+    --width and --height options.  Output resolution will be either the original
+    input resolution (if --width/--height was not specified) or the one specified with
+    --width/height.
+
+    Use the --transform=center-crop or --transform=center-crop-wide options to apply a
+    center crop transform on the input image.  These options should be used with the
+    --width and --height options.  For example:
+
+    \b
+    python dataset_tool.py --source LSUN/raw/cat_lmdb --dest /tmp/lsun_cat \\
+        --transform=center-crop-wide --width 512 --height=384
+    """
+
+    PIL.Image.init() # type: ignore
+
+    if dest == '':
+        ctx.fail('--dest output filename or directory must not be an empty string')
+
+    print(f"Convert {source} to {dest}")
+    num_files, input_iter = open_dataset(source, max_images=max_images)
+    archive_root_dir, save_bytes, close_dest = open_dest(dest)
+
+    transform_image = make_transform(transform, width, height, resize_filter)
+
+    dataset_attrs = None
+
+    labels = []
+    for idx, image in tqdm(enumerate(input_iter), total=num_files):
+        idx_str = f'{idx:08d}'
+        archive_fname = f'{idx_str[:5]}/img{idx_str}.png'
+
+        # Apply crop and resize.
+        img = transform_image(image['img'])
+
+        # Transform may drop images.
+        if img is None:
+            continue
+
+        # Error check to require uniform image attributes across
+        # the whole dataset.
+        channels = img.shape[2] if img.ndim == 3 else 1
+        cur_image_attrs = {
+            'width': img.shape[1],
+            'height': img.shape[0],
+            'channels': channels
+        }
+        if dataset_attrs is None:
+            dataset_attrs = cur_image_attrs
+            width = dataset_attrs['width']
+            height = dataset_attrs['height']
+            if width != height:
+                error(f'Image dimensions after scale and crop are required to be square.  Got {width}x{height}')
+            if dataset_attrs['channels'] not in [1, 3]:
+                error('Input images must be stored as RGB or grayscale')
+            # if width != 2 ** int(np.floor(np.log2(width))):
+            #     error('Image width/height after scale and crop are required to be power-of-two')
+        elif dataset_attrs != cur_image_attrs:
+            err = [f'  dataset {k}/cur image {k}: {dataset_attrs[k]}/{cur_image_attrs[k]}' for k in dataset_attrs.keys()]
+            error(f'Image {archive_fname} attributes must be equal across all images of the dataset.  Got:\n' + '\n'.join(err))
+
+        # Save the image as an uncompressed PNG.
+        img = PIL.Image.fromarray(img, { 1: 'L', 3: 'RGB' }[channels])
+        image_bits = io.BytesIO()
+        img.save(image_bits, format='png', compress_level=0, optimize=False)
+        save_bytes(os.path.join(archive_root_dir, archive_fname), image_bits.getbuffer())
+        labels.append([archive_fname, image['label']] if image['label'] is not None else None)
+        if global_cfg.tl_debug:
+            break
+
+    metadata = {
+        'labels': labels if all(x is not None for x in labels) else None
+    }
+    save_bytes(os.path.join(archive_root_dir, 'dataset.json'), json.dumps(metadata))
+    close_dest()
+
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_upload', {}), global_cfg=global_cfg, download=False)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    pass
+#----------------------------------------------------------------------------
+
+if __name__ == "__main__":
+    from tl2.launch.launch_utils import update_parser_defaults_from_yaml, global_cfg
+    update_parser_defaults_from_yaml(parser=None)
+
+    # modelarts_utils.setup_tl_outdir_obs(global_cfg)
+    # modelarts_utils.modelarts_sync_results_dir(global_cfg, join=True)
+    # modelarts_utils.prepare_dataset(global_cfg.get('modelarts_download', {}), global_cfg=global_cfg)
+
+    convert_dataset() # pylint: disable=no-value-for-parameter
+
```

### Comparing `tl2-0.1.0/tl2/tools/get_data_list.py` & `tl2-0.1.1/tl2/tools/get_data_list.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,46 +1,46 @@
-import pprint
-import argparse
-import sys
-sys.path.insert(0, '.')
-import tempfile
-
-from tl2.tl2_utils import get_filelist_recursive
-from tl2.proj.logger.logger_utils import get_file_logger
-
-
-def main(source_dir,
-         outfile,
-         ext
-         ):
-
-  file_list = get_filelist_recursive(directory=source_dir, ext=ext, sort=True)
-  print("")
-
-  if not outfile:
-    fd, path = tempfile.mkstemp()
-    outfile = path
-
-  out_f = get_file_logger(outfile, stream=True)
-  for path in file_list:
-    out_f.info_msg(path)
-
-  print(f"\noutfile: {outfile}")
-  print(f"number of items: {len(file_list)}\n")
-  pass
-
-if __name__ == '__main__':
-  """
-  python3 -m tl2.tools.get_data_list \
-    --source_dir  \
-    --outfile  \
-    --ext *.png
-  """
-  parser = argparse.ArgumentParser()
-  parser.add_argument('--source_dir', type=str, default="")
-  parser.add_argument('--outfile', type=str, default="")
-  parser.add_argument('--ext', type=str, nargs='+', default=["*.png", "*.jpg", "*.jfif", "*.jpeg"])
-
-  args = parser.parse_args()
-  pprint.pprint(vars(args))
-  main(**vars(args))
-
+import pprint
+import argparse
+import sys
+sys.path.insert(0, '.')
+import tempfile
+
+from tl2.tl2_utils import get_filelist_recursive
+from tl2.proj.logger.logger_utils import get_file_logger
+
+
+def main(source_dir,
+         outfile,
+         ext
+         ):
+
+  file_list = get_filelist_recursive(directory=source_dir, ext=ext, sort=True)
+  print("")
+
+  if not outfile:
+    fd, path = tempfile.mkstemp()
+    outfile = path
+
+  out_f = get_file_logger(outfile, stream=True)
+  for path in file_list:
+    out_f.info_msg(path)
+
+  print(f"\noutfile: {outfile}")
+  print(f"number of items: {len(file_list)}\n")
+  pass
+
+if __name__ == '__main__':
+  """
+  python3 -m tl2.tools.get_data_list \
+    --source_dir  \
+    --outfile  \
+    --ext *.png
+  """
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--source_dir', type=str, default="")
+  parser.add_argument('--outfile', type=str, default="")
+  parser.add_argument('--ext', type=str, nargs='+', default=["*.png", "*.jpg", "*.jfif", "*.jpeg"])
+
+  args = parser.parse_args()
+  pprint.pprint(vars(args))
+  main(**vars(args))
+
```

### Comparing `tl2-0.1.0/tl2/tools/label_smoothing.py` & `tl2-0.1.1/tl2/tools/label_smoothing.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,100 +1,100 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-# """
-# https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch
-#
-# """
-
-
-class LabelSmoothingLoss(torch.nn.Module):
-  def __init__(self,
-               reduction="mean",
-               weight=None):
-    super(LabelSmoothingLoss, self).__init__()
-
-    self.reduction = reduction
-    self.weight = weight
-    pass
-
-  def reduce_loss(self, loss):
-    return loss.mean() if self.reduction == 'mean' else loss.sum() \
-      if self.reduction == 'sum' else loss
-
-  def linear_combination(self,
-                         x,
-                         y,
-                         smoothing):
-    return smoothing * x + (1 - smoothing) * y
-
-  def forward(self,
-              preds,
-              target,
-              smoothing=0.1):
-    assert 0 <= smoothing < 1
-
-    if self.weight is not None:
-      self.weight = self.weight.to(preds.device)
-
-    n = preds.size(-1)
-    log_preds = F.log_softmax(preds, dim=-1)
-    loss = self.reduce_loss(-log_preds.sum(dim=-1))
-
-    nll = F.nll_loss(log_preds, target, reduction=self.reduction, weight=self.weight)
-
-    return self.linear_combination(loss / n, nll, smoothing=smoothing)
-
-
-
-class LabelSmoothing(nn.Module):
-  """NLL loss with label smoothing.
-  """
-
-  def __init__(self):
-    """Constructor for the LabelSmoothing module.
-    :param smoothing: label smoothing factor
-    """
-    super(LabelSmoothing, self).__init__()
-
-    pass
-
-  def forward(self,
-              x,
-              target,
-              smoothing=0.1):
-    """
-
-    :param x: (b, c)
-    :param target:  (b, )
-    :param smoothing:
-    :return:
-    """
-    confidence = 1.0 - smoothing
-
-    logprobs = torch.nn.functional.log_softmax(x, dim=-1)
-    nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
-    nll_loss = nll_loss.squeeze(1)
-    smooth_loss = -logprobs.mean(dim=-1)
-    loss = confidence * nll_loss + smoothing * smooth_loss
-    return loss.mean()
-
-
-if __name__=="__main__":
-    # Wangleiofficial
-
-    predict = torch.tensor([[0, 0.2, 0.7, 0.1, 0],
-                            [0, 0.9, 0.2, 0.2, 1],
-                            [1, 0.2, 0.7, 0.9, 1]])
-    label = torch.tensor([2, 1, 0])
-
-    crit1 = LabelSmoothingLoss(reduction="mean")
-    v1 = crit1(predict, label, smoothing=0.3, )
-
-    # NVIDIA
-    crit2 = LabelSmoothing()
-    v2 = crit2(predict, label, smoothing=0.3)
-
-    assert v1 == v2
-    pass
-
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+# """
+# https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch
+#
+# """
+
+
+class LabelSmoothingLoss(torch.nn.Module):
+  def __init__(self,
+               reduction="mean",
+               weight=None):
+    super(LabelSmoothingLoss, self).__init__()
+
+    self.reduction = reduction
+    self.weight = weight
+    pass
+
+  def reduce_loss(self, loss):
+    return loss.mean() if self.reduction == 'mean' else loss.sum() \
+      if self.reduction == 'sum' else loss
+
+  def linear_combination(self,
+                         x,
+                         y,
+                         smoothing):
+    return smoothing * x + (1 - smoothing) * y
+
+  def forward(self,
+              preds,
+              target,
+              smoothing=0.1):
+    assert 0 <= smoothing < 1
+
+    if self.weight is not None:
+      self.weight = self.weight.to(preds.device)
+
+    n = preds.size(-1)
+    log_preds = F.log_softmax(preds, dim=-1)
+    loss = self.reduce_loss(-log_preds.sum(dim=-1))
+
+    nll = F.nll_loss(log_preds, target, reduction=self.reduction, weight=self.weight)
+
+    return self.linear_combination(loss / n, nll, smoothing=smoothing)
+
+
+
+class LabelSmoothing(nn.Module):
+  """NLL loss with label smoothing.
+  """
+
+  def __init__(self):
+    """Constructor for the LabelSmoothing module.
+    :param smoothing: label smoothing factor
+    """
+    super(LabelSmoothing, self).__init__()
+
+    pass
+
+  def forward(self,
+              x,
+              target,
+              smoothing=0.1):
+    """
+
+    :param x: (b, c)
+    :param target:  (b, )
+    :param smoothing:
+    :return:
+    """
+    confidence = 1.0 - smoothing
+
+    logprobs = torch.nn.functional.log_softmax(x, dim=-1)
+    nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
+    nll_loss = nll_loss.squeeze(1)
+    smooth_loss = -logprobs.mean(dim=-1)
+    loss = confidence * nll_loss + smoothing * smooth_loss
+    return loss.mean()
+
+
+if __name__=="__main__":
+    # Wangleiofficial
+
+    predict = torch.tensor([[0, 0.2, 0.7, 0.1, 0],
+                            [0, 0.9, 0.2, 0.2, 1],
+                            [1, 0.2, 0.7, 0.9, 1]])
+    label = torch.tensor([2, 1, 0])
+
+    crit1 = LabelSmoothingLoss(reduction="mean")
+    v1 = crit1(predict, label, smoothing=0.3, )
+
+    # NVIDIA
+    crit2 = LabelSmoothing()
+    v2 = crit2(predict, label, smoothing=0.3)
+
+    assert v1 == v2
+    pass
+
```

### Comparing `tl2-0.1.0/tl2/tools/test_tools.py` & `tl2-0.1.1/tl2/tools/test_tools.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,67 +1,67 @@
-import os
-import sys
-import unittest
-import argparse
-
-
-class Testing_dataset_tool(unittest.TestCase):
-
-  def test_FFHQ_1024_to_256(self, debug=True):
-    """
-    Usage:
-        ssh -o ServerAliveInterval=30 -o ServerAliveCountMax=2 root@localhost -p 2232
-
-        export CUDA_VISIBLE_DEVICES=0
-        export TIME_STR=0
-        export PYTHONPATH=.:tl2_lib
-        python -c "from tl2_lib.tl2.tools.test_tools import Testing_dataset_tool;\
-          Testing_dataset_tool().test_FFHQ_1024_to_256(debug=False)"
-
-    :return:
-    """
-    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
-      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
-    if 'TIME_STR' not in os.environ:
-      os.environ['TIME_STR'] = '0'
-    from tl2 import tl2_utils
-    from tl2.launch.launch_utils import \
-      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
-
-    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
-    argv_str = f"""
-                --tl_config_file none
-                --tl_command none
-                --tl_outdir {outdir}
-                """
-    args = setup_outdir_and_yaml(argv_str)
-
-    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
-    print(f'tl_opts:\n {tl_opts}')
-
-    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
-    cmd_str = f"""
-        python 
-        tl2_lib/tl2/tools/dataset_tool.py
-        --source=datasets/ffhq/images1024x1024
-        --dest=datasets/ffhq/downsample_ffhq_256x256.zip
-        --width=256 --height=256
-        {get_append_cmd_str(args)}
-        """
-    if debug:
-      cmd_str = f"""
-              python 
-              tl2_lib/tl2/tools/dataset_tool.py
-              --source=datasets/ffhq/images1024x1024
-              --dest=datasets/ffhq/downsample_ffhq_256x256_debug.zip
-              --width=256 --height=256
-              --tl_debug
-              {get_append_cmd_str(args)}
-              """
-    # else:
-    #   cmd_str += f"""
-    #               {get_append_cmd_str(args)}
-    #               --tl_opts {tl_opts}
-    #               """
-    start_cmd_run(cmd_str)
-    pass
-
+import os
+import sys
+import unittest
+import argparse
+
+
+class Testing_dataset_tool(unittest.TestCase):
+
+  def test_FFHQ_1024_to_256(self, debug=True):
+    """
+    Usage:
+        ssh -o ServerAliveInterval=30 -o ServerAliveCountMax=2 root@localhost -p 2232
+
+        export CUDA_VISIBLE_DEVICES=0
+        export TIME_STR=0
+        export PYTHONPATH=.:tl2_lib
+        python -c "from tl2_lib.tl2.tools.test_tools import Testing_dataset_tool;\
+          Testing_dataset_tool().test_FFHQ_1024_to_256(debug=False)"
+
+    :return:
+    """
+    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
+      os.environ['CUDA_VISIBLE_DEVICES'] = '0'
+    if 'TIME_STR' not in os.environ:
+      os.environ['TIME_STR'] = '0'
+    from tl2 import tl2_utils
+    from tl2.launch.launch_utils import \
+      (get_command_and_outdir, setup_outdir_and_yaml, get_append_cmd_str, start_cmd_run)
+
+    command, outdir = get_command_and_outdir(self, func_name=sys._getframe().f_code.co_name, file=__file__)
+    argv_str = f"""
+                --tl_config_file none
+                --tl_command none
+                --tl_outdir {outdir}
+                """
+    args = setup_outdir_and_yaml(argv_str)
+
+    tl_opts = ' '.join(sys.argv[sys.argv.index('--tl_opts') + 1:]) if '--tl_opts' in sys.argv else ''
+    print(f'tl_opts:\n {tl_opts}')
+
+    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
+    cmd_str = f"""
+        python 
+        tl2_lib/tl2/tools/dataset_tool.py
+        --source=datasets/ffhq/images1024x1024
+        --dest=datasets/ffhq/downsample_ffhq_256x256.zip
+        --width=256 --height=256
+        {get_append_cmd_str(args)}
+        """
+    if debug:
+      cmd_str = f"""
+              python 
+              tl2_lib/tl2/tools/dataset_tool.py
+              --source=datasets/ffhq/images1024x1024
+              --dest=datasets/ffhq/downsample_ffhq_256x256_debug.zip
+              --width=256 --height=256
+              --tl_debug
+              {get_append_cmd_str(args)}
+              """
+    # else:
+    #   cmd_str += f"""
+    #               {get_append_cmd_str(args)}
+    #               --tl_opts {tl_opts}
+    #               """
+    start_cmd_run(cmd_str)
+    pass
+
```

### Comparing `tl2-0.1.0/tl2.egg-info/PKG-INFO` & `tl2-0.1.1/tl2.egg-info/PKG-INFO`

 * *Files 5% similar despite different names*

```diff
@@ -1,16 +1,14 @@
 Metadata-Version: 2.1
 Name: tl2
-Version: 0.1.0
+Version: 0.1.1
 Summary: A personal package for research
 Home-page: https://github.com/PeterouZh/tl2
 Author: Peterou
 Author-email: pengzhoucv@gmail.com
-License: UNKNOWN
-Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
 Description-Content-Type: text/markdown
 License-File: LICENSE
 
 ## [tl2](https://github.com/PeterouZh/tl2)
@@ -48,9 +46,7 @@
 
 ### fvcore
 
 
 
 
 
-
-
```

### Comparing `tl2-0.1.0/tl2.egg-info/SOURCES.txt` & `tl2-0.1.1/tl2.egg-info/SOURCES.txt`

 * *Files 4% similar despite different names*

```diff
@@ -29,28 +29,26 @@
 tl2/launch/__pycache__/launch_utils.cpython-38.pyc
 tl2/launch/configs/Launch_v1.yaml
 tl2/launch/scripts/run_ddp.py
 tl2/launch/tests/test_launch.py
 tl2/modelarts/__init__.py
 tl2/modelarts/modelarts_utils.py
 tl2/modelarts/moxing_utils.py
-tl2/modelarts/__pycache__/__init__.cpython-38.pyc
-tl2/modelarts/__pycache__/modelarts_utils.cpython-38.pyc
-tl2/modelarts/__pycache__/moxing_utils.cpython-38.pyc
 tl2/modelarts/configs/run.yaml
 tl2/modelarts/scripts/copy_file_list.py
 tl2/modelarts/scripts/copy_tool.py
 tl2/modelarts/scripts/make_log_dirs.py
 tl2/modelarts/scripts/plot_results_obs.py
 tl2/modelarts/scripts/run.py
 tl2/modelarts/scripts/run.sh
 tl2/modelarts/scripts/run_v2_modelarts.sh
 tl2/modelarts/scripts/s3_downloader.py
 tl2/modelarts/scripts/s3_uploader.py
 tl2/modelarts/scripts/setup_env.sh
+tl2/modelarts/scripts/setup_env_debug.sh
 tl2/modelarts/scripts/start_modelarts_v2.py
 tl2/modelarts/scripts/test_bash.py
 tl2/modelarts/scripts/test_bash.sh
 tl2/modelarts/scripts/test_resnet.py
 tl2/modelarts/sources/pip.conf.modelarts
 tl2/modelarts/sources/sources.list.modelarts
 tl2/modelarts/tests/test_run.py
@@ -58,15 +56,14 @@
 tl2/proj/GAN/eval_FID_given_imgdir.py
 tl2/proj/GAN/frequency_spectrum.py
 tl2/proj/GAN/plot_freq_spectrum.py
 tl2/proj/__pycache__/__init__.cpython-38.pyc
 tl2/proj/argparser/argparser_utils.py
 tl2/proj/argparser/__pycache__/argparser_utils.cpython-38.pyc
 tl2/proj/cv2/cv2_utils.py
-tl2/proj/cv2/__pycache__/cv2_utils.cpython-38.pyc
 tl2/proj/dlib/__init__.py
 tl2/proj/dlib/dlib_utils.py
 tl2/proj/dlib/configs/dlib_web.yaml
 tl2/proj/dlib/datasets/dlib_landmarks_68.png
 tl2/proj/dlib/datasets/hand_convex_hull.jpg
 tl2/proj/dlib/datasets/raw_face_list.txt
 tl2/proj/dlib/datasets/raw_face/21242213255_abde1622df_o.jpg
@@ -92,21 +89,24 @@
 tl2/proj/fvcore/checkpoint.py
 tl2/proj/fvcore/config.py
 tl2/proj/fvcore/dummy_model.py
 tl2/proj/fvcore/logger.py
 tl2/proj/fvcore/registry.py
 tl2/proj/fvcore/timer.py
 tl2/proj/fvcore/__pycache__/__init__.cpython-38.pyc
-tl2/proj/fvcore/__pycache__/checkpoint.cpython-38.pyc
 tl2/proj/fvcore/__pycache__/config.cpython-38.pyc
-tl2/proj/fvcore/__pycache__/logger.cpython-38.pyc
 tl2/proj/fvcore/__pycache__/registry.cpython-38.pyc
 tl2/proj/fvcore/configs/Registry.yaml
 tl2/proj/fvcore/configs/TLCfgNode.yaml
 tl2/proj/fvcore/tests/test_checkpoint.py
+tl2/proj/gradio/gradio_utils.py
+tl2/proj/gradio/configs/Quickstart.yaml
+tl2/proj/gradio/examples/blocks.py
+tl2/proj/gradio/tests/test_gradio.py
+tl2/proj/gradio/tests/__pycache__/test_gradio.cpython-38.pyc
 tl2/proj/logger/__init__.py
 tl2/proj/logger/logger_utils.py
 tl2/proj/logger/logging_utils_v2.py
 tl2/proj/logger/plot_utils.py
 tl2/proj/logger/textlogger.py
 tl2/proj/logger/__pycache__/__init__.cpython-38.pyc
 tl2/proj/logger/__pycache__/logger_utils.cpython-38.pyc
@@ -117,37 +117,37 @@
 tl2/proj/matplot/plt_utils.py
 tl2/proj/matplot/configs/Plot.yaml
 tl2/proj/matplot/data/OmniGAN_ImageNet128_results.pkl
 tl2/proj/matplot/data/pigan_nerfgan_r64r128_gradpoints.pkl
 tl2/proj/matplot/scripts/__init__.py
 tl2/proj/matplot/scripts/parse_results_dict_pkl.py
 tl2/proj/numpy/np_utils.py
-tl2/proj/numpy/__pycache__/np_utils.cpython-38.pyc
 tl2/proj/pil/pil_utils.py
-tl2/proj/pil/__pycache__/pil_utils.cpython-38.pyc
+tl2/proj/pl/configs/lightning_2_steps.yaml
+tl2/proj/pl/configs/step_by_step.yaml
+tl2/proj/pl/examples/lightning_2_steps/module.py
+tl2/proj/pl/examples/step_by_step/module.py
+tl2/proj/pl/tests/test_pytorch_lightning.py
 tl2/proj/pytorch/__init__.py
 tl2/proj/pytorch/downsampler.py
 tl2/proj/pytorch/init_func.py
 tl2/proj/pytorch/optim.py
 tl2/proj/pytorch/pytorch_hook.py
 tl2/proj/pytorch/torch_utils.py
 tl2/proj/pytorch/__pycache__/__init__.cpython-38.pyc
-tl2/proj/pytorch/__pycache__/pytorch_hook.cpython-38.pyc
-tl2/proj/pytorch/__pycache__/torch_utils.cpython-38.pyc
 tl2/proj/pytorch/datasets/__init__.py
 tl2/proj/pytorch/datasets/dataset_celeba_align.py
 tl2/proj/pytorch/datasets/dataset_danbooru2019_portraits.py
 tl2/proj/pytorch/datasets/dataset_image_list.py
 tl2/proj/pytorch/datasets/tests/test_datasets.py
 tl2/proj/pytorch/ddp/__init__.py
 tl2/proj/pytorch/ddp/d2_comm.py
 tl2/proj/pytorch/ddp/ddp_utils.py
 tl2/proj/pytorch/ddp/__pycache__/__init__.cpython-38.pyc
 tl2/proj/pytorch/ddp/__pycache__/d2_comm.cpython-38.pyc
-tl2/proj/pytorch/ddp/__pycache__/ddp_utils.cpython-38.pyc
 tl2/proj/pytorch/examples/cips3d/pigan_gen_celeba.py
 tl2/proj/pytorch/examples/cips3d/pigan_gen_celeba.yaml
 tl2/proj/pytorch/examples/cips3d/test_pigan_gen_celeba.py
 tl2/proj/pytorch/examples/cips3d/train_v6.py
 tl2/proj/pytorch/examples/dataset_stylegan3/dataset.py
 tl2/proj/pytorch/examples/dataset_stylegan3/dataset.yaml
 tl2/proj/pytorch/examples/dataset_stylegan3/test_dataset.py
@@ -196,21 +196,21 @@
 tl2/proj/pytorch/examples/networks/test_siren_net_pigan.py
 tl2/proj/pytorch/examples/networks/test_siren_net_v1.py
 tl2/proj/pytorch/ops/__init__.py
 tl2/proj/pytorch/ops/grid_sample.py
 tl2/proj/pytorch/ops/grid_sample_gradfix.py
 tl2/proj/pytorch/scripts/resize_antialias.py
 tl2/proj/skimage/skimage_utils.py
-tl2/proj/skimage/__pycache__/skimage_utils.cpython-38.pyc
 tl2/proj/streamlit/SessionState.py
 tl2/proj/streamlit/st_utils.py
-tl2/proj/streamlit/__pycache__/SessionState.cpython-38.pyc
-tl2/proj/streamlit/__pycache__/st_utils.cpython-38.pyc
 tl2/proj/streamlit/configs/Streamlit.yaml
+tl2/proj/streamlit/configs/Streamlit_v2.yaml
+tl2/proj/streamlit/scripts/create_an_app.py
 tl2/proj/streamlit/scripts/run_web.py
+tl2/proj/streamlit/scripts/st_two_column.py
 tl2/proj/streamlit/tests/test_streamlit.py
 tl2/proj/stylegan2_ada/__init__.py
 tl2/proj/stylegan2_ada/ada_utils.py
 tl2/proj/stylegan2_ada/persistence.py
 tl2/proj/stylegan2_ada/test/test_ada.py
 tl2/proj/tools3d/camera_pose_visualizer.py
 tl2/proj/tools3d/compute_normal_from_depth.py
```

