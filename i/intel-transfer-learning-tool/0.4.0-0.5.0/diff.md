# Comparing `tmp/intel_transfer_learning_tool-0.4.0-py3-none-any.whl.zip` & `tmp/intel_transfer_learning_tool-0.5.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,138 +1,135 @@
-Zip file size: 273255 bytes, number of entries: 136
--rw-r--r--  2.0 unx      723 b- defN 23-May-03 01:10 downloader/__init__.py
--rw-r--r--  2.0 unx     5236 b- defN 23-May-03 01:10 downloader/datasets.py
--rw-r--r--  2.0 unx     4478 b- defN 23-May-03 01:10 downloader/models.py
--rw-r--r--  2.0 unx     2982 b- defN 23-May-03 01:10 downloader/types.py
--rw-r--r--  2.0 unx     2600 b- defN 23-May-03 01:10 downloader/utils.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 downloader/tests/__init__.py
--rw-r--r--  2.0 unx     4444 b- defN 23-May-03 01:10 downloader/tests/test_dataset_download.py
--rw-r--r--  2.0 unx     2910 b- defN 23-May-03 01:10 downloader/tests/test_model_download.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tests/__init__.py
--rw-r--r--  2.0 unx     2891 b- defN 23-May-03 01:10 tests/conftest.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tests/pytorch_tests/__init__.py
--rw-r--r--  2.0 unx     9919 b- defN 23-May-03 01:10 tests/pytorch_tests/test_image_anomaly_detection.py
--rw-r--r--  2.0 unx    15505 b- defN 23-May-03 01:10 tests/pytorch_tests/test_image_classification.py
--rw-r--r--  2.0 unx    11830 b- defN 23-May-03 01:10 tests/pytorch_tests/test_text_classification.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tests/pytorch_tests/unit/__init__.py
--rw-r--r--  2.0 unx    29095 b- defN 23-May-03 01:10 tests/pytorch_tests/unit/test_datasets.py
--rw-r--r--  2.0 unx    21420 b- defN 23-May-03 01:10 tests/pytorch_tests/unit/test_inc.py
--rw-r--r--  2.0 unx    14730 b- defN 23-May-03 01:10 tests/pytorch_tests/unit/test_models.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tests/tensorflow_tests/__init__.py
--rw-r--r--  2.0 unx    18416 b- defN 23-May-03 01:10 tests/tensorflow_tests/test_image_classification.py
--rw-r--r--  2.0 unx    10952 b- defN 23-May-03 01:10 tests/tensorflow_tests/test_text_classification.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tests/tensorflow_tests/unit/__init__.py
--rw-r--r--  2.0 unx    25833 b- defN 23-May-03 01:10 tests/tensorflow_tests/unit/test_datasets.py
--rw-r--r--  2.0 unx    22206 b- defN 23-May-03 01:10 tests/tensorflow_tests/unit/test_inc.py
--rw-r--r--  2.0 unx    20697 b- defN 23-May-03 01:10 tests/tensorflow_tests/unit/test_models.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tests/test_utils/__init__.py
--rw-r--r--  2.0 unx     2602 b- defN 23-May-03 01:10 tests/test_utils/platform_config.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tests/tools/__init__.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tests/tools/cli/__init__.py
--rw-r--r--  2.0 unx    12098 b- defN 23-May-03 01:10 tests/tools/cli/test_benchmark_cli.py
--rw-r--r--  2.0 unx    12028 b- defN 23-May-03 01:10 tests/tools/cli/test_eval_cli.py
--rw-r--r--  2.0 unx     7473 b- defN 23-May-03 01:10 tests/tools/cli/test_optimize_cli.py
--rw-r--r--  2.0 unx    14758 b- defN 23-May-03 01:10 tests/tools/cli/test_quantize_cli.py
--rw-r--r--  2.0 unx    19166 b- defN 23-May-03 01:10 tests/tools/cli/test_train_cli.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tests/utils/__init__.py
--rw-r--r--  2.0 unx     2388 b- defN 23-May-03 01:10 tests/utils/test_file_utils.py
--rw-r--r--  2.0 unx    13817 b- defN 23-May-03 01:10 tests/utils/test_platform_util.py
--rw-r--r--  2.0 unx      727 b- defN 23-May-03 01:10 tlt/__init__.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tlt/datasets/__init__.py
--rw-r--r--  2.0 unx     2292 b- defN 23-May-03 01:10 tlt/datasets/dataset.py
--rw-r--r--  2.0 unx     9139 b- defN 23-May-03 01:10 tlt/datasets/dataset_factory.py
--rw-r--r--  2.0 unx    13179 b- defN 23-May-03 01:10 tlt/datasets/hf_dataset.py
--rw-r--r--  2.0 unx    10214 b- defN 23-May-03 01:10 tlt/datasets/pytorch_dataset.py
--rw-r--r--  2.0 unx     4965 b- defN 23-May-03 01:10 tlt/datasets/tf_dataset.py
--rw-r--r--  2.0 unx     1969 b- defN 23-May-03 01:10 tlt/datasets/configs/hf_text_classification_datasets.json
--rw-r--r--  2.0 unx      533 b- defN 23-May-03 01:10 tlt/datasets/configs/tf_text_classification_datasets.json
--rw-r--r--  2.0 unx      764 b- defN 23-May-03 01:10 tlt/datasets/image_anomaly_detection/__init__.py
--rw-r--r--  2.0 unx    25566 b- defN 23-May-03 01:10 tlt/datasets/image_anomaly_detection/pytorch_custom_image_anomaly_detection_dataset.py
--rw-r--r--  2.0 unx      764 b- defN 23-May-03 01:10 tlt/datasets/image_classification/__init__.py
--rw-r--r--  2.0 unx     1238 b- defN 23-May-03 01:10 tlt/datasets/image_classification/image_classification_dataset.py
--rw-r--r--  2.0 unx     6340 b- defN 23-May-03 01:10 tlt/datasets/image_classification/pytorch_custom_image_classification_dataset.py
--rw-r--r--  2.0 unx    10183 b- defN 23-May-03 01:10 tlt/datasets/image_classification/tf_custom_image_classification_dataset.py
--rw-r--r--  2.0 unx     6681 b- defN 23-May-03 01:10 tlt/datasets/image_classification/tfds_image_classification_dataset.py
--rw-r--r--  2.0 unx     5755 b- defN 23-May-03 01:10 tlt/datasets/image_classification/torchvision_image_classification_dataset.py
--rw-r--r--  2.0 unx     8859 b- defN 23-May-03 01:10 tlt/datasets/text_classification/hf_custom_text_classification_dataset.py
--rw-r--r--  2.0 unx     6189 b- defN 23-May-03 01:10 tlt/datasets/text_classification/hf_text_classification_dataset.py
--rw-r--r--  2.0 unx     2161 b- defN 23-May-03 01:10 tlt/datasets/text_classification/text_classification_dataset.py
--rw-r--r--  2.0 unx     8623 b- defN 23-May-03 01:10 tlt/datasets/text_classification/tf_custom_text_classification_dataset.py
--rw-r--r--  2.0 unx     4997 b- defN 23-May-03 01:10 tlt/datasets/text_classification/tfds_text_classification_dataset.py
--rw-r--r--  2.0 unx     2049 b- defN 23-May-03 01:10 tlt/distributed/README.md
--rw-r--r--  2.0 unx      734 b- defN 23-May-03 01:10 tlt/distributed/__init__.py
--rw-r--r--  2.0 unx     1770 b- defN 23-May-03 01:10 tlt/distributed/pytorch/README.md
--rw-r--r--  2.0 unx      671 b- defN 23-May-03 01:10 tlt/distributed/pytorch/__init__.py
--rw-r--r--  2.0 unx      416 b- defN 23-May-03 01:10 tlt/distributed/pytorch/run_install.sh
--rw-r--r--  2.0 unx     2922 b- defN 23-May-03 01:10 tlt/distributed/pytorch/run_train_pyt.py
--rw-r--r--  2.0 unx      892 b- defN 23-May-03 01:10 tlt/distributed/pytorch/deploy/install_torch_ccl.sh
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tlt/distributed/pytorch/utils/__init__.py
--rw-r--r--  2.0 unx     9240 b- defN 23-May-03 01:10 tlt/distributed/pytorch/utils/pyt_distributed_utils.py
--rw-r--r--  2.0 unx     2585 b- defN 23-May-03 01:10 tlt/distributed/tensorflow/README.md
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tlt/distributed/tensorflow/__init__.py
--rw-r--r--  2.0 unx       52 b- defN 23-May-03 01:10 tlt/distributed/tensorflow/requirements.txt
--rw-r--r--  2.0 unx     2726 b- defN 23-May-03 01:10 tlt/distributed/tensorflow/run_train_tf.py
--rw-r--r--  2.0 unx      963 b- defN 23-May-03 01:10 tlt/distributed/tensorflow/tf_hvd_setup.sh
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tlt/distributed/tensorflow/utils/__init__.py
--rw-r--r--  2.0 unx     6264 b- defN 23-May-03 01:10 tlt/distributed/tensorflow/utils/tf_distributed_util.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tlt/models/__init__.py
--rw-r--r--  2.0 unx     3197 b- defN 23-May-03 01:10 tlt/models/hf_model.py
--rw-r--r--  2.0 unx     7155 b- defN 23-May-03 01:10 tlt/models/model.py
--rw-r--r--  2.0 unx    15254 b- defN 23-May-03 01:10 tlt/models/model_factory.py
--rw-r--r--  2.0 unx     8413 b- defN 23-May-03 01:10 tlt/models/pytorch_model.py
--rw-r--r--  2.0 unx    12703 b- defN 23-May-03 01:10 tlt/models/tf_model.py
--rw-r--r--  2.0 unx      929 b- defN 23-May-03 01:10 tlt/models/configs/hf_text_classification_models.json
--rw-r--r--  2.0 unx     4894 b- defN 23-May-03 01:10 tlt/models/configs/pytorch_hub_image_classification_models.json
--rw-r--r--  2.0 unx     4762 b- defN 23-May-03 01:10 tlt/models/configs/tf_keras_image_classification_models.json
--rw-r--r--  2.0 unx     6343 b- defN 23-May-03 01:10 tlt/models/configs/tfhub_image_classification_models.json
--rw-r--r--  2.0 unx    13993 b- defN 23-May-03 01:10 tlt/models/configs/tfhub_text_classification_models.json
--rw-r--r--  2.0 unx      865 b- defN 23-May-03 01:10 tlt/models/configs/torchvision_image_anomaly_detection_models.json
--rw-r--r--  2.0 unx    10856 b- defN 23-May-03 01:10 tlt/models/configs/torchvision_image_classification_models.json
--rw-r--r--  2.0 unx     3044 b- defN 23-May-03 01:10 tlt/models/configs/inc/image_classification_template.yaml
--rw-r--r--  2.0 unx      216 b- defN 23-May-03 01:10 tlt/models/configs/inc/text_classification_template.yaml
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tlt/models/image_anomaly_detection/__init__.py
--rw-r--r--  2.0 unx    28493 b- defN 23-May-03 01:10 tlt/models/image_anomaly_detection/pytorch_image_anomaly_detection_model.py
--rw-r--r--  2.0 unx     1983 b- defN 23-May-03 01:10 tlt/models/image_anomaly_detection/torchvision_image_anomaly_detection_model.py
--rw-r--r--  2.0 unx     5769 b- defN 23-May-03 01:10 tlt/models/image_anomaly_detection/utils.py
--rw-r--r--  2.0 unx     6977 b- defN 23-May-03 01:10 tlt/models/image_anomaly_detection/cutpaste/cutpaste.py
--rw-r--r--  2.0 unx     2390 b- defN 23-May-03 01:10 tlt/models/image_anomaly_detection/cutpaste/model.py
--rw-r--r--  2.0 unx     2535 b- defN 23-May-03 01:10 tlt/models/image_anomaly_detection/simsiam/builder.py
--rw-r--r--  2.0 unx     1380 b- defN 23-May-03 01:10 tlt/models/image_anomaly_detection/simsiam/loader.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tlt/models/image_classification/__init__.py
--rw-r--r--  2.0 unx     3421 b- defN 23-May-03 01:10 tlt/models/image_classification/image_classification_model.py
--rw-r--r--  2.0 unx     3152 b- defN 23-May-03 01:10 tlt/models/image_classification/keras_image_classification_model.py
--rw-r--r--  2.0 unx     2325 b- defN 23-May-03 01:10 tlt/models/image_classification/pytorch_hub_image_classification_model.py
--rw-r--r--  2.0 unx    36544 b- defN 23-May-03 01:10 tlt/models/image_classification/pytorch_image_classification_model.py
--rw-r--r--  2.0 unx    30727 b- defN 23-May-03 01:10 tlt/models/image_classification/tf_image_classification_model.py
--rw-r--r--  2.0 unx    12103 b- defN 23-May-03 01:10 tlt/models/image_classification/tfhub_image_classification_model.py
--rw-r--r--  2.0 unx    14508 b- defN 23-May-03 01:10 tlt/models/image_classification/torchvision_image_classification_model.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tlt/models/text_classification/__init__.py
--rw-r--r--  2.0 unx    43619 b- defN 23-May-03 01:10 tlt/models/text_classification/hf_text_classification_model.py
--rw-r--r--  2.0 unx     3295 b- defN 23-May-03 01:10 tlt/models/text_classification/text_classification_model.py
--rw-r--r--  2.0 unx    28112 b- defN 23-May-03 01:10 tlt/models/text_classification/tf_text_classification_model.py
--rw-r--r--  2.0 unx    12405 b- defN 23-May-03 01:10 tlt/models/text_classification/tfhub_text_classification_model.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tlt/tools/cli/__init__.py
--rw-r--r--  2.0 unx     1300 b- defN 23-May-03 01:10 tlt/tools/cli/main.py
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tlt/tools/cli/commands/__init__.py
--rw-r--r--  2.0 unx     5830 b- defN 23-May-03 01:10 tlt/tools/cli/commands/benchmark.py
--rw-r--r--  2.0 unx     6682 b- defN 23-May-03 01:10 tlt/tools/cli/commands/eval.py
--rw-r--r--  2.0 unx     2399 b- defN 23-May-03 01:10 tlt/tools/cli/commands/list.py
--rw-r--r--  2.0 unx     3691 b- defN 23-May-03 01:10 tlt/tools/cli/commands/optimize.py
--rw-r--r--  2.0 unx     7954 b- defN 23-May-03 01:10 tlt/tools/cli/commands/quantize.py
--rw-r--r--  2.0 unx    13169 b- defN 23-May-03 01:10 tlt/tools/cli/commands/train.py
--rw-r--r--  2.0 unx     8362 b- defN 23-May-03 01:10 tlt/tools/docker/README.md
--rwxr-xr-x  2.0 unx     3624 b- defN 23-May-03 01:10 tlt/tools/docker/build.sh
--rw-r--r--  2.0 unx     1729 b- defN 23-May-03 01:10 tlt/tools/docker/dockerfiles/pyt-tests.Dockerfile
--rw-r--r--  2.0 unx     1832 b- defN 23-May-03 01:10 tlt/tools/docker/dockerfiles/pyt.Dockerfile
--rw-r--r--  2.0 unx     1790 b- defN 23-May-03 01:10 tlt/tools/docker/dockerfiles/tf-tests.Dockerfile
--rw-r--r--  2.0 unx     1909 b- defN 23-May-03 01:10 tlt/tools/docker/dockerfiles/tf.Dockerfile
--rw-r--r--  2.0 unx      674 b- defN 23-May-03 01:10 tlt/utils/__init__.py
--rw-r--r--  2.0 unx     5298 b- defN 23-May-03 01:10 tlt/utils/file_utils.py
--rw-r--r--  2.0 unx    18975 b- defN 23-May-03 01:10 tlt/utils/platform_util.py
--rw-r--r--  2.0 unx     2577 b- defN 23-May-03 01:10 tlt/utils/types.py
--rw-r--r--  2.0 unx    11348 b- defN 23-May-03 01:38 intel_transfer_learning_tool-0.4.0.dist-info/LICENSE
--rw-r--r--  2.0 unx    16866 b- defN 23-May-03 01:38 intel_transfer_learning_tool-0.4.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-03 01:38 intel_transfer_learning_tool-0.4.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       53 b- defN 23-May-03 01:38 intel_transfer_learning_tool-0.4.0.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       21 b- defN 23-May-03 01:38 intel_transfer_learning_tool-0.4.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    13611 b- defN 23-May-03 01:38 intel_transfer_learning_tool-0.4.0.dist-info/RECORD
-136 files, 949247 bytes uncompressed, 251057 bytes compressed:  73.6%
+Zip file size: 274452 bytes, number of entries: 133
+-rw-r--r--  2.0 unx      723 b- defN 23-Jun-24 00:56 downloader/__init__.py
+-rw-r--r--  2.0 unx     5618 b- defN 23-Jun-24 00:56 downloader/datasets.py
+-rw-r--r--  2.0 unx     4911 b- defN 23-Jun-24 00:56 downloader/models.py
+-rw-r--r--  2.0 unx     3213 b- defN 23-Jun-24 00:56 downloader/types.py
+-rw-r--r--  2.0 unx     2615 b- defN 23-Jun-24 00:56 downloader/utils.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 downloader/tests/__init__.py
+-rw-r--r--  2.0 unx     4725 b- defN 23-Jun-24 00:56 downloader/tests/test_dataset_download.py
+-rw-r--r--  2.0 unx     3631 b- defN 23-Jun-24 00:56 downloader/tests/test_model_download.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tests/__init__.py
+-rw-r--r--  2.0 unx     2891 b- defN 23-Jun-24 00:56 tests/conftest.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tests/pytorch_tests/__init__.py
+-rw-r--r--  2.0 unx    10026 b- defN 23-Jun-24 00:56 tests/pytorch_tests/test_image_anomaly_detection.py
+-rw-r--r--  2.0 unx    15476 b- defN 23-Jun-24 00:56 tests/pytorch_tests/test_image_classification.py
+-rw-r--r--  2.0 unx    11921 b- defN 23-Jun-24 00:56 tests/pytorch_tests/test_text_classification.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tests/pytorch_tests/unit/__init__.py
+-rw-r--r--  2.0 unx    28829 b- defN 23-Jun-24 00:56 tests/pytorch_tests/unit/test_datasets.py
+-rw-r--r--  2.0 unx     8471 b- defN 23-Jun-24 00:56 tests/pytorch_tests/unit/test_inc.py
+-rw-r--r--  2.0 unx    18751 b- defN 23-Jun-24 00:56 tests/pytorch_tests/unit/test_models.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tests/tensorflow_tests/__init__.py
+-rw-r--r--  2.0 unx    23067 b- defN 23-Jun-24 00:56 tests/tensorflow_tests/test_image_classification.py
+-rw-r--r--  2.0 unx    15761 b- defN 23-Jun-24 00:56 tests/tensorflow_tests/test_text_classification.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tests/tensorflow_tests/unit/__init__.py
+-rw-r--r--  2.0 unx    25739 b- defN 23-Jun-24 00:56 tests/tensorflow_tests/unit/test_datasets.py
+-rw-r--r--  2.0 unx     8354 b- defN 23-Jun-24 00:56 tests/tensorflow_tests/unit/test_inc.py
+-rw-r--r--  2.0 unx    23080 b- defN 23-Jun-24 00:56 tests/tensorflow_tests/unit/test_models.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tests/test_utils/__init__.py
+-rw-r--r--  2.0 unx     2602 b- defN 23-Jun-24 00:56 tests/test_utils/platform_config.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tests/tools/__init__.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tests/tools/cli/__init__.py
+-rw-r--r--  2.0 unx    11342 b- defN 23-Jun-24 00:56 tests/tools/cli/test_benchmark_cli.py
+-rw-r--r--  2.0 unx    12021 b- defN 23-Jun-24 00:56 tests/tools/cli/test_eval_cli.py
+-rw-r--r--  2.0 unx     7473 b- defN 23-Jun-24 00:56 tests/tools/cli/test_optimize_cli.py
+-rw-r--r--  2.0 unx    14830 b- defN 23-Jun-24 00:56 tests/tools/cli/test_quantize_cli.py
+-rw-r--r--  2.0 unx    20046 b- defN 23-Jun-24 00:56 tests/tools/cli/test_train_cli.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tests/utils/__init__.py
+-rw-r--r--  2.0 unx     2531 b- defN 23-Jun-24 00:56 tests/utils/test_file_utils.py
+-rw-r--r--  2.0 unx     3491 b- defN 23-Jun-24 00:56 tests/utils/test_inc_utils.py
+-rw-r--r--  2.0 unx    17490 b- defN 23-Jun-24 00:56 tests/utils/test_platform_util.py
+-rw-r--r--  2.0 unx      727 b- defN 23-Jun-24 00:56 tlt/__init__.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tlt/datasets/__init__.py
+-rw-r--r--  2.0 unx     2292 b- defN 23-Jun-24 00:56 tlt/datasets/dataset.py
+-rw-r--r--  2.0 unx     9141 b- defN 23-Jun-24 00:56 tlt/datasets/dataset_factory.py
+-rw-r--r--  2.0 unx    14370 b- defN 23-Jun-24 00:56 tlt/datasets/hf_dataset.py
+-rw-r--r--  2.0 unx    10599 b- defN 23-Jun-24 00:56 tlt/datasets/pytorch_dataset.py
+-rw-r--r--  2.0 unx     6356 b- defN 23-Jun-24 00:56 tlt/datasets/tf_dataset.py
+-rw-r--r--  2.0 unx     1969 b- defN 23-Jun-24 00:56 tlt/datasets/configs/hf_text_classification_datasets.json
+-rw-r--r--  2.0 unx      533 b- defN 23-Jun-24 00:56 tlt/datasets/configs/tf_text_classification_datasets.json
+-rw-r--r--  2.0 unx      764 b- defN 23-Jun-24 00:56 tlt/datasets/image_anomaly_detection/__init__.py
+-rw-r--r--  2.0 unx    25566 b- defN 23-Jun-24 00:56 tlt/datasets/image_anomaly_detection/pytorch_custom_image_anomaly_detection_dataset.py
+-rw-r--r--  2.0 unx      764 b- defN 23-Jun-24 00:56 tlt/datasets/image_classification/__init__.py
+-rw-r--r--  2.0 unx     1238 b- defN 23-Jun-24 00:56 tlt/datasets/image_classification/image_classification_dataset.py
+-rw-r--r--  2.0 unx     6341 b- defN 23-Jun-24 00:56 tlt/datasets/image_classification/pytorch_custom_image_classification_dataset.py
+-rw-r--r--  2.0 unx    11071 b- defN 23-Jun-24 00:56 tlt/datasets/image_classification/tf_custom_image_classification_dataset.py
+-rw-r--r--  2.0 unx     7274 b- defN 23-Jun-24 00:56 tlt/datasets/image_classification/tfds_image_classification_dataset.py
+-rw-r--r--  2.0 unx     5755 b- defN 23-Jun-24 00:56 tlt/datasets/image_classification/torchvision_image_classification_dataset.py
+-rw-r--r--  2.0 unx     8924 b- defN 23-Jun-24 00:56 tlt/datasets/text_classification/hf_custom_text_classification_dataset.py
+-rw-r--r--  2.0 unx     6189 b- defN 23-Jun-24 00:56 tlt/datasets/text_classification/hf_text_classification_dataset.py
+-rw-r--r--  2.0 unx     2163 b- defN 23-Jun-24 00:56 tlt/datasets/text_classification/text_classification_dataset.py
+-rw-r--r--  2.0 unx     9488 b- defN 23-Jun-24 00:56 tlt/datasets/text_classification/tf_custom_text_classification_dataset.py
+-rw-r--r--  2.0 unx     5861 b- defN 23-Jun-24 00:56 tlt/datasets/text_classification/tfds_text_classification_dataset.py
+-rw-r--r--  2.0 unx     2115 b- defN 23-Jun-24 00:56 tlt/distributed/README.md
+-rw-r--r--  2.0 unx      734 b- defN 23-Jun-24 00:56 tlt/distributed/__init__.py
+-rw-r--r--  2.0 unx     2671 b- defN 23-Jun-24 00:56 tlt/distributed/pytorch/README.md
+-rw-r--r--  2.0 unx      671 b- defN 23-Jun-24 00:56 tlt/distributed/pytorch/__init__.py
+-rw-r--r--  2.0 unx      138 b- defN 23-Jun-24 00:56 tlt/distributed/pytorch/requirements.txt
+-rw-r--r--  2.0 unx      474 b- defN 23-Jun-24 00:56 tlt/distributed/pytorch/run_install.sh
+-rw-r--r--  2.0 unx     3394 b- defN 23-Jun-24 00:56 tlt/distributed/pytorch/run_train_pyt.py
+-rw-r--r--  2.0 unx      892 b- defN 23-Jun-24 00:56 tlt/distributed/pytorch/deploy/install_torch_ccl.sh
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tlt/distributed/pytorch/utils/__init__.py
+-rw-r--r--  2.0 unx     8926 b- defN 23-Jun-24 00:56 tlt/distributed/pytorch/utils/pyt_distributed_utils.py
+-rw-r--r--  2.0 unx     6191 b- defN 23-Jun-24 00:56 tlt/distributed/tensorflow/README.md
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tlt/distributed/tensorflow/__init__.py
+-rw-r--r--  2.0 unx       28 b- defN 23-Jun-24 00:56 tlt/distributed/tensorflow/requirements.txt
+-rw-r--r--  2.0 unx     8550 b- defN 23-Jun-24 00:56 tlt/distributed/tensorflow/run_train_tf.py
+-rw-r--r--  2.0 unx      963 b- defN 23-Jun-24 00:56 tlt/distributed/tensorflow/tf_hvd_setup.sh
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tlt/distributed/tensorflow/utils/__init__.py
+-rw-r--r--  2.0 unx    13588 b- defN 23-Jun-24 00:56 tlt/distributed/tensorflow/utils/tf_distributed_util.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tlt/models/__init__.py
+-rw-r--r--  2.0 unx    10827 b- defN 23-Jun-24 00:56 tlt/models/hf_model.py
+-rw-r--r--  2.0 unx     7200 b- defN 23-Jun-24 00:56 tlt/models/model.py
+-rw-r--r--  2.0 unx    15515 b- defN 23-Jun-24 00:56 tlt/models/model_factory.py
+-rw-r--r--  2.0 unx    15053 b- defN 23-Jun-24 00:56 tlt/models/pytorch_model.py
+-rw-r--r--  2.0 unx    21348 b- defN 23-Jun-24 00:56 tlt/models/tf_model.py
+-rw-r--r--  2.0 unx      929 b- defN 23-Jun-24 00:56 tlt/models/configs/pytorch_hf_text_classification_models.json
+-rw-r--r--  2.0 unx     4894 b- defN 23-Jun-24 00:56 tlt/models/configs/pytorch_hub_image_classification_models.json
+-rw-r--r--  2.0 unx     5464 b- defN 23-Jun-24 00:56 tlt/models/configs/tf_hf_text_classification_models.json
+-rw-r--r--  2.0 unx     4882 b- defN 23-Jun-24 00:56 tlt/models/configs/tf_keras_image_classification_models.json
+-rw-r--r--  2.0 unx     6343 b- defN 23-Jun-24 00:56 tlt/models/configs/tfhub_image_classification_models.json
+-rw-r--r--  2.0 unx      865 b- defN 23-Jun-24 00:56 tlt/models/configs/torchvision_image_anomaly_detection_models.json
+-rw-r--r--  2.0 unx    10856 b- defN 23-Jun-24 00:56 tlt/models/configs/torchvision_image_classification_models.json
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tlt/models/image_anomaly_detection/__init__.py
+-rw-r--r--  2.0 unx    28493 b- defN 23-Jun-24 00:56 tlt/models/image_anomaly_detection/pytorch_image_anomaly_detection_model.py
+-rw-r--r--  2.0 unx     1983 b- defN 23-Jun-24 00:56 tlt/models/image_anomaly_detection/torchvision_image_anomaly_detection_model.py
+-rw-r--r--  2.0 unx     5769 b- defN 23-Jun-24 00:56 tlt/models/image_anomaly_detection/utils.py
+-rw-r--r--  2.0 unx     7172 b- defN 23-Jun-24 00:56 tlt/models/image_anomaly_detection/cutpaste/cutpaste.py
+-rw-r--r--  2.0 unx     2390 b- defN 23-Jun-24 00:56 tlt/models/image_anomaly_detection/cutpaste/model.py
+-rw-r--r--  2.0 unx     2535 b- defN 23-Jun-24 00:56 tlt/models/image_anomaly_detection/simsiam/builder.py
+-rw-r--r--  2.0 unx     1395 b- defN 23-Jun-24 00:56 tlt/models/image_anomaly_detection/simsiam/loader.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tlt/models/image_classification/__init__.py
+-rw-r--r--  2.0 unx     2297 b- defN 23-Jun-24 00:56 tlt/models/image_classification/image_classification_model.py
+-rw-r--r--  2.0 unx     3568 b- defN 23-Jun-24 00:56 tlt/models/image_classification/keras_image_classification_model.py
+-rw-r--r--  2.0 unx     2325 b- defN 23-Jun-24 00:56 tlt/models/image_classification/pytorch_hub_image_classification_model.py
+-rw-r--r--  2.0 unx    23436 b- defN 23-Jun-24 00:56 tlt/models/image_classification/pytorch_image_classification_model.py
+-rw-r--r--  2.0 unx    17651 b- defN 23-Jun-24 00:56 tlt/models/image_classification/tf_image_classification_model.py
+-rw-r--r--  2.0 unx    13778 b- defN 23-Jun-24 00:56 tlt/models/image_classification/tfhub_image_classification_model.py
+-rw-r--r--  2.0 unx    14951 b- defN 23-Jun-24 00:56 tlt/models/image_classification/torchvision_image_classification_model.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tlt/models/text_classification/__init__.py
+-rw-r--r--  2.0 unx    35167 b- defN 23-Jun-24 00:56 tlt/models/text_classification/pytorch_hf_text_classification_model.py
+-rw-r--r--  2.0 unx     1673 b- defN 23-Jun-24 00:56 tlt/models/text_classification/text_classification_model.py
+-rw-r--r--  2.0 unx    13806 b- defN 23-Jun-24 00:56 tlt/models/text_classification/tf_hf_text_classification_model.py
+-rw-r--r--  2.0 unx    17550 b- defN 23-Jun-24 00:56 tlt/models/text_classification/tf_text_classification_model.py
+-rw-r--r--  2.0 unx    12629 b- defN 23-Jun-24 00:56 tlt/models/text_classification/tfhub_text_classification_model.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tlt/tools/cli/__init__.py
+-rw-r--r--  2.0 unx     1300 b- defN 23-Jun-24 00:56 tlt/tools/cli/main.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tlt/tools/cli/commands/__init__.py
+-rw-r--r--  2.0 unx     6597 b- defN 23-Jun-24 00:56 tlt/tools/cli/commands/benchmark.py
+-rw-r--r--  2.0 unx     6682 b- defN 23-Jun-24 00:56 tlt/tools/cli/commands/eval.py
+-rw-r--r--  2.0 unx     2399 b- defN 23-Jun-24 00:56 tlt/tools/cli/commands/list.py
+-rw-r--r--  2.0 unx     3725 b- defN 23-Jun-24 00:56 tlt/tools/cli/commands/optimize.py
+-rw-r--r--  2.0 unx     9307 b- defN 23-Jun-24 00:56 tlt/tools/cli/commands/quantize.py
+-rw-r--r--  2.0 unx    13307 b- defN 23-Jun-24 00:56 tlt/tools/cli/commands/train.py
+-rw-r--r--  2.0 unx      674 b- defN 23-Jun-24 00:56 tlt/utils/__init__.py
+-rw-r--r--  2.0 unx     1838 b- defN 23-Jun-24 00:56 tlt/utils/dataset_utils.py
+-rw-r--r--  2.0 unx     5392 b- defN 23-Jun-24 00:56 tlt/utils/file_utils.py
+-rw-r--r--  2.0 unx     5146 b- defN 23-Jun-24 00:56 tlt/utils/inc_utils.py
+-rw-r--r--  2.0 unx    30611 b- defN 23-Jun-24 00:56 tlt/utils/platform_util.py
+-rw-r--r--  2.0 unx     2577 b- defN 23-Jun-24 00:56 tlt/utils/types.py
+-rw-r--r--  2.0 unx    11348 b- defN 23-Jun-24 01:10 intel_transfer_learning_tool-0.5.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     6245 b- defN 23-Jun-24 01:10 intel_transfer_learning_tool-0.5.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-24 01:10 intel_transfer_learning_tool-0.5.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       53 b- defN 23-Jun-24 01:10 intel_transfer_learning_tool-0.5.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       21 b- defN 23-Jun-24 01:10 intel_transfer_learning_tool-0.5.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    13293 b- defN 23-Jun-24 01:10 intel_transfer_learning_tool-0.5.0.dist-info/RECORD
+133 files, 943179 bytes uncompressed, 252778 bytes compressed:  73.2%
```

## zipnote {}

```diff
@@ -102,14 +102,17 @@
 
 Filename: tests/utils/__init__.py
 Comment: 
 
 Filename: tests/utils/test_file_utils.py
 Comment: 
 
+Filename: tests/utils/test_inc_utils.py
+Comment: 
+
 Filename: tests/utils/test_platform_util.py
 Comment: 
 
 Filename: tlt/__init__.py
 Comment: 
 
 Filename: tlt/datasets/__init__.py
@@ -183,14 +186,17 @@
 
 Filename: tlt/distributed/pytorch/README.md
 Comment: 
 
 Filename: tlt/distributed/pytorch/__init__.py
 Comment: 
 
+Filename: tlt/distributed/pytorch/requirements.txt
+Comment: 
+
 Filename: tlt/distributed/pytorch/run_install.sh
 Comment: 
 
 Filename: tlt/distributed/pytorch/run_train_pyt.py
 Comment: 
 
 Filename: tlt/distributed/pytorch/deploy/install_torch_ccl.sh
@@ -237,41 +243,35 @@
 
 Filename: tlt/models/pytorch_model.py
 Comment: 
 
 Filename: tlt/models/tf_model.py
 Comment: 
 
-Filename: tlt/models/configs/hf_text_classification_models.json
+Filename: tlt/models/configs/pytorch_hf_text_classification_models.json
 Comment: 
 
 Filename: tlt/models/configs/pytorch_hub_image_classification_models.json
 Comment: 
 
-Filename: tlt/models/configs/tf_keras_image_classification_models.json
+Filename: tlt/models/configs/tf_hf_text_classification_models.json
 Comment: 
 
-Filename: tlt/models/configs/tfhub_image_classification_models.json
+Filename: tlt/models/configs/tf_keras_image_classification_models.json
 Comment: 
 
-Filename: tlt/models/configs/tfhub_text_classification_models.json
+Filename: tlt/models/configs/tfhub_image_classification_models.json
 Comment: 
 
 Filename: tlt/models/configs/torchvision_image_anomaly_detection_models.json
 Comment: 
 
 Filename: tlt/models/configs/torchvision_image_classification_models.json
 Comment: 
 
-Filename: tlt/models/configs/inc/image_classification_template.yaml
-Comment: 
-
-Filename: tlt/models/configs/inc/text_classification_template.yaml
-Comment: 
-
 Filename: tlt/models/image_anomaly_detection/__init__.py
 Comment: 
 
 Filename: tlt/models/image_anomaly_detection/pytorch_image_anomaly_detection_model.py
 Comment: 
 
 Filename: tlt/models/image_anomaly_detection/torchvision_image_anomaly_detection_model.py
@@ -315,20 +315,23 @@
 
 Filename: tlt/models/image_classification/torchvision_image_classification_model.py
 Comment: 
 
 Filename: tlt/models/text_classification/__init__.py
 Comment: 
 
-Filename: tlt/models/text_classification/hf_text_classification_model.py
+Filename: tlt/models/text_classification/pytorch_hf_text_classification_model.py
 Comment: 
 
 Filename: tlt/models/text_classification/text_classification_model.py
 Comment: 
 
+Filename: tlt/models/text_classification/tf_hf_text_classification_model.py
+Comment: 
+
 Filename: tlt/models/text_classification/tf_text_classification_model.py
 Comment: 
 
 Filename: tlt/models/text_classification/tfhub_text_classification_model.py
 Comment: 
 
 Filename: tlt/tools/cli/__init__.py
@@ -354,56 +357,44 @@
 
 Filename: tlt/tools/cli/commands/quantize.py
 Comment: 
 
 Filename: tlt/tools/cli/commands/train.py
 Comment: 
 
-Filename: tlt/tools/docker/README.md
-Comment: 
-
-Filename: tlt/tools/docker/build.sh
-Comment: 
-
-Filename: tlt/tools/docker/dockerfiles/pyt-tests.Dockerfile
-Comment: 
-
-Filename: tlt/tools/docker/dockerfiles/pyt.Dockerfile
-Comment: 
-
-Filename: tlt/tools/docker/dockerfiles/tf-tests.Dockerfile
+Filename: tlt/utils/__init__.py
 Comment: 
 
-Filename: tlt/tools/docker/dockerfiles/tf.Dockerfile
+Filename: tlt/utils/dataset_utils.py
 Comment: 
 
-Filename: tlt/utils/__init__.py
+Filename: tlt/utils/file_utils.py
 Comment: 
 
-Filename: tlt/utils/file_utils.py
+Filename: tlt/utils/inc_utils.py
 Comment: 
 
 Filename: tlt/utils/platform_util.py
 Comment: 
 
 Filename: tlt/utils/types.py
 Comment: 
 
-Filename: intel_transfer_learning_tool-0.4.0.dist-info/LICENSE
+Filename: intel_transfer_learning_tool-0.5.0.dist-info/LICENSE
 Comment: 
 
-Filename: intel_transfer_learning_tool-0.4.0.dist-info/METADATA
+Filename: intel_transfer_learning_tool-0.5.0.dist-info/METADATA
 Comment: 
 
-Filename: intel_transfer_learning_tool-0.4.0.dist-info/WHEEL
+Filename: intel_transfer_learning_tool-0.5.0.dist-info/WHEEL
 Comment: 
 
-Filename: intel_transfer_learning_tool-0.4.0.dist-info/entry_points.txt
+Filename: intel_transfer_learning_tool-0.5.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: intel_transfer_learning_tool-0.4.0.dist-info/top_level.txt
+Filename: intel_transfer_learning_tool-0.5.0.dist-info/top_level.txt
 Comment: 
 
-Filename: intel_transfer_learning_tool-0.4.0.dist-info/RECORD
+Filename: intel_transfer_learning_tool-0.5.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## downloader/datasets.py

```diff
@@ -18,14 +18,15 @@
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import os
 from pydoc import locate
 import tarfile
 import zipfile
+import inspect
 
 from downloader.types import DatasetType
 from downloader import utils
 
 
 class DataDownloader():
     """
@@ -85,19 +86,24 @@
             os.environ['NO_GCE_CHECK'] = 'true'
             return tfds.load(self._dataset_name,
                              data_dir=self._dataset_dir,
                              split=split,
                              **self._args)
 
         elif self._type == DatasetType.TORCHVISION:
+            from torchvision.datasets import __all__ as torchvision_datasets
             dataset_class = locate('torchvision.datasets.{}'.format(self._dataset_name))
-            try:
-                return dataset_class(self._dataset_dir, download=True, split=split)
-            except TypeError:
-                return dataset_class(self._dataset_dir, download=True, train=split == 'train')
+            if dataset_class:
+                params = inspect.signature(dataset_class).parameters
+                kwargs = dict(download=True, split=split, train=split == 'train')
+                kwargs = dict([(k, v) for k, v in kwargs.items() if k in params])
+                return dataset_class(self._dataset_dir, **kwargs)
+            else:
+                raise ValueError("Torchvision dataset {} not found in following: {}"
+                                 .format(self._dataset_name, torchvision_datasets))
 
         elif self._type == DatasetType.HUGGING_FACE:
             from datasets import load_dataset
             if 'subset' in self._args:
                 return load_dataset(self._dataset_name, self._args['subset'], split=split, cache_dir=self._dataset_dir)
             else:
                 return load_dataset(self._dataset_name, split=split, cache_dir=self._dataset_dir)
```

## downloader/models.py

```diff
@@ -105,7 +105,15 @@
             try:
                 pretrained_model_class = locate('keras.applications.{}'.format(self._model_name))
             except TypeError:
                 pretrained_model_class = locate('keras.applications.{}.{}'.format(self._model_name.lower(),
                                                                                   self._model_name))
 
             return pretrained_model_class(**self._args)
+
+        elif self._type == ModelType.TF_BERT_HUGGINGFACE:
+            if self._model_dir is not None:
+                os.environ['TRANSFORMERS_CACHE'] = self._model_dir
+            from transformers import BertConfig, TFBertModel
+
+            config = BertConfig.from_pretrained(self._model_name, output_hidden_states=True)
+            return TFBertModel.from_pretrained(self._model_name, config=config, from_pt=True, **self._args)
```

## downloader/types.py

```diff
@@ -54,14 +54,15 @@
 
 class ModelType(Enum):
     TF_HUB = auto()
     TORCHVISION = auto()
     PYTORCH_HUB = auto()
     HUGGING_FACE = auto()
     KERAS_APPLICATIONS = auto()
+    TF_BERT_HUGGINGFACE = auto()
     GENERIC = auto()
 
     def __str__(self):
         return self.name.lower()
 
     @staticmethod
     def from_str(model_str):
@@ -76,12 +77,15 @@
             return ModelType.TORCHVISION
         elif model_str in ["pytorch_hub", "pyt_hub", "torch_hub", "torch hub", "pytorch hub"]:
             return ModelType.PYTORCH_HUB
         elif model_str in ["huggingface", "hugging_face", "hugging face"]:
             return ModelType.HUGGING_FACE
         elif model_str in ["keras", "keras_applications", "keras applications"]:
             return ModelType.KERAS_APPLICATIONS
+        elif model_str in ["tf_bert_huggingface", "tf bert huggingface", "tf_bert_hugging_face",
+                           "tf bert hugging face"]:
+            return ModelType.TF_BERT_HUGGINGFACE
         elif model_str in ["generic"]:
             return ModelType.GENERIC
         else:
             options = [e.name for e in ModelType]
             raise ValueError("Unsupported model type: {} (Select from: {})".format(model_str, options))
```

## downloader/utils.py

```diff
@@ -15,33 +15,34 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import os
-import shutil
 import tarfile
-import urllib.request
+import requests
+import shutil
 import zipfile
 
 
 def download_file(download_url, destination_directory):
     """
     Downloads a file using the specified url to the destination directory. Returns the
     path to the downloaded file.
     """
     if not os.path.isdir(destination_directory):
         os.makedirs(destination_directory)
 
     destination_file_path = os.path.join(destination_directory, os.path.basename(download_url))
 
     print("Downloading {} to {}".format(download_url, destination_directory))
-    with urllib.request.urlopen(download_url) as response, open(destination_file_path, 'wb') as out_file:
-        shutil.copyfileobj(response, out_file)
+    response = requests.get(download_url, stream=True, timeout=30)
+    with open(destination_file_path, 'wb') as out_file:
+        shutil.copyfileobj(response.raw, out_file)
 
     return destination_file_path
 
 
 def extract_tar_file(tar_file_path, destination_directory):
     """
     Extracts a tar file on the local file system to the destination directory. Returns a list
```

## downloader/tests/test_dataset_download.py

```diff
@@ -1,15 +1,26 @@
 import os
 import pytest
 import shutil
 import tempfile
 
-from datasets.arrow_dataset import Dataset as HF_Dataset
-from torch.utils.data import Dataset as TV_Dataset
-from tensorflow.data import Dataset as TF_Dataset
+try:
+    from datasets.arrow_dataset import Dataset as HF_Dataset
+except ModuleNotFoundError:
+    print("WARNING: datasets may not be installed")
+
+try:
+    from torch.utils.data import Dataset as TV_Dataset
+except ModuleNotFoundError:
+    print("WARNING: torch may not be installed")
+
+try:
+    from tensorflow.data import Dataset as TF_Dataset
+except ModuleNotFoundError:
+    print("WARNING: tensorflow may not be installed")
 
 from downloader import datasets
 from downloader.types import DatasetType
 
 
 @pytest.mark.parametrize('dataset_name,catalog,url',
                          [['foo', 'tfds', 'https:...'],
@@ -24,15 +35,15 @@
 
 
 class TestDatasetDownload:
     """
     Tests the dataset downloader with a temp download directory that is initialized and cleaned up
     """
     URLS = {'sms_spam_collection':
-            'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip',
+            'https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip',
             'flowers':
             'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
             'imagenet_labels':
             'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt',
             'peacock':
             'https://c8.staticflickr.com/8/7095/7210797228_c7fe51c3cb_z.jpg',
             'pennfudan':
@@ -44,14 +55,15 @@
 
     @classmethod
     def teardown_class(cls):
         if os.path.exists(cls._dataset_dir):
             print("Deleting test directory:", cls._dataset_dir)
             shutil.rmtree(cls._dataset_dir)
 
+    @pytest.mark.integration
     @pytest.mark.parametrize('dataset_name,catalog,split,kwargs,size',
                              [['tf_flowers', 'tfds', 'train', {}, 3670],
                               ['CIFAR10', 'torchvision', 'train', {}, 50000],
                               ['CIFAR10', 'torchvision', 'val', {}, 10000],
                               ['imdb', 'huggingface', 'train', {}, 25000],
                               ['glue', 'huggingface', 'test', {'subset': 'sst2'}, 1821]])
     def test_catalog_download(self, dataset_name, catalog, split, kwargs, size):
```

## downloader/tests/test_model_download.py

```diff
@@ -1,15 +1,26 @@
 import os
 import pytest
 import shutil
 import tempfile
 
-from torch.nn import Module
-from tensorflow_hub.keras_layer import KerasLayer
-from tensorflow.keras import Model
+try:
+    from torch.nn import Module
+except ModuleNotFoundError:
+    print("WARNING: Unable to import torch. Torch may not be installed")
+
+try:
+    from tensorflow_hub.keras_layer import KerasLayer
+except ModuleNotFoundError:
+    print("WARNING: Unable to import KerasLayer. Tensorflow Hub may not be installed")
+
+try:
+    from tensorflow.keras import Model
+except ModuleNotFoundError:
+    print("WARNING: Unable to import Keras Model. Tensorflow may not be installed")
 
 from downloader import models
 from downloader.types import ModelType
 
 
 @pytest.mark.parametrize('hub',
                          [['foo'],
@@ -34,26 +45,29 @@
 
     @classmethod
     def teardown_class(cls):
         if os.path.exists(cls._model_dir):
             print("Deleting test directory:", cls._model_dir)
             shutil.rmtree(cls._model_dir)
 
+    # Has previously been skipped due to HTTP Error 403: rate limit exceeded')
     @pytest.mark.parametrize('model_name,hub,kwargs',
                              [['https://tfhub.dev/google/efficientnet/b0/feature-vector/1', 'tf_hub', {}],
                               ['https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3', 'tfhub',
                                {'name': 'encoder', 'trainable': True}],
                               ['resnet34', 'torchvision', {}],
                               ['mobilenet_v2', 'torchvision', {}],
                               ['resnet18_ssl', 'pytorch_hub', {}],
                               ['resnet50_swsl', 'pytorch_hub', {}],
                               ['distilbert-base-uncased', 'huggingface', {}],
                               ['bert-base-cased', 'hugging_face', {}],
                               ['Xception', 'keras_applications', {}],
-                              ['ResNet50', 'keras', {'weights': 'imagenet', 'include_top': False}]])
+                              ['ResNet50', 'keras', {'weights': 'imagenet', 'include_top': False}],
+                              ['google/bert_uncased_L-2_H-128_A-2', 'tf_bert_huggingface', {}],
+                              ['bert-base-uncased', 'tf_bert_hugging_face', {}]])
     def test_hub_download(self, model_name, hub, kwargs):
         """
         Tests downloader for different model hubs
         """
         downloader = models.ModelDownloader(model_name, hub, model_dir=self._model_dir, **kwargs)
         model = downloader.download()
 
@@ -64,12 +78,14 @@
             assert isinstance(model, Module)
         elif downloader._type == ModelType.PYTORCH_HUB:
             assert isinstance(model, Module)
         elif downloader._type == ModelType.HUGGING_FACE:
             assert isinstance(model, Module)
         elif downloader._type == ModelType.KERAS_APPLICATIONS:
             assert isinstance(model, Model)
+        elif downloader._type == ModelType.TF_BERT_HUGGINGFACE:
+            assert isinstance(model, Model)
         else:
             assert False
 
         # Check that the directory is not empty
         assert os.listdir(self._model_dir) is not None
```

## tests/pytorch_tests/test_image_anomaly_detection.py

```diff
@@ -13,36 +13,46 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
+try:
+    import torch
+    import torch.nn as nn
+    import torch.nn.functional as functional
+except ModuleNotFoundError:
+    print("WARNING: Unable to import torch. Torch may not be installed")
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as functional
 
 import os
 import pytest
 import shutil
 import tempfile
 
 from tlt.datasets import dataset_factory
 from tlt.models import model_factory
 from tlt.utils.file_utils import download_and_extract_tar_file
-from tlt.models.image_anomaly_detection.pytorch_image_anomaly_detection_model import extract_features
 
+try:
+    from tlt.models.image_anomaly_detection.pytorch_image_anomaly_detection_model import extract_features
+except ModuleNotFoundError:
+    print("WARNING: Unable to import torch. Torch may not be installed")
 
+
+@pytest.mark.integration
+@pytest.mark.pytorch
 class TestImageAnomalyDetectionCustomDataset:
     """
     Tests for PyTorch image anomaly detection using a custom dataset using the flowers dataset
     """
     @classmethod
     def setup_class(cls):
+        os.makedirs('/tmp/data', exist_ok=True)
         temp_dir = tempfile.mkdtemp(dir='/tmp/data')
         custom_dataset_path = os.path.join(temp_dir, "flower_photos")
 
         if not os.path.exists(custom_dataset_path):
             download_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
             download_and_extract_tar_file(download_url, temp_dir)
             # Rename daisy to "good" and delete all but one other kind to make the dataset small
@@ -60,16 +70,14 @@
     def teardown_class(cls):
         # remove directories
         for dir in [cls._output_dir, cls._temp_dir]:
             if os.path.exists(dir):
                 print("Deleting test directory:", dir)
                 shutil.rmtree(dir)
 
-    @pytest.mark.integration
-    @pytest.mark.pytorch
     @pytest.mark.parametrize('model_name',
                              ['resnet18'])
     def test_custom_dataset_workflow(self, model_name):
         """
         Tests the workflow for PYT image anomaly detection using a custom dataset
         """
         framework = 'pytorch'
@@ -101,16 +109,14 @@
         threshold, auroc = model.evaluate(dataset, pca_components)
         assert isinstance(auroc, float)
 
         # Predict with a batch
         predictions = model.predict(images, pca_components)
         assert len(predictions) == 32
 
-    @pytest.mark.integration
-    @pytest.mark.pytorch
     def test_custom_model_workflow(self):
         """
         Tests the workflow for PYT image anomaly detection using a custom model and custom dataset
         """
         framework = 'pytorch'
         use_case = 'image_anomaly_detection'
 
@@ -161,16 +167,14 @@
         threshold, auroc = model.evaluate(dataset, pca_components)
         assert isinstance(auroc, float)
 
         # Predict with a batch
         predictions = model.predict(images, pca_components)
         assert len(predictions) == 32
 
-    @pytest.mark.integration
-    @pytest.mark.pytorch
     @pytest.mark.parametrize('model_name',
                              ['resnet18'])
     def test_simsiam_workflow(self, model_name):
         """
         Tests the workflow for PYT image anomaly detection using a custom dataset
         and simsiam feature extractor enabled
         """
@@ -200,16 +204,14 @@
         assert isinstance(auroc, float)
 
         # Predict with a batch
         images, labels = dataset.get_batch(subset='validation')
         predictions = model.predict(images, pca_components)
         assert len(predictions) == 32
 
-    @pytest.mark.integration
-    @pytest.mark.pytorch
     @pytest.mark.parametrize('model_name',
                              ['resnet18'])
     def test_cutpaste_workflow(self, model_name):
         """
         Tests the workflow for PYT image anomaly detection using a custom dataset
         and cutpaste feature extractor enabled
         """
```

## tests/pytorch_tests/test_image_classification.py

```diff
@@ -20,31 +20,35 @@
 
 import os
 import numpy as np
 import pytest
 import shutil
 import tempfile
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as functional
+try:
+    import torch
+    import torch.nn as nn
+    import torch.nn.functional as functional
+except ModuleNotFoundError:
+    print("WARNING: Unable to import torch. Torch may not be installed")
+
 
 from tlt.datasets import dataset_factory
 from tlt.models import model_factory
 from tlt.utils.file_utils import download_and_extract_tar_file
 
 
-@pytest.mark.integration
+@pytest.mark.skip(reason='TODO: Solve test fails with urllib.error.HTTPError: HTTP Error 403: rate limit exceeded')
 @pytest.mark.pytorch
-@pytest.mark.parametrize('model_name,dataset_name,extra_layers,correct_num_layers',
-                         [['efficientnet_b0', 'CIFAR10', None, 2],
-                          ['resnet18_ssl', 'CIFAR10', None, 1],
-                          ['efficientnet_b0', 'CIFAR10', [1024, 512], 6],
-                          ['resnet18', 'CIFAR10', [1024, 512], 5]])
-def test_pyt_image_classification(model_name, dataset_name, extra_layers, correct_num_layers):
+@pytest.mark.parametrize('model_name,dataset_name,extra_layers,correct_num_layers,test_inc',
+                         [['efficientnet_b0', 'CIFAR10', None, 2, False],
+                          ['resnet18_ssl', 'CIFAR10', None, 1, False],
+                          ['efficientnet_b0', 'CIFAR10', [1024, 512], 6, False],
+                          ['resnet18', 'CIFAR10', [1024, 512], 5, True]])
+def test_pyt_image_classification(model_name, dataset_name, extra_layers, correct_num_layers, test_inc):
     """
     Tests basic transfer learning functionality for PyTorch image classification models using a torchvision dataset
     """
     framework = 'pytorch'
     output_dir = tempfile.mkdtemp()
     os.environ["TORCH_HOME"] = output_dir
 
@@ -92,26 +96,30 @@
 
     # Evaluate
     reload_metrics = reload_model.evaluate(dataset)
     assert reload_metrics == trained_metrics
 
     # Ensure we get not implemented errors for graph_optimization
     with pytest.raises(NotImplementedError):
-        model.optimize_graph(saved_model_dir, os.path.join(saved_model_dir, 'optimized'))
+        model.optimize_graph(os.path.join(saved_model_dir, 'optimized'))
+
+    # Test quantization and benchmarking
+    if test_inc:
+        inc_output_dir = os.path.join(output_dir, "quantized", 'resnet18')
+        os.makedirs(inc_output_dir, exist_ok=True)
+        model.quantize(inc_output_dir, dataset)
+        assert os.path.exists(os.path.join(inc_output_dir, "model.pt"))
+        model.benchmark(dataset=dataset, saved_model_dir=inc_output_dir)
 
     # Delete the temp output directory
     if os.path.exists(output_dir) and os.path.isdir(output_dir):
         shutil.rmtree(output_dir)
 
-    # Ensure we get not implemented errors for quantization
-    inc_config_file_path = os.path.join(output_dir, "pytorch_{}.yaml".format(model_name))
-    with pytest.raises(NotImplementedError):
-        model.write_inc_config_file(inc_config_file_path, dataset, batch_size=32)
-
 
+@pytest.mark.integration
 @pytest.mark.pytorch
 def test_pyt_image_classification_custom_model():
     """
     Tests basic transfer learning functionality for custom PyTorch image classification models using a Torchvision
     dataset
     """
     framework = 'pytorch'
@@ -180,26 +188,29 @@
 
     # Evaluate
     reload_metrics = reload_model.evaluate(dataset)
     assert reload_metrics == trained_metrics
 
     # Ensure we get not implemented errors for graph_optimization
     with pytest.raises(NotImplementedError):
-        model.optimize_graph(saved_model_dir, os.path.join(saved_model_dir, 'optimized'))
+        model.optimize_graph(os.path.join(saved_model_dir, 'optimized'))
+
+    # Test quantization and benchmarking
+    inc_output_dir = os.path.join(output_dir, "quantized", 'Net')
+    os.makedirs(inc_output_dir, exist_ok=True)
+    model.quantize(inc_output_dir, dataset)
+    assert os.path.exists(os.path.join(inc_output_dir, "model.pt"))
+    model.benchmark(dataset=dataset, saved_model_dir=inc_output_dir)
 
     # Delete the temp output directory
     if os.path.exists(output_dir) and os.path.isdir(output_dir):
         shutil.rmtree(output_dir)
 
-    # Ensure we get not implemented errors for quantization
-    inc_config_file_path = os.path.join(output_dir, "pytorch_{}.yaml".format('custom_model'))
-    with pytest.raises(NotImplementedError):
-        model.write_inc_config_file(inc_config_file_path, dataset, batch_size=32)
-
 
+@pytest.mark.pytorch
 class TestImageClassificationCustomDataset:
     """
     Tests for PyTorch image classification using a custom dataset using the flowers dataset
     """
     @classmethod
     def setup_class(cls):
         temp_dir = tempfile.mkdtemp(dir='/tmp/data')
@@ -219,21 +230,22 @@
     def teardown_class(cls):
         # remove directories
         for dir in [cls._output_dir, cls._temp_dir]:
             if os.path.exists(dir):
                 print("Deleting test directory:", dir)
                 shutil.rmtree(dir)
 
-    @pytest.mark.pytorch
-    @pytest.mark.parametrize('model_name,add_aug,ipex_optimize',
-                             [['efficientnet_b0', ['hflip'], True],
-                              ['resnet18', ['rotate'], True],
-                              ['resnet18_ssl', ['rotate'], True],
-                              ['vit_b_16', None, False]])
-    def test_custom_dataset_workflow(self, model_name, add_aug, ipex_optimize):
+    @pytest.mark.skip(reason='TODO: Solve test fails with urllib.error.HTTPError: HTTP Error 403: rate limit exceeded')
+    @pytest.mark.parametrize('model_name,add_aug,ipex_optimize,test_inc',
+                             [['efficientnet_b0', ['hflip'], True, False],
+                              ['resnet18', ['rotate'], False, True],
+                              ['resnet18', None, True, True],
+                              ['resnet18_ssl', ['rotate'], True, False],
+                              ['vit_b_16', None, False, False]])
+    def test_custom_dataset_workflow(self, model_name, add_aug, ipex_optimize, test_inc):
         """
         Tests the full workflow for PYT image classification using a custom dataset
         """
         framework = 'pytorch'
         use_case = 'image_classification'
 
         # Get the dataset
@@ -268,27 +280,21 @@
         reload_model = model_factory.get_model(model_name, framework)
         reload_model.load_from_directory(saved_model_dir)
 
         # Evaluate
         metrics = reload_model.evaluate(dataset)
         assert len(metrics) > 0
 
-        # Test benchmarking and quantization with non-IPEX ResNet18
-        if model_name == "resnet18" and not ipex_optimize:
-            inc_config_file_path = os.path.join(self._output_dir, "pyt_{}.yaml".format(model_name))
-            nc_workspace = os.path.join(self._output_dir, "nc_workspace")
-            model.write_inc_config_file(inc_config_file_path, dataset, batch_size=32, overwrite=True,
-                                        accuracy_criterion_relative=0.1, exit_policy_max_trials=10,
-                                        exit_policy_timeout=0, tuning_workspace=nc_workspace)
-            model.benchmark(saved_model_dir, inc_config_file_path, model_type='fp32')
-            quantization_output = os.path.join(self._output_dir, "quantized", model_name)
-            os.makedirs(quantization_output, exist_ok=True)
-            model.quantize(saved_model_dir, quantization_output, inc_config_file_path)
-            assert os.path.exists(os.path.join(quantization_output, "model.pt"))
-            model.benchmark(quantization_output, inc_config_file_path, model_type='int8')
+        # Test benchmarking and quantization
+        if test_inc:
+            inc_output_dir = os.path.join(self._output_dir, "quantized", model_name)
+            os.makedirs(inc_output_dir, exist_ok=True)
+            model.quantize(inc_output_dir, dataset)
+            assert os.path.exists(os.path.join(inc_output_dir, "model.pt"))
+            model.benchmark(saved_model_dir=inc_output_dir, dataset=dataset)
 
 
 @pytest.mark.integration
 @pytest.mark.pytorch
 @pytest.mark.parametrize('model_name,dataset_name,epochs,lr,do_eval,early_stopping,lr_decay,final_lr,final_acc',
                          [['efficientnet_b0', 'CIFAR10', 10, 0.005, True, False, True, 0.001, 0.9888],
                           ['resnet18', 'CIFAR10', 1, 0.005, True, False, False, None, 0.2688],
@@ -324,14 +330,15 @@
         assert model._lr_scheduler.optimizer.param_groups[0]['lr'] == final_lr
     else:
         assert model._lr_scheduler is None
 
     assert history['Acc'][-1] == final_acc
 
 
+@pytest.mark.integration
 @pytest.mark.pytorch
 def test_pyt_freeze():
     """
     Tests layer freezing functionality for PyTorch image classification models using a torchvision dataset
     """
     dataset_name = 'CIFAR10'
     framework = 'pytorch'
```

## tests/pytorch_tests/test_text_classification.py

```diff
@@ -22,23 +22,26 @@
 import pytest
 import shutil
 import tempfile
 from unittest.mock import MagicMock
 
 from tlt.datasets import dataset_factory
 from tlt.models import model_factory
-from tlt.datasets.text_classification.hf_custom_text_classification_dataset import HFCustomTextClassificationDataset
+try:
+    from tlt.datasets.text_classification.hf_custom_text_classification_dataset import HFCustomTextClassificationDataset
+except ModuleNotFoundError:
+    print("WARNING: Unable to import HFCustomTextClassificationDataset.")
 
 
 @pytest.mark.integration
 @pytest.mark.pytorch
-@pytest.mark.parametrize('model_name,dataset_name,extra_layers,correct_num_layers',
-                         [['bert-base-cased', 'imdb', None, 1],
-                          ['distilbert-base-uncased', 'imdb', [384, 192], 5]])
-def test_pyt_text_classification(model_name, dataset_name, extra_layers, correct_num_layers):
+@pytest.mark.parametrize('model_name,dataset_name,extra_layers,correct_num_layers,test_inc',
+                         [['bert-base-cased', 'imdb', None, 1, False],
+                          ['distilbert-base-uncased', 'imdb', [384, 192], 5, True]])
+def test_pyt_text_classification(model_name, dataset_name, extra_layers, correct_num_layers, test_inc):
     """
     Tests basic transfer learning functionality for PyTorch text classification models using a hugging face dataset
     """
     framework = 'pytorch'
     output_dir = tempfile.mkdtemp()
 
     # Get the dataset
@@ -46,53 +49,65 @@
                                           'huggingface', split=["train"], shuffle_files=False)
 
     # Get the model
     model = model_factory.get_model(model_name, framework)
 
     # Preprocess the dataset
     dataset.preprocess(model_name, batch_size=32)
-    dataset.shuffle_split(train_pct=0.01, val_pct=0.01, seed=10)
+    dataset.shuffle_split(train_pct=0.02, val_pct=0.01, seed=6)
     assert dataset._validation_type == 'shuffle_split'
 
     # Evaluate before training
     pretrained_metrics = model.evaluate(dataset)
     assert len(pretrained_metrics) > 0
 
     # Train
-    model.train(dataset, output_dir=output_dir, epochs=1, do_eval=False, extra_layers=extra_layers)
+    train_history = model.train(dataset, output_dir=output_dir, epochs=1, do_eval=False, extra_layers=extra_layers)
+    assert train_history is not None and isinstance(train_history, dict)
+    assert 'Loss' in train_history
+    assert 'Acc' in train_history
+    assert 'train_runtime' in train_history
+    assert 'train_samples_per_second' in train_history
     classifier_layer = getattr(model._model, "classifier")
     try:
         # If extra_layers given, the classifier is a Sequential layer with given input
         n_layers = len(classifier_layer)
     except TypeError:
         # If not given, the classifer is just a single Linear layer
         n_layers = 1
     assert n_layers == correct_num_layers
 
     # Evaluate
     trained_metrics = model.evaluate(dataset)
-    assert trained_metrics[0] <= pretrained_metrics[0]  # loss
-    assert trained_metrics[1] >= pretrained_metrics[1]  # accuracy
+    assert trained_metrics['eval_loss'] <= pretrained_metrics['eval_loss']
+    assert trained_metrics['eval_accuracy'] >= pretrained_metrics['eval_accuracy']
 
     # Export the saved model
     saved_model_dir = model.export(output_dir)
     assert os.path.isdir(saved_model_dir)
     assert os.path.isfile(os.path.join(saved_model_dir, "model.pt"))
 
     # Reload the saved model
     reload_model = model_factory.get_model(model_name, framework)
-    reload_model.load_from_directory(saved_model_dir, num_classes=len(dataset.class_names))
+    reload_model.load_from_directory(saved_model_dir)
 
     # Evaluate
     reload_metrics = reload_model.evaluate(dataset)
-    assert reload_metrics == trained_metrics
+    assert reload_metrics['eval_accuracy'] == trained_metrics['eval_accuracy']
 
     # Ensure we get 'NotImplementedError' for graph_optimization
     with pytest.raises(NotImplementedError):
-        model.optimize_graph(saved_model_dir, os.path.join(saved_model_dir, 'optimized'))
+        model.optimize_graph(os.path.join(saved_model_dir, 'optimized'))
+
+    # Quantization
+    if test_inc:
+        inc_output_dir = os.path.join(output_dir, "quantized", model_name)
+        os.makedirs(inc_output_dir, exist_ok=True)
+        model.quantize(inc_output_dir, dataset)
+        assert os.path.exists(os.path.join(inc_output_dir, "model.pt"))
 
     # Delete the temp output directory
     if os.path.exists(output_dir) and os.path.isdir(output_dir):
         shutil.rmtree(output_dir)
 
 
 @pytest.mark.integration
@@ -128,36 +143,26 @@
     # export the saved model
     saved_model_dir = model.export(output_dir)
     assert os.path.isdir(saved_model_dir)
     assert os.path.isfile(os.path.join(saved_model_dir, "model.pt"))
 
     # Reload the saved model
     reload_model = model_factory.get_model(model_name, 'pytorch')
-    reload_model.load_from_directory(saved_model_dir, 2)
+    reload_model.load_from_directory(saved_model_dir)
 
     # Evaluate
     metrics = reload_model.evaluate(mock_dataset)
     assert len(metrics) > 0
 
-    # Quantization
-    inc_config_file_path = 'tlt/models/configs/inc/text_classification_template.yaml'
-    nc_workspace = os.path.join(output_dir, "nc_workspace")
-    model.write_inc_config_file(inc_config_file_path, mock_dataset, batch_size=32, overwrite=True,
-                                accuracy_criterion_relative=0.1, exit_policy_max_trials=10,
-                                exit_policy_timeout=0, tuning_workspace=nc_workspace)
-    quantization_output = os.path.join(output_dir, "quantized", model_name)
-    os.makedirs(quantization_output, exist_ok=True)
-    model.quantize(saved_model_dir, quantization_output, inc_config_file_path)
-    assert os.path.exists(os.path.join(quantization_output, "model.pt"))
-
     # Delete the temp output directory
     if os.path.exists(output_dir) and os.path.isdir(output_dir):
         shutil.rmtree(output_dir)
 
 
+@pytest.mark.integration
 @pytest.mark.pytorch
 @pytest.mark.parametrize('model_name,dataset_name',
                          [['distilbert-base-uncased', 'imdb']])
 def test_initial_checkpoints(model_name, dataset_name):
     framework = 'pytorch'
     output_dir = tempfile.mkdtemp()
     checkpoint_dir = os.path.join(output_dir, model_name + '_checkpoints')
@@ -184,22 +189,23 @@
 
     model = model_factory.get_model(model_name, framework)
     model.train(dataset, output_dir=output_dir, epochs=2, do_eval=False,
                 initial_checkpoints=os.path.join(checkpoint_dir, 'checkpoint.pt'))
 
     improved_metrics = model.evaluate(dataset)
 
-    assert improved_metrics[0] < trained_metrics[0]  # loss
-    assert improved_metrics[1] > trained_metrics[1]  # accuracy
+    assert improved_metrics['eval_loss'] < trained_metrics['eval_loss']
+    assert improved_metrics['eval_accuracy'] > trained_metrics['eval_accuracy']
 
     # Delete the temp output directory
     if os.path.exists(output_dir) and os.path.isdir(output_dir):
         shutil.rmtree(output_dir)
 
 
+@pytest.mark.integration
 @pytest.mark.pytorch
 @pytest.mark.parametrize('model_name,dataset_name',
                          [['distilbert-base-uncased', 'imdb']])
 def test_freeze_bert(model_name, dataset_name):
     framework = 'pytorch'
     output_dir = tempfile.mkdtemp()
 
@@ -227,14 +233,15 @@
                 assert param.requires_grad is False
 
     # Delete the temp output directory
     if os.path.exists(output_dir) and os.path.isdir(output_dir):
         shutil.rmtree(output_dir)
 
 
+@pytest.mark.integration
 @pytest.mark.pytorch
 @pytest.mark.parametrize('model_name,dataset_name',
                          [['distilbert-base-uncased', 'imdb']])
 def test_unfreeze_bert(model_name, dataset_name):
     framework = 'pytorch'
     output_dir = tempfile.mkdtemp()
 
@@ -260,14 +267,15 @@
                 assert param.requires_grad is True
 
     # Delete the temp output directory
     if os.path.exists(output_dir) and os.path.isdir(output_dir):
         shutil.rmtree(output_dir)
 
 
+@pytest.mark.integration
 @pytest.mark.pytorch
 @pytest.mark.parametrize('model_name,dataset_name',
                          [['distilbert-base-uncased', 'imdb']])
 def test_list_layers_bert(model_name, dataset_name):
     import io
     import unittest.mock as mock
```

## tests/pytorch_tests/unit/test_datasets.py

```diff
@@ -64,38 +64,38 @@
 @pytest.mark.pytorch
 def test_torchvision_subset():
     """
     Checks that a torchvision test subset can be loaded
     """
     data = get_dataset('/tmp/data', 'image_classification', 'pytorch', 'CIFAR10', 'torchvision', split=["test"])
     assert type(data) == TorchvisionImageClassificationDataset
-    assert len(data.dataset) < 50000
+    assert len(data.dataset) > 0
 
 
 @pytest.mark.pytorch
 def test_defined_split():
     """
     Checks that dataset can be loaded into train and test subsets based on torchvision splits and then
     re-partitioned with shuffle-split
     """
     data = get_dataset('/tmp/data', 'image_classification', 'pytorch', 'CIFAR10',
                        'torchvision', split=['train', 'test'])
-    assert len(data.dataset) == 60000
-    assert len(data.train_subset) == 50000
-    assert len(data.test_subset) == 10000
+
+    dataset_size = len(data.dataset)
+    assert dataset_size > 0
+    assert len(data.train_subset) <= dataset_size
+    assert len(data.test_subset) <= len(data.train_subset)
     assert data.validation_subset is None
-    assert data._train_indices == range(50000)
-    assert data._test_indices == range(50000, 60000)
     assert data._validation_type == 'defined_split'
 
     # Apply shuffle split and verify new subset sizes
     data.shuffle_split(.6, .2, .2, seed=10)
-    assert len(data.train_subset) == 36000
-    assert len(data.validation_subset) == 12000
-    assert len(data.test_subset) == 12000
+    assert len(data.train_subset) == dataset_size * .6
+    assert len(data.validation_subset) == dataset_size * .2
+    assert len(data.test_subset) == dataset_size * .2
     assert data._validation_type == 'shuffle_split'
 
 
 @pytest.mark.pytorch
 def test_shuffle_split():
     """
     Checks that dataset can be split into train, validation, and test subsets
@@ -104,14 +104,15 @@
     data.shuffle_split(seed=10)
     assert len(data.train_subset) == 37500
     assert len(data.validation_subset) == 12500
     assert data.test_subset is None
     assert data._validation_type == 'shuffle_split'
 
 
+@pytest.mark.integration
 @pytest.mark.pytorch
 def test_shuffle_split_deterministic_tv():
     """
     Checks that dataset can be split into train, validation, and test subsets in a way that is reproducible
     """
     data = get_dataset('/tmp/data', 'image_classification', 'pytorch', 'DTD', 'torchvision', split=['test'])
     data.preprocess(224, 128)
@@ -159,14 +160,15 @@
     finally:
         if ic_dataset1:
             ic_dataset1.cleanup()
         if ic_dataset2:
             ic_dataset2.cleanup()
 
 
+@pytest.mark.integration
 @pytest.mark.pytorch
 @pytest.mark.parametrize('dataset_dir,dataset_name,dataset_catalog,class_names,batch_size',
                          [['/tmp/data', 'DTD', 'torchvision', None, 32],
                           ['/tmp/data', 'DTD', 'torchvision', None, 1],
                           ['/tmp/data', None, None, ['foo', 'bar'], 8],
                           ['/tmp/data', None, None, ['foo', 'bar'], 1]])
 def test_batching(dataset_dir, dataset_name, dataset_catalog, class_names, batch_size):
@@ -180,14 +182,15 @@
 
         tlt_dataset.preprocess(224, batch_size)
         assert len(tlt_dataset.get_batch()[0]) == batch_size
     finally:
         ic_dataset.cleanup()
 
 
+@pytest.mark.integration
 @pytest.mark.pytorch
 @pytest.mark.parametrize('dataset_dir,dataset_name,dataset_catalog,class_names',
                          [['/tmp/data', 'DTD', 'torchvision', None],
                           ['/tmp/data', None, None, ['foo', 'bar']]])
 def test_batching_error(dataset_dir, dataset_name, dataset_catalog, class_names):
     """
     Checks that preprocessing cannot be run twice
@@ -304,15 +307,14 @@
 @pytest.mark.pytorch
 class TestImageClassificationDataset:
     """
     This class contains image classification dataset tests that only require the dataset to be initialized once. These
     tests will be run once for each of the dataset defined in the dataset_params list.
     """
 
-    @pytest.mark.pytorch
     def test_class_names_and_size(self, image_classification_data):
         """
         Verify the class type, dataset class names, and dataset length after initializaion
         """
         tlt_dataset, dataset_name, dataset_classes, splits = image_classification_data
 
         if dataset_name is None:
@@ -323,70 +325,65 @@
             else:
                 assert len(tlt_dataset.dataset) == len(dataset_classes) * len(splits) * 50
         else:
             assert type(tlt_dataset) == TorchvisionImageClassificationDataset
             assert len(tlt_dataset.class_names) == len(torchvision_metadata[dataset_name]['class_names'])
             assert len(tlt_dataset.dataset) == torchvision_metadata[dataset_name]['size']
 
-    @pytest.mark.pytorch
     @pytest.mark.parametrize('batch_size',
                              ['foo',
                               -17,
                               20.5])
     def test_invalid_batch_sizes(self, batch_size, image_classification_data):
         """
         Ensures that a ValueError is raised when an invalid batch size is passed
         """
         tlt_dataset, dataset_name, dataset_classes, splits = image_classification_data
         with pytest.raises(ValueError):
             tlt_dataset.preprocess(224, batch_size)
 
-    @pytest.mark.pytorch
     @pytest.mark.parametrize('image_size',
                              ['foo',
                               -17,
                               20.5])
     def test_invalid_image_size(self, image_size, image_classification_data):
         """
         Ensures that a ValueError is raised when an invalid image size is passed
         """
         tlt_dataset, dataset_name, dataset_classes, splits = image_classification_data
         with pytest.raises(ValueError):
             tlt_dataset.preprocess(image_size, batch_size=8)
 
-    @pytest.mark.pytorch
     def test_preprocessing(self, image_classification_data):
         """
         Checks that dataset can be preprocessed only once
         """
         tlt_dataset, dataset_name, dataset_classes, splits = image_classification_data
         tlt_dataset.preprocess(224, 8)
         preprocessing_inputs = {'image_size': 224, 'batch_size': 8}
         assert tlt_dataset._preprocessed == preprocessing_inputs
         # Trying to preprocess again should throw an exception
         with pytest.raises(Exception) as e:
             tlt_dataset.preprocess(324, 32)
         assert 'Data has already been preprocessed: {}'.format(preprocessing_inputs) == str(e.value)
         print(tlt_dataset.info)
 
-    @pytest.mark.pytorch
     def test_shuffle_split_errors(self, image_classification_data):
         """
         Checks that splitting into train, validation, and test subsets will error if inputs are wrong
         """
         tlt_dataset, dataset_name, dataset_classes, splits = image_classification_data
 
         with pytest.raises(Exception) as e:
             tlt_dataset.shuffle_split(train_pct=.5, val_pct=.5, test_pct=.2)
         assert 'Sum of percentage arguments must be less than or equal to 1.' == str(e.value)
         with pytest.raises(Exception) as e:
             tlt_dataset.shuffle_split(train_pct=1, val_pct=0)
         assert 'Percentage arguments must be floats.' == str(e.value)
 
-    @pytest.mark.pytorch
     def test_shuffle_split(self, image_classification_data):
         """
         Checks that dataset can be split into train, validation, and test subsets
         """
         tlt_dataset, dataset_name, dataset_classes, splits = image_classification_data
 
         # Before the shuffle split, validation type should be recall
@@ -462,56 +459,52 @@
 @pytest.mark.pytorch
 class TestImageAnomalyDetectionDataset:
     """
     This class contains image anomaly detection dataset tests that only require the dataset to be initialized once.
     These tests will be run once for each of the dataset defined in the anomaly_dataset_params list.
     """
 
-    @pytest.mark.pytorch
     def test_classes_defects_and_size(self, anomaly_detection_data):
         """
         Verify the class type, dataset class names, defect_names, and dataset length after initializaion
         """
         tlt_dataset, dataset_name, dataset_classes, use_case = anomaly_detection_data
 
         assert type(tlt_dataset) == PyTorchCustomImageAnomalyDetectionDataset
         assert len(tlt_dataset.class_names) == 2  # Always 2 for anomaly detection
         assert len(tlt_dataset.defect_names) == len(dataset_classes) - 1  # Subtract 1 for the "good" class
         assert len(tlt_dataset.dataset) == len(dataset_classes) * 50
 
-    @pytest.mark.pytorch
     def test_preprocessing(self, anomaly_detection_data):
         """
         Checks that dataset can be preprocessed only once
         """
         tlt_dataset, dataset_name, dataset_classes, use_case = anomaly_detection_data
         tlt_dataset.preprocess(224, 8)
         preprocessing_inputs = {'image_size': 224, 'batch_size': 8}
         assert tlt_dataset._preprocessed == preprocessing_inputs
         # Trying to preprocess again should throw an exception
         with pytest.raises(Exception) as e:
             tlt_dataset.preprocess(324, 32)
         assert 'Data has already been preprocessed: {}'.format(preprocessing_inputs) == str(e.value)
         print(tlt_dataset.info)
 
-    @pytest.mark.pytorch
     def test_shuffle_split_errors(self, anomaly_detection_data):
         """
         Checks that splitting into train, validation, and test subsets will error if inputs are wrong
         """
         tlt_dataset, dataset_name, dataset_classes, use_case = anomaly_detection_data
 
         with pytest.raises(Exception) as e:
             tlt_dataset.shuffle_split(train_pct=.5, val_pct=.5, test_pct=.2)
         assert 'Sum of percentage arguments must be less than or equal to 1.' == str(e.value)
         with pytest.raises(Exception) as e:
             tlt_dataset.shuffle_split(train_pct=1, val_pct=0)
         assert 'Percentage arguments must be floats.' == str(e.value)
 
-    @pytest.mark.pytorch
     def test_shuffle_split(self, anomaly_detection_data):
         """
         Checks that dataset can be split into train, validation, and test subsets
         """
         tlt_dataset, dataset_name, dataset_classes, use_case = anomaly_detection_data
 
         # Before the shuffle split, validation type should be None
@@ -632,60 +625,57 @@
 
     request.addfinalizer(cleanup)
 
     # Return the tlt dataset along with metadata that tests might need
     return (tc_dataset.tlt_dataset, dataset_dir, dataset_name, dataset_catalog, class_names)
 
 
+@pytest.mark.integration
 @pytest.mark.pytorch
 class TestTextClassificationDataset:
     """
     This class contains text classification dataset tests that only require the dataset to be initialized once. These
     tests will be run once for each of the datasets defined in the dataset_params list.
     """
 
-    @pytest.mark.pytorch
     def test_tlt_dataset(self, text_classification_data):
         """
         Tests whether a matching Intel Transfer Learning Tool dataset object is returned
         """
         tlt_dataset, _, dataset_name, _, _ = text_classification_data
         if dataset_name is None:
             assert type(tlt_dataset) == HFCustomTextClassificationDataset
         else:
             assert type(tlt_dataset) == HFTextClassificationDataset
 
-    @pytest.mark.pytorch
     def test_class_names_and_size(self, text_classification_data):
         """
         Verify the class type, dataset class names, and dataset length after initializaion
         """
         tlt_dataset, _, dataset_name, _, class_names = text_classification_data
         if dataset_name is None:
             assert len(tlt_dataset.class_names) == len(class_names)
             assert len(tlt_dataset.dataset) == 50
         else:
             assert tlt_dataset.class_names == class_names
             assert len(tlt_dataset.dataset) == hf_metadata[dataset_name]['size']
 
-    @pytest.mark.pytorch
     @pytest.mark.parametrize('batch_size',
                              ['foo',  # A string
                               -17,  # A negative int
                               20.5,  # A float
                               ])
     def test_invalid_batch_size_type(self, batch_size, text_classification_data):
         """
         Ensures that a ValueError is raised when an invalid batch size type is passed
         """
         tlt_dataset, _, _, _, _ = text_classification_data
         with pytest.raises(ValueError):
             tlt_dataset.preprocess('', batch_size)
 
-    @pytest.mark.pytorch
     def test_shuffle_split_errors(self, text_classification_data):
         """
         Checks that splitting into train, validation, and test subsets will error if inputs are wrong
         """
         tlt_dataset, _, _, _, _ = text_classification_data
         with pytest.raises(ValueError) as sum_err_message:
             tlt_dataset.shuffle_split(train_pct=.5, val_pct=.5, test_pct=.2)
```

## tests/pytorch_tests/unit/test_inc.py

```diff
@@ -21,39 +21,26 @@
 import os
 import pytest
 import shutil
 import tempfile
 import uuid
 
 from pathlib import Path
-from unittest.mock import patch
+from unittest.mock import patch, MagicMock
 
 from tlt.models import model_factory
+from tlt.datasets.image_classification.pytorch_custom_image_classification_dataset import PyTorchCustomImageClassificationDataset  # noqa: E501
+
 
 try:
     # Do PyTorch specific imports in a try/except to prevent pytest test loading from failing when running in a TF env
     from tlt.models.image_classification.torchvision_image_classification_model import TorchvisionImageClassificationModel  # noqa: F401, E501
 except ModuleNotFoundError:
     print("WARNING: Unable to import TorchvisionImageClassificationModel. PyTorch or torchvision may not be installed")
 
-from tlt.datasets import dataset_factory
-from tlt.utils.file_utils import download_and_extract_tar_file
-from tlt.datasets.image_classification.pytorch_custom_image_classification_dataset import PyTorchCustomImageClassificationDataset  # noqa: E501
-
-# Load a custom PyTorch dataset that can be re-used for tests
-dataset_dir = tempfile.mkdtemp()
-custom_dataset_path = os.path.join(dataset_dir, "flower_photos")
-if not os.path.exists(custom_dataset_path):
-    download_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
-    download_and_extract_tar_file(download_url, dataset_dir)
-# Load the dataset from the custom dataset path
-dataset = dataset_factory.load_dataset(dataset_dir=custom_dataset_path,
-                                       use_case='image_classification',
-                                       framework='pytorch')
-
 
 @pytest.mark.pytorch
 def test_torchvision_image_classification_optimize_graph_not_implemented():
     """
     Verifies the error that gets raise if graph optimization is attempted with a PyTorch model
     """
     try:
@@ -63,15 +50,15 @@
         Path(dummy_config_file).touch()
         model = model_factory.get_model('resnet50', 'pytorch')
         # The torchvision model is not present until training, so call _get_hub_model()
         model._get_hub_model(3)
         # Graph optimization is not enabled for PyTorch, so this should fail
         with patch('neural_compressor.experimental.Graph_Optimization'):
             with pytest.raises(NotImplementedError):
-                model.optimize_graph(saved_model_dir, output_dir)
+                model.optimize_graph(output_dir)
 
         # Verify that the installed version of Intel Neural Compressor throws a SystemError
         from neural_compressor.experimental import Graph_Optimization, common
         # set_backend API is no longer available in Neural Compressor v2.0
         # from neural_compressor.experimental.common.model import set_backend
         # set_backend('pytorch')
         graph_optimizer = Graph_Optimization()
@@ -82,371 +69,124 @@
         if os.path.exists(output_dir):
             shutil.rmtree(output_dir)
         if os.path.exists(saved_model_dir):
             shutil.rmtree(saved_model_dir)
 
 
 @pytest.mark.pytorch
-def test_pyt_image_classification_config_file_overwrite():
-    """
-    Tests writing an INC config file for image classification models with a mock custom dataset. Checks that the
-    overwrite flag lets you overwrite a config file that already exists.
-    """
-    try:
-        temp_dir = tempfile.mkdtemp()
-        model = model_factory.get_model('efficientnet_b0', 'pytorch')
-        with patch('tlt.datasets.image_classification.pytorch_custom_image_classification_dataset.PyTorchCustomImageClassificationDataset') as mock_dataset:  # noqa: E501
-            mock_dataset.__class__ = PyTorchCustomImageClassificationDataset
-            config_file = os.path.join(temp_dir, "config.yaml")
-            batch_size = 4
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
-            nc_workspace = os.path.join(temp_dir, "nc_workspace")
-            model.write_inc_config_file(config_file, dataset, batch_size=batch_size, tuning_workspace=nc_workspace)
-            assert os.path.exists(config_file)
-
-            # If overwrite=False this should fail, since the config file already exists
-            with pytest.raises(FileExistsError):
-                model.write_inc_config_file(config_file, dataset, batch_size=batch_size, overwrite=False)
-
-            # Writing the config file again should work with overwrite=True
-            model.write_inc_config_file(config_file, dataset, batch_size=batch_size, overwrite=True)
-    finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
-
-
-@pytest.mark.pytorch
-@pytest.mark.parametrize('batch_size,valid',
-                         [[1, True],
-                          [-1, False],
-                          ['abc', False],
-                          [1.434, False],
-                          [0, False],
-                          [128, True]])
-def test_pyt_image_classification_config_file_batch_size(batch_size, valid):
-    """
-    Tests writing an INC config file with good and bad batch sizes
-    """
-    try:
-        temp_dir = tempfile.mkdtemp()
-        nc_workspace = os.path.join(temp_dir, "nc_workspace")
-        model = model_factory.get_model('efficientnet_b0', 'pytorch')
-        with patch('tlt.datasets.image_classification.pytorch_custom_image_classification_dataset.PyTorchCustomImageClassificationDataset') as mock_dataset:  # noqa: E501
-            config_file = os.path.join(temp_dir, "config.yaml")
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-            if not valid:
-                with pytest.raises(ValueError):
-                    model.write_inc_config_file(config_file, dataset, batch_size=batch_size, overwrite=True,
-                                                tuning_workspace=nc_workspace)
-            else:
-                model.write_inc_config_file(config_file, dataset, batch_size=batch_size, overwrite=True,
-                                            tuning_workspace=nc_workspace)
-    finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
-
-
-@pytest.mark.pytorch
-@pytest.mark.parametrize('resize_interpolation,valid',
-                         [['bilinear', True],
-                          [-1, False],
-                          ['nearest', True],
-                          [1.434, False],
-                          ['bicubic', True],
-                          ['foo', False]])
-def test_pyt_image_classification_config_file_resize_interpolation(resize_interpolation, valid):
-    """
-    Tests writing an INC config file with good and bad resize_interpolation values
-    """
-    try:
-        temp_dir = tempfile.mkdtemp()
-        nc_workspace = os.path.join(temp_dir, "nc_workspace")
-        model = model_factory.get_model('efficientnet_b0', 'pytorch')
-        with patch('tlt.datasets.image_classification.pytorch_custom_image_classification_dataset.PyTorchCustomImageClassificationDataset') as mock_dataset:  # noqa: E501
-            config_file = os.path.join(temp_dir, "config.yaml")
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-            if not valid:
-                with pytest.raises(ValueError):
-                    model.write_inc_config_file(config_file, dataset, batch_size=1, overwrite=True,
-                                                resize_interpolation=resize_interpolation,
-                                                tuning_workspace=nc_workspace)
-            else:
-                model.write_inc_config_file(config_file, dataset, batch_size=1, overwrite=True,
-                                            resize_interpolation=resize_interpolation, tuning_workspace=nc_workspace)
-    finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
-
-
-@pytest.mark.pytorch
-@pytest.mark.parametrize('accuracy_criterion,valid',
-                         [[0.1, True],
-                          [-1, False],
-                          [0.01, True],
-                          [1.434, False],
-                          ['foo', False]])
-def test_pyt_image_classification_config_file_accuracy_criterion(accuracy_criterion, valid):
+@patch('tlt.models.image_classification.torchvision_image_classification_model.ModelDownloader')
+@patch('tlt.models.pytorch_model.quantization.fit')
+def test_pyt_image_classification_quantize_overwrite_saved_model(mock_quantization_fit, mock_model_downloader):
     """
-    Tests writing an INC config file with good and bad accuracy_criterion_relative values
+    Given a valid directory for the output dir, test the quantize function with the actual Intel Neural
+    Compressor call mocked out. Tests that the model will be overwritten or not using the overwrite_model flag.
     """
-    try:
-        temp_dir = tempfile.mkdtemp()
-        nc_workspace = os.path.join(temp_dir, "nc_workspace")
-        model = model_factory.get_model('efficientnet_b0', 'pytorch')
-        with patch('tlt.datasets.image_classification.pytorch_custom_image_classification_dataset.PyTorchCustomImageClassificationDataset') as mock_dataset:  # noqa: E501
-            config_file = os.path.join(temp_dir, "config.yaml")
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-            if not valid:
-                with pytest.raises(ValueError):
-                    model.write_inc_config_file(config_file, dataset, batch_size=1, overwrite=True,
-                                                accuracy_criterion_relative=accuracy_criterion,
-                                                tuning_workspace=nc_workspace)
-            else:
-                model.write_inc_config_file(config_file, dataset, batch_size=1, overwrite=True,
-                                            accuracy_criterion_relative=accuracy_criterion,
-                                            tuning_workspace=nc_workspace)
-    finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
 
+    # tlt imports
+    from tlt.datasets.image_classification.pytorch_custom_image_classification_dataset \
+        import PyTorchCustomImageClassificationDataset
+    from tlt.models import model_factory
 
-@pytest.mark.pytorch
-@pytest.mark.parametrize('timeout,valid',
-                         [[0.1, False],
-                          [-1, False],
-                          [0, True],
-                          [60, True],
-                          ['foo', False]])
-def test_pyt_image_classification_config_file_timeout(timeout, valid):
-    """
-    Tests writing an INC config file with good and bad exit_policy_timeout values
-    """
     try:
-        temp_dir = tempfile.mkdtemp()
-        nc_workspace = os.path.join(temp_dir, "nc_workspace")
-        model = model_factory.get_model('efficientnet_b0', 'pytorch')
-        with patch('tlt.datasets.image_classification.pytorch_custom_image_classification_dataset.PyTorchCustomImageClassificationDataset') as mock_dataset:  # noqa: E501
-            config_file = os.path.join(temp_dir, "config.yaml")
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-            if not valid:
-                with pytest.raises(ValueError):
-                    model.write_inc_config_file(config_file, dataset, batch_size=1, overwrite=True,
-                                                exit_policy_timeout=timeout, tuning_workspace=nc_workspace)
-            else:
-                model.write_inc_config_file(config_file, dataset, batch_size=1, overwrite=True,
-                                            exit_policy_timeout=timeout, tuning_workspace=nc_workspace)
-    finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
-
-
-@pytest.mark.pytorch
-@pytest.mark.parametrize('max_trials,valid',
-                         [[0.1, False],
-                          [-1, False],
-                          [0, False],
-                          [1, True],
-                          [60, True],
-                          ['foo', False]])
-def test_pyt_image_classification_config_file_max_trials(max_trials, valid):
-    """
-    Tests writing an INC config file with good and bad exit_policy_max_trials values
-    """
-    try:
-        temp_dir = tempfile.mkdtemp()
-        nc_workspace = os.path.join(temp_dir, "nc_workspace")
-        model = model_factory.get_model('efficientnet_b0', 'pytorch')
-        with patch('tlt.datasets.image_classification.pytorch_custom_image_classification_dataset.PyTorchCustomImageClassificationDataset') as mock_dataset:  # noqa: E501
-            config_file = os.path.join(temp_dir, "config.yaml")
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
+        # Specify a directory for output
+        output_dir = tempfile.mkdtemp()
 
-            if not valid:
-                with pytest.raises(ValueError):
-                    model.write_inc_config_file(config_file, dataset, batch_size=1, overwrite=True,
-                                                exit_policy_max_trials=max_trials, tuning_workspace=nc_workspace)
-            else:
-                model.write_inc_config_file(config_file, dataset, batch_size=1, overwrite=True,
-                                            exit_policy_max_trials=max_trials, tuning_workspace=nc_workspace)
-    finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
+        model = model_factory.get_model(model_name='efficientnet_b0', framework='pytorch')
 
+        # Mock the dataset
+        mock_dataset = MagicMock()
+        mock_dataset.__class__ = PyTorchCustomImageClassificationDataset
+        mock_dataset.get_inc_dataloaders.return_value = 1, 2
+
+        # Method to create a dummy model.pt file in the specified directory
+        def create_dummy_file(output_dir):
+            with open(os.path.join(output_dir, 'model.pt'), 'w') as _:
+                pass
+
+        # Mock an INC quantized model that will create a dummy file when saved
+        mock_quantized_model = MagicMock()
+        mock_quantized_model.save.side_effect = create_dummy_file
+
+        # Mock the INC quantization.fit method
+        def mock_fit(**args):
+            return mock_quantized_model
+        mock_quantization_fit.side_effect = mock_fit
+
+        # Call quantize when a model does not exist
+        model.quantize(output_dir=output_dir, dataset=mock_dataset, overwrite_model=False)
+
+        # Call quantize when the model exists, but overwrite_model=True
+        model.quantize(output_dir=output_dir, dataset=mock_dataset, overwrite_model=True)
+        model.quantize(output_dir=output_dir, dataset=mock_dataset, overwrite_model=True)
 
-@pytest.mark.pytorch
-@pytest.mark.parametrize('seed,valid',
-                         [[0.1, False],
-                          [-1, False],
-                          [0, True],
-                          [1, True],
-                          [123, True],
-                          ['foo', False]])
-def test_pyt_image_classification_config_file_seed(seed, valid):
-    """
-    Tests writing an INC config file with good and bad tuning_random_seed values
-    """
-    try:
-        temp_dir = tempfile.mkdtemp()
-        nc_workspace = os.path.join(temp_dir, "nc_workspace")
-        model = model_factory.get_model('efficientnet_b0', 'pytorch')
-        with patch('tlt.datasets.image_classification.pytorch_custom_image_classification_dataset.PyTorchCustomImageClassificationDataset') as mock_dataset:  # noqa: E501
-            config_file = os.path.join(temp_dir, "config.yaml")
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
+        with pytest.raises(FileExistsError):  # Model exists, so this should be true
+            model.quantize(output_dir=output_dir, dataset=mock_dataset, overwrite_model=False)
 
-            if not valid:
-                with pytest.raises(ValueError):
-                    model.write_inc_config_file(config_file, dataset, batch_size=1, overwrite=True,
-                                                tuning_random_seed=seed, tuning_workspace=nc_workspace)
-            else:
-                model.write_inc_config_file(config_file, dataset, batch_size=1, overwrite=True,
-                                            tuning_random_seed=seed, tuning_workspace=nc_workspace)
     finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
+        if os.path.exists(output_dir):
+            shutil.rmtree(output_dir)
 
 
 @pytest.mark.pytorch
 def test_pyt_image_classification_quantization():
     """
-    Given valid directories for the saved model, output dir, and config file, test the quantization function with
-    the actual INC called mocked out.
+    Given a valid directory for output dir, test the quantization function with the actual INC called mocked out.
     """
     try:
         output_dir = tempfile.mkdtemp()
-        saved_model_dir = tempfile.mkdtemp()
-        saved_model_file = os.path.join(saved_model_dir, "model.pt")
-        Path(saved_model_file).touch()
-        dummy_config_file = os.path.join(saved_model_dir, "config.yaml")
-        Path(dummy_config_file).touch()
-
         model = model_factory.get_model('efficientnet_b0', 'pytorch')
         with patch('tlt.datasets.image_classification.pytorch_custom_image_classification_dataset.PyTorchCustomImageClassificationDataset') as mock_dataset:  # noqa: E501
-            with patch('neural_compressor.experimental.Quantization') as mock_q:
+            with patch('neural_compressor.quantization.fit') as mock_q:
                 mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-                model.quantize(saved_model_dir, output_dir, dummy_config_file)
-                mock_q.assert_called_with(dummy_config_file)
+                mock_dataset.__class__ = PyTorchCustomImageClassificationDataset
+                mock_dataset.get_inc_dataloaders.return_value = (1, 2)
+                model.quantize(output_dir, mock_dataset)
+                mock_q.assert_called_once()
     finally:
         if os.path.exists(output_dir):
             shutil.rmtree(output_dir)
-        if os.path.exists(saved_model_dir):
-            shutil.rmtree(saved_model_dir)
 
 
 @pytest.mark.pytorch
-def test_pyt_image_classification_quantization_model_does_not_exist():
+def test_pyt_image_classification_benchmark_model_does_not_exist():
     """
-    Verifies the error that gets raise if quantization or INC benchmarking is done with a model that does not exist
+    Verifies the error that gets raise if benchmarking is done with a model that does not exist
     """
     try:
         output_dir = tempfile.mkdtemp()
-        dummy_config_file = os.path.join(output_dir, "config.yaml")
-        Path(dummy_config_file).touch()
         model = model_factory.get_model('efficientnet_b0', 'pytorch')
         with patch('tlt.datasets.image_classification.pytorch_custom_image_classification_dataset.PyTorchCustomImageClassificationDataset') as mock_dataset:  # noqa: E501
             mock_dataset.dataset_dir = "/tmp/data/my_photos"
-            with patch('neural_compressor.experimental.Quantization'):
-
-                # Generate a random name that wouldn't exist
-                random_dir = str(uuid.uuid4())
-
-                # It's not a directory, so we expect an error
-                with pytest.raises(NotADirectoryError):
-                    model.quantize(random_dir, output_dir, dummy_config_file)
-
-                saved_model_dir = tempfile.mkdtemp()
-
-                # An empty directory with no saved model should also generate an error
-                with pytest.raises(FileNotFoundError):
-                    model.quantize(saved_model_dir, output_dir, dummy_config_file)
+            mock_dataset.__class__ = PyTorchCustomImageClassificationDataset
+            random_dir = str(uuid.uuid4())
+            saved_model_dir = tempfile.mkdtemp()
 
             with patch('neural_compressor.experimental.Benchmark'):
                 # It's not a directory, so we expect an error
                 with pytest.raises(NotADirectoryError):
-                    model.benchmark(random_dir, dummy_config_file, model_type='int8')
+                    model.benchmark(mock_dataset, saved_model_dir=random_dir)
 
                 # An empty directory with no saved model should also generate an error
                 with pytest.raises(FileNotFoundError):
-                    model.benchmark(saved_model_dir, dummy_config_file, model_type='int8')
+                    model.benchmark(mock_dataset, saved_model_dir=saved_model_dir)
 
     finally:
         if os.path.exists(output_dir):
             shutil.rmtree(output_dir)
         if os.path.exists(saved_model_dir):
             shutil.rmtree(saved_model_dir)
 
 
-def test_pyt_image_classification_inc_benchmark():
-    """
-    Verifies that if we have valid parameters for the saved model, config file, and mode, benchmarking is called. The
-    actual benchmarking calls to INC are mocked out.
-    """
-    try:
-        output_dir = tempfile.mkdtemp()
-        saved_model_dir = tempfile.mkdtemp()
-        saved_model_file = os.path.join(saved_model_dir, "model.pt")
-        Path(saved_model_file).touch()
-        dummy_config_file = os.path.join(saved_model_dir, "config.yaml")
-        Path(dummy_config_file).touch()
-
-        model = model_factory.get_model('efficientnet_b0', 'pytorch')
-        with patch('tlt.datasets.image_classification.pytorch_custom_image_classification_dataset.PyTorchCustomImageClassificationDataset') as mock_dataset:  # noqa: E501
-            with patch('neural_compressor.experimental.Benchmark') as mock_bench:
-                mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-                model.benchmark(saved_model_dir, dummy_config_file)
-                mock_bench.assert_called_with(dummy_config_file)
-    finally:
-        if os.path.exists(output_dir):
-            shutil.rmtree(output_dir)
-        if os.path.exists(saved_model_dir):
-            shutil.rmtree(saved_model_dir)
-
-
 @pytest.mark.pytorch
-@pytest.mark.parametrize('mode,valid',
-                         [['abc', False],
-                          [1, False],
-                          [0, False],
-                          ['performance', True],
-                          ['accuracy', True]])
-def test_pyt_image_classification_inc_benchmark_mode(mode, valid):
+def test_pyt_image_classification_inc_benchmark():
     """
-    Checks error handling for the benchmarking mode
+    Verifies that if we have a valid model and dataset, benchmarking is called. The actual benchmarking calls to INC
+    are mocked out.
     """
-    output_dir = tempfile.mkdtemp()
-    model = model_factory.get_model(model_name='efficientnet_b0', framework='pytorch')
-    dataset = dataset_factory.get_dataset(dataset_dir=dataset_dir,
-                                          use_case='image_classification',
-                                          framework='pytorch',
-                                          dataset_name='CIFAR10',
-                                          dataset_catalog='torchvision')
-    batch_size = 456
-    dataset.preprocess(model.image_size, batch_size=batch_size)
-    dataset.shuffle_split(train_pct=0.05, val_pct=0.05, shuffle_files=False)
-    model.train(dataset, output_dir=output_dir, epochs=1)
-    try:
-        saved_model_dir = tempfile.mkdtemp()
-        dummy_config_file = os.path.join(saved_model_dir, "config.yaml")
-        Path(dummy_config_file).touch()
-        import dill
-        import torch
-        model_copy = dill.dumps(model._model)
-        torch.save(model_copy, os.path.join(saved_model_dir, 'model.pt'))
-        with patch('tlt.datasets.image_classification.pytorch_custom_image_classification_dataset.PyTorchCustomImageClassificationDataset') as mock_dataset:  # noqa: E501
-            with patch('neural_compressor.experimental.Benchmark') as mock_bench:
-                mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-                if not valid:
-                    with pytest.raises(ValueError):
-                        model.benchmark(saved_model_dir, dummy_config_file, mode=mode)
-                else:
-                    model.benchmark(saved_model_dir, dummy_config_file, mode=mode)
-                    mock_bench.assert_called_with(dummy_config_file)
-    finally:
-        if os.path.exists(output_dir):
-            shutil.rmtree(output_dir)
-        if os.path.exists(saved_model_dir):
-            shutil.rmtree(saved_model_dir)
+    model = model_factory.get_model('efficientnet_b0', 'pytorch')
+    with patch('tlt.datasets.image_classification.pytorch_custom_image_classification_dataset.PyTorchCustomImageClassificationDataset') as mock_dataset:  # noqa: E501
+        with patch('neural_compressor.benchmark.fit') as mock_bench:
+            mock_dataset.dataset_dir = "/tmp/data/my_photos"
+            mock_dataset.__class__ = PyTorchCustomImageClassificationDataset
+            mock_dataset.get_inc_dataloaders.return_value = (1, 2)
+            model.benchmark(mock_dataset)
+            mock_bench.assert_called_once()
```

## tests/pytorch_tests/unit/test_models.py

```diff
@@ -17,37 +17,45 @@
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import pytest
 import numpy
 
-from unittest.mock import MagicMock, patch
+from unittest import mock
+from unittest.mock import ANY, MagicMock, patch
 from sklearn import decomposition
 
 from tlt.models import model_factory
 from tlt.utils.types import FrameworkType, UseCaseType
-from tlt.models.image_anomaly_detection.pytorch_image_anomaly_detection_model import extract_features, pca, get_feature_extraction_model  # noqa: E501
 
+try:
+    from tlt.models.image_anomaly_detection.pytorch_image_anomaly_detection_model import extract_features, pca, get_feature_extraction_model  # noqa: E501
+except ModuleNotFoundError:
+    print("WARNING: Unable to import PytorchImageAnomolyDetectionModel. Pytorch may not be installed")
+
+# This is necessary to protect from import errors when testing in a pytorch only environment
+# True when imports are successful, False when imports are unsuccessful
+torch_env = True
 
 try:
     # Do torch specific imports in a try/except to prevent pytest test loading from failing when running in a TF env
     import torch
     import torch.nn as nn
 except ModuleNotFoundError:
     print("WARNING: Unable to import torch. Torch may not be installed")
-
+    torch_env = False
 
 try:
     # Do torch specific imports in a try/except to prevent pytest test loading from failing when running in a TF env
     from tlt.models.image_classification.torchvision_image_classification_model import TorchvisionImageClassificationModel  # noqa: E501
     from tlt.datasets.image_classification.torchvision_image_classification_dataset import TorchvisionImageClassificationDataset   # noqa: E501
     from tlt.datasets.image_classification.pytorch_custom_image_classification_dataset import \
         PyTorchCustomImageClassificationDataset  # noqa: E501
-    from tlt.models.text_classification.hf_text_classification_model import HFTextClassificationModel  # noqa: E501
+    from tlt.models.text_classification.pytorch_hf_text_classification_model import PyTorchHFTextClassificationModel  # noqa: E501
 except ModuleNotFoundError:
     print("WARNING: Unable to import TorchvisionImageClassificationModel and TorchvisionImageClassificationDataset. "
           "Torch may not be installed")
 
 try:
     from tlt.models.image_anomaly_detection.torchvision_image_anomaly_detection_model import \
         TorchvisionImageAnomalyDetectionModel
@@ -207,30 +215,40 @@
             assert return_val == expected_return_value_history_no_val
             mock_model.eval.assert_not_called()
 
 
 @pytest.mark.pytorch
 def test_bert_train():
     model = model_factory.get_model('distilbert-base-uncased', 'pytorch')
-    assert type(model) == HFTextClassificationModel
+    assert type(model) == PyTorchHFTextClassificationModel
     with patch('tlt.datasets.text_classification.hf_text_classification_dataset.HFTextClassificationDataset') as mock_dataset:  # noqa: E501
         mock_dataset.__class__ = HFTextClassificationDataset
         mock_dataset.train_subset = ['1', '2', '3']
         mock_dataset.validation_subset = ['4', '5', '6']
         expected_return_value_history_no_val = {'Acc': [0.0], 'Loss': [0.0]}
         expected_return_value_history_val = {'Acc': [0.0], 'Loss': [0.0], 'Val Acc': [0.0], 'Val Loss': [0.0]}
 
         # Scenario 1: Call train without validation
         return_val = model.train(mock_dataset, output_dir="/tmp/output/pytorch", do_eval=False, lr_decay=False)
-        assert return_val == expected_return_value_history_no_val
+        assert return_val['Acc'] == expected_return_value_history_no_val['Acc']
+        assert return_val['Loss'] == expected_return_value_history_no_val['Loss']
+        assert 'train_runtime' in return_val
+        assert 'train_samples_per_second' in return_val
+        assert 'Val Acc' not in return_val
+        assert 'Val Loss' not in return_val
 
         # Scenario 2: Call train with validation
         mock_dataset.validation_loader.__class__ = HFTextClassificationDataset
         return_val = model.train(mock_dataset, output_dir="/tmp/output/pytorch", do_eval=True, lr_decay=False)
-        assert return_val == expected_return_value_history_val
+        assert return_val['Acc'] == expected_return_value_history_val['Acc']
+        assert return_val['Loss'] == expected_return_value_history_val['Loss']
+        assert return_val['Val Acc'] == expected_return_value_history_val['Val Acc']
+        assert return_val['Val Loss'] == expected_return_value_history_val['Val Loss']
+        assert 'train_runtime' in return_val
+        assert 'train_samples_per_second' in return_val
 
 
 @pytest.mark.pytorch
 def test_resnet50_anomaly_extract_pca():
     model = model_factory.get_model(model_name="resnet50", framework="pytorch", use_case="anomaly_detection")
     assert type(model) == TorchvisionImageAnomalyDetectionModel
 
@@ -247,80 +265,139 @@
     if not numpy.isnan(data_mats_orig).any():
         with torch.no_grad():
             components = pca(data_mats_orig, 0.97)
         assert type(components) == decomposition._pca.PCA
         assert components.n_components == 0.97
 
 
-@pytest.mark.pytorch
-@pytest.mark.parametrize('model_name,use_case,dataset_type,optimizer,loss',
-                         [['efficientnet_b0', 'image_classification', PyTorchCustomImageClassificationDataset,
-                           torch.optim.Adam, torch.nn.L1Loss],
-                          ['resnet18', 'image_classification', PyTorchCustomImageClassificationDataset,
-                           torch.optim.AdamW, torch.nn.MSELoss],
-                          ['custom', 'image_classification', PyTorchCustomImageClassificationDataset,
-                           torch.optim.SGD, torch.nn.L1Loss],
-                          ['distilbert-base-uncased', 'text_classification', HFTextClassificationDataset,
-                           torch.optim.Adam, torch.nn.MSELoss]])
-def test_pytorch_optimizer_loss(model_name, use_case, dataset_type, optimizer, loss):
-    """
-    Tests initializing and training a model with configurable optimizers and loss functions
-    """
-
-    # Define a model
-    class Net(nn.Module):
-        def __init__(self):
-            super().__init__()
-            self.conv1 = nn.Conv2d(3, 6, 5)
-            self.pool = nn.MaxPool2d(2, 2)
-            self.conv2 = nn.Conv2d(6, 16, 5)
-            self.fc1 = nn.Linear(16 * 5 * 5, 120)
-            self.fc2 = nn.Linear(120, 84)
-            self.fc3 = nn.Linear(84, 3)
-
-        def forward(self, x):
-            x = self.pool(nn.functional.relu(self.conv1(x)))
-            x = self.pool(nn.functional.relu(self.conv2(x)))
-            x = torch.flatten(x, 1)
-            x = nn.functional.relu(self.fc1(x))
-            x = nn.functional.relu(self.fc2(x))
-            x = self.fc3(x)
-            return x
+# This is necessary to protect from import errors when testing in a pytorch only environment
+if torch_env:
+    @pytest.mark.pytorch
+    @pytest.mark.parametrize('model_name,use_case,dataset_type,optimizer,loss',
+                             [['efficientnet_b0', 'image_classification', PyTorchCustomImageClassificationDataset,
+                               torch.optim.Adam, torch.nn.L1Loss],
+                              ['resnet18', 'image_classification', PyTorchCustomImageClassificationDataset,
+                               torch.optim.AdamW, torch.nn.MSELoss],
+                              ['custom', 'image_classification', PyTorchCustomImageClassificationDataset,
+                               torch.optim.SGD, torch.nn.L1Loss],
+                              ['distilbert-base-uncased', 'text_classification', HFTextClassificationDataset,
+                               torch.optim.Adam, torch.nn.MSELoss]])
+    def test_pytorch_optimizer_loss(model_name, use_case, dataset_type, optimizer, loss):
+        """
+        Tests initializing and training a model with configurable optimizers and loss functions
+        """
+
+        # Define a model
+        class Net(nn.Module):
+            def __init__(self):
+                super().__init__()
+                self.conv1 = nn.Conv2d(3, 6, 5)
+                self.pool = nn.MaxPool2d(2, 2)
+                self.conv2 = nn.Conv2d(6, 16, 5)
+                self.fc1 = nn.Linear(16 * 5 * 5, 120)
+                self.fc2 = nn.Linear(120, 84)
+                self.fc3 = nn.Linear(84, 3)
+
+            def forward(self, x):
+                x = self.pool(nn.functional.relu(self.conv1(x)))
+                x = self.pool(nn.functional.relu(self.conv2(x)))
+                x = torch.flatten(x, 1)
+                x = nn.functional.relu(self.fc1(x))
+                x = nn.functional.relu(self.fc2(x))
+                x = self.fc3(x)
+                return x
+
+        net = Net()
+
+        if model_name == 'custom':
+            model = model_factory.load_model(model_name, net, 'pytorch', use_case, optimizer=optimizer, loss=loss)
+        else:
+            model = model_factory.get_model(model_name, 'pytorch', optimizer=optimizer, loss=loss)
+
+        model._generate_checkpoints = False
+        model._fit = MagicMock()
+        assert model._optimizer_class == optimizer
+        assert model._loss_class == loss
+        assert type(model._loss) == loss
+
+        mock_dataset = MagicMock()
+        mock_dataset.__class__ = dataset_type
+        mock_dataset.class_names = ['a', 'b', 'c']
+        mock_dataset.train_subset = [1, 2, 3]
+        mock_dataset.validation_subset = [4, 5, 6]
+
+        # Train is called and optimizer and loss objects should match the input types
+        model.train(mock_dataset, output_dir="/tmp/output/pytorch")
+        assert model._optimizer_class == optimizer
+        assert type(model._optimizer) == optimizer
+        assert model._loss_class == loss
+        assert type(model._loss) == loss
+
+
+# This is necessary to protect from import errors when testing in a pytorch only environment
+if torch_env:
+    @pytest.mark.pytorch
+    @pytest.mark.parametrize('model_name,optimizer',
+                             [['efficientnet_b0', 1],
+                              ['resnet18', 'foo'],
+                              ['distilbert-base-uncased', torch.nn.MSELoss]])
+    def test_pytorch_optimizer_wrong_type(model_name, optimizer):
+        """
+        Tests that an exception is thrown when the input optimizer is the wrong type
+        """
+        with pytest.raises(TypeError):
+            model_factory.get_model(model_name, 'pytorch', optimizer=optimizer)
 
-    net = Net()
 
-    if model_name == 'custom':
-        model = model_factory.load_model(model_name, net, 'pytorch', use_case, optimizer=optimizer, loss=loss)
-    else:
-        model = model_factory.get_model(model_name, 'pytorch', optimizer=optimizer, loss=loss)
+@pytest.mark.pytorch
+@patch('tlt.models.text_classification.pytorch_hf_text_classification_model.torch.optim.AdamW')
+@patch('tlt.models.text_classification.pytorch_hf_text_classification_model.Trainer')
+@patch('tlt.models.text_classification.pytorch_hf_text_classification_model.ModelDownloader')
+def test_pytorch_hf_text_classification_trainer_return_values(mock_downloader, mock_trainer, mock_optimizer):
+    """
+    Tests the PyTorch Text Classification model with the Hugging Face Trainer to verify that the value returned
+    by Trainer.train() is returned by the model.train() method
+    """
 
-    model._generate_checkpoints = False
-    model._fit = MagicMock()
-    assert model._optimizer_class == optimizer
-    assert model._loss_class == loss
-    assert type(model._loss) == loss
+    model = model_factory.get_model(model_name='bert-base-cased', framework='pytorch')
 
     mock_dataset = MagicMock()
-    mock_dataset.__class__ = dataset_type
+    mock_dataset.__class__ = HFTextClassificationDataset
     mock_dataset.class_names = ['a', 'b', 'c']
     mock_dataset.train_subset = [1, 2, 3]
     mock_dataset.validation_subset = [4, 5, 6]
 
-    # Train is called and optimizer and loss objects should match the input types
-    model.train(mock_dataset, output_dir="/tmp/output/pytorch")
-    assert model._optimizer_class == optimizer
-    assert type(model._optimizer) == optimizer
-    assert model._loss_class == loss
-    assert type(model._loss) == loss
+    expected_value = "a"
+
+    mock_trainer().train.return_value = expected_value
+
+    return_val = model.train(mock_dataset, output_dir="/tmp", use_trainer=True, seed=10)
+    assert mock_trainer().train.call_count == 1
+
+    assert return_val == expected_value
 
 
 @pytest.mark.pytorch
-@pytest.mark.parametrize('model_name,optimizer',
-                         [['efficientnet_b0', 1],
-                          ['resnet18', 'foo'],
-                          ['distilbert-base-uncased', torch.nn.MSELoss]])
-def test_pytorch_optimizer_wrong_type(model_name, optimizer):
+@patch('tlt.models.text_classification.pytorch_hf_text_classification_model.torch.optim.AdamW')
+@patch('tlt.models.text_classification.pytorch_hf_text_classification_model.Trainer')
+@patch('tlt.models.text_classification.pytorch_hf_text_classification_model.ModelDownloader')
+def test_pytorch_hf_text_classification_trainer_without_val_subset(mock_downloader, mock_trainer, mock_optimizer):
     """
-    Tests that an exception is thrown when the input optimizer is the wrong type
+    Tests the PyTorch Text Classification model with the Hugging Face Trainer is able to run evaluation with a test
+    subset when a validation subset does not exist.
     """
-    with pytest.raises(TypeError):
-        model_factory.get_model(model_name, 'pytorch', optimizer=optimizer)
+
+    model = model_factory.get_model(model_name='bert-base-cased', framework='pytorch')
+
+    mock_dataset = MagicMock()
+    mock_dataset.__class__ = HFTextClassificationDataset
+    mock_dataset.class_names = ['a', 'b', 'c']
+    mock_dataset.train_subset = [1, 2, 3]
+    mock_dataset.test_subset = [4, 5, 6]
+    type(mock_dataset).validation_subset = mock.PropertyMock(side_effect=ValueError)
+
+    with pytest.raises(ValueError):
+        mock_dataset.validation_subset
+
+    model.train(mock_dataset, output_dir="/tmp", use_trainer=True, seed=10)
+    mock_trainer.assert_called_with(model=model._model, args=ANY, train_dataset=[1, 2, 3], eval_dataset=[4, 5, 6],
+                                    compute_metrics=ANY, tokenizer=ANY)
```

## tests/tensorflow_tests/test_image_classification.py

```diff
@@ -19,33 +19,43 @@
 #
 
 import os
 import pytest
 import shutil
 import tempfile
 import numpy as np
-from tensorflow import keras
+
 
 from tlt.datasets import dataset_factory
 from tlt.models import model_factory
 from tlt.utils.file_utils import download_and_extract_tar_file
 from unittest.mock import MagicMock, patch
 
 from tlt.datasets.image_classification.image_classification_dataset import ImageClassificationDataset
 
+# This is necessary to protect from import errors when testing in a tensorflow only environment
+keras_env = True
+
+try:
+    from tensorflow import keras
+except ModuleNotFoundError:
+    print("WARNING: Unable to import Keras. Tensorflow may not be installed")
+    keras_env = False
+
 
 @pytest.mark.integration
 @pytest.mark.tensorflow
-@pytest.mark.parametrize('model_name,dataset_name,train_accuracy,retrain_accuracy,extra_layers,correct_num_layers',
-                         [['efficientnet_b0', 'tf_flowers', 0.3125, 0.53125, None, 2],
-                          ['resnet_v1_50', 'tf_flowers', 0.40625, 0.59375, None, 2],
-                          ['efficientnet_b0', 'tf_flowers', 0.8125, 0.96875, [1024, 512], 4],
-                          ['ResNet50', 'tf_flowers', 0.40625, 0.15625, None, 4]])
+@pytest.mark.parametrize('model_name,dataset_name,train_accuracy,retrain_accuracy,extra_layers,correct_num_layers,'
+                         'test_optimization',
+                         [['efficientnet_b0', 'tf_flowers', 0.3125, 0.53125, None, 2, False],
+                          ['resnet_v1_50', 'tf_flowers', 0.40625, 0.59375, None, 2, True],
+                          ['efficientnet_b0', 'tf_flowers', 0.8125, 0.96875, [1024, 512], 4, False],
+                          ['ResNet50', 'tf_flowers', 0.34375, 0.625, None, 4, True]])
 def test_tf_image_classification(model_name, dataset_name, train_accuracy, retrain_accuracy, extra_layers,
-                                 correct_num_layers):
+                                 correct_num_layers, test_optimization):
     """
     Tests basic transfer learning functionality for TensorFlow image classification models using TF Datasets
     """
     framework = 'tensorflow'
     use_case = 'image_classification'
     output_dir = tempfile.mkdtemp()
 
@@ -53,15 +63,15 @@
     dataset = dataset_factory.get_dataset('/tmp/data', use_case, framework, dataset_name,
                                           'tf_datasets', split=["train[:5%]"], seed=10, shuffle_files=False)
 
     # Get the model
     model = model_factory.get_model(model_name, framework)
 
     # Preprocess the dataset
-    dataset.preprocess(model.image_size, 32)
+    dataset.preprocess(model.image_size, 32, preprocessor=model.preprocessor)
     dataset.shuffle_split(shuffle_files=False)
 
     # Evaluate before training
     pretrained_metrics = model.evaluate(dataset)
     assert len(pretrained_metrics) > 0
 
     # Train
@@ -99,127 +109,127 @@
     reload_model.load_from_directory(saved_model_dir)
 
     # Evaluate
     reload_metrics = reload_model.evaluate(dataset)
     np.testing.assert_almost_equal(reload_metrics, trained_metrics)
 
     # Optimize the graph
-    if model_name in ['resnet_v1_50', 'ResNet50']:
-        optimized_model_dir = os.path.join(output_dir, "optimized")
-        os.makedirs(optimized_model_dir, exist_ok=True)
-        model.optimize_graph(saved_model_dir, optimized_model_dir)
-        assert os.path.isfile(os.path.join(optimized_model_dir, "saved_model.pb"))
-
-    # Test generating an Intel Neural Compressor config file (not implemented yet for TFDS)
-    inc_config_file_path = os.path.join(output_dir, "tf_{}.yaml".format(model_name))
-    with pytest.raises(NotImplementedError):
-        model.write_inc_config_file(inc_config_file_path, dataset, batch_size=32, tuning_workspace=output_dir)
+    if test_optimization:
+        inc_output_dir = os.path.join(output_dir, "optimized")
+        os.makedirs(inc_output_dir, exist_ok=True)
+        model.optimize_graph(inc_output_dir)
+        assert os.path.isfile(os.path.join(inc_output_dir, "saved_model.pb"))
 
     # Retrain from checkpoints and verify that we have better accuracy than the original training
     retrain_model = model_factory.load_model(model_name, saved_model_dir, framework, use_case)
     retrain_history = retrain_model.train(dataset, output_dir=output_dir, epochs=1, initial_checkpoints=checkpoint_dir,
                                           shuffle_files=False, seed=10, do_eval=False)
     np.testing.assert_almost_equal(retrain_history['acc'], [retrain_accuracy])
 
     # Delete the temp output directory
     if os.path.exists(output_dir) and os.path.isdir(output_dir):
         shutil.rmtree(output_dir)
 
 
-@pytest.mark.tensorflow
-def test_tf_image_classification_custom_model():
-    """
-    Tests basic transfer learning functionality for a custom TensorFlow image classification model using TF Datasets
-    """
-    framework = 'tensorflow'
-    use_case = 'image_classification'
-    output_dir = tempfile.mkdtemp()
-    model_name = 'custom_model'
-    image_size = 227
-
-    # Get the dataset
-    dataset = dataset_factory.get_dataset('/tmp/data', use_case, framework, 'tf_flowers',
-                                          'tf_datasets', split=["train[:5%]"], shuffle_files=False)
-
-    # Define a custom model
-    alexnet = keras.models.Sequential([
-        keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), activation='relu',
-                            input_shape=(image_size, image_size, 3)),
-        keras.layers.BatchNormalization(),
-        keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),
-        keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding="same"),
-        keras.layers.BatchNormalization(),
-        keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),
-        keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
-        keras.layers.BatchNormalization(),
-        keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
-        keras.layers.BatchNormalization(),
-        keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
-        keras.layers.BatchNormalization(),
-        keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),
-        keras.layers.Flatten(),
-        keras.layers.Dense(4096, activation='relu'),
-        keras.layers.Dropout(0.5),
-        keras.layers.Dense(4096, activation='relu'),
-        keras.layers.Dropout(0.5),
-        keras.layers.Dense(5, activation='softmax')
-    ])
-
-    model = model_factory.load_model(model_name=model_name, model=alexnet, framework=framework, use_case=use_case)
-    assert model.num_classes == 5
-    assert model._image_size == 227
+# This is necessary to protect from import errors when testing in a tensorflow only environment
+if keras_env:
+    @pytest.mark.integration
+    @pytest.mark.tensorflow
+    def test_tf_image_classification_custom_model():
+        """
+        Tests basic transfer learning functionality for a custom TensorFlow image classification model using TF Datasets
+        """
+        framework = 'tensorflow'
+        use_case = 'image_classification'
+        output_dir = tempfile.mkdtemp()
+        model_name = 'custom_model'
+        image_size = 227
 
-    # Preprocess the dataset
-    dataset.preprocess(image_size, 32)
-    dataset.shuffle_split(seed=10)
+        # Get the dataset
+        dataset = dataset_factory.get_dataset('/tmp/data', use_case, framework, 'tf_flowers',
+                                              'tf_datasets', split=["train[:5%]"], shuffle_files=False)
 
-    # Train
-    history = model.train(dataset, output_dir=output_dir, epochs=1, shuffle_files=False, seed=10)
-    assert history is not None
+        # Define a custom model
+        alexnet = keras.models.Sequential([
+            keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), activation='relu',
+                                input_shape=(image_size, image_size, 3)),
+            keras.layers.BatchNormalization(),
+            keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),
+            keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding="same"),
+            keras.layers.BatchNormalization(),
+            keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),
+            keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
+            keras.layers.BatchNormalization(),
+            keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
+            keras.layers.BatchNormalization(),
+            keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
+            keras.layers.BatchNormalization(),
+            keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),
+            keras.layers.Flatten(),
+            keras.layers.Dense(4096, activation='relu'),
+            keras.layers.Dropout(0.5),
+            keras.layers.Dense(4096, activation='relu'),
+            keras.layers.Dropout(0.5),
+            keras.layers.Dense(5, activation='softmax')
+        ])
+
+        model = model_factory.load_model(model_name=model_name, model=alexnet, framework=framework, use_case=use_case)
+        assert model.num_classes == 5
+        assert model._image_size == 227
+
+        # Preprocess the dataset
+        dataset.preprocess(image_size, 32)
+        dataset.shuffle_split(seed=10)
 
-    # Verify that checkpoints were generated
-    checkpoint_dir = os.path.join(output_dir, "{}_checkpoints".format(model_name))
-    assert os.path.isdir(checkpoint_dir)
-    assert len(os.listdir(checkpoint_dir))
+        # Train
+        history = model.train(dataset, output_dir=output_dir, epochs=1, shuffle_files=False, seed=10)
+        assert history is not None
 
-    # Evaluate
-    trained_metrics = model.evaluate(dataset)
-    assert trained_metrics is not None
+        # Verify that checkpoints were generated
+        checkpoint_dir = os.path.join(output_dir, "{}_checkpoints".format(model_name))
+        assert os.path.isdir(checkpoint_dir)
+        assert len(os.listdir(checkpoint_dir))
 
-    # Predict with a batch
-    images, labels = dataset.get_batch()
-    predictions = model.predict(images)
-    assert len(predictions) == 32
-    probabilities = model.predict(images, return_type='probabilities')
-    assert probabilities.shape == (32, 5)  # tf_flowers has 5 classes
-    np.testing.assert_almost_equal(np.sum(probabilities), np.float32(32), decimal=4)
+        # Evaluate
+        trained_metrics = model.evaluate(dataset)
+        assert trained_metrics is not None
 
-    # Export the saved model
-    saved_model_dir = model.export(output_dir)
-    assert os.path.isdir(saved_model_dir)
-    assert os.path.isfile(os.path.join(saved_model_dir, "saved_model.pb"))
+        # Predict with a batch
+        images, labels = dataset.get_batch()
+        predictions = model.predict(images)
+        assert len(predictions) == 32
+        probabilities = model.predict(images, return_type='probabilities')
+        assert probabilities.shape == (32, 5)  # tf_flowers has 5 classes
+        np.testing.assert_almost_equal(np.sum(probabilities), np.float32(32), decimal=4)
 
-    # Reload the saved model
-    reload_model = model_factory.load_model(model_name, saved_model_dir, framework, use_case)
+        # Export the saved model
+        saved_model_dir = model.export(output_dir)
+        assert os.path.isdir(saved_model_dir)
+        assert os.path.isfile(os.path.join(saved_model_dir, "saved_model.pb"))
 
-    # Evaluate
-    reload_metrics = reload_model.evaluate(dataset)
-    np.testing.assert_almost_equal(reload_metrics, trained_metrics)
+        # Reload the saved model
+        reload_model = model_factory.load_model(model_name, saved_model_dir, framework, use_case)
 
-    # Retrain from checkpoints and verify that we have better accuracy than the original training
-    retrain_model = model_factory.load_model(model_name, saved_model_dir, framework, use_case)
-    retrain_history = retrain_model.train(dataset, output_dir=output_dir, epochs=1, initial_checkpoints=checkpoint_dir,
-                                          shuffle_files=False, seed=10)
-    assert retrain_history is not None
+        # Evaluate
+        reload_metrics = reload_model.evaluate(dataset)
+        np.testing.assert_almost_equal(reload_metrics, trained_metrics)
 
-    # Delete the temp output directory
-    if os.path.exists(output_dir) and os.path.isdir(output_dir):
-        shutil.rmtree(output_dir)
+        # Retrain from checkpoints and verify that we have better accuracy than the original training
+        retrain_model = model_factory.load_model(model_name, saved_model_dir, framework, use_case)
+        retrain_history = retrain_model.train(dataset, output_dir=output_dir, epochs=1,
+                                              initial_checkpoints=checkpoint_dir, shuffle_files=False, seed=10)
+        assert retrain_history is not None
+
+        # Delete the temp output directory
+        if os.path.exists(output_dir) and os.path.isdir(output_dir):
+            shutil.rmtree(output_dir)
 
 
+@pytest.mark.integration
+@pytest.mark.tensorflow
 class TestImageClassificationCustomDataset:
     """
     Tests for TensorFlow image classification using a custom dataset using the flowers dataset
     """
     @classmethod
     def setup_class(cls):
         temp_dir = tempfile.mkdtemp(dir='/tmp/data')
@@ -237,20 +247,19 @@
     def teardown_class(cls):
         # remove directories
         for dir in [cls._output_dir, cls._temp_dir]:
             if os.path.exists(dir):
                 print("Deleting test directory:", dir)
                 shutil.rmtree(dir)
 
-    @pytest.mark.integration
-    @pytest.mark.tensorflow
-    @pytest.mark.parametrize('model_name,train_accuracy,retrain_accuracy',
-                             [['efficientnet_b0', 0.9333333, 1.0],
-                              ['resnet_v1_50', 1.0, 1.0]])
-    def test_custom_dataset_workflow(self, model_name, train_accuracy, retrain_accuracy):
+    @pytest.mark.parametrize('model_name,train_accuracy,retrain_accuracy,test_inc',
+                             [['efficientnet_b0', 0.9333333, 1.0, False],
+                              ['resnet_v1_50', 1.0, 1.0, True],
+                              ['resnet_v2_50', 1.0, 1.0, False]])
+    def test_custom_dataset_workflow(self, model_name, train_accuracy, retrain_accuracy, test_inc):
         """
         Tests the full workflow for TF image classification using a custom dataset
         """
         framework = 'tensorflow'
         use_case = 'image_classification'
 
         # Get the dataset
@@ -259,15 +268,15 @@
         assert ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] == dataset.class_names
 
         # Get the model
         model = model_factory.get_model(model_name, framework)
 
         # Preprocess the dataset and split to get small subsets for training and validation
         dataset.shuffle_split(train_pct=0.1, val_pct=0.1, shuffle_files=False)
-        dataset.preprocess(model.image_size, 32)
+        dataset.preprocess(model.image_size, 32, preprocessor=model.preprocessor)
 
         # Train for 1 epoch
         history = model.train(dataset, output_dir=self._output_dir, epochs=1, shuffle_files=False, seed=10,
                               do_eval=False)
         assert history is not None
         np.testing.assert_almost_equal(history['acc'], [train_accuracy])
 
@@ -300,26 +309,21 @@
         # Retrain from checkpoints and verify that we have better accuracy than the original training
         retrain_model = model_factory.get_model(model_name, framework)
         retrain_history = retrain_model.train(dataset, output_dir=self._output_dir, epochs=1,
                                               initial_checkpoints=checkpoint_dir, shuffle_files=False, seed=10,
                                               do_eval=False)
         np.testing.assert_almost_equal(retrain_history['acc'], [retrain_accuracy])
 
-        # Test benchmarking, quantization, and graph optimization with ResNet50
-        if model_name == "resnet_v1_50":
-            inc_config_file_path = os.path.join(self._output_dir, "tf_{}.yaml".format(model_name))
-            nc_workspace = os.path.join(self._output_dir, "nc_workspace")
-            model.write_inc_config_file(inc_config_file_path, dataset, batch_size=32, accuracy_criterion_relative=0.1,
-                                        exit_policy_max_trials=10, exit_policy_timeout=0, tuning_workspace=nc_workspace)
-
-            quantization_output = os.path.join(self._output_dir, "quantized", model_name)
-            os.makedirs(quantization_output)
-            model.quantize(saved_model_dir, quantization_output, inc_config_file_path)
-            assert os.path.exists(os.path.join(quantization_output, "saved_model.pb"))
-            model.benchmark(quantization_output, inc_config_file_path)
+        # Test benchmarking, quantization
+        if test_inc:
+            inc_output_dir = os.path.join(self._output_dir, "quantized", model_name)
+            os.makedirs(inc_output_dir)
+            model.quantize(inc_output_dir, dataset=dataset)
+            assert os.path.exists(os.path.join(inc_output_dir, "saved_model.pb"))
+            model.benchmark(saved_model_dir=inc_output_dir, dataset=dataset)
 
 
 @pytest.mark.integration
 @pytest.mark.tensorflow
 @pytest.mark.parametrize('model_name,dataset_name,epochs,learning_rate,do_eval,early_stopping,lr_decay,accuracy,\
                           val_accuracy,lr_final',
                          [['efficientnet_b0', 'tf_flowers', 4, 0.001, False, False, False, 0.9, None, 0.001],
@@ -408,7 +412,118 @@
         # add basic preprocessing with add aug set to 'zoom'
         mock_dataset.preprocess(model.image_size, 32, add_aug=[add_aug])
         mock_dataset.shuffle_split(shuffle_files=False)
 
         # Test train without eval
         return_val = model.train(mock_dataset, output_dir="/tmp/output", do_eval=False)
         assert return_val == expected_return_value
+
+
+@pytest.mark.tensorflow
+def test_custom_callback():
+    """
+    Tests passing custom callbacks to the TensorFlow image classification train, evaluate, and predict functions.
+    """
+    model = model_factory.get_model('efficientnet_b0', 'tensorflow')
+
+    with patch('tlt.models.image_classification.tfhub_image_classification_model.'
+               'TFHubImageClassificationModel._get_hub_model') as mock_get_hub_model:
+        mock_dataset = MagicMock()
+        mock_dataset.__class__ = ImageClassificationDataset
+        mock_dataset.validation_subset = [1, 2, 3]
+
+        mock_dataset.class_names = ['a', 'b', 'c']
+        mock_model = MagicMock()
+        expected_return_value = {"result": True}
+        mock_history = MagicMock()
+        mock_history.history = expected_return_value
+
+        class TestCallbackMethod(keras.callbacks.Callback):
+            pass
+
+        test_callback = TestCallbackMethod()
+
+        def mock_fit(dataset, epochs, shuffle, callbacks, validation_data=None):
+            # We should have more than one callback since TLT them and we added a custom one
+            assert isinstance(callbacks, list)
+            assert len(callbacks) > 1
+
+            # We should have one callback that's our test callback
+            assert (len([x for x in callbacks if x.__class__.__name__ == 'TestCallbackMethod']) == 1)
+
+            return mock_history
+
+        def mock_evaluate(dataset, callbacks=None):
+            assert isinstance(callbacks, list)
+            assert len(callbacks) == 1
+            assert (len([x for x in callbacks if x.__class__.__name__ == 'TestCallbackMethod']) == 1)
+            return [.98, 0.13]
+
+        def mock_predict(input_samples, callbacks=None):
+            assert isinstance(callbacks, list)
+            assert len(callbacks) == 1
+            assert (len([x for x in callbacks if x.__class__.__name__ == 'TestCallbackMethod']) == 1)
+            return [1.0, 0.5]
+
+        mock_model.fit.side_effect = mock_fit
+        mock_model.evaluate.side_effect = mock_evaluate
+        mock_model.predict.side_effect = mock_predict
+        mock_get_hub_model.return_value = mock_model
+
+        # Test custom callback as a single item, list, or tuple
+        custom_callbacks = [test_callback, [test_callback], (test_callback)]
+
+        for custom_callback in custom_callbacks:
+            # Test train with custom callback
+            return_val = model.train(mock_dataset, output_dir="/tmp/output", do_eval=False, callbacks=custom_callback)
+            assert return_val == expected_return_value
+            mock_model.fit.assert_called_once()
+            mock_model.fit.reset_mock()
+
+            # Test evaluate with custom callback
+            model.evaluate(mock_dataset, callbacks=custom_callback)
+            mock_model.evaluate.assert_called_once()
+            mock_model.evaluate.reset_mock()
+
+            # Test predict with custom callback
+            model.predict([], callbacks=custom_callback)
+            mock_model.predict.assert_called_once()
+            mock_model.predict.reset_mock()
+
+
+@pytest.mark.tensorflow
+@patch('tlt.models.image_classification.tfhub_image_classification_model.TFHubImageClassificationModel._get_hub_model')
+def test_invalid_callback_types(mock_get_hub_model):
+    """
+    Tests passing custom callbacks of the wrong type to train, predict, and evaluate
+    """
+    model = model_factory.get_model('efficientnet_b0', 'tensorflow')
+
+    mock_dataset = MagicMock()
+    mock_dataset.__class__ = ImageClassificationDataset
+    mock_dataset.validation_subset = [1, 2, 3]
+
+    mock_dataset.class_names = ['a', 'b', 'c']
+    mock_model = MagicMock()
+    expected_return_value = {"result": True}
+    mock_history = MagicMock()
+    mock_history.history = expected_return_value
+
+    class TestCallbackMethod(keras.callbacks.Callback):
+        pass
+
+    good_callback = TestCallbackMethod()
+    bad_callback = 1
+
+    mock_model.fit = MagicMock()
+    mock_model.evaluate = MagicMock()
+    mock_model.predict = MagicMock()
+    mock_get_hub_model.return_value = mock_model
+
+    with pytest.raises(TypeError, match="Callbacks must be tf.keras.callbacks.Callback instances"):
+        model.train(mock_dataset, output_dir="/tmp/output", do_eval=False, callbacks=[good_callback, bad_callback])
+
+    with pytest.raises(TypeError, match="Callbacks must be tf.keras.callbacks.Callback instances"):
+        model.evaluate(mock_dataset, callbacks=[good_callback, bad_callback])
+
+    with pytest.raises(TypeError, match="Callbacks must be tf.keras.callbacks.Callback instances"):
+        model.predict([], callbacks=[good_callback, bad_callback])
```

## tests/tensorflow_tests/test_text_classification.py

```diff
@@ -19,26 +19,131 @@
 #
 
 import os
 import pytest
 import shutil
 import tempfile
 
-from tlt.utils.file_utils import validate_model_name
+from tlt.utils.file_utils import validate_model_name, download_and_extract_zip_file
 from tlt.datasets import dataset_factory
 from tlt.models import model_factory
 
 
 @pytest.mark.integration
 @pytest.mark.tensorflow
-@pytest.mark.parametrize('model_name,dataset_name,extra_layers,correct_num_layers',
-                         [['small_bert/bert_en_uncased_L-2_H-128_A-2', 'imdb_reviews', None, 3],
-                          ['small_bert/bert_en_uncased_L-2_H-256_A-4', 'glue/sst2', None, 3],
-                          ['small_bert/bert_en_uncased_L-2_H-128_A-2', 'imdb_reviews', [512, 128], 5]])
-def test_tf_binary_text_classification(model_name, dataset_name, extra_layers, correct_num_layers):
+@pytest.mark.parametrize('model_name,dataset_name,extra_layers,correct_num_layers,model_hub',
+                         [['google/bert_uncased_L-2_H-128_A-2', 'ag_news_subset', None, 5, 'huggingface']])
+def test_tf_multi_text_classification(model_name, dataset_name, extra_layers, correct_num_layers, model_hub):
+    """
+    Tests basic transfer learning functionality for TensorFlow multi text classification using TF Datasets
+    """
+    framework = 'tensorflow'
+    output_dir = tempfile.mkdtemp()
+    os.environ["TENSORFLOW_HOME"] = output_dir
+
+    try:
+        # Get the dataset
+        dataset = dataset_factory.get_dataset(output_dir, 'text_classification', framework, dataset_name,
+                                              'tf_datasets', split=["train[:8%]"], shuffle_files=False)
+
+        # Get the model
+        model = model_factory.get_model(model_name, framework)
+
+        # Preprocess the dataset
+        batch_size = 32
+        dataset.preprocess(batch_size)
+        dataset.shuffle_split(seed=10)
+
+        # This model does not support evaluate/predict before training
+        with pytest.raises(ValueError) as e:
+            model.evaluate(dataset)
+        assert "model must be trained" in str(e)
+        with pytest.raises(ValueError) as e:
+            model.predict(dataset)
+        assert "model must be trained" in str(e)
+
+        # Train
+        history = model.train(dataset, output_dir=output_dir, epochs=1,
+                              shuffle_files=False, do_eval=False,
+                              extra_layers=extra_layers)
+        assert history is not None
+        assert len(model._model.layers) == correct_num_layers
+
+        # Verify that checkpoints were generated
+        cleaned_name = validate_model_name(model_name)
+        checkpoint_dir = os.path.join(output_dir, "{}_checkpoints".format(cleaned_name))
+        assert os.path.isdir(checkpoint_dir)
+        assert len(os.listdir(checkpoint_dir))
+
+        # Evaluate
+        trained_metrics = model.evaluate(dataset)
+        assert len(trained_metrics) == 2  # expect to get loss and accuracy metrics
+
+        # Predict with a batch
+        input, labels = dataset.get_batch()
+        predictions = model.predict(input)
+        assert len(predictions) == batch_size
+
+        text1 = ('Oil and Economy Cloud Stocks Outlook (Reuters) Reuters - '
+                 'Soaring crude prices plus worries about the economy and the'
+                 'outlook for earnings are expected to hang over the stock market'
+                 'next week during the depth of the summer doldrums')
+        text2 = ('Wall St. Bears Claw Back Into the Black (Reuters) Reuters -'
+                 'Short-sellers, Wall Streets dwindlingband of ultra-cynics,'
+                 'are seeing green again.')
+        text3 = ('Expansion slows in Japan Economic growth in Japan slows down'
+                 'as the country experiences a drop in domestic and corporate spending.'
+                 'outlook for earnings are expected to hang over the stock market'
+                 'next week during the depth of the summer doldrums')
+        # Predict with raw text input
+        raw_text_input = [text1, text2, text3]
+        predictions = model.predict(raw_text_input)
+        assert len(predictions) == len(raw_text_input)
+
+        # export the saved model
+        saved_model_dir = model.export(output_dir)
+        assert os.path.isdir(saved_model_dir)
+        assert os.path.isfile(os.path.join(saved_model_dir, "saved_model.pb"))
+
+        # Reload the saved model
+        reload_model = model_factory.load_model(model_name, saved_model_dir, framework, 'text_classification',
+                                                model_hub)
+
+        # Evaluate
+        reload_metrics = reload_model.evaluate(dataset)
+        assert reload_metrics == trained_metrics
+
+        # Predict with the raw text input
+        reload_predictions = reload_model.predict(raw_text_input)
+        assert (reload_predictions == predictions).all()
+
+        # Retrain from checkpoints and verify that accuracy metric is the expected type
+        retrain_model = model_factory.load_model(model_name, saved_model_dir, framework, 'text_classification',
+                                                 model_hub)
+        retrain_model.train(dataset, output_dir=output_dir, epochs=1, initial_checkpoints=checkpoint_dir,
+                            shuffle_files=False, do_eval=False)
+
+        retrain_metrics = retrain_model.evaluate(dataset)
+        accuracy_index = next(id for id, k in enumerate(model._model.metrics_names) if 'acc' in k)
+        # BERT model results are not deterministic, so the commented assertion doesn't reliably pass
+        assert isinstance(retrain_metrics[accuracy_index], float)
+
+    finally:
+        # Delete the temp output directory
+        if os.path.exists(output_dir) and os.path.isdir(output_dir):
+            shutil.rmtree(output_dir)
+
+
+@pytest.mark.integration
+@pytest.mark.tensorflow
+@pytest.mark.parametrize('model_name,dataset_name,extra_layers,correct_num_layers,model_hub',
+                         [['google/bert_uncased_L-2_H-128_A-2', 'imdb_reviews', None, 5, 'huggingface'],
+                          ['google/bert_uncased_L-2_H-256_A-4', 'glue/sst2', None, 5, 'huggingface'],
+                          ['google/bert_uncased_L-2_H-128_A-2', 'imdb_reviews', [512, 128], 7, 'huggingface']])
+def test_tf_binary_text_classification(model_name, dataset_name, extra_layers, correct_num_layers, model_hub):
     """
     Tests basic transfer learning functionality for TensorFlow binary text classification using TF Datasets
     """
     framework = 'tensorflow'
     output_dir = tempfile.mkdtemp()
 
     try:
@@ -91,56 +196,52 @@
 
         # export the saved model
         saved_model_dir = model.export(output_dir)
         assert os.path.isdir(saved_model_dir)
         assert os.path.isfile(os.path.join(saved_model_dir, "saved_model.pb"))
 
         # Reload the saved model
-        reload_model = model_factory.load_model(model_name, saved_model_dir, framework, 'text_classification')
+        reload_model = model_factory.load_model(model_name, saved_model_dir, framework, 'text_classification',
+                                                model_hub)
 
         # Evaluate
         reload_metrics = reload_model.evaluate(dataset)
         assert reload_metrics == trained_metrics
 
         # Predict with the raw text input
         reload_predictions = reload_model.predict(raw_text_input)
         assert (reload_predictions == predictions).all()
 
         # Retrain from checkpoints and verify that accuracy metric is the expected type
-        retrain_model = model_factory.load_model(model_name, saved_model_dir, framework, 'text_classification')
+        retrain_model = model_factory.load_model(model_name, saved_model_dir, framework, 'text_classification',
+                                                 model_hub)
         retrain_model.train(dataset, output_dir=output_dir, epochs=1, initial_checkpoints=checkpoint_dir,
                             shuffle_files=False, do_eval=False)
 
         retrain_metrics = retrain_model.evaluate(dataset)
         accuracy_index = next(id for id, k in enumerate(model._model.metrics_names) if 'acc' in k)
         # BERT model results are not deterministic, so the commented assertion doesn't reliably pass
         # assert retrain_metrics[accuracy_index] > trained_metrics[accuracy_index]
         assert isinstance(retrain_metrics[accuracy_index], float)
 
-        # Test generating an Intel Neural Compressor config file (not implemented yet)
-        inc_config_file_path = os.path.join(output_dir, "tf_{}.yaml".format(model_name))
-        with pytest.raises(NotImplementedError):
-            model.write_inc_config_file(inc_config_file_path, dataset, batch_size=batch_size,
-                                        tuning_workspace=output_dir)
-
     finally:
         # Delete the temp output directory
         if os.path.exists(output_dir) and os.path.isdir(output_dir):
             shutil.rmtree(output_dir)
 
 
 @pytest.mark.integration
 @pytest.mark.tensorflow
 @pytest.mark.parametrize('model_name, dataset_name, epochs, learning_rate, do_eval, \
                          lr_decay, accuracy, val_accuracy, lr_final',
-                         [['small_bert/bert_en_uncased_L-2_H-128_A-2', 'glue/sst2', 1,
+                         [['google/bert_uncased_L-2_H-128_A-2', 'glue/sst2', 1,
                            .005, False, False, None, None, 0.005],
-                          ['small_bert/bert_en_uncased_L-2_H-256_A-4', 'glue/sst2',
+                          ['google/bert_uncased_L-2_H-256_A-4', 'glue/sst2',
                            1, .001, True, True, 0.34375, 0.4256, 0.001],
-                          ['small_bert/bert_en_uncased_L-2_H-128_A-2', 'imdb_reviews',
+                          ['google/bert_uncased_L-2_H-128_A-2', 'imdb_reviews',
                            15, .005, True, True, None, None, 0.001]])
 def test_tf_binary_text_classification_with_lr_options(model_name, dataset_name,
                                                        epochs, learning_rate, do_eval,
                                                        lr_decay, accuracy, val_accuracy, lr_final):
     """
     Tests transfer learning for TensorFlow binary text classification with different learning rate options
     """
@@ -186,27 +287,37 @@
         if os.path.exists(output_dir) and os.path.isdir(output_dir):
             shutil.rmtree(output_dir)
 
 
 @pytest.mark.integration
 @pytest.mark.tensorflow
 @pytest.mark.parametrize('model_name',
-                         ['small_bert/bert_en_uncased_L-2_H-128_A-2'])
+                         ['google/bert_uncased_L-2_H-128_A-2'])
 def test_custom_dataset_workflow(model_name):
     """
     Tests the full workflow for TF text classification using a custom dataset
     """
     output_dir = tempfile.mkdtemp()
+    dataset_dir = '/tmp/data'
 
     def label_map_func(x):
         return int(x == "spam")
 
     try:
         # Get the dataset
-        dataset = dataset_factory.load_dataset('/tmp/data/sms_spam_collection', use_case="text_classification",
+        zip_file_url = "https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip"
+        sms_data_directory = os.path.join(dataset_dir, "sms_spam_collection")
+        csv_file_name = "SMSSpamCollection"
+
+        # If the SMS Spam collection csv file is not found, download and extract the file:
+        if not os.path.exists(os.path.join(sms_data_directory, csv_file_name)):
+            # Download the zip file with the SMS Spam collection dataset
+            download_and_extract_zip_file(zip_file_url, sms_data_directory)
+
+        dataset = dataset_factory.load_dataset(sms_data_directory, use_case="text_classification",
                                                framework="tensorflow", csv_file_name="SMSSpamCollection",
                                                class_names=["ham", "spam"], shuffle_files=False,
                                                delimiter='\t', header=False, label_map_func=label_map_func)
         # Get the model
         model = model_factory.get_model(model_name, "tensorflow")
 
         # Preprocess the dataset and split to get small subsets for training and validation
@@ -230,21 +341,16 @@
         reload_model.load_from_directory(saved_model_dir)
 
         # Evaluate
         metrics = reload_model.evaluate(dataset)
         assert len(metrics) > 0
 
         # Quantization
-        inc_config_file_path = 'tlt/models/configs/inc/text_classification_template.yaml'
-        nc_workspace = os.path.join(output_dir, "nc_workspace")
-        model.write_inc_config_file(inc_config_file_path, dataset, batch_size=32, overwrite=True,
-                                    accuracy_criterion_relative=0.1, exit_policy_max_trials=10,
-                                    exit_policy_timeout=0, tuning_workspace=nc_workspace)
-        quantization_output = os.path.join(output_dir, "quantized", "mocked")
-        os.makedirs(quantization_output, exist_ok=True)
-        model.quantize(saved_model_dir, quantization_output, inc_config_file_path)
-        assert os.path.exists(os.path.join(quantization_output, "saved_model.pb"))
+        inc_output_dir = os.path.join(output_dir, "quantized", "mocked")
+        os.makedirs(inc_output_dir, exist_ok=True)
+        model.quantize(inc_output_dir, dataset)
+        assert os.path.exists(os.path.join(inc_output_dir, "saved_model.pb"))
 
     finally:
         # Delete the temp output directory
         if os.path.exists(output_dir) and os.path.isdir(output_dir):
             shutil.rmtree(output_dir)
```

## tests/tensorflow_tests/unit/test_datasets.py

```diff
@@ -115,14 +115,15 @@
     flowers.shuffle_split(seed=10)
     assert len(flowers.train_subset) == train_len
     assert len(flowers.validation_subset) == val_len
     assert flowers.test_subset is None
     assert flowers._validation_type == 'shuffle_split'
 
 
+@pytest.mark.integration
 @pytest.mark.tensorflow
 @pytest.mark.parametrize('dataset_name,use_case,image_size',
                          [['tf_flowers', 'image_classification', 224],
                           ['glue/cola', 'text_classification', None]])
 def test_shuffle_split_deterministic_tfds(dataset_name, use_case, image_size):
     """
     Checks that tfds datasets can be split into train, validation, and test subsets in a way that is reproducible
@@ -241,14 +242,15 @@
                 tlt_dataset.preprocess(batch_size=32)
 
         assert 'Data has already been preprocessed: {}'.format(tlt_dataset._preprocessed) == str(e.value)
     finally:
         ic_dataset.cleanup()
 
 
+@pytest.mark.integration
 @pytest.mark.tensorflow
 @pytest.mark.parametrize('dataset_name,use_case,expected_class_names',
                          [['glue/cola', 'text_classification', ['unacceptable', 'acceptable']],
                           ['glue/sst2', 'text_classification', ['negative', 'positive']],
                           ['imdb_reviews', 'text_classification', ['neg', 'pos']]])
 def test_supported_tfds_datasets(dataset_name, use_case, expected_class_names):
     """
@@ -322,14 +324,15 @@
 
     finally:
         # Clean up after the test by deleting the temp dataset directory
         if os.path.exists(dataset_dir):
             shutil.rmtree(dataset_dir)
 
 
+@pytest.mark.tensorflow
 def test_custom_text_classification_extra_columns():
     """
     Tests load_dataset with a text classification csv file that has 3 columns and uses select_cols and exclude_cols to
     make the resulting dataset only have 2 columns.
     """
     dataset_dir = tempfile.mkdtemp()
     csv_file_name = "test.csv"
@@ -482,15 +485,14 @@
 @pytest.mark.tensorflow
 class TestImageClassificationDataset:
     """
     This class contains image classification dataset tests that only require the dataset to be initialized once. These
     tests will be run once for each of the dataset defined in the dataset_params list.
     """
 
-    @pytest.mark.tensorflow
     def test_class_names_and_size(self, test_data):
         """
         Verify the class type, dataset class names, and dataset length after initialization
         """
         tlt_dataset, dataset_name, dataset_classes, use_case, splits = test_data
 
         if dataset_name is None:
@@ -505,15 +507,14 @@
                 assert type(tlt_dataset) == TFDSImageClassificationDataset
             elif use_case == 'text_classification':
                 assert type(tlt_dataset) == TFDSTextClassificationDataset
 
             assert len(tlt_dataset.class_names) == len(tfds_metadata[dataset_name]['class_names'])
             assert len(tlt_dataset.dataset) == tfds_metadata[dataset_name]['size']
 
-    @pytest.mark.tensorflow
     @pytest.mark.parametrize('batch_size',
                              ['foo',
                               -17,
                               20.5])
     def test_invalid_batch_sizes(self, batch_size, test_data):
         """
         Ensures that a ValueError is raised when an invalid batch size is passed
@@ -521,15 +522,14 @@
         tlt_dataset, dataset_name, dataset_classes, use_case, splits = test_data
         with pytest.raises(ValueError):
             if use_case == 'image_classification':
                 tlt_dataset.preprocess(224, batch_size)
             else:
                 tlt_dataset.preprocess(batch_size=batch_size)
 
-    @pytest.mark.tensorflow
     @pytest.mark.parametrize('image_size',
                              ['foo',
                               -17,
                               20.5])
     def test_invalid_image_size(self, image_size, test_data):
         """
         Ensures that a ValueError is raised when an invalid image size is passed. This test only applies to
@@ -537,15 +537,14 @@
         """
         tlt_dataset, dataset_name, dataset_classes, use_case, splits = test_data
 
         if use_case == 'image_classification':
             with pytest.raises(ValueError):
                 tlt_dataset.preprocess(image_size, batch_size=8)
 
-    @pytest.mark.tensorflow
     def test_preprocessing(self, test_data):
         """
         Checks that dataset can be preprocessed only once
         """
         tlt_dataset, dataset_name, dataset_classes, use_case, splits = test_data
 
         if use_case == 'image_classification':
@@ -562,29 +561,27 @@
             if use_case == 'image_classification':
                 tlt_dataset.preprocess(324, 32)
             else:
                 tlt_dataset.preprocess(batch_size=32)
         assert 'Data has already been preprocessed: {}'.format(preprocessing_inputs) == str(e.value)
         print(tlt_dataset.info)
 
-    @pytest.mark.tensorflow
     def test_shuffle_split_errors(self, test_data):
         """
         Checks that splitting into train, validation, and test subsets will error if inputs are wrong
         """
         tlt_dataset, dataset_name, dataset_classes, use_case, splits = test_data
 
         with pytest.raises(Exception) as e:
             tlt_dataset.shuffle_split(train_pct=.5, val_pct=.5, test_pct=.2)
         assert 'Sum of percentage arguments must be less than or equal to 1.' == str(e.value)
         with pytest.raises(Exception) as e:
             tlt_dataset.shuffle_split(train_pct=1, val_pct=0)
         assert 'Percentage arguments must be floats.' == str(e.value)
 
-    @pytest.mark.tensorflow
     def test_shuffle_split(self, test_data):
         """
         Checks that dataset can be split into train, validation, and test subsets
         """
         tlt_dataset, dataset_name, dataset_classes, use_case, splits = test_data
 
         # Before the shuffle split, validation type should be None or defined_split
```

## tests/tensorflow_tests/unit/test_inc.py

```diff
@@ -17,473 +17,182 @@
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import os
 import pytest
 import shutil
-import uuid
 import tempfile
+import uuid
 
-from pathlib import Path
-from unittest.mock import patch
-
+from unittest.mock import patch, MagicMock
 from tlt.models import model_factory
 
 try:
     # Do TF specific imports in a try/except to prevent pytest test loading from failing when running in a PyTorch env
     from tlt.models.image_classification.tf_image_classification_model import TFImageClassificationModel  # noqa: F401
+    from tlt.models.image_classification.tf_image_classification_model import TFCustomImageClassificationDataset
 except ModuleNotFoundError:
     print("WARNING: Unable to import TFImageClassificationModel. TensorFlow may not be installed")
 
-from tlt.datasets import dataset_factory
-from tlt.utils.file_utils import download_and_extract_tar_file
-
-# Load a custom PyTorch dataset that can be re-used for tests
-dataset_dir = tempfile.mkdtemp()
-custom_dataset_path = os.path.join(dataset_dir, "flower_photos")
-if not os.path.exists(custom_dataset_path):
-    download_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
-    download_and_extract_tar_file(download_url, dataset_dir)
-# Load the dataset from the custom dataset path
-dataset = dataset_factory.load_dataset(dataset_dir=custom_dataset_path,
-                                       use_case='image_classification',
-                                       framework='pytorch')
-
-
-@pytest.mark.tensorflow
-def test_tf_image_classification_config_file_overwrite():
-    """
-    Tests writing an Intel Neural Compressor config file for image classification models with a mock custom dataset.
-    Checks that the overwrite flag lets you overwrite a config file that already exists.
-    """
-    try:
-        temp_dir = tempfile.mkdtemp()
-        model = model_factory.get_model('efficientnet_b0', 'tensorflow')
-        with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
-                as mock_dataset:
-            config_file = os.path.join(temp_dir, "config.yaml")
-            batch_size = 24
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
-            nc_workspace = os.path.join(temp_dir, "nc_workspace")
-            model.write_inc_config_file(config_file, mock_dataset, batch_size=batch_size, tuning_workspace=nc_workspace)
-            assert os.path.exists(config_file)
-
-            # If overwrite=False this should fail, since the config file already exists
-            with pytest.raises(FileExistsError):
-                model.write_inc_config_file(config_file, mock_dataset, batch_size=batch_size, overwrite=False)
-
-            # Writing the config file again should work with overwrite=True
-            model.write_inc_config_file(config_file, mock_dataset, batch_size=batch_size, overwrite=True)
-    finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
-
 
 @pytest.mark.tensorflow
-@pytest.mark.parametrize('batch_size,valid',
-                         [[1, True],
-                          [-1, False],
-                          ['abc', False],
-                          [1.434, False],
-                          [0, False],
-                          [128, True]])
-def test_tf_image_classification_config_file_batch_size(batch_size, valid):
+def test_tf_image_classification_quantization():
     """
-    Tests writing an Intel Neural Compressor config file with good and bad batch sizes
+    Given a valid directory for the output dir, test the quantization function with the actual Intel Neural Compressor
+    call mocked out.
     """
     try:
-        temp_dir = tempfile.mkdtemp()
-        nc_workspace = os.path.join(temp_dir, "nc_workspace")
-        model = model_factory.get_model('efficientnet_b0', 'tensorflow')
-        with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
-                as mock_dataset:
-            config_file = os.path.join(temp_dir, "config.yaml")
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-            if not valid:
-                with pytest.raises(ValueError):
-                    model.write_inc_config_file(config_file, mock_dataset, batch_size=batch_size, overwrite=True,
-                                                tuning_workspace=nc_workspace)
-            else:
-                model.write_inc_config_file(config_file, mock_dataset, batch_size=batch_size, overwrite=True,
-                                            tuning_workspace=nc_workspace)
-    finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
-
+        output_dir = tempfile.mkdtemp()
 
-@pytest.mark.tensorflow
-@pytest.mark.parametrize('resize_interpolation,valid',
-                         [['bilinear', True],
-                          [-1, False],
-                          ['nearest', True],
-                          [1.434, False],
-                          ['bicubic', True],
-                          ['foo', False]])
-def test_tf_image_classification_config_file_resize_interpolation(resize_interpolation, valid):
-    """
-    Tests writing an Intel Neural Compressor config file with good and bad resize_interpolation values
-    """
-    try:
-        temp_dir = tempfile.mkdtemp()
-        nc_workspace = os.path.join(temp_dir, "nc_workspace")
         model = model_factory.get_model('efficientnet_b0', 'tensorflow')
         with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
                 as mock_dataset:
-            config_file = os.path.join(temp_dir, "config.yaml")
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-            if not valid:
-                with pytest.raises(ValueError):
-                    model.write_inc_config_file(config_file, mock_dataset, batch_size=1, overwrite=True,
-                                                resize_interpolation=resize_interpolation,
-                                                tuning_workspace=nc_workspace)
-            else:
-                model.write_inc_config_file(config_file, mock_dataset, batch_size=1, overwrite=True,
-                                            resize_interpolation=resize_interpolation, tuning_workspace=nc_workspace)
+            with patch('neural_compressor.quantization.fit') as mock_q:
+                mock_dataset.dataset_dir = "/tmp/data/my_photos"
+                mock_dataset.__class__ = TFCustomImageClassificationDataset
+                mock_dataset.get_inc_dataloaders.return_value = (1, 2)
+                model.quantize(output_dir, mock_dataset)
+                mock_q.assert_called_once()
     finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
+        if os.path.exists(output_dir):
+            shutil.rmtree(output_dir)
 
 
 @pytest.mark.tensorflow
-@pytest.mark.parametrize('accuracy_criterion,valid',
-                         [[0.1, True],
-                          [-1, False],
-                          [0.01, True],
-                          [1.434, False],
-                          ['foo', False]])
-def test_tf_image_classification_config_file_accuracy_criterion(accuracy_criterion, valid):
+@patch('tlt.models.tf_model.quantization.fit')
+def test_tf_image_classification_quantize_overwrite_saved_model(mock_quantization_fit):
     """
-    Tests writing an Intel Neural Compressor config file with good and bad accuracy_criterion_relative values
+    Given a valid directory for the output dir, test the quantize function with the actual Intel Neural
+    Compressor call mocked out. Tests that the model will be overwritten or not using the overwrite_model flag.
     """
-    try:
-        temp_dir = tempfile.mkdtemp()
-        nc_workspace = os.path.join(temp_dir, "nc_workspace")
-        model = model_factory.get_model('efficientnet_b0', 'tensorflow')
-        with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
-                as mock_dataset:
-            config_file = os.path.join(temp_dir, "config.yaml")
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-            if not valid:
-                with pytest.raises(ValueError):
-                    model.write_inc_config_file(config_file, mock_dataset, batch_size=1, overwrite=True,
-                                                accuracy_criterion_relative=accuracy_criterion,
-                                                tuning_workspace=nc_workspace)
-            else:
-                model.write_inc_config_file(config_file, mock_dataset, batch_size=1, overwrite=True,
-                                            accuracy_criterion_relative=accuracy_criterion,
-                                            tuning_workspace=nc_workspace)
-    finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
 
+    from tlt.models import model_factory
 
-@pytest.mark.tensorflow
-@pytest.mark.parametrize('timeout,valid',
-                         [[0.1, False],
-                          [-1, False],
-                          [0, True],
-                          [60, True],
-                          ['foo', False]])
-def test_tf_image_classification_config_file_timeout(timeout, valid):
-    """
-    Tests writing an Intel Neural Compressor config file with good and bad exit_policy_timeout values
-    """
     try:
-        temp_dir = tempfile.mkdtemp()
-        nc_workspace = os.path.join(temp_dir, "nc_workspace")
-        model = model_factory.get_model('efficientnet_b0', 'tensorflow')
-        with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
-                as mock_dataset:
-            config_file = os.path.join(temp_dir, "config.yaml")
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
+        # Specify a directory for output
+        output_dir = tempfile.mkdtemp()
 
-            if not valid:
-                with pytest.raises(ValueError):
-                    model.write_inc_config_file(config_file, mock_dataset, batch_size=1, overwrite=True,
-                                                exit_policy_timeout=timeout, tuning_workspace=nc_workspace)
-            else:
-                model.write_inc_config_file(config_file, mock_dataset, batch_size=1, overwrite=True,
-                                            exit_policy_timeout=timeout, tuning_workspace=nc_workspace)
-    finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
+        model = model_factory.get_model(model_name='resnet_v1_50', framework='tensorflow')
 
+        # Mock the dataset
+        mock_dataset = MagicMock()
+        mock_dataset.__class__ = TFCustomImageClassificationDataset
+        mock_dataset.get_inc_dataloaders.return_value = 1, 2
 
-@pytest.mark.tensorflow
-@pytest.mark.parametrize('max_trials,valid',
-                         [[0.1, False],
-                          [-1, False],
-                          [0, False],
-                          [1, True],
-                          [60, True],
-                          ['foo', False]])
-def test_tf_image_classification_config_file_max_trials(max_trials, valid):
-    """
-    Tests writing an Intel Neural Compressor config file with good and bad exit_policy_max_trials values
-    """
-    try:
-        temp_dir = tempfile.mkdtemp()
-        nc_workspace = os.path.join(temp_dir, "nc_workspace")
-        model = model_factory.get_model('efficientnet_b0', 'tensorflow')
-        with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
-                as mock_dataset:
-            config_file = os.path.join(temp_dir, "config.yaml")
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
+        # Method to create a dummy model.pt file in the specified directory
+        def create_dummy_file(output_dir):
+            with open(os.path.join(output_dir, 'saved_model.pb'), 'w') as fp:
+                fp.close()
 
-            if not valid:
-                with pytest.raises(ValueError):
-                    model.write_inc_config_file(config_file, mock_dataset, batch_size=1, overwrite=True,
-                                                exit_policy_max_trials=max_trials, tuning_workspace=nc_workspace)
-            else:
-                model.write_inc_config_file(config_file, mock_dataset, batch_size=1, overwrite=True,
-                                            exit_policy_max_trials=max_trials, tuning_workspace=nc_workspace)
-    finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
+        # Mock an INC quantized model that will create a dummy file when saved
+        mock_quantized_model = MagicMock()
+        mock_quantized_model.save.side_effect = create_dummy_file
 
+        # Mock the INC quantization.fit method
+        def mock_fit(**args):
+            return mock_quantized_model
+        mock_quantization_fit.side_effect = mock_fit
 
-@pytest.mark.tensorflow
-@pytest.mark.parametrize('seed,valid',
-                         [[0.1, False],
-                          [-1, False],
-                          [0, True],
-                          [1, True],
-                          [123, True],
-                          ['foo', False]])
-def test_tf_image_classification_config_file_seed(seed, valid):
-    """
-    Tests writing an Intel Neural Compressor config file with good and bad tuning_random_seed values
-    """
-    try:
-        temp_dir = tempfile.mkdtemp()
-        nc_workspace = os.path.join(temp_dir, "nc_workspace")
-        model = model_factory.get_model('efficientnet_b0', 'tensorflow')
-        with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
-                as mock_dataset:
-            config_file = os.path.join(temp_dir, "config.yaml")
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
+        # Call quantize when a model does not exist
+        model.quantize(output_dir=output_dir, dataset=mock_dataset, overwrite_model=False)
 
-            if not valid:
-                with pytest.raises(ValueError):
-                    model.write_inc_config_file(config_file, mock_dataset, batch_size=1, overwrite=True,
-                                                tuning_random_seed=seed, tuning_workspace=nc_workspace)
-            else:
-                model.write_inc_config_file(config_file, mock_dataset, batch_size=1, overwrite=True,
-                                            tuning_random_seed=seed, tuning_workspace=nc_workspace)
-    finally:
-        if os.path.exists(temp_dir):
-            shutil.rmtree(temp_dir)
+        # Call quantize when the model exists, but overwrite_model=True
+        model.quantize(output_dir=output_dir, dataset=mock_dataset, overwrite_model=True)
+        model.quantize(output_dir=output_dir, dataset=mock_dataset, overwrite_model=True)
 
+        with pytest.raises(FileExistsError):  # Model exists, so this should be true
+            model.quantize(output_dir=output_dir, dataset=mock_dataset, overwrite_model=False)
 
-@pytest.mark.tensorflow
-def test_tf_image_classification_quantization():
-    """
-    Given valid directories for the saved model, output dir, and config file, test the quantization function with
-    the actual Intel Neural Compressor called mocked out.
-    """
-    try:
-        output_dir = tempfile.mkdtemp()
-        saved_model_dir = tempfile.mkdtemp()
-        saved_model_file = os.path.join(saved_model_dir, "saved_model.pb")
-        Path(saved_model_file).touch()
-        dummy_config_file = os.path.join(saved_model_dir, "config.yaml")
-        Path(dummy_config_file).touch()
-
-        model = model_factory.get_model('efficientnet_b0', 'tensorflow')
-        with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
-                as mock_dataset:
-            with patch('neural_compressor.experimental.Quantization') as mock_q:
-                mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-                model.quantize(saved_model_dir, output_dir, dummy_config_file)
-                mock_q.assert_called_with(dummy_config_file)
     finally:
         if os.path.exists(output_dir):
             shutil.rmtree(output_dir)
-        if os.path.exists(saved_model_dir):
-            shutil.rmtree(saved_model_dir)
 
 
+@patch('tlt.models.tf_model.Graph_Optimization')
 @pytest.mark.tensorflow
-def test_tf_image_classification_quantization_model_does_not_exist():
+def test_tf_image_classification_optimize_graph_overwrite_saved_model(mock_graph_optimization):
     """
-    Verifies the error that gets raise if quantization or Intel Neural Compressor benchmarking is done with a model
-    that does not exist
+    Given a valid directory for the output dir, test the quantize function with the actual Intel Neural
+    Compressor call mocked out. Tests that the model will be overwritten or not using the overwrite_model flag.
     """
+
+    # tlt imports
+    from tlt.models.image_classification.tf_image_classification_model import TFCustomImageClassificationDataset
+    from tlt.models import model_factory
+
     try:
+        # Specify a directory for output
         output_dir = tempfile.mkdtemp()
-        dummy_config_file = os.path.join(output_dir, "config.yaml")
-        Path(dummy_config_file).touch()
-        model = model_factory.get_model('efficientnet_b0', 'tensorflow')
-        with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
-                as mock_dataset:
-            mock_dataset.dataset_dir = "/tmp/data/my_photos"
-            with patch('neural_compressor.experimental.Quantization'):
 
-                # Generate a random name that wouldn't exist
-                random_dir = str(uuid.uuid4())
+        model = model_factory.get_model(model_name='resnet_v1_50', framework='tensorflow')
 
-                # It's not a directory, so we expect an error
-                with pytest.raises(NotADirectoryError):
-                    model.quantize(random_dir, output_dir, dummy_config_file)
+        # Mock the dataset
+        mock_dataset = MagicMock()
+        mock_dataset.__class__ = TFCustomImageClassificationDataset
+        mock_dataset.get_inc_dataloaders.return_value = 1, 2
 
-                saved_model_dir = tempfile.mkdtemp()
+        # Method to create a dummy model.pt file in the specified directory
+        def create_dummy_file():
+            with open(os.path.join(output_dir, 'saved_model.pb'), 'w') as fp:
+                fp.close()
+            return MagicMock()
 
-                # An empty directory with no saved model should alos generate an error
-                with pytest.raises(FileNotFoundError):
-                    model.quantize(saved_model_dir, output_dir, dummy_config_file)
+        # Mock an INC quantized model that will create a dummy file when saved
+        mock_graph_optimization.side_effect = create_dummy_file
 
-            with patch('neural_compressor.experimental.Benchmark'):
-                # It's not a directory, so we expect an error
-                with pytest.raises(NotADirectoryError):
-                    model.benchmark(random_dir, dummy_config_file)
+        # Call optimize_graph when a model does not exist
+        model.optimize_graph(output_dir=output_dir)
 
-                # An empty directory with no saved model should alos generate an error
-                with pytest.raises(FileNotFoundError):
-                    model.benchmark(saved_model_dir, dummy_config_file)
+        # Call optimize_graph when the model exists, but overwrite_model=True
+        model.optimize_graph(output_dir=output_dir, overwrite_model=True)
+        model.optimize_graph(output_dir=output_dir, overwrite_model=True)
 
-    finally:
-        if os.path.exists(output_dir):
-            shutil.rmtree(output_dir)
-        if os.path.exists(saved_model_dir):
-            shutil.rmtree(saved_model_dir)
-
-
-@pytest.mark.tensorflow
-def test_tf_image_classification_optimize_graph():
-    """
-    Given valid directories for the saved model, output dir, and config file, test the graph optimization function with
-    the actual Intel Neural Compressorcalled mocked out.
-    """
-    try:
-        output_dir = tempfile.mkdtemp()
-        saved_model_dir = tempfile.mkdtemp()
-        saved_model_file = os.path.join(saved_model_dir, "saved_model.pb")
-        Path(saved_model_file).touch()
+        with pytest.raises(FileExistsError):  # Model exists, so this should be true
+            model.optimize_graph(output_dir=output_dir, overwrite_model=False)
 
-        model = model_factory.get_model('efficientnet_b0', 'tensorflow')
-        with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
-                as mock_dataset:
-            with patch('neural_compressor.experimental.Graph_Optimization') as mock_o:
-                mock_dataset.dataset_dir = "/tmp/data/my_photos"
-                model.optimize_graph(saved_model_dir, output_dir)
-                mock_o.assert_called()
     finally:
         if os.path.exists(output_dir):
             shutil.rmtree(output_dir)
-        if os.path.exists(saved_model_dir):
-            shutil.rmtree(saved_model_dir)
 
 
 @pytest.mark.tensorflow
-def test_tf_image_classification_optimize_graph_model_does_not_exist():
+def test_tf_image_classification_benchmark_model_does_not_exist():
     """
-    Verifies the error that gets raise if graph optimization is done with a model that does not exist
+    Verifies the error that gets raise if benchmarking is done with a model that does not exist
     """
     try:
-        output_dir = tempfile.mkdtemp()
-        dummy_config_file = os.path.join(output_dir, "config.yaml")
-        Path(dummy_config_file).touch()
         model = model_factory.get_model('efficientnet_b0', 'tensorflow')
         with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
                 as mock_dataset:
             mock_dataset.dataset_dir = "/tmp/data/my_photos"
-            with patch('neural_compressor.experimental.Graph_Optimization'):
-
-                # Generate a random name that wouldn't exist
-                random_dir = str(uuid.uuid4())
-
-                # It's not a directory, so we expect an error
-                with pytest.raises(NotADirectoryError):
-                    model.optimize_graph(random_dir, output_dir)
-
-                saved_model_dir = tempfile.mkdtemp()
-
-                # An empty directory with no saved model should alos generate an error
-                with pytest.raises(FileNotFoundError):
-                    model.optimize_graph(saved_model_dir, output_dir)
-
-            with patch('neural_compressor.experimental.Benchmark'):
+            mock_dataset.__class__ = TFCustomImageClassificationDataset
+            random_dir = str(uuid.uuid4())
+            saved_model_dir = tempfile.mkdtemp()
+            with patch('neural_compressor.benchmark.fit'):
                 # It's not a directory, so we expect an error
                 with pytest.raises(NotADirectoryError):
-                    model.benchmark(random_dir, dummy_config_file)
+                    model.benchmark(mock_dataset, saved_model_dir=random_dir)
 
-                # An empty directory with no saved model should alos generate an error
+                # An empty directory with no saved model should also generate an error
                 with pytest.raises(FileNotFoundError):
-                    model.benchmark(saved_model_dir, dummy_config_file)
-
+                    model.benchmark(mock_dataset, saved_model_dir=saved_model_dir)
     finally:
-        if os.path.exists(output_dir):
-            shutil.rmtree(output_dir)
         if os.path.exists(saved_model_dir):
             shutil.rmtree(saved_model_dir)
 
 
 @pytest.mark.tensorflow
 def test_tf_image_classification_inc_benchmark():
     """
-    Verifies that if we have valid parameters for the saved model, config file, and mode, benchmarking is called. The
-    actual benchmarking calls to Intel Neural Compressor are mocked out.
+    Verifies that if we have a valid model and dataset, benchmarking is called. The actual benchmarking calls to Intel
+    Neural Compressor are mocked out.
     """
-    try:
-        output_dir = tempfile.mkdtemp()
-        saved_model_dir = tempfile.mkdtemp()
-        saved_model_file = os.path.join(saved_model_dir, "saved_model.pb")
-        Path(saved_model_file).touch()
-        dummy_config_file = os.path.join(saved_model_dir, "config.yaml")
-        Path(dummy_config_file).touch()
-
-        model = model_factory.get_model('efficientnet_b0', 'tensorflow')
-        with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
-                as mock_dataset:
-            with patch('neural_compressor.experimental.Benchmark') as mock_bench:
-                mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-                model.benchmark(saved_model_dir, dummy_config_file)
-                mock_bench.assert_called_with(dummy_config_file)
-    finally:
-        if os.path.exists(output_dir):
-            shutil.rmtree(output_dir)
-        if os.path.exists(saved_model_dir):
-            shutil.rmtree(saved_model_dir)
-
-
-@pytest.mark.tensorflow
-@pytest.mark.parametrize('mode,valid',
-                         [['abc', False],
-                          [1, False],
-                          [0, False],
-                          ['performance', True],
-                          ['accuracy', True]])
-def test_tf_image_classification_inc_benchmark_mode(mode, valid):
-    """
-    Checks error handling for the benchmarking mode
-    """
-    try:
-        output_dir = tempfile.mkdtemp()
-        saved_model_dir = tempfile.mkdtemp()
-        saved_model_file = os.path.join(saved_model_dir, "saved_model.pb")
-        Path(saved_model_file).touch()
-        dummy_config_file = os.path.join(saved_model_dir, "config.yaml")
-        Path(dummy_config_file).touch()
-
-        model = model_factory.get_model('efficientnet_b0', 'tensorflow')
-        with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
-                as mock_dataset:
-            with patch('neural_compressor.experimental.Benchmark') as mock_bench:
-                mock_dataset.dataset_dir = "/tmp/data/my_photos"
-
-                if not valid:
-                    with pytest.raises(ValueError):
-                        model.benchmark(saved_model_dir, dummy_config_file, mode=mode)
-                else:
-                    model.benchmark(saved_model_dir, dummy_config_file, mode=mode)
-                    mock_bench.assert_called_with(dummy_config_file)
-    finally:
-        if os.path.exists(output_dir):
-            shutil.rmtree(output_dir)
-        if os.path.exists(saved_model_dir):
-            shutil.rmtree(saved_model_dir)
+    model = model_factory.get_model('efficientnet_b0', 'tensorflow')
+    with patch('tlt.models.image_classification.tf_image_classification_model.TFCustomImageClassificationDataset') \
+            as mock_dataset:
+        with patch('neural_compressor.benchmark.fit') as mock_bench:
+            mock_dataset.dataset_dir = "/tmp/data/my_photos"
+            mock_dataset.__class__ = TFCustomImageClassificationDataset
+            mock_dataset.get_inc_dataloaders.return_value = (1, 2)
+            model.benchmark(mock_dataset)
+            mock_bench.assert_called_once()
```

## tests/tensorflow_tests/unit/test_models.py

```diff
@@ -16,119 +16,136 @@
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import pytest
 from unittest.mock import MagicMock, patch
-from tensorflow import keras
 
 from test_utils import platform_config
 from tlt.models import model_factory
 from tlt.utils.types import FrameworkType, UseCaseType
+from tlt.datasets.image_classification.image_classification_dataset import ImageClassificationDataset
+from tlt.datasets.text_classification.text_classification_dataset import TextClassificationDataset
+
+# True when all imports are successful, false when an import fails
+# This is necessary to protect from import errors when testing in a tensorflow only environment
+tf_env = True
+
+try:
+    from tensorflow import keras
+except ModuleNotFoundError:
+    print("WARNING: Unable to import Keras. Tensorflow may not be installed")
+    tf_env = False
+
 
 try:
     # Do TF specific imports in a try/except to prevent pytest test loading from failing when running in a PyTorch env
     from tlt.models.image_classification.tfhub_image_classification_model import TFHubImageClassificationModel
     from tlt.models.image_classification.keras_image_classification_model import KerasImageClassificationModel
     from tlt.models.image_classification.tf_image_classification_model import TFImageClassificationModel
 except ModuleNotFoundError:
     TFHubImageClassificationModel = None
     KerasImageClassificationModel = None
     TFImageClassificationModel = None
     print("WARNING: Unable to import TFHubImageClassificationModel or TFImageClassificationModel. "
           "TensorFlow may not be installed")
+    tf_env = False
 
 
 try:
     # Do TF specific imports in a try/except to prevent pytest test loading from failing when running in a PyTorch env
-    from tlt.models.text_classification.tfhub_text_classification_model import TFHubTextClassificationModel
+    from tlt.models.text_classification.tf_hf_text_classification_model import TFHFTextClassificationModel
     from tlt.models.text_classification.tf_text_classification_model import TFTextClassificationModel
 except ModuleNotFoundError:
-    TFHubTextClassificationModel = None
+    TFHFTextClassificationModel = None
     TFTextClassificationModel = None
-    print("WARNING: Unable to import TFHubTextClassificationModel. TensorFlow may not be installed")
+    print("WARNING: Unable to import TFHFTextClassificationModel. TensorFlow may not be installed")
+    tf_env = False
 
-from tlt.datasets.image_classification.image_classification_dataset import ImageClassificationDataset
-from tlt.datasets.text_classification.text_classification_dataset import TextClassificationDataset
 
-
-# Define a custom model
-ALEXNET = keras.models.Sequential([
-    keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), activation='relu',
-                        input_shape=(227, 227, 3)),
-    keras.layers.BatchNormalization(),
-    keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),
-    keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding="same"),
-    keras.layers.BatchNormalization(),
-    keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),
-    keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
-    keras.layers.BatchNormalization(),
-    keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
-    keras.layers.BatchNormalization(),
-    keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
-    keras.layers.BatchNormalization(),
-    keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),
-    keras.layers.Flatten(),
-    keras.layers.Dense(4096, activation='relu'),
-    keras.layers.Dropout(0.5),
-    keras.layers.Dense(4096, activation='relu'),
-    keras.layers.Dropout(0.5),
-    keras.layers.Dense(3, activation='softmax')
-])
+# This is necessary to protect from import errors when testing in a tensorflow only environment
+if tf_env:
+    # Define a custom model
+    ALEXNET = keras.models.Sequential([
+        keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), activation='relu',
+                            input_shape=(227, 227, 3)),
+        keras.layers.BatchNormalization(),
+        keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),
+        keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding="same"),
+        keras.layers.BatchNormalization(),
+        keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),
+        keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
+        keras.layers.BatchNormalization(),
+        keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
+        keras.layers.BatchNormalization(),
+        keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
+        keras.layers.BatchNormalization(),
+        keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),
+        keras.layers.Flatten(),
+        keras.layers.Dense(4096, activation='relu'),
+        keras.layers.Dropout(0.5),
+        keras.layers.Dense(4096, activation='relu'),
+        keras.layers.Dropout(0.5),
+        keras.layers.Dense(3, activation='softmax')
+    ])
 
 
 @pytest.mark.tensorflow
 @pytest.mark.parametrize('model_name,expected_class,expected_image_size',
                          [['efficientnet_b0', TFHubImageClassificationModel, 224],
-                          ['small_bert/bert_en_uncased_L-2_H-128_A-2', TFHubTextClassificationModel, None]])
-def test_tfhub_model_load(model_name, expected_class, expected_image_size):
+                          ['google/bert_uncased_L-2_H-128_A-2', TFHFTextClassificationModel, None]])
+def test_tf_model_load(model_name, expected_class, expected_image_size):
     """
-    Checks that a model can be downloaded form TF Hub
+    Checks that a model can be downloaded
     """
     model = model_factory.get_model(model_name, 'tensorflow')
     assert type(model) == expected_class
     if expected_image_size:
         assert model.image_size == expected_image_size
 
 
-@pytest.mark.tensorflow
-@pytest.mark.parametrize('model_name,expected_class,expected_image_size',
-                         [['ResNet50', KerasImageClassificationModel, 224],
-                          ['Xception', KerasImageClassificationModel, 299]])
-def test_keras_model_load(model_name, expected_class, expected_image_size):
-    """
-    Checks that a model can be downloaded from Keras.applications
-    """
-    model = model_factory.get_model(model_name, 'tensorflow')
-    assert type(model) == expected_class
-    if expected_image_size:
-        assert model.image_size == expected_image_size
-
-
-@pytest.mark.tensorflow
-@pytest.mark.parametrize('model_name,use_case,expected_class,expected_image_size,expected_num_classes',
-                         [['alexnet', 'image_classification', TFImageClassificationModel, 227, 3],
-                          ['alexnet', 'text_classification', TFTextClassificationModel, None, 3]])
-def test_custom_model_load(model_name, use_case, expected_class, expected_image_size, expected_num_classes):
-    """
-    Checks that a custom model can be loaded
-    """
-    model = model_factory.load_model(model_name, ALEXNET, 'tensorflow', use_case)
-    assert type(model) == expected_class
-    assert model.num_classes == expected_num_classes
-    if use_case == 'image_classification':
-        assert model.image_size == expected_image_size
+# This is necessary to protect from import errors when testing in a tensorflow only environment
+if tf_env:
+    @pytest.mark.tensorflow
+    @pytest.mark.parametrize('model_name,expected_class,expected_image_size',
+                             [['ResNet50', KerasImageClassificationModel, 224],
+                              ['Xception', KerasImageClassificationModel, 299]])
+    def test_keras_model_load(model_name, expected_class, expected_image_size):
+        """
+        Checks that a model can be downloaded from Keras.applications
+        """
+        model = model_factory.get_model(model_name, 'tensorflow')
+        assert type(model) == expected_class
+        if expected_image_size:
+            assert model.image_size == expected_image_size
+        assert callable(model.preprocessor)
+
+# This is necessary to protect from import errors when testing in a tensorflow only environment
+if tf_env:
+    @pytest.mark.tensorflow
+    @pytest.mark.parametrize('model_name,use_case,expected_class,expected_image_size,expected_num_classes',
+                             [['alexnet', 'image_classification', TFImageClassificationModel, 227, 3],
+                              ['alexnet', 'text_classification', TFTextClassificationModel, None, 3]])
+    def test_custom_model_load(model_name, use_case, expected_class, expected_image_size, expected_num_classes):
+        """
+        Checks that a custom model can be loaded
+        """
+        model = model_factory.load_model(model_name, ALEXNET, 'tensorflow', use_case)
+        assert type(model) == expected_class
+        assert model.num_classes == expected_num_classes
+        if use_case == 'image_classification':
+            assert model.image_size == expected_image_size
 
 
 @pytest.mark.tensorflow
 @pytest.mark.parametrize('model_name,use_case,hub',
                          [['ResNet50', 'image_classification', 'Keras'],
                           ['efficientnet_b0', 'image_classification', 'TFHub'],
-                          ['small_bert/bert_en_uncased_L-2_H-128_A-2', 'text_classification', 'TFHub']])
+                          ['google/bert_uncased_L-2_H-128_A-2', 'text_classification', 'huggingface']])
 def test_get_supported_models(model_name, use_case, hub):
     """
     Call get supported models and checks to make sure the dictionary has keys for each use case,
     and checks for a known supported model.
     """
     model_dict = model_factory.get_supported_models()
 
@@ -205,124 +222,132 @@
     Ensure that the proper error is raised when a bad use case is passed in
     """
     with pytest.raises(ValueError) as e:
         model_factory.get_supported_models(use_case=bad_use_case)
         assert "Unsupported use case: {}".format(bad_use_case) in str(e)
 
 
-@pytest.mark.tensorflow
-@pytest.mark.parametrize('model_name,dataset_type,get_hub_model_patch,class_names',
-                         [['efficientnet_b0', ImageClassificationDataset,
-                           'tlt.models.image_classification.tfhub_image_classification_model.'
-                           'TFHubImageClassificationModel._get_hub_model', ['a', 'b', 'c']],
-                          ['small_bert/bert_en_uncased_L-2_H-128_A-2',
-                           TextClassificationDataset, 'tlt.models.text_classification.tfhub_text_classification_model.'
-                           'TFHubTextClassificationModel._get_hub_model', ['a', 'b']],
-                          ['ResNet50', ImageClassificationDataset,
-                           'tlt.models.image_classification.keras_image_classification_model.'
-                           'KerasImageClassificationModel._get_hub_model', ['a', 'b', 'c']]
-                          ])
-def test_tf_model_train(model_name, dataset_type, get_hub_model_patch, class_names):
-    """
-    Tests calling train on an TFHub or Keras model with a mock dataset and mock model and verifies we get back the
-    return value from the fit function.
-    """
-    model = model_factory.get_model(model_name, 'tensorflow')
+# This is necessary to protect from import errors when testing in a tensorflow only environment
+if tf_env:
+    @pytest.mark.tensorflow
+    @pytest.mark.parametrize('model_name,dataset_type,get_hub_model_patch,class_names',
+                             [['efficientnet_b0', ImageClassificationDataset,
+                               'tlt.models.image_classification.tfhub_image_classification_model.'
+                               'TFHubImageClassificationModel._get_hub_model', ['a', 'b', 'c']],
+                              ['google/bert_uncased_L-2_H-128_A-2',
+                              TextClassificationDataset, 'tlt.models.text_classification.tf_hf_text_classification_model.'  # noqa: E501
+                               'TFHFTextClassificationModel._get_hub_model', ['a', 'b']],
+                              ['ResNet50', ImageClassificationDataset,
+                               'tlt.models.image_classification.keras_image_classification_model.'
+                               'KerasImageClassificationModel._get_hub_model', ['a', 'b', 'c']]
+                              ])
+    @patch('tlt.models.text_classification.tf_hf_text_classification_model.prepare_huggingface_input_data')
+    def test_tf_model_train(mock_tokenizer, model_name, dataset_type, get_hub_model_patch, class_names):
+        """
+        Tests calling train on an TFHub or Keras model with a mock dataset and mock model and verifies we get back the
+        return value from the fit function.
+        """
+        model = model_factory.get_model(model_name, 'tensorflow')
+
+        with patch(get_hub_model_patch) as mock_get_hub_model:
+            mock_dataset = MagicMock()
+            mock_dataset.__class__ = dataset_type
+            mock_dataset.validation_subset = [1, 2, 3]
+
+            mock_dataset.class_names = class_names
+            mock_model = MagicMock()
+            expected_return_value = {"result": True}
+            mock_history = MagicMock()
+            mock_history.history = expected_return_value
+
+            def mock_fit(x=None, y=None, epochs=1, shuffle=True, callbacks=[], validation_data=None, batch_size=None):
+                assert x is not None
+                assert isinstance(epochs, int)
+                assert isinstance(shuffle, bool)
+                assert len(callbacks) > 0
+
+                if eval_expected:
+                    assert validation_data is not None
+                else:
+                    assert validation_data is None
+
+                return mock_history
+
+            # Mock internal function to tokenize input data
+            mock_tokenizer.return_value = mock_dataset, []
+
+            mock_model.fit = mock_fit
+            mock_get_hub_model.return_value = mock_model
+
+            # Test train with eval
+            eval_expected = True
+            return_val = model.train(mock_dataset, output_dir="/tmp/output", do_eval=True)
+            assert return_val == expected_return_value
+
+            # Test train without eval
+            eval_expected = False
+            return_val = model.train(mock_dataset, output_dir="/tmp/output", do_eval=False)
+            assert return_val == expected_return_value
+
+            # Test train with eval, but no validation subset
+            eval_expected = False
+            mock_dataset.validation_subset = None
+            return_val = model.train(mock_dataset, output_dir="/tmp/output", do_eval=True)
+            assert return_val == expected_return_value
+
+
+# This is necessary to protect from import errors when testing in a tensorflow only environment
+if tf_env:
+    @pytest.mark.tensorflow
+    def test_custom_model_train():
+        """
+        Tests calling train on a custom TF model with a mock dataset and mock model and verifies we get back the return
+        value from the fit function.
+        """
+        model = model_factory.load_model('custom_model', ALEXNET, 'tensorflow', 'image_classification')
 
-    with patch(get_hub_model_patch) as mock_get_hub_model:
         mock_dataset = MagicMock()
-        mock_dataset.__class__ = dataset_type
-        mock_dataset.validation_subset = [1, 2, 3]
+        mock_dataset.__class__ = ImageClassificationDataset
 
-        mock_dataset.class_names = class_names
-        mock_model = MagicMock()
+        mock_dataset.class_names = ['1', '2', '3']
+        model._model = MagicMock()
         expected_return_value = {"result": True}
         mock_history = MagicMock()
         mock_history.history = expected_return_value
 
         def mock_fit(dataset, epochs, shuffle, callbacks, validation_data=None):
             assert dataset is not None
             assert isinstance(epochs, int)
             assert isinstance(shuffle, bool)
             assert len(callbacks) > 0
 
-            if eval_expected:
-                assert validation_data is not None
-            else:
-                assert validation_data is None
-
             return mock_history
 
-        mock_model.fit = mock_fit
-        mock_get_hub_model.return_value = mock_model
+        model._model.fit = mock_fit
 
-        # Test train with eval
-        eval_expected = True
-        return_val = model.train(mock_dataset, output_dir="/tmp/output", do_eval=True)
+        return_val = model.train(mock_dataset, output_dir="/tmp/output")
         assert return_val == expected_return_value
 
-        # Test train without eval
-        eval_expected = False
-        return_val = model.train(mock_dataset, output_dir="/tmp/output", do_eval=False)
-        assert return_val == expected_return_value
-
-        # Test train with eval, but no validation subset
-        eval_expected = False
-        mock_dataset.validation_subset = None
-        return_val = model.train(mock_dataset, output_dir="/tmp/output", do_eval=True)
-        assert return_val == expected_return_value
-
-
-@pytest.mark.tensorflow
-def test_custom_model_train():
-    """
-    Tests calling train on a custom TF model with a mock dataset and mock model and verifies we get back the return
-    value from the fit function.
-    """
-    model = model_factory.load_model('custom_model', ALEXNET, 'tensorflow', 'image_classification')
-
-    mock_dataset = MagicMock()
-    mock_dataset.__class__ = ImageClassificationDataset
-
-    mock_dataset.class_names = ['1', '2', '3']
-    model._model = MagicMock()
-    expected_return_value = {"result": True}
-    mock_history = MagicMock()
-    mock_history.history = expected_return_value
-
-    def mock_fit(dataset, epochs, shuffle, callbacks, validation_data=None):
-        assert dataset is not None
-        assert isinstance(epochs, int)
-        assert isinstance(shuffle, bool)
-        assert len(callbacks) > 0
-
-        return mock_history
-
-    model._model.fit = mock_fit
-
-    return_val = model.train(mock_dataset, output_dir="/tmp/output")
-    assert return_val == expected_return_value
-
 
 @pytest.mark.tensorflow
 @pytest.mark.parametrize(
     'cpu_model,enable_auto_mixed_precision,expected_auto_mixed_precision_parameter,tf_version,model_name,dataset_type',
     [['85', None, False, '2.9.0', 'efficientnet_b0', ImageClassificationDataset],
      ['143', None, True, '2.9.0', 'efficientnet_b0', ImageClassificationDataset],
      ['123', None, False, '2.9.0', 'efficientnet_b0', ImageClassificationDataset],
      ['85', True, True, '2.9.0', 'efficientnet_b0', ImageClassificationDataset],
      ['143', True, True, '2.9.0', 'efficientnet_b0', ImageClassificationDataset],
      ['123', True, True, '2.9.0', 'efficientnet_b0', ImageClassificationDataset],
      ['85', True, True, '2.10.0', 'efficientnet_b0', ImageClassificationDataset],
-     ['85', None, False, '2.9.0', 'bert_en_wwm_uncased_L-24_H-1024_A-16', TextClassificationDataset],
-     ['143', None, True, '2.9.0', 'bert_en_wwm_uncased_L-24_H-1024_A-16', TextClassificationDataset],
-     ['123', None, False, '2.9.0', 'bert_en_wwm_uncased_L-24_H-1024_A-16', TextClassificationDataset],
-     ['85', True, True, '2.9.0', 'bert_en_wwm_uncased_L-24_H-1024_A-16', TextClassificationDataset],
-     ['143', True, True, '2.9.0', 'bert_en_wwm_uncased_L-24_H-1024_A-16', TextClassificationDataset],
-     ['123', True, True, '2.9.0', 'bert_en_wwm_uncased_L-24_H-1024_A-16', TextClassificationDataset],
+     ['85', None, False, '2.9.0', 'bert-base-uncased', TextClassificationDataset],
+     ['143', None, True, '2.9.0', 'bert-base-uncased', TextClassificationDataset],
+     ['123', None, False, '2.9.0', 'bert-base-uncased', TextClassificationDataset],
+     ['85', True, True, '2.9.0', 'bert-base-uncased', TextClassificationDataset],
+     ['143', True, True, '2.9.0', 'bert-base-uncased', TextClassificationDataset],
+     ['123', True, True, '2.9.0', 'bert-base-uncased', TextClassificationDataset],
      ['85', True, True, '2.10.0', 'efficientnet_b0', ImageClassificationDataset],
      ['143', True, True, '2.10.0', 'efficientnet_b0', ImageClassificationDataset],
      ['123', True, True, '2.10.0', 'efficientnet_b0', ImageClassificationDataset],
      ['85', False, False, '2.9.1', 'efficientnet_b0', ImageClassificationDataset],
      ['143', False, False, '2.9.1', 'efficientnet_b0', ImageClassificationDataset],
      ['123', False, False, '2.9.1', 'efficientnet_b0', ImageClassificationDataset],
      ['123', False, None, '2.8.0', 'efficientnet_b0', ImageClassificationDataset],
@@ -333,15 +358,16 @@
      ['143', None, True, '3.1.0', 'efficientnet_b0', ImageClassificationDataset]])
 @patch("tlt.models.tf_model.tf.version")
 @patch("tlt.models.tf_model.tf.config.optimizer.set_experimental_options")
 @patch("tlt.utils.platform_util.PlatformUtil._get_cpuset")
 @patch("tlt.utils.platform_util.os")
 @patch("tlt.utils.platform_util.system_platform")
 @patch("tlt.utils.platform_util.subprocess")
-def test_tfhub_auto_mixed_precision(mock_subprocess, mock_platform, mock_os, mock_get_cpuset,
+@patch('tlt.models.text_classification.tf_hf_text_classification_model.prepare_huggingface_input_data')
+def test_tfhub_auto_mixed_precision(mock_tokenizer, mock_subprocess, mock_platform, mock_os, mock_get_cpuset,
                                     mock_set_experimental_options, mock_tf_version, cpu_model,
                                     enable_auto_mixed_precision, expected_auto_mixed_precision_parameter,
                                     tf_version, model_name, dataset_type):
     """
     Verifies that auto mixed precision is enabled by default for SPR (cpu model 85), but disabled by default for other
     CPU types like SKX (cpu model 143).  The default auto mixed precision setting is used when
     enable_auto_mixed_precision=None. Auto mixed precision was enabled for TF 2.9.0 and later, so don't expect the call
@@ -365,68 +391,78 @@
     mock_dataset.class_names = ['a', 'b']
 
     mock_tf_version.VERSION = tf_version
 
     model = model_factory.get_model(model_name, 'tensorflow')
     model._get_hub_model = MagicMock()
 
+    # Mock internal function to tokenize input data
+    mock_tokenizer.return_value = mock_dataset, []
+
     model.train(mock_dataset, output_dir="/tmp/output", enable_auto_mixed_precision=enable_auto_mixed_precision)
 
     if expected_auto_mixed_precision_parameter is not None:
         expected_parameter = {'auto_mixed_precision_mkl': expected_auto_mixed_precision_parameter}
         mock_set_experimental_options.assert_called_with(expected_parameter)
     else:
         # We expect that the auto mixed prercision config is not called (due to TF version unsupported)
         assert not mock_set_experimental_options.called
 
 
-@pytest.mark.tensorflow
-@pytest.mark.parametrize('model_name,use_case,dataset_type,optimizer,loss',
-                         [['efficientnet_b0', 'image_classification', ImageClassificationDataset,
-                           keras.optimizers.Adagrad, keras.losses.MeanSquaredError],
-                          ['custom', 'image_classification', ImageClassificationDataset,
-                           keras.optimizers.SGD, keras.losses.CategoricalCrossentropy],
-                          ['bert_en_wwm_uncased_L-24_H-1024_A-16', 'text_classification', TextClassificationDataset,
-                           keras.optimizers.RMSprop, keras.losses.BinaryCrossentropy]])
-def test_tf_optimizer_loss(model_name, use_case, dataset_type, optimizer, loss):
-    """
-    Tests initializing and training a model with configurable optimizers and loss functions
-    """
-
-    if model_name == 'custom':
-        model = model_factory.load_model(model_name, ALEXNET, 'tensorflow', use_case, optimizer=optimizer, loss=loss)
-    else:
-        model = model_factory.get_model(model_name, 'tensorflow', optimizer=optimizer, loss=loss)
-
-    model._generate_checkpoints = False
-    model._get_hub_model = MagicMock()
-    model._model = MagicMock()
-    model._model.fit = MagicMock()
-    assert model._optimizer_class == optimizer
-    assert model._loss_class == loss
-
-    mock_dataset = MagicMock()
-    mock_dataset.__class__ = dataset_type
-    if dataset_type == TextClassificationDataset:
-        mock_dataset.class_names = ['a', 'b']
-    else:
-        mock_dataset.class_names = ['a', 'b', 'c']
-
-    # Train is called and optimizer and loss objects should match the input types
-    model.train(mock_dataset, output_dir="/tmp/output/tf")
-    assert model._optimizer_class == optimizer
-    assert type(model._optimizer) == optimizer
-    assert model._loss_class == loss
-    assert type(model._loss) == loss
-
+# This is necessary to protect from import errors when testing in a tensorflow only environment
+if tf_env:
+    @pytest.mark.tensorflow
+    @pytest.mark.parametrize('model_name,use_case,dataset_type,optimizer,loss',
+                             [['efficientnet_b0', 'image_classification', ImageClassificationDataset,
+                              keras.optimizers.Adagrad, keras.losses.MeanSquaredError],
+                              ['custom', 'image_classification', ImageClassificationDataset,
+                              keras.optimizers.SGD, keras.losses.CategoricalCrossentropy],
+                              ['bert-base-uncased', 'text_classification', TextClassificationDataset,
+                              keras.optimizers.RMSprop, keras.losses.BinaryCrossentropy]])
+    @patch('tlt.models.text_classification.tf_hf_text_classification_model.prepare_huggingface_input_data')
+    def test_tf_optimizer_loss(mock_tokenizer, model_name, use_case, dataset_type, optimizer, loss):
+        """
+        Tests initializing and training a model with configurable optimizers and loss functions
+        """
+
+        if model_name == 'custom':
+            model = model_factory.load_model(model_name, ALEXNET, 'tensorflow', use_case, optimizer=optimizer, loss=loss)  # noqa: E501
+        else:
+            model = model_factory.get_model(model_name, 'tensorflow', optimizer=optimizer, loss=loss)
+
+        model._generate_checkpoints = False
+        model._get_hub_model = MagicMock()
+        model._model = MagicMock()
+        model._model.fit = MagicMock()
+        assert model._optimizer_class == optimizer
+        assert model._loss_class == loss
 
-@pytest.mark.tensorflow
-@pytest.mark.parametrize('model_name,loss',
-                         [['efficientnet_b0', 1],
-                          ['efficientnet_b0', 'foo'],
-                          ['bert_en_wwm_uncased_L-24_H-1024_A-16', keras.optimizers.Adam]])
-def test_tf_loss_wrong_type(model_name, loss):
-    """
-    Tests that an exception is thrown when the input loss function is the wrong type
-    """
-    with pytest.raises(TypeError):
-        model_factory.get_model(model_name, 'tensorflow', loss=loss)
+        mock_dataset = MagicMock()
+        mock_dataset.__class__ = dataset_type
+        if dataset_type == TextClassificationDataset:
+            mock_dataset.class_names = ['a', 'b']
+        else:
+            mock_dataset.class_names = ['a', 'b', 'c']
+
+        # Mock internal function to tokenize input data
+        mock_tokenizer.return_value = mock_dataset, []
+
+        # Train is called and optimizer and loss objects should match the input types
+        model.train(mock_dataset, output_dir="/tmp/output/tf")
+        assert model._optimizer_class == optimizer
+        assert type(model._optimizer) == optimizer
+        assert model._loss_class == loss
+        assert type(model._loss) == loss
+
+# This is necessary to protect from import errors when testing in a tensorflow only environment
+if tf_env:
+    @pytest.mark.tensorflow
+    @pytest.mark.parametrize('model_name,loss',
+                             [['efficientnet_b0', 1],
+                              ['efficientnet_b0', 'foo'],
+                              ['bert-base-uncased', keras.optimizers.Adam]])
+    def test_tf_loss_wrong_type(model_name, loss):
+        """
+        Tests that an exception is thrown when the input loss function is the wrong type
+        """
+        with pytest.raises(TypeError):
+            model_factory.get_model(model_name, 'tensorflow', loss=loss)
```

## tests/tools/cli/test_benchmark_cli.py

```diff
@@ -24,90 +24,94 @@
 import tempfile
 
 from click.testing import CliRunner
 from pathlib import Path
 from unittest.mock import MagicMock, patch
 from tlt.tools.cli.commands.benchmark import benchmark
 from tlt.utils.types import FrameworkType
+from tlt.utils.file_utils import download_and_extract_zip_file
 
 
 @pytest.mark.common
 @pytest.mark.parametrize('model_name,framework,batch_size,mode',
                          [['efficientnet_b0', FrameworkType.TENSORFLOW, 512, 'performance'],
                           ['inception_v3', FrameworkType.TENSORFLOW, 32, 'accuracy'],
                           ['resnet50', FrameworkType.PYTORCH, 128, 'performance'],
-                          ['efficientnet_b2', FrameworkType.PYTORCH, 256, 'accuracy']])
+                          ['bert-base-cased', FrameworkType.PYTORCH, 256, 'accuracy']])
 @patch("tlt.models.model_factory.get_model")
 @patch("tlt.datasets.dataset_factory.load_dataset")
 def test_benchmark(mock_load_dataset, mock_get_model, model_name, framework, batch_size, mode):
     """
-    Tests the benchmark comamnd with an without an Intel Neural Compressor config file and verifies that the
+    Tests the benchmark command and verifies that the
     expected calls are made on the tlt model object. The call parameters also verify that the benchmark command
     is able to properly identify the model's name based on the directory and the framework type based on the
     type of saved model.
     """
     runner = CliRunner()
 
     tmp_dir = tempfile.mkdtemp()
     model_dir = os.path.join(tmp_dir, model_name, '3')
     dataset_dir = os.path.join(tmp_dir, 'data')
     output_dir = os.path.join(tmp_dir, 'output')
 
+    if model_name == "bert-base-cased":
+        # Get the dataset
+        zip_file_url = "https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip"
+        csv_dir = os.path.join(dataset_dir, "sms_spam_collection")
+        csv_file_name = "SMSSpamCollection"
+        delimiter = '\t'
+
+        # If the SMS Spam collection csv file is not found, download and extract the file:
+        if not os.path.exists(os.path.join(csv_dir, csv_file_name)):
+            # Download the zip file with the SMS Spam collection dataset
+            download_and_extract_zip_file(zip_file_url, csv_dir)
+
     try:
         for new_dir in [model_dir, dataset_dir]:
-            os.makedirs(new_dir)
+            os.makedirs(new_dir, exist_ok=True)
 
         if framework == FrameworkType.TENSORFLOW:
             Path(os.path.join(model_dir, 'saved_model.pb')).touch()
         elif framework == FrameworkType.PYTORCH:
             Path(os.path.join(model_dir, 'model.pt')).touch()
 
         model_mock = MagicMock()
         data_mock = MagicMock()
 
+        if model_name == "bert-base-cased":
+            model_mock.use_case = "text_classification"
+        else:
+            model_mock.use_case = "image_classification"
+
         mock_get_model.return_value = model_mock
         mock_load_dataset.return_value = data_mock
 
-        # Call the benchmark command without an Intel Neural Compressor config file
-        result = runner.invoke(benchmark,
-                               ["--model-dir", model_dir, "--dataset_dir", dataset_dir, "--mode", mode,
-                                "--batch-size", batch_size, "--output-dir", output_dir])
+        # Call the benchmark command
+        if model_mock.use_case == "image_classification":
+            result = runner.invoke(benchmark,
+                                   ["--model-dir", model_dir, "--dataset_dir", dataset_dir,
+                                    "--batch-size", batch_size, "--output-dir", output_dir])
+        else:
+            result = runner.invoke(benchmark,
+                                   ["--model-dir", model_dir, "--dataset_dir", dataset_dir,
+                                    "--batch-size", batch_size, "--output-dir", output_dir,
+                                    "--dataset-file", csv_file_name, "--delimiter", delimiter])
 
-        # Verify that the expected calls were made, including to create an Intel Neural Compressor config file
+        # Verify that the expected calls were made
         mock_get_model.assert_called_once_with(model_name, framework)
-        mock_load_dataset.assert_called_once_with(dataset_dir, model_mock.use_case, model_mock.framework)
-        assert model_mock.write_inc_config_file.called
+        if model_mock.use_case == "image_classification":
+            mock_load_dataset.assert_called_once_with(dataset_dir, model_mock.use_case, model_mock.framework)
+        else:
+            mock_load_dataset.assert_called_once_with(dataset_dir, model_mock.use_case, model_mock.framework,
+                                                      csv_file_name=csv_file_name, delimiter=delimiter)
         assert model_mock.benchmark.called
 
         # Verify a successful exit code
         assert result.exit_code == 0
 
-        # Reset mocks to do another experiment with an Intel Neural Compressor confijg file
-        model_mock.reset_mock()
-        data_mock.reset_mock()
-        mock_get_model.reset_mock()
-        mock_load_dataset.reset_mock()
-
-        # Create a temp inc config yaml file
-        inc_config = os.path.join(tmp_dir, 'inc_config.yaml')
-        Path(inc_config).touch()
-
-        # Call benchmark with a config file
-        result = runner.invoke(benchmark,
-                               ["--model-dir", model_dir, "--dataset_dir", dataset_dir, "--inc-config", inc_config])
-
-        mock_get_model.assert_called_once_with(model_name, framework)
-        mock_load_dataset.assert_called_once_with(dataset_dir, model_mock.use_case, model_mock.framework)
-        model_mock.benchmark.called_once_with(model_dir, inc_config, mode)
-
-        # Function to create an Intel Neural Compressor config file shouldn't have been called, since yaml was provided
-        model_mock.write_inc_config_file.assert_not_called()
-
-        # Verify a successful exit code
-        assert result.exit_code == 0
     finally:
         if os.path.exists(tmp_dir):
             shutil.rmtree(tmp_dir)
 
 
 @pytest.mark.common
 @pytest.mark.parametrize('model_name,model_file',
@@ -279,29 +283,7 @@
                                      ["--model-dir", self._model_dir,
                                       "--dataset_dir", self._dataset_dir,
                                       "--output-dir", self._output_dir,
                                       "--batch-size", batch_size])
 
         assert result.exit_code == 2
         assert "Invalid value for '--batch-size'" in result.output
-
-    @pytest.mark.common
-    @pytest.mark.parametrize('mode',
-                             ['foo', 'benchmark', '0'])
-    def test_benchmark_invalid_mode(self, mode):
-        """
-        Verifies that benchmark command fails if the mode value is invalid (the choices are: accuracy, performance)
-        """
-
-        # Create the model file
-        Path(os.path.join(self._model_dir, 'saved_model.pt')).touch()
-
-        # Call the benchmark command with the model directory
-        result = self._runner.invoke(benchmark,
-                                     ["--model-dir", self._model_dir,
-                                      "--dataset_dir", self._dataset_dir,
-                                      "--output-dir", self._output_dir,
-                                      "--mode", mode])
-
-        assert result.exit_code == 2
-        assert "Invalid value for '--mode'" in result.output
-        assert "'{}' is not one of 'performance', 'accuracy'".format(mode) in result.output
```

## tests/tools/cli/test_eval_cli.py

```diff
@@ -87,15 +87,15 @@
     finally:
         if os.path.exists(tmp_dir):
             shutil.rmtree(tmp_dir)
 
 
 @pytest.mark.common
 @pytest.mark.parametrize('model_name,framework',
-                         [['small_bert/bert_en_uncased_L-10_H-128_A-2', FrameworkType.TENSORFLOW],
+                         [['google/bert_uncased_L-10_H-128_A-2', FrameworkType.TENSORFLOW],
                           ['bert_en_uncased_L-12_H-768_A-12', FrameworkType.PYTORCH]])
 @patch("tlt.models.model_factory.get_model")
 @patch("tlt.datasets.dataset_factory.load_dataset")
 @patch("inspect.getfullargspec")
 def test_eval_preprocess_without_image_size(mock_inspect, mock_load_dataset, mock_get_model, model_name, framework):
     """
     Tests the eval command with a dataset preprocessing method that just has a batch size arg. Actual calls for the
```

## tests/tools/cli/test_quantize_cli.py

```diff
@@ -24,92 +24,96 @@
 import tempfile
 
 from click.testing import CliRunner
 from pathlib import Path
 from unittest.mock import MagicMock, patch
 from tlt.tools.cli.commands.quantize import quantize
 from tlt.utils.types import FrameworkType
+from tlt.utils.file_utils import download_and_extract_zip_file
 
 
 @pytest.mark.common
 @pytest.mark.parametrize('model_name,framework,batch_size',
                          [['efficientnet_b0', FrameworkType.TENSORFLOW, 512],
                           ['inception_v3', FrameworkType.TENSORFLOW, 32],
                           ['resnet50', FrameworkType.PYTORCH, 128],
-                          ['efficientnet_b2', FrameworkType.PYTORCH, 256]])
+                          ['bert-base-cased', FrameworkType.PYTORCH, 256]])
 @patch("tlt.models.model_factory.get_model")
 @patch("tlt.datasets.dataset_factory.load_dataset")
 def test_quantize(mock_load_dataset, mock_get_model, model_name, framework, batch_size):
     """
-    Tests the quantize comamnd with an without an Intel Neural Compressor config file and verifies that the
+    Tests the quantize command and verifies that the
     expected calls are made on the tlt model object. The call parameters also verify that the quantize command
     is able to properly identify the model's name based on the directory and the framework type based on the
     type of saved model.
     """
     runner = CliRunner()
 
     tmp_dir = tempfile.mkdtemp()
     model_dir = os.path.join(tmp_dir, model_name, '3')
     dataset_dir = os.path.join(tmp_dir, 'data')
     output_dir = os.path.join(tmp_dir, 'output')
 
+    if model_name == "bert-base-cased":
+        # Get the dataset
+        zip_file_url = "https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip"
+        csv_dir = os.path.join(dataset_dir, "sms_spam_collection")
+        csv_file_name = "SMSSpamCollection"
+        delimiter = '\t'
+
+        # If the SMS Spam collection csv file is not found, download and extract the file:
+        if not os.path.exists(os.path.join(csv_dir, csv_file_name)):
+            # Download the zip file with the SMS Spam collection dataset
+            download_and_extract_zip_file(zip_file_url, csv_dir)
+
     try:
         for new_dir in [model_dir, dataset_dir]:
-            os.makedirs(new_dir)
+            os.makedirs(new_dir, exist_ok=True)
 
         if framework == FrameworkType.TENSORFLOW:
             Path(os.path.join(model_dir, 'saved_model.pb')).touch()
         elif framework == FrameworkType.PYTORCH:
             Path(os.path.join(model_dir, 'model.pt')).touch()
 
         model_mock = MagicMock()
         data_mock = MagicMock()
 
+        if model_name == "bert-base-cased":
+            model_mock.use_case = "text_classification"
+        else:
+            model_mock.use_case = "image_classification"
+
         mock_get_model.return_value = model_mock
         mock_load_dataset.return_value = data_mock
 
-        # Call the quantize command without an Intel Neural Compressor config file
-        result = runner.invoke(quantize,
-                               ["--model-dir", model_dir, "--dataset_dir", dataset_dir,
-                                "--batch-size", batch_size, "--output-dir", output_dir])
+        # Call the quantize command
+        if model_mock.use_case == "image_classification":
+            result = runner.invoke(quantize,
+                                   ["--model-dir", model_dir, "--dataset_dir", dataset_dir,
+                                    "--batch-size", batch_size, "--output-dir", output_dir])
+        else:
+            result = runner.invoke(quantize,
+                                   ["--model-dir", model_dir, "--dataset_dir", dataset_dir,
+                                    "--batch-size", batch_size, "--output-dir", output_dir,
+                                    "--dataset-file", csv_file_name, "--delimiter", delimiter])
 
         # Verify that the expected calls were made, including to create an Intel Neural Compressor config file
         mock_get_model.assert_called_once_with(model_name, framework)
-        mock_load_dataset.assert_called_once_with(dataset_dir, model_mock.use_case, model_mock.framework)
-        assert model_mock.write_inc_config_file.called
-        assert model_mock.quantize.called
 
-        # Verify a successful exit code
-        assert result.exit_code == 0
-
-        # Reset mocks to do another experiment with an Intel Neural Compressor config file
-        model_mock.reset_mock()
-        data_mock.reset_mock()
-        mock_get_model.reset_mock()
-        mock_load_dataset.reset_mock()
-
-        # Create a temp inc config yaml file
-        inc_config = os.path.join(tmp_dir, 'inc_config.yaml')
-        Path(inc_config).touch()
+        if model_mock.use_case == "image_classification":
+            mock_load_dataset.assert_called_once_with(dataset_dir, model_mock.use_case, model_mock.framework)
+        else:
+            mock_load_dataset.assert_called_once_with(dataset_dir, model_mock.use_case, model_mock.framework,
+                                                      csv_file_name=csv_file_name, delimiter=delimiter)
 
-        # Call quantize with a config file
-        result = runner.invoke(quantize,
-                               ["--model-dir", model_dir, "--dataset_dir", dataset_dir, "--inc-config", inc_config,
-                                "--output-dir", output_dir])
-        expected_quantization_dir = os.path.join(output_dir, "quantized", model_name, "1")
-
-        mock_get_model.assert_called_once_with(model_name, framework)
-        mock_load_dataset.assert_called_once_with(dataset_dir, model_mock.use_case, model_mock.framework)
-        model_mock.quantize.called_once_with(model_dir, expected_quantization_dir)
-
-        # Function to create an Intel Neural Compressor config file shouldn't have been called, since yaml was provided
-        model_mock.write_inc_config_file.assert_not_called()
+        assert model_mock.quantize.called
 
         # Verify a successful exit code
         assert result.exit_code == 0
+
     finally:
         if os.path.exists(tmp_dir):
             shutil.rmtree(tmp_dir)
 
 
 @pytest.mark.common
 @pytest.mark.parametrize('model_name,model_file',
@@ -258,32 +262,30 @@
         for new_dir in [model_dir, dataset_dir]:
             os.makedirs(new_dir)
 
         Path(os.path.join(model_dir, 'saved_model.pb')).touch()
 
         model_mock = MagicMock()
         data_mock = MagicMock()
+        model_mock.use_case = "image_classification"
+        data_mock.use_case = "image_classification"
 
         mock_get_model.return_value = model_mock
         mock_load_dataset.return_value = data_mock
 
-        # Create a temp inc config yaml file
-        inc_config = os.path.join(tmp_dir, 'inc_config.yaml')
-        Path(inc_config).touch()
-
         for i in range(1, 5):
             # Call the quantize command
             result = runner.invoke(quantize,
                                    ["--model-dir", model_dir, "--dataset_dir", dataset_dir,
-                                    "--output-dir", output_dir, "--inc-config", inc_config])
+                                    "--output-dir", output_dir])
             assert result.exit_code == 0
 
             # Check for an expected quantization output dir with the folder number incrementing
             expected_quantize_dir = os.path.join(output_dir, "quantize", model_name, str(i))
-            model_mock.quantize.called_once_with(model_dir, expected_quantize_dir, inc_config)
+            model_mock.quantize.called_once_with(model_dir, expected_quantize_dir)
 
             model_mock.reset_mock()
 
     finally:
         if os.path.exists(tmp_dir):
             shutil.rmtree(tmp_dir)
```

## tests/tools/cli/test_train_cli.py

```diff
@@ -83,15 +83,15 @@
     finally:
         if os.path.exists(tmp_dir):
             shutil.rmtree(tmp_dir)
 
 
 @pytest.mark.common
 @pytest.mark.parametrize('model_name,framework',
-                         [['small_bert/bert_en_uncased_L-10_H-128_A-2', FrameworkType.TENSORFLOW],
+                         [['google/bert_uncased_L-10_H-128_A-2', FrameworkType.TENSORFLOW],
                           ['bert_en_uncased_L-12_H-768_A-12', FrameworkType.PYTORCH]])
 @patch("tlt.models.model_factory.get_model")
 @patch("tlt.datasets.dataset_factory.load_dataset")
 @patch("inspect.getfullargspec")
 def test_train_preprocess_without_image_size(mock_inspect, mock_load_dataset, mock_get_model, model_name, framework):
     """
     Tests the train command with a dataset preprocessing method that just has a batch size arg. Actual calls for the
@@ -275,18 +275,32 @@
         model_mock = MagicMock()
         data_mock = MagicMock()
         mock_get_model.return_value = model_mock
         mock_load_dataset.return_value = data_mock
         model_mock.export.return_value = output_dir
 
         # Call the train command
-        result = runner.invoke(train,
-                               ["--framework", str(framework), "--model-name", model_name, "--dataset_dir", dataset_dir,
-                                "--output-dir", output_dir, "--epochs", epochs, "--early_stopping", early_stopping,
-                                "--lr_decay", lr_decay])
+        if early_stopping and lr_decay:
+            result = runner.invoke(train,
+                                   ["--framework", str(framework), "--model-name", model_name, "--dataset_dir",
+                                    dataset_dir, "--output-dir", output_dir, "--epochs", epochs, "--early_stopping",
+                                    "--lr_decay"])
+        elif early_stopping:
+            result = runner.invoke(train,
+                                   ["--framework", str(framework), "--model-name", model_name, "--dataset_dir",
+                                    dataset_dir, "--output-dir", output_dir, "--epochs", epochs, "--early_stopping"])
+        elif lr_decay:
+            result = runner.invoke(train,
+                                   ["--framework", str(framework), "--model-name", model_name, "--dataset_dir",
+                                    dataset_dir, "--output-dir", output_dir, "--epochs", epochs, "--lr_decay"])
+
+        else:
+            result = runner.invoke(train,
+                                   ["--framework", str(framework), "--model-name", model_name, "--dataset_dir",
+                                    dataset_dir, "--output-dir", output_dir, "--epochs", epochs])
 
         # Verify that the expected calls were made
         mock_get_model.assert_called_once_with(model_name, str(framework))
         mock_load_dataset.assert_called_once_with(dataset_dir, model_mock.use_case, model_mock.framework)
         assert data_mock.shuffle_split.called
         assert model_mock.train.called
```

## tests/utils/test_file_utils.py

```diff
@@ -46,23 +46,27 @@
 
 @pytest.mark.common
 def test_download():
     output_dir = tempfile.mkdtemp()
     expected = os.path.join(output_dir, 'example.txt')
 
     # Set up mock return value
+    mock_file_contents = MagicMock()
+    mock_file_contents.read.return_value = b''
+
     mock_file = MagicMock(spec=io.BytesIO)
     mock_file.__enter__.return_value = mock_file
-    mock_file.read.return_value = b''
+    mock_file.raw = mock_file_contents
 
-    # Patch urlopen
-    with mock.patch('urllib.request.urlopen', return_value=mock_file) as mock_urlopen:
+    # Patch requests.get
+    with mock.patch('requests.get', return_value=mock_file) as mock_get:
         result = download_file('https://example-files.online-convert.com/document/txt/example.txt', output_dir)
         assert result == expected
 
         # Check that the mock was called as expected
-        mock_urlopen.assert_called_with('https://example-files.online-convert.com/document/txt/example.txt')
-        mock_file.read.assert_called_once()
+        mock_get.assert_called_with('https://example-files.online-convert.com/document/txt/example.txt',
+                                    stream=True, timeout=30)
+        mock_file_contents.read.assert_called_once()
 
     # Delete the temp output directory
     if os.path.exists(output_dir) and os.path.isdir(output_dir):
         shutil.rmtree(output_dir)
```

## tests/utils/test_platform_util.py

```diff
@@ -15,17 +15,17 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 import json
 import pytest
 import os
-from mock import MagicMock, mock_open, patch
+from mock import mock_open, patch
 
-from tlt.utils.platform_util import PlatformUtil, CPUInfo
+from tlt.utils.platform_util import PlatformUtil, OptimizedPlatformUtil, CPUInfo
 from test_utils import platform_config
 
 
 def setup_mock_values(mock_platform, mock_os, mock_subprocess):
     platform_config.set_mock_system_type(mock_platform)
     platform_config.set_mock_os_access(mock_os)
     platform_config.set_mock_lscpu_subprocess_values(mock_subprocess)
@@ -63,15 +63,15 @@
     Verifies that platform_utils gives us the proper values that we expect
     based on the lscpu_output string provided.
     """
     platform_mock.return_value = platform_config.SYSTEM_TYPE
     os_mock.return_value = True
     get_cpuset_mock.return_value = "0-111"
     subprocess_mock.return_value = platform_config.LSCPU_OUTPUT
-    platform_util = PlatformUtil(MagicMock(verbose=True))
+    platform_util = PlatformUtil(verbose=True)
     platform_util.linux_init()
     assert platform_util.num_cpu_sockets == 2
     assert platform_util.num_cores_per_socket == 28
     assert platform_util.num_threads_per_core == 2
     assert platform_util.num_logical_cpus == 112
     assert platform_util.num_numa_nodes == 2
     assert platform_util.cpu_family == '6'
@@ -98,15 +98,15 @@
     # get the lscpu sample output, but replace in the parameterized cpu model id
     lscpu_value = platform_config.LSCPU_OUTPUT
     original_model_value = "Model:                 143\n"  # model test value from the test platform config
     new_model_value = "Model:                 {}\n".format(cpu_model)
     lscpu_value = lscpu_value.replace(original_model_value, new_model_value)
 
     subprocess_mock.return_value = lscpu_value
-    platform_util = PlatformUtil(MagicMock(verbose=True))
+    platform_util = PlatformUtil(verbose=True)
     platform_util.linux_init()
     assert platform_util.cpu_type == expected_type
 
 
 @pytest.mark.common
 def test_platform_util_unsupported_os(platform_mock, subprocess_mock, os_mock):
     """
@@ -114,15 +114,15 @@
     based on the lscpu_output string provided.
     """
     os_mock.return_value = True
     subprocess_mock.return_value = platform_config.LSCPU_OUTPUT
     # Mac is not supported yet
     platform_mock.return_value = "Mac"
     with pytest.raises(NotImplementedError) as e:
-        PlatformUtil(MagicMock(verbose=True))
+        PlatformUtil(verbose=True)
     assert "Mac Support not yet implemented" in str(e)
 
 
 @pytest.mark.common
 def test_cpu_info_binding_information(subprocess_mock):
     """
     Verifies that cpu_info binding_information property gives us the proper values
@@ -183,15 +183,15 @@
     """ Test the platform utils to ensure that we are getting the proper core lists """
     subprocess_mock.return_value = platform_config.LSCPU_OUTPUT
     subprocess_popen_mock.return_value.stdout.readlines.return_value = platform_config.NUMA_CORES_OUTPUT
     platform_mock.return_value = platform_config.SYSTEM_TYPE
     get_cpuset_mock.return_value = "0-111"
     os_mock.return_value = True
     subprocess_mock.return_value = platform_config.LSCPU_OUTPUT
-    platform_util = PlatformUtil(MagicMock(verbose=True))
+    platform_util = PlatformUtil(verbose=True, numa_cores_per_instance="socket")
 
     # ensure there are 2 items in the list since there are 2 sockets
     assert len(platform_util.cpu_core_list) == 2
 
     # ensure each list of cores has the length of the number of cores per socket
     for core_list in platform_util.cpu_core_list:
         assert len(core_list) == platform_util.num_cores_per_socket
@@ -202,15 +202,15 @@
     """
     Verifies that platform_utils gives us the proper values that we expect
     based on the wmic_output string provided.
     """
     platform_mock.return_value = "Windows"
     os_mock.return_value = True
     subprocess_mock.return_value = platform_config.WMIC_OUTPUT
-    platform_util = PlatformUtil(MagicMock(verbose=True))
+    platform_util = PlatformUtil(verbose=True)
     platform_util.windows_init()
     assert platform_util.num_cpu_sockets == 2
     assert platform_util.num_cores_per_socket == 28
     assert platform_util.num_threads_per_core == 28
     assert platform_util.num_logical_cpus == 56
     assert platform_util.num_numa_nodes == 0
 
@@ -236,15 +236,15 @@
     Tests the PlatformUtils _get_list_from_string_ranges function that converts string
     number ranges to an integer list.
     """
     platform_mock.return_value = platform_config.SYSTEM_TYPE
     subprocess_mock.return_value = platform_config.LSCPU_OUTPUT
     get_cpuset_mock.return_value = cpuset_range
     os_mock.return_value = True
-    platform_util = PlatformUtil(MagicMock())
+    platform_util = PlatformUtil()
     result = platform_util._get_list_from_string_ranges(cpuset_range)
     assert result == expected_list
 
 
 @pytest.mark.common
 @pytest.mark.parametrize('cpuset_range,expected_core_list',
                          [["0-7,28-35",
@@ -261,15 +261,15 @@
     subprocess_popen_mock.return_value.stdout.readlines.return_value = platform_config.NUMA_CORES_OUTPUT
     platform_mock.return_value = platform_config.SYSTEM_TYPE
     os_mock.return_value = True
     subprocess_mock.return_value = platform_config.LSCPU_OUTPUT
     path_exists_mock.return_value = True
     cpuset_mock = mock_open(read_data=cpuset_range)
     with patch("builtins.open", cpuset_mock):
-        platform_util = PlatformUtil(MagicMock(verbose=True, numa_cores_per_instance=4))
+        platform_util = PlatformUtil(verbose=True, numa_cores_per_instance=4)
 
     # ensure there are 2 items in the list since there are 2 sockets
     assert len(platform_util.cpu_core_list) == 2
 
     # Check that the core list matches the ranges defined for the cpuset file read
     assert platform_util.cpu_core_list == expected_core_list
 
@@ -292,22 +292,75 @@
     then the num_cpu_sockets should be 1, even if the system itself has 2 sockets (since the
     container only has access to 1).
     """
     platform_mock.return_value = platform_config.SYSTEM_TYPE
     os_mock.return_value = True
     get_cpuset_mock.return_value = cpuset_range
     subprocess_mock.return_value = platform_config.LSCPU_OUTPUT
-    platform_util = PlatformUtil(MagicMock(verbose=True))
+    platform_util = PlatformUtil(verbose=True)
     platform_util.linux_init()
     assert platform_util.num_cpu_sockets == expected_num_sockets
 
 
 @pytest.mark.common
 def test_platform_util_with_no_args(platform_mock, subprocess_mock):
     """
     Verifies that PlatformUtil object can be created with an empty string, as needed
     by the performance Jupyter notebooks.
     """
     platform_mock.return_value = platform_config.SYSTEM_TYPE
     subprocess_mock.return_value = platform_config.LSCPU_OUTPUT
-    platform_util = PlatformUtil("")
+    platform_util = PlatformUtil()
     assert platform_util.num_logical_cpus == 112
+
+
+@pytest.mark.common
+@pytest.mark.parametrize('omp_num_threads,omp_thread_limit,kmp_blocktime,kmp_affinity,'
+                         'tf_num_intraop_threads,tf_num_interop_threads,'
+                         'tf_enable_mkl_native_format,ld_preload',
+                         [[-1, None, None, None, None, None, None, None],
+                          [1000, None, None, None, None, None, None, None],
+                          [None, 0, None, None, None, None, None, None],
+                          [None, 1000, None, None, None, None, None, None],
+                          [None, None, -1, None, None, None, None, None],
+                          [None, None, None, 'garbage_string', None, None, None, None],
+                          [None, None, None, None, -1, None, None, None],
+                          [None, None, None, None, None, -1, None, None],
+                          [None, None, None, None, None, None, -1, None],
+                          [None, None, None, None, None, None, None, 'path/to/non_so_file.txt'],
+                          [None, None, None, None, None, None, None, 'path/to/invalid_file.so']])
+def test_optimized_platform_util_invalid_args(omp_num_threads, omp_thread_limit, kmp_blocktime,
+                                              kmp_affinity, tf_num_intraop_threads, tf_num_interop_threads,
+                                              tf_enable_mkl_native_format, ld_preload):
+    with pytest.raises((ValueError, FileNotFoundError)):
+        OptimizedPlatformUtil(omp_num_threads, omp_thread_limit, kmp_blocktime, kmp_affinity,
+                              tf_num_intraop_threads, tf_num_interop_threads,
+                              tf_enable_mkl_native_format, ld_preload)
+
+
+@pytest.mark.common
+@pytest.mark.parametrize('omp_num_threads,omp_thread_limit,kmp_blocktime,kmp_affinity,'
+                         'tf_num_intraop_threads,tf_num_interop_threads,'
+                         'tf_enable_mkl_native_format,ld_preload',
+                         [[28, 112, 0, 'granularity=fine', 28, 2, 1, '/tmp/valid_file.so']])
+def test_optimized_platform_util_set_env_vars(omp_num_threads, omp_thread_limit, kmp_blocktime,
+                                              kmp_affinity, tf_num_intraop_threads, tf_num_interop_threads,
+                                              tf_enable_mkl_native_format, ld_preload):
+    try:
+        with open('/tmp/valid_file.so', 'x'):
+            OptimizedPlatformUtil(omp_num_threads, omp_thread_limit, kmp_blocktime, kmp_affinity,
+                                  tf_num_intraop_threads, tf_num_interop_threads,
+                                  tf_enable_mkl_native_format, ld_preload)
+
+            assert 'OMP_NUM_THREADS' in os.environ and os.environ.get('OMP_NUM_THREADS') == str(omp_num_threads)
+            assert 'OMP_THREAD_LIMIT' in os.environ and os.environ.get('OMP_THREAD_LIMIT') == str(omp_thread_limit)
+            assert 'KMP_BLOCKTIME' in os.environ and os.environ.get('KMP_BLOCKTIME') == str(kmp_blocktime)
+            assert 'KMP_AFFINITY' in os.environ and os.environ.get('KMP_AFFINITY') == kmp_affinity
+            assert 'TF_NUM_INTRAOP_THREADS' in os.environ and os.environ.get(
+                'TF_NUM_INTRAOP_THREADS') == str(tf_num_intraop_threads)
+            assert 'TF_NUM_INTEROP_THREADS' in os.environ and os.environ.get(
+                'TF_NUM_INTEROP_THREADS') == str(tf_num_interop_threads)
+            assert 'TF_ENABLE_MKL_NATIVE_FORMAT' in os.environ and os.environ.get(
+                'TF_ENABLE_MKL_NATIVE_FORMAT') == str(tf_enable_mkl_native_format)
+            assert 'LD_PRELOAD' in os.environ and os.environ.get('LD_PRELOAD') == ld_preload
+    finally:
+        os.remove('/tmp/valid_file.so')
```

## tlt/datasets/dataset_factory.py

```diff
@@ -108,15 +108,15 @@
         dataset_name (str): optional; name of the dataset used for informational purposes
         kwargs: optional; additional keyword arguments depending on the type of dataset being loaded
 
     Returns:
         (dataset)
 
     Raises:
-        NotImplementedError if the type of dataset being loaded is not supported
+        NotImplementedError: if the type of dataset being loaded is not supported
 
     Example:
         >>> from tlt.datasets.dataset_factory import load_dataset
         >>> data = load_dataset('/tmp/data/flower_photos', 'image_classification', 'tensorflow')
         Found 3670 files belonging to 5 classes.
         >>> data.class_names
         ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']
@@ -162,15 +162,15 @@
                                datasets for PyTorch NLP models or Hugging Face models.
         **kwargs: optional; additional keyword arguments for the framework or dataset_catalog
 
     Returns:
         (dataset)
 
     Raises:
-        NotImplementedError if the dataset requested is not supported yet
+        NotImplementedError: if the dataset requested is not supported yet
 
     Example:
         >>> from tlt.datasets.dataset_factory import get_dataset
         >>> data = get_dataset('/tmp/data/', 'image_classification', 'tensorflow', 'tf_flowers', 'tf_datasets')
         >>> sorted(data.class_names)
         ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']
```

## tlt/datasets/hf_dataset.py

```diff
@@ -46,15 +46,15 @@
             Args:
                 subset (str): default "all", can also be "train", "validation", or "test"
 
             Returns:
                 (examples, labels)
 
             Raises:
-                ValueError if the dataset is not defined yet or the given subset is not valid
+                ValueError: if the dataset is not defined yet or the given subset is not valid
         """
 
         if subset == 'all' and self._dataset is not None:
             return next(iter(self._data_loader))
         elif subset == 'train' and self.train_subset is not None:
             return next(iter(self._train_loader))
         elif subset == 'validation' and self.validation_subset is not None:
@@ -81,15 +81,15 @@
                 batch_size (int): Number of batches to split the data.
                 padding (str): desired padding. (default: "max_length")
                 max_length (int): desired max length. (default: 64)
                 truncation (bool): Boolean specifying to truncate the word tokens to match with the
                 longest sentence. (default: True)
                 max_length (int): Maximum sequence length
             Raises:
-                ValueError if data has already been preprocessed (or) non integer batch size given (or)
+                ValueError: if data has already been preprocessed (or) non integer batch size given (or)
                 given dataset hasn't been implemented into the API yet.
         """
 
         # Sanity checks
         if not isinstance(batch_size, int) or batch_size < 1:
             raise ValueError("batch_size should be an positive integer")
 
@@ -144,15 +144,15 @@
                 train_pct (float): default .75, percentage of dataset to use for training
                 val_pct (float):  default .25, percentage of dataset to use for validation
                 test_pct (float): default 0.0, percentage of dataset to use for testing
                 shuffle_files (bool): default True, optionally control whether shuffling occurs
                 seed (None or int): default None, can be set for pseudo-randomization
 
             Raises:
-                ValueError if percentage input args are not floats or sum to greater than 1
+                ValueError: if percentage input args are not floats or sum to greater than 1
                 """
         # Sanity checks
         if not (isinstance(train_pct, float) and isinstance(val_pct, float) and isinstance(test_pct, float)):
             raise ValueError("Percentage arguments must be floats.")
 
         if train_pct + val_pct + test_pct > 1.0:
             raise ValueError("Sum of percentage arguments must be less than or equal to 1.")
@@ -313,7 +313,30 @@
 
     @property
     def validation_loader(self):
         if self._validation_loader:
             return self._validation_loader
         else:
             raise ValueError("validation split not specified")
+
+    def get_inc_dataloaders(self):
+        calib_dataset = self.train_subset
+        if self.validation_loader is not None:
+            eval_dataset = self.validation_subset
+        elif self.test_loader is not None:
+            eval_dataset = self.test_subset
+        else:
+            eval_dataset = self.train_subset
+
+        # Drop the label column because Intel Neural Compressor does not like it embedded with the features
+        # If we need to compute metrics from the labels, we can improve this with a subclass of
+        # torch.utils.data.Dataset or neural_compressor.data.datasets.bert_dataset.PytorchBertDataset that
+        # also returns the labels from __getitem__
+        for label_col_name in ['labels', 'label']:
+            if label_col_name in self._dataset.features.keys():
+                calib_dataset = calib_dataset.remove_columns(label_col_name)
+                eval_dataset = eval_dataset.remove_columns(label_col_name)
+
+        calib_dataloader = loader(calib_dataset, batch_size=self._preprocessed['batch_size'])
+        eval_dataloader = loader(eval_dataset, batch_size=self._preprocessed['batch_size'])
+
+        return calib_dataloader, eval_dataloader
```

## tlt/datasets/pytorch_dataset.py

```diff
@@ -95,15 +95,15 @@
             Args:
                 subset (str): default "all", can also be "train", "validation", or "test"
 
             Returns:
                 (examples, labels)
 
             Raises:
-                ValueError if the dataset is not defined yet or the given subset is not valid
+                ValueError: if the dataset is not defined yet or the given subset is not valid
         """
         if subset == 'all' and self._dataset is not None:
             return next(iter(self._data_loader))
         elif subset == 'train' and self._train_loader is not None:
             return next(iter(self._train_loader))
         elif subset == 'validation' and self._validation_loader is not None:
             return next(iter(self._validation_loader))
@@ -120,15 +120,15 @@
                 train_pct (float): default .75, percentage of dataset to use for training
                 val_pct (float):  default .25, percentage of dataset to use for validation
                 test_pct (float): default 0.0, percentage of dataset to use for testing
                 shuffle_files (bool): default True, optionally control whether shuffling occurs
                 seed (None or int): default None, can be set for pseudo-randomization
 
             Raises:
-                ValueError if percentage input args are not floats or sum to greater than 1
+                ValueError: if percentage input args are not floats or sum to greater than 1
         """
         if not (isinstance(train_pct, float) and isinstance(val_pct, float) and isinstance(test_pct, float)):
             raise ValueError("Percentage arguments must be floats.")
         if train_pct + val_pct + test_pct > 1.0:
             raise ValueError("Sum of percentage arguments must be less than or equal to 1.")
 
         length = len(self._dataset)
@@ -229,7 +229,18 @@
             transforms.append(T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], **normalize_args))
 
             return T.Compose(transforms)
 
         self._dataset.transform = get_transform(image_size, add_aug)
         self._preprocessed = {'image_size': image_size, 'batch_size': batch_size}
         self._make_data_loaders(batch_size=batch_size)
+
+    def get_inc_dataloaders(self):
+        calib_dataloader = self.train_loader
+        if self.validation_loader is not None:
+            eval_dataloader = self.validation_loader
+        elif self.test_loader is not None:
+            eval_dataloader = self.test_loader
+        else:
+            eval_dataloader = self.train_loader
+
+        return calib_dataloader, eval_dataloader
```

## tlt/datasets/tf_dataset.py

```diff
@@ -14,14 +14,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
+from neural_compressor.data import DataLoader
 import tensorflow as tf
 
 from tlt.datasets.dataset import BaseDataset
 
 
 class TFDataset(BaseDataset):
     """
@@ -65,15 +66,15 @@
             Args:
                 subset (str): default "all", can also be "train", "validation", or "test"
 
             Returns:
                 (examples, labels)
 
             Raises:
-                ValueError if the dataset is not defined yet or the given subset is not valid
+                ValueError: if the dataset is not defined yet or the given subset is not valid
         """
         if subset == 'all' and self._dataset is not None:
             return next(iter(self._dataset))
         elif subset == 'train' and self._train_subset is not None:
             return next(iter(self._train_subset))
         elif subset == 'validation' and self._validation_subset is not None:
             return next(iter(self._validation_subset))
@@ -90,15 +91,15 @@
                 train_pct (float): default .75, percentage of dataset to use for training
                 val_pct (float):  default .25, percentage of dataset to use for validation
                 test_pct (float): default 0.0, percentage of dataset to use for testing
                 shuffle_files (bool): default True, optionally control whether shuffling occurs
                 seed (None or int): default None, can be set for pseudo-randomization
 
             Raises:
-                ValueError if percentage input args are not floats or sum to greater than 1
+                ValueError: if percentage input args are not floats or sum to greater than 1
         """
         if not (isinstance(train_pct, float) and isinstance(val_pct, float) and isinstance(test_pct, float)):
             raise ValueError("Percentage arguments must be floats.")
         if train_pct + val_pct + test_pct > 1.0:
             raise ValueError("Sum of percentage arguments must be less than or equal to 1.")
 
         cardinality = self._dataset.cardinality()
@@ -121,7 +122,32 @@
         self._train_subset = self._dataset.take(train_size)
         self._validation_subset = self._dataset.skip(train_size).take(val_size)
         if test_pct:
             self._test_subset = self._dataset.skip(train_size + val_size)
         else:
             self._test_subset = None
         self._validation_type = 'shuffle_split'
+
+    def get_inc_dataloaders(self):
+        # The added dimension of a batched TF dataset throws Intel Neural Compressor off, so use unbatched dataset
+        batched = self._preprocessed and 'batch_size' in self._preprocessed
+        if batched:
+            calib_dataloader = DataLoader('tensorflow_itex', self.train_subset.unbatch(),
+                                          batch_size=self._preprocessed['batch_size'])
+        else:
+            calib_dataloader = DataLoader('tensorflow_itex', self.train_subset)
+        if self.validation_subset is not None:
+            if batched:
+                eval_dataloader = DataLoader('tensorflow_itex', self.validation_subset.unbatch(),
+                                             batch_size=self._preprocessed['batch_size'])
+            else:
+                eval_dataloader = DataLoader('tensorflow_itex', self.validation_subset)
+        elif self.test_subset is not None:
+            if batched:
+                eval_dataloader = DataLoader('tensorflow_itex', self.test_subset.unbatch(),
+                                             batch_size=self._preprocessed['batch_size'])
+            else:
+                eval_dataloader = DataLoader('tensorflow_itex', self.test_subset)
+        else:
+            eval_dataloader = calib_dataloader
+
+        return calib_dataloader, eval_dataloader
```

## tlt/datasets/image_classification/pytorch_custom_image_classification_dataset.py

```diff
@@ -63,15 +63,15 @@
                            each class.
         dataset_name (str): optional; Name of the dataset. If no dataset name is given, the dataset_dir folder name
                             will be used as the dataset name.
         num_workers (int): optional; Number of processes to use for data loading, default is 0
         shuffle_files (bool): optional; Whether to shuffle the data. Defaults to True.
 
     Raises:
-        FileNotFoundError if dataset directory does not exist
+        FileNotFoundError: if dataset directory does not exist
 
     """
 
     def __init__(self, dataset_dir, dataset_name=None, num_workers=0, shuffle_files=True):
         """
         Class constructor
         """
```

## tlt/datasets/image_classification/tf_custom_image_classification_dataset.py

```diff
@@ -63,15 +63,15 @@
         dataset_name (str): optional; Name of the dataset. If no dataset name is given, the dataset_dir folder name
                             will be used as the dataset name.
         color_mode (str): optional; Specify the color mode as "greyscale", "rgb", or "rgba". Defaults to "rgb".
         shuffle_files (bool): optional; Whether to shuffle the data. Defaults to True.
         seed (int): optional; Random seed for shuffling
 
     Raises:
-        FileNotFoundError if dataset directory does not exist
+        FileNotFoundError: if dataset directory does not exist
 
     """
 
     def __init__(self, dataset_dir, dataset_name=None, color_mode="rgb", shuffle_files=True, seed=None, **kwargs):
         """
         Class constructor
         """
@@ -159,27 +159,34 @@
     @property
     def dataset(self):
         """
         Returns the framework dataset object (tf.data.Dataset)
         """
         return self._dataset
 
-    def preprocess(self, image_size, batch_size, add_aug=None):
+    def preprocess(self, image_size, batch_size, add_aug=None, preprocessor=None):
         """
-        Preprocess the dataset to convert to float32, resize, and batch the images
+        Preprocess the dataset to convert to float32, resize, normalize, and batch the images
 
             Args:
                 image_size (int): desired square image size
                 batch_size (int): desired batch size
+                normalize (bool): rescale the image between (1, 255), default True; should be disabled when using keras
+                                  applications because the model's initial layers apply the requisite model-specific
+                                  normalization
                 add_aug (None or list[str]): Choice of augmentations (RandomHorizontalandVerticalFlip,
                                              RandomHorizontalFlip, RandomVerticalFlip, RandomZoom, RandomRotation) to be
                                              applied during training
+                preprocessor (None or preprocess_input function from keras.applications): Should be provided when using
+                                             Keras Applications models, which have model-specific preprocessors;
+                                             otherwise, use None (the default) to apply generic normalization and
+                                             resizing
 
             Raises:
-                ValueError if the dataset is not defined or has already been processed
+                ValueError: if the dataset is not defined or has already been processed
         """
         if self._preprocessed:
             raise ValueError("Data has already been preprocessed: {}".format(self._preprocessed))
         if not isinstance(batch_size, int) or batch_size < 1:
             raise ValueError("batch_size should be an positive integer")
         if not isinstance(image_size, int) or image_size < 1:
             raise ValueError("image_size should be an positive integer")
@@ -206,22 +213,25 @@
                         Supported augmentations are {}".format(option, aug_list))
                 data_augmentation.add(aug_dict[option])
 
         normalization_layer = tf.keras.layers.Rescaling(1. / 255)
 
         def preprocess_image(image, label):
             image = tf.image.resize_with_pad(image, image_size, image_size)
-            image = normalization_layer(image)
+            if preprocessor is None:
+                image = normalization_layer(image)
             return (image, label)
 
         # Get the non-None splits
         split_list = ['_dataset', '_train_subset', '_validation_subset', '_test_subset']
         subsets = [s for s in split_list if getattr(self, s, None)]
         for subset in subsets:
             setattr(self, subset, getattr(self, subset).map(preprocess_image))
+            if preprocessor:
+                setattr(self, subset, getattr(self, subset).map(lambda x, y: (preprocessor(x), y)))
             setattr(self, subset, getattr(self, subset).cache())
             setattr(self, subset, getattr(self, subset).batch(batch_size))
             setattr(self, subset, getattr(self, subset).prefetch(tf.data.AUTOTUNE))
             if add_aug is not None and subset in ['_dataset', '_train_subset']:
                 setattr(self, subset, getattr(self, subset).map(lambda x, y: (data_augmentation(x, training=True), y),
                                                                 num_parallel_calls=tf.data.AUTOTUNE))
         self._preprocessed = {'image_size': image_size, 'batch_size': batch_size}
```

## tlt/datasets/image_classification/tfds_image_classification_dataset.py

```diff
@@ -77,27 +77,31 @@
     @property
     def dataset(self):
         """
         Returns the framework dataset object (tf.data.Dataset)
         """
         return self._dataset
 
-    def preprocess(self, image_size, batch_size, add_aug=None):
+    def preprocess(self, image_size, batch_size, add_aug=None, preprocessor=None):
         """
         Preprocess the dataset to convert to float32, resize, and batch the images
 
             Args:
                 image_size (int): desired square image size
                 batch_size (int): desired batch size
                 add_aug (None or list[str]): Choice of augmentations (RandomHorizontalandVerticalFlip,
                                              RandomHorizontalFlip, RandomVerticalFlip, RandomZoom, RandomRotation) to
                                              be applied during training
+                preprocessor (None or preprocess_input function from keras.applications): Should be provided when using
+                                             Keras Applications models, which have model-specific preprocessors;
+                                             otherwise, use None (the default) to apply generic type conversion and
+                                             resizing
 
             Raises:
-                ValueError if the dataset is not defined or has already been processed
+                ValueError: if the dataset is not defined or has already been processed
         """
         if self._preprocessed:
             raise ValueError("Data has already been preprocessed: {}".format(self._preprocessed))
         if not isinstance(batch_size, int) or batch_size < 1:
             raise ValueError("batch_size should be a positive integer")
         if not isinstance(image_size, int) or image_size < 1:
             raise ValueError("image_size should be a positive integer")
@@ -121,23 +125,26 @@
             for option in add_aug:
                 if option not in aug_list:
                     raise ValueError("Unsupported augmentation for TensorFlow:{}. \
                     Supported augmentations are {}".format(option, aug_list))
                 data_augmentation.add(aug_dict[option])
 
         def preprocess_image(image, label):
-            image = tf.image.convert_image_dtype(image, tf.float32)
+            if preprocessor is None:
+                image = tf.image.convert_image_dtype(image, tf.float32)
             image = tf.image.resize_with_pad(image, image_size, image_size)
             return (image, label)
 
         # Get the non-None splits
         split_list = ['_dataset', '_train_subset', '_validation_subset', '_test_subset']
         subsets = [s for s in split_list if getattr(self, s, None)]
         for subset in subsets:
             setattr(self, subset, getattr(self, subset).map(preprocess_image))
+            if preprocessor:
+                setattr(self, subset, getattr(self, subset).map(lambda x, y: (preprocessor(x), y)))
             setattr(self, subset, getattr(self, subset).cache())
             setattr(self, subset, getattr(self, subset).batch(batch_size))
             setattr(self, subset, getattr(self, subset).prefetch(tf.data.AUTOTUNE))
             if add_aug is not None and subset in ['_dataset', '_train_subset']:
                 setattr(self, subset, getattr(self, subset).map(lambda x, y: (data_augmentation(x, training=True), y),
                                                                 num_parallel_calls=tf.data.AUTOTUNE))
         self._preprocessed = {'image_size': image_size, 'batch_size': batch_size}
```

## tlt/datasets/text_classification/hf_custom_text_classification_dataset.py

```diff
@@ -72,21 +72,21 @@
             exclude_cols (list): optional; Specify a list of sorted indices for columns from the dataset file(s) that
                 should be excluded from parsing. Defaults to parsing all columns. At most one of select_cols and
                 exclude_cols can be specified.
             shuffle_files (bool): optional; Whether to shuffle the data. Defaults to True.
             num_workers (int): Number of workers to pass into a DataLoader.
 
         Raises:
-            FileNotFoundError if the csv file is not found in the dataset directory
-            TypeError if label_map_func is not callable
-            ValueError if class_names list is empty
-            ValueError if column_names list does not contain the value 'label'
-            ValueError if index of 'label' in column_names and label_col mismatch
-            ValueError if the values of column_names are not strings.
-            ValueError if column_names contains more than one value as 'label'
+            FileNotFoundError: if the csv file is not found in the dataset directory
+            TypeError: if label_map_func is not callable
+            ValueError: if class_names list is empty
+            ValueError: if column_names list does not contain the value 'label'
+            ValueError: if index of 'label' in column_names and label_col mismatch
+            ValueError: if the values of column_names are not strings.
+            ValueError: if column_names contains more than one value as 'label'
 
         """
         # Sanity checks
         dataset_file = os.path.join(dataset_dir, csv_file_name)
         if not os.path.exists(dataset_file):
             raise FileNotFoundError("The dataset file ({}) does not exist".format(dataset_file))
 
@@ -110,16 +110,18 @@
             if column_names.index('label') != label_col:
                 raise ValueError("The label_col index ({}) does not match with column_names {}."
                                  "Either specify label_col argument (or) make the first value "
                                  "in your column_names as 'label'".format(label_col, column_names))
 
         TextClassificationDataset.__init__(self, dataset_dir, dataset_name, dataset_catalog=None)
 
-        print("WARNING: Using column {} as label column. To change this behavior, \
-               specify the label_col argument".format(label_col))
+        print("WARNING: Using column {} as label column. To change this behavior, "
+              "specify the label_col argument".format(label_col))
+        if delimiter == 't':
+            delimiter = '\t'
         if header:
             dataset_df = pd.read_csv(dataset_file, delimiter=delimiter, encoding='utf-8', dtype=str, names=column_names,
                                      header=0)
         else:
             dataset_df = pd.read_csv(dataset_file, delimiter=delimiter, encoding='utf-8', dtype=str, names=column_names,
                                      header=None)
             if not column_names:
```

## tlt/datasets/text_classification/text_classification_dataset.py

```diff
@@ -40,16 +40,16 @@
             Returns the string label (class name) associated with the specified numerical value. If the numerical
             value provided is a float, it will be rounded to the nearest integer.
 
             Args:
                 numerical_value (int or float): Numerical label value
 
             Raises:
-                TypeError if the numerical value is not a float or an integer
-                ValueError if the numerical value does not map to a class label
+                TypeError: if the numerical value is not a float or an integer
+                ValueError: if the numerical value does not map to a class label
         """
         if isinstance(numerical_value, float):
             numerical_value = int(round(numerical_value))
 
         if not isinstance(numerical_value, int):
             raise TypeError("Invalid type for the numerical value. Expected an integer or float value.")
```

## tlt/datasets/text_classification/tf_custom_text_classification_dataset.py

```diff
@@ -19,14 +19,16 @@
 #
 
 import os
 import tensorflow as tf
 
 from tlt.datasets.tf_dataset import TFDataset
 from tlt.datasets.text_classification.text_classification_dataset import TextClassificationDataset
+from tlt.utils.dataset_utils import prepare_huggingface_input_data
+from tlt.utils.inc_utils import INCTFDataLoader
 
 
 class TFCustomTextClassificationDataset(TextClassificationDataset, TFDataset):
     """
     A custom text classification dataset that can be used with TensorFlow models.
     Note that this dataset class expects a .csv file with two columns where the first column is the label and
     the second column is the text/sentence to classify.
@@ -63,17 +65,17 @@
         exclude_cols (list): optional; Specify a list of sorted indices for columns from the dataset file(s) that should
                              be excluded from parsing. Defaults to parsing all columns. At most one of select_cols and
                              exclude_cols can be specified.
         shuffle_files (bool): optional; Whether to shuffle the data. Defaults to True.
         seed (int): optional; Random seed for shuffling
 
     Raises:
-        FileNotFoundError if the csv file is not found in the dataset directory
-        TypeError if the class_names parameter is not a list or the label_map_func is not callable
-        ValueError if the class_names list is empty
+        FileNotFoundError: if the csv file is not found in the dataset directory
+        TypeError: if the class_names parameter is not a list or the label_map_func is not callable
+        ValueError: if the class_names list is empty
 
     """
 
     def __init__(self, dataset_dir, dataset_name, csv_file_name, class_names, label_map_func=None,
                  defaults=[tf.string, tf.string], delimiter=",", header=False, select_cols=None, exclude_cols=None,
                  shuffle_files=True, seed=None, **kwargs):
         """
@@ -188,7 +190,22 @@
         split_list = ['_dataset', '_train_subset', '_validation_subset', '_test_subset']
         subsets = [s for s in split_list if getattr(self, s, None)]
         for subset in subsets:
             setattr(self, subset, getattr(self, subset).cache())
             setattr(self, subset, getattr(self, subset).batch(batch_size))
             setattr(self, subset, getattr(self, subset).prefetch(tf.data.AUTOTUNE))
         self._preprocessed = {'batch_size': batch_size}
+
+    def get_inc_dataloaders(self, hub_name, max_seq_length):
+        calib_data, calib_labels = prepare_huggingface_input_data(self.train_subset, hub_name, max_seq_length)
+        calib_data['label'] = tf.convert_to_tensor(calib_labels)
+
+        eval_data, eval_labels = prepare_huggingface_input_data(self.validation_subset, hub_name, max_seq_length)
+        eval_data['label'] = tf.convert_to_tensor(eval_labels)
+
+        calib_data.pop('token_type_ids')
+        eval_data.pop('token_type_ids')
+
+        calib_dataloader = INCTFDataLoader(calib_data, batch_size=self._preprocessed['batch_size'])
+        eval_dataloader = INCTFDataLoader(eval_data, batch_size=self._preprocessed['batch_size'])
+
+        return calib_dataloader, eval_dataloader
```

## tlt/datasets/text_classification/tfds_text_classification_dataset.py

```diff
@@ -20,15 +20,17 @@
 
 import os
 import tensorflow as tf
 
 from tlt import TLT_BASE_DIR
 from tlt.datasets.tf_dataset import TFDataset
 from tlt.datasets.text_classification.text_classification_dataset import TextClassificationDataset
+from tlt.utils.dataset_utils import prepare_huggingface_input_data
 from tlt.utils.file_utils import read_json_file
+from tlt.utils.inc_utils import INCTFDataLoader
 from downloader.datasets import DataDownloader
 
 DATASET_CONFIG_DIR = os.path.join(TLT_BASE_DIR, "datasets/configs")
 config_file = os.path.join(DATASET_CONFIG_DIR, "tf_text_classification_datasets.json")
 config_dict = read_json_file(config_file)
 DATASETS = list(config_dict.keys())
 
@@ -102,16 +104,16 @@
         """
             Batch the dataset
 
             Args:
                 batch_size (int): desired batch size
 
             Raises:
-                TypeError if the batch_size is not a positive integer
-                ValueError if the dataset is not defined or has already been processed
+                TypeError: if the batch_size is not a positive integer
+                ValueError: if the dataset is not defined or has already been processed
         """
         if not isinstance(batch_size, int) or batch_size < 1:
             raise ValueError("batch_size should be a positive integer")
 
         if self._preprocessed:
             raise ValueError("Data has already been preprocessed: {}".format(self._preprocessed))
 
@@ -119,7 +121,22 @@
         split_list = ['_dataset', '_train_subset', '_validation_subset', '_test_subset']
         subsets = [s for s in split_list if getattr(self, s, None)]
         for subset in subsets:
             setattr(self, subset, getattr(self, subset).cache())
             setattr(self, subset, getattr(self, subset).batch(batch_size))
             setattr(self, subset, getattr(self, subset).prefetch(tf.data.AUTOTUNE))
         self._preprocessed = {'batch_size': batch_size}
+
+    def get_inc_dataloaders(self, hub_name, max_seq_length):
+        calib_data, calib_labels = prepare_huggingface_input_data(self.train_subset, hub_name, max_seq_length)
+        calib_data['label'] = tf.convert_to_tensor(calib_labels)
+
+        eval_data, eval_labels = prepare_huggingface_input_data(self.validation_subset, hub_name, max_seq_length)
+        eval_data['label'] = tf.convert_to_tensor(eval_labels)
+
+        calib_data.pop('token_type_ids')
+        eval_data.pop('token_type_ids')
+
+        calib_dataloader = INCTFDataLoader(calib_data, batch_size=self._preprocessed['batch_size'])
+        eval_dataloader = INCTFDataLoader(eval_data, batch_size=self._preprocessed['batch_size'])
+
+        return calib_dataloader, eval_dataloader
```

## tlt/distributed/README.md

```diff
@@ -1,13 +1,15 @@
-# Distributed Training with Intel® Transfer Learning Tool
+# Distributed Training
+
+Here are instructions for using distributed/multinode Training with Intel® Transfer Learning Tool.
 
 ## Prerequisites
 
 - Participating nodes should have Intel® oneAPI Base Toolkit installed. Verify the files under `/opt/intel/oneapi`
-- Participating nodes should have passwordless SSH setup. Instructions to setup are given below.
+- Participating nodes should have passwordless SSH setup. Instructions to set up are given below.
 
 ### Passwordless SSH setup
 
 - Use an existing (or create an) SSH key pair.
 
     - Check under `~/.ssh` and see if they exist. If present, make sure they have default names `(id_rsa.pub id_rsa)` and they don't have any passphrase.
```

## tlt/distributed/pytorch/README.md

```diff
@@ -1,41 +1,73 @@
 # Distributed Training with PyTorch and Intel® Transfer Learning Tool
 
 ## Multinode setup
 
+### Create and activate a Python3 virtual environment
+
+We encourage you to use a python virtual environment (virtualenv or conda) for consistent package management. Make sure to follow only the chosen method on all the nodes. Mixing those configurations is not supported. 
+
+There are two ways to do this:
+
+a. Using `virtualenv`:
+
 1. Login to one of the participating nodes.
 
-2. Create a new conda environment called `multi-node`
+2. Create and activate a new python3 virtualenv
+
+```
+virtualenv -p python3 tlt_dev_venv
+source tlt_dev_venv/bin/activate
+```
+
+3. Install Intel® Transfer Learning Tool (see main [README](/README.md))
+```
+pip install --editable .
+```
+
+4. Install multinode dependencies from the requirements text file. You can also compile `torch_ccl` manually from [here](https://github.com/intel/torch-ccl)
+```
+pip install -r tlt/distributed/pytorch/requirements.txt
 ```
-conda create -n multi-node python=3.8 --yes
 
-conda activate multi-node
+b. Or `conda`:
+
+1. Login to one of the participating nodes.
+
+2. Create and activate a new conda environment
+```
+conda create -n tlt_dev_venv python=3.8 --yes
+conda activate tlt_dev_venv
 ```
 
-3. Install dependencies from the shell script
+3. Install Intel® Transfer Learning Tool (see main [README](/README.md))
 ```
-sh run_install.sh
+pip install --editable .
 ```
 
-4. Install Intel® Transfer Learning Tool excluding framework dependencies (see main [README](/README.md))
+4. Install dependencies from the shell script
 ```
-EXCLUDE_FRAMEWORK=True pip install --editable .
+bash tlt/distributed/pytorch/run_install.sh
 ```
 
 ## Verify multinode setup
 
 Create a `hostfile` with a list of IP addresses of the participating nodes and type the following command. You should see a list of hostnames of the nodes.
 ```
 mpiexec.hydra -ppn 1 -f hostfile hostname
 ```
+**Note:** If the above command errors out as `'mpiexec.hydra' command not found`, activate the oneAPI environment:
+```
+source /opt/intel/oneapi/setvars.sh
+```
 
 ## Launch a distributed training job with TLT CLI
 
 **Step 1:** Create a `hostfile` with a list of IP addresses of the participating nodes. Make sure 
-the first IP address to be of the current node. For testing, you can use the nodes present in this [hostfile](hostfile)
+the first IP address to be of the current node.
 
 **Step 2:** Launch a distributed training job with TLT CLI using the appropriate flags.
 ```
 tlt train \
     -f pytorch \
     --model_name resnet50 \
     --dataset_name CIFAR10 \
@@ -52,8 +84,8 @@
 - "Port already in use" - Might happen when you keyboard interrupt training.
 
 **Fix:** Release the port from the terminal (or) log out and log in again to free the port.
 
 - "HTTP Connection error" - Might happen if there are several attempts to train text classification model
 as it uses Hugging Face API to make calls to get dataset, model, tokenizer.
 
-**Fix:** Wait for about few seconds and try again.
+**Fix:** Wait for about few seconds and try again.
```

## tlt/distributed/pytorch/run_install.sh

```diff
@@ -1,26 +1,26 @@
 #!/usr/bin/env bash
 
 conda install -y \
-  pyyaml \
+  'numpy==1.23.5' \
+  'pytorch==1.13.1' \
+  'pyyaml==6.0' \
+  'scikit-learn==1.2.2' \
+  'torchaudio==1.13.1' \
+  'torchvision==0.14.1' \
+  'tqdm==4.65.0' \
   cmake \
   cpuonly \
   future \
   gperftools \
   intel-openmp \
   ninja \
-  numpy \
   pydot \
-  'pytorch==1.13.1' \
-  scikit-learn \
   setuptools \
-  'torchaudio==1.13.1' \
-  'torchvision==0.14.1' \
-  tqdm \
   -c pytorch -c intel -c conda-forge
 
 pip install \
-  datasets \
-  'intel_extension_for_pytorch==1.13.0' \
-  transformers
+  'datasets~=2.12.0' \
+  'intel_extension_for_pytorch==1.13.100' \
+  'transformers~=4.30.0'
 
 bash deploy/install_torch_ccl.sh
```

## tlt/distributed/pytorch/run_train_pyt.py

```diff
@@ -14,25 +14,31 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
+import os
 import argparse
 
 from tlt.distributed.pytorch.utils.pyt_distributed_utils import (
     DistributedTorch,
     DistributedTrainingArguments
 )
 
 
 if __name__ == "__main__":
 
-    # Program arguments
+    def directory_path(path):
+        if os.path.isdir(path):
+            return path
+        else:
+            raise argparse.ArgumentTypeError("'{}' is not a valid directory path.".format(path))
+
     print("******Distributed Training*****")
 
     description = 'Distributed training with PyTorch.'
 
     parser = argparse.ArgumentParser(description=description)
 
     parser.add_argument('--master_addr', type=str, required=True, help="Master node to run this script")
@@ -42,31 +48,33 @@
     parser.add_argument('--use_case', type=str, required=True,
                         help='Use case (image_classification|text_classification)')
     parser.add_argument('--epochs', type=int, required=False, default=1, help='Total epochs to train the model')
     parser.add_argument('--batch_size', type=int, required=False, default=128,
                         help='Global batch size to distribute data (default: 128)')
     parser.add_argument('--disable_ipex', action='store_true', required=False, help="Disables IPEX optimization to "
                         "the model")
+    parser.add_argument('--tlt_saved_objects_dir', type=directory_path, required=False, help='Path to TLT saved '
+                        'distributed objects. The path must be accessible to all the nodes. For example: mounted '
+                        'NFS drive. This arg is helpful when using TLT API/CLI. '
+                        'See DistributedTorch.load_saved_objects() for more information.')
 
     args = parser.parse_args()
 
-    # Load the saved dataset and model objects
-    loaded_objects = DistributedTorch.load_saved_objects(use_case=args.use_case)
-
-    dataset = loaded_objects['dataset']
-    train_subset = loaded_objects.get('train_subset', dataset)
-    test_subset = loaded_objects.get('test_subset', dataset)
-    validation_subset = loaded_objects.get('validation_subset', dataset)
-    model = loaded_objects['model']
-    loss = loaded_objects['loss']
-    optimizer = loaded_objects['optimizer']
+    if args.tlt_saved_objects_dir is not None:
+        # Load the saved dataset and model objects
+        loaded_objects = DistributedTorch.load_saved_objects(args.tlt_saved_objects_dir)
+
+        train_data = loaded_objects.get('train_data')
+        model = loaded_objects['model']
+        loss = loaded_objects['loss']
+        optimizer = loaded_objects['optimizer']
 
     # Launch distributed job
     training_args = DistributedTrainingArguments(
-        dataset=train_subset,
+        dataset=train_data,
         model=model,
         criterion=loss,
         optimizer=optimizer,
         epochs=args.epochs,
         batch_size=args.batch_size,
         disable_ipex=args.disable_ipex
     )
```

## tlt/distributed/pytorch/utils/pyt_distributed_utils.py

```diff
@@ -23,15 +23,14 @@
 import torch
 import torch.distributed as dist
 
 from tqdm import tqdm
 from random import Random
 from torch.utils.data import DataLoader
 from torch.nn.parallel import DistributedDataParallel as DDP
-from tlt.distributed import TLT_DISTRIBUTED_DIR
 
 import oneccl_bindings_for_pytorch  # noqa # pylint: disable=unused-import
 import intel_extension_for_pytorch as ipex
 
 """ Dataset partitioning helper classes and methods """
 
 
@@ -241,25 +240,20 @@
 
     @classmethod
     def cleanup_ddp(cls):
         if dist.is_initialized():
             dist.destroy_process_group()
 
     @classmethod
-    def load_saved_objects(cls, use_case: str):
+    def load_saved_objects(cls, saved_objects_dir):
         """
         Helper function to load saved dataset and model objects
 
         Args:
             use_case (str): Use case of the saved datasets and models.
 
         Returns:
             dict with loaded dataset and model objects
         """
-        if use_case == 'text_classification':
-            saved_objects_file = 'hf_saved_objects.obj'
-        elif use_case == 'image_classification':
-            saved_objects_file = 'torch_saved_objects.obj'
-        else:
-            raise ValueError("Distributed PyTorch for {} is not implemented yet".format(use_case))
+        saved_objects_file = 'torch_saved_objects.obj'
 
-        return torch.load(os.path.join(TLT_DISTRIBUTED_DIR, saved_objects_file))
+        return torch.load(os.path.join(saved_objects_dir, saved_objects_file))
```

## tlt/distributed/tensorflow/README.md

```diff
@@ -16,16 +16,16 @@
 ## Multinode setup
 
 You can choose to install multinode dependencies in your existing TLT virtualenv (or) you can create a new virtualenv and install TLT and multinode dependencies.
 
 Step 1: Create a virtualenv and activate it (or) activate your existing TLT virtualenv
 
 ```
-virtualenv -p python3 tlt_tf_multinode
-source tlt_tf_multinode/bin/activate
+virtualenv -p python3 tlt_dev_venv
+source tlt_dev_venv/bin/activate
 ```
 
 Step 2: Install TLT from the `setup.py` script (You can skip this step if you already have TLT installed)
 
 ```
 pip install --editable .
 ```
@@ -40,24 +40,24 @@
 
 ## Verify multinode setup
 
 Run any of the following commands on a head node by providing required env variables and IP addresses. You should see a list of hostnames of the nodes.
 
 ### Using `mpirun`
 ```
-source tlt_tf_multinode/bin/activate && \
+source tlt_dev_venv/bin/activate && \
 mpirun --allow-run-as-root -bind-to none -map-by slot -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x NCCL_SOCKET_IFNAME=^lo,docker0 -mca pml ob1 -mca btl ^openib -mca btl_tcp_if_exclude lo,docker0 \
     -np 4 \
     -H node_01:2,node_02:2 \
     hostname
 ```
 
 ### Using `horovodrun`
 ```
-source tlt_tf_multinode/bin/activate && \
+source tlt_dev_venv/bin/activate && \
 horovodrun \
     -np 4 \
     -H node_01:2,node_02:2 \
     hostname
 ```
 
 ## Launch a distributed training job with TLT CLI
@@ -82,8 +82,79 @@
     --model-name efficientnet_b0 \
     --distributed \
     --hostfile hostfile \
     --nnodes 2 \
     --nproc_per_node 2
 ```
 
-(Optional): Use the `--use_horovod` flag to run using horovodrun instead of default mpirun.
+(Optional): Use the `--use_horovod` flag to run using horovodrun instead of default mpirun.
+
+## Launch a distributed training job with `horovodrun`
+
+You can also use the `run_train_tf.py` script alone with `horovodrun` to do distributed training for TensorFlow hub/Huggingface models on TensorFlow Datasets. 
+
+```
+Distributed training with TensorFlow.
+
+optional arguments:
+  -h, --help            show this help message and exit
+  --use-case {image_classification,text_classification}, --use_case {image_classification,text_classification}
+                        Use case (image_classification|text_classification)
+  --epochs EPOCHS       Total epochs to train the model
+  --batch_size BATCH_SIZE
+                        Global batch size to distribute data (default: 128)
+  --batch_denom BATCH_DENOM
+                        Batch denominator to be used to divide global batch size (default: 1)
+  --shuffle             Shuffle dataset while training
+  --scaling {weak,strong}
+                        Weak or Strong scaling. For weak scaling, lr is scaled by a factor of sqrt(batch_size/batch_denom) and uses global batch size for
+                        all the processes. For strong scaling, lr is scaled by world size and divides global batch size by world size (default: weak)
+  --tlt_saved_objects_dir TLT_SAVED_OBJECTS_DIR
+                        Path to TLT saved distributed objects. The path must be accessible to all the nodes. For example: mounted NFS drive. This arg is
+                        helpful when using TLT API/CLI. See DistributedTF.load_saved_objects() for more information.
+  --max_seq_length MAX_SEQ_LENGTH
+                        Maximum sequence length that the model will be used with
+  --hf_bert_tokenizer HF_BERT_TOKENIZER
+                        Name of the Hugging Face BertTokenizer to use to prepare the data.
+  --dataset-dir DATASET_DIR, --dataset_dir DATASET_DIR
+                        Path to dataset directory to save/load tfds dataset. This arg is helpful if you plan to use this as a stand-alone script. Custom
+                        dataset is not supported yet!
+  --output-dir OUTPUT_DIR, --output_dir OUTPUT_DIR
+                        Path to save the trained model and store logs. This arg is helpful if you plan to use this as a stand-alone script
+  --dataset-name DATASET_NAME, --dataset_name DATASET_NAME
+                        Dataset name to load from tfds. This arg is helpful if you plan to use this as a stand-alone script. Custom dataset is not
+                        supported yet!
+  --model-name MODEL_NAME, --model_name MODEL_NAME
+                        TensorFlow image classification model url/ feature vector url from TensorFlow Hub (or) Huggingface hub name for text
+                        classification models. This arg is helpful if you plan to use this as a stand-alone script.
+  --image-size IMAGE_SIZE, --image_size IMAGE_SIZE
+                        Input image size to the given model, for which input shape is determined as (image_size, image_size, 3). This arg is helpful if
+                        you plan to use this as a stand-alone script.
+```
+
+Here are some examples:
+
+**For image classification:**
+
+```
+horovodrun \
+    -np 10 \
+    -H server_1:6,server_2:4 \
+    python tlt/distributed/tensorflow/run_train_tf.py \
+    --use-case image_classification \
+    --model-name https://tfhub.dev/google/efficientnet/b1/feature-vector/1 \
+    --dataset-name cifar10
+ 
+ ```
+
+ **For text classification**:
+
+ ```
+ horovodrun \
+    -np 10 \
+    -H server_1:6,server_2:4 \
+    python tlt/distributed/tensorflow/run_train_tf.py \
+    --use-case text_classification \
+    --model-name bert-base-uncased \
+    --dataset-name imdb_reviews
+ ```
+
```

## tlt/distributed/tensorflow/requirements.txt

```diff
@@ -1,3 +1,2 @@
-dill==0.3.6
-tensorflow-text==2.11.0
-horovod==0.26.1
+dill~=0.3.6
+horovod~=0.27.0
```

## tlt/distributed/tensorflow/run_train_tf.py

```diff
@@ -14,57 +14,150 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
+import os
+import tempfile
 import argparse
 
+import tensorflow as tf
+import tensorflow_datasets as tfds
+
 from tlt.distributed.tensorflow.utils.tf_distributed_util import (
     DistributedTF,
     DistributedTrainingArguments
 )
 
 
-dtf = DistributedTF()
-model, optimizer, loss, train_data, val_data = dtf.load_saved_objects()
-
 if __name__ == '__main__':
 
+    default_data_dir = os.path.join(tempfile.gettempdir(), 'data')
+    default_output_dir = os.path.join(tempfile.gettempdir(), 'output')
+
+    for d in [default_data_dir, default_output_dir]:
+        if not os.path.exists(d):
+            os.makedirs(d)
+
+    def directory_path(path):
+        if os.path.isdir(path):
+            return path
+        else:
+            raise argparse.ArgumentTypeError("'{}' is not a valid directory path.".format(path))
+
     print("******Distributed Training*****")
 
     description = 'Distributed training with TensorFlow.'
 
     parser = argparse.ArgumentParser(description=description)
 
-    parser.add_argument('--use_case', type=str, required=True,
-                        help='Use case (image_classification|text_classification)')
+    parser.add_argument('--use-case', '--use_case', type=str, required=True, choices=['image_classification',
+                        'text_classification'], help='Use case (image_classification|text_classification)')
     parser.add_argument('--epochs', type=int, required=False, default=1, help='Total epochs to train the model')
     parser.add_argument('--batch_size', type=int, required=False, default=128,
                         help='Global batch size to distribute data (default: 128)')
     parser.add_argument("--batch_denom", type=int, required=False, default=1,
                         help="Batch denominator to be used to divide global batch size (default: 1)")
     parser.add_argument('--shuffle', action='store_true', required=False, help="Shuffle dataset while training")
-    parser.add_argument('--scaling', type=str, required=False, default='weak',
+    parser.add_argument('--scaling', type=str, required=False, default='weak', choices=['weak', 'strong'],
                         help='Weak or Strong scaling. For weak scaling, lr is scaled by a factor of '
                         'sqrt(batch_size/batch_denom) and uses global batch size for all the processes. For '
                         'strong scaling, lr is scaled by world size and divides global batch size by world size '
                         '(default: weak)')
+    parser.add_argument('--tlt_saved_objects_dir', type=directory_path, required=False, help='Path to TLT saved '
+                        'distributed objects. The path must be accessible to all the nodes. For example: mounted '
+                        'NFS drive. This arg is helpful when using TLT API/CLI. See DistributedTF.load_saved_objects()'
+                        ' for more information.')
+    parser.add_argument('--max_seq_length', type=int, default=128,
+                        help='Maximum sequence length that the model will be used with')
+    parser.add_argument('--dataset-dir', '--dataset_dir', type=directory_path, default=default_data_dir,
+                        help="Path to dataset directory to save/load tfds dataset. This arg is helpful if you "
+                        "plan to use this as a stand-alone script. Custom dataset is not supported yet!")
+    parser.add_argument('--output-dir', '--output_dir', type=directory_path, default=default_output_dir,
+                        help="Path to save the trained model and store logs. This arg is helpful if you "
+                        "plan to use this as a stand-alone script")
+    parser.add_argument('--dataset-name', '--dataset_name', type=str, default=None,
+                        help="Dataset name to load from tfds. This arg is helpful if you "
+                        "plan to use this as a stand-alone script. Custom dataset is not supported yet!")
+    parser.add_argument('--model-name', '--model_name', type=str, default=None,
+                        help="TensorFlow image classification model url/ feature vector url from TensorFlow Hub "
+                        "(or) Huggingface hub name for text classification models. This arg is helpful if you "
+                        "plan to use this as a stand-alone script.")
+    parser.add_argument('--image-size', '--image_size', type=int, default=None,
+                        help="Input image size to the given model, for which input shape is determined as "
+                        "(image_size, image_size, 3). This arg is helpful if you "
+                        "plan to use this as a stand-alone script.")
 
     args = parser.parse_args()
 
+    dtf = DistributedTF()
+
+    model = None
+    optimizer, loss = None, None
+    train_data, train_labels = None, None
+    val_data, val_labels = None, None
+
+    if args.tlt_saved_objects_dir is not None:
+        model, optimizer, loss, train_data, val_data = dtf.load_saved_objects(args.tlt_saved_objects_dir)
+    else:
+        if args.dataset_name is None:
+            raise argparse.ArgumentError(args.dataset_name, "Please provide a dataset name to load from tfds "
+                                         "using --dataset-name")
+        if args.model_name is None:
+            raise argparse.ArgumentError(args.model_name, "Please provide TensorFlow Hub's model url/feature "
+                                         "vector url (or) Huggingface hub name using --model-name")
+
+        train_data, data_info = tfds.load(args.dataset_name, data_dir=args.dataset_dir, split='train',
+                                          as_supervised=True, with_info=True)
+        val_data = tfds.load(args.dataset_name, data_dir=args.dataset_dir, split='test', as_supervised=True)
+        num_classes = data_info.features['label'].num_classes
+
+        if args.use_case == 'image_classification':
+            if args.image_size is not None:
+                input_shape = (args.image_size, args.image_size, 3)
+            else:
+                try:
+                    input_shape = data_info.features['image'].shape
+                except (KeyError, AttributeError):
+                    raise argparse.ArgumentError(args.image_size, "Unable to determine input_shape, please "
+                                                 "provide --image-size/--image_size")
+
+            train_data = dtf.prepare_dataset(train_data, args.use_case, args.batch_size, args.scaling)
+            val_data = dtf.prepare_dataset(val_data, args.use_case, args.batch_size, args.scaling)
+
+            model = dtf.prepare_model(args.model_name, args.use_case, input_shape, num_classes)
+
+        elif args.use_case == 'text_classification':
+            input_shape = (args.max_seq_length,)
+            from transformers import BertTokenizer
+            hf_bert_tokenizer = BertTokenizer.from_pretrained(args.model_name)
+
+            train_data = dtf.prepare_dataset(train_data, args.use_case, args.batch_size, args.scaling,
+                                             max_seq_length=args.max_seq_length, hf_bert_tokenizer=hf_bert_tokenizer)
+            val_data = dtf.prepare_dataset(val_data, args.use_case, args.batch_size, args.scaling,
+                                           max_seq_length=args.max_seq_length, hf_bert_tokenizer=hf_bert_tokenizer)
+            model = dtf.prepare_model(args.model_name, args.use_case, input_shape, num_classes)
+
+        optimizer = tf.keras.optimizers.Adam()
+        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True) if num_classes == 2 else \
+            tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
+
     training_args = DistributedTrainingArguments(
         use_case=args.use_case,
         model=model,
         optimizer=optimizer,
         loss=loss,
         train_data=train_data,
         val_data=val_data,
         epochs=args.epochs,
         scaling=args.scaling,
         batch_size=args.batch_size,
         batch_denom=args.batch_denom,
-        shuffle=args.shuffle
+        shuffle=args.shuffle,
+        max_seq_length=args.max_seq_length,
+        hf_bert_tokenizer=args.model_name if args.tlt_saved_objects_dir is not None and
+        args.use_case == 'text_classification' else None
     )
 
     dtf.launch_distributed_job(training_args)
```

## tlt/distributed/tensorflow/utils/tf_distributed_util.py

```diff
@@ -15,58 +15,178 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import os
-import dill
+import dill  # nosec: B403
 import time
 
 import tensorflow as tf
-import tensorflow_text  # noqa: F401
+import tensorflow_hub as hub
+
 import numpy as np
 
-# import horovod
-import horovod.tensorflow.keras as hvd
+from transformers import TFBertModel, BertConfig
+
+from pydoc import locate
+from tlt.utils.dataset_utils import prepare_huggingface_input_data
+from tlt.models.model_factory import get_model_info
 
-from tlt.distributed import TLT_DISTRIBUTED_DIR
+
+# This needs to be imported last to avoid "free(): invalid pointer" error
+import horovod.tensorflow.keras as hvd
 
 
 class DistributedTrainingArguments:
-    def __init__(self, **kwargs) -> None:
-        self.__dict__ = dict(kwargs)
+
+    def __init__(self, use_case, train_data, model, optimizer, loss, test_data=None, val_data=None,
+                 epochs=1, global_batch_size=128, shuffle=True, scaling='weak', **kwargs) -> None:
+
+        self.use_case = use_case
+
+        # Model related arguments
+        self.model = model
+        self.optimizer = optimizer
+        self.loss = loss
+
+        # Data related arguments
+        self.train_data = train_data
+        self.test_data = test_data
+        self.val_data = val_data
+        self.num_classes = kwargs.get('num_classes', None)
+
+        # Training related arguments
+        self.epochs = epochs
+        self.scaling = scaling
+        self.global_batch_size = global_batch_size
+        self.batch_denom = kwargs.get('batch_denom', 1)
+        self.shuffle = shuffle
+
+        # Use case related arguments
+        # For image classification
+        self.image_size = kwargs.get('image_size', None)
+        self.image_shape = kwargs.get('image_shape', None)
+        # For text classification
+        self.max_seq_length = kwargs.get('max_seq_length', None)
+        self.padding = kwargs.get('padding', None)
+        self.truncation = kwargs.get('truncation', None)
+        self.hf_bert_tokenizer = kwargs.get('hf_bert_tokenizer', None)
 
 
 class DistributedTF:
 
     def __init__(self) -> None:
-        pass
-
-    def launch_distributed_job(self, training_args: DistributedTrainingArguments):
         hvd.init()
 
+    def prepare_dataset(self, dataset, use_case, global_batch_size, scaling, **kwargs):
+        if scaling.lower() == 'weak':
+            batch_size = global_batch_size
+        elif scaling.lower() == 'strong':
+            batch_size = global_batch_size // hvd.size()
+
+        if use_case == 'image_classification':
+            dataset = dataset.shard(num_shards=hvd.size(), index=hvd.rank())
+            dataset = dataset.cache()
+            if 'map_func' in kwargs:
+                dataset = dataset.map(map_func=kwargs.get('map_func'), num_parallel_calls=tf.data.AUTOTUNE)
+            dataset = dataset.batch(batch_size)
+            dataset = dataset.prefetch(tf.data.AUTOTUNE)
+        elif use_case == 'text_classification':
+            max_seq_length = kwargs.get('max_seq_length')
+            bert_tokenizer = kwargs.get('hf_bert_tokenizer')
+
+            input_ids_shape = (len(dataset), max_seq_length)
+            attention_mask_shape = (len(dataset), max_seq_length)
+
+            input_ids = tf.zeros(input_ids_shape, dtype=tf.int32)
+            attention_mask = tf.zeros(attention_mask_shape, dtype=tf.int32)
+            labels = tf.ones(len(dataset), dtype=tf.int32)
+
+            # Preprocessing text could be done only on one worker and the tensors are synced later among workers
+            if hvd.rank() == 0:
+                dataset = [(sentence.numpy().decode(), label.numpy()) for sentence, label in dataset]
+
+                sentences = [x[0] for x in dataset]
+                labels = [x[1] for x in dataset]
+
+                print('Tokenizing the dataset...')
+                tokenized_dataset = bert_tokenizer(sentences, padding='max_length', max_length=max_seq_length,
+                                                   truncation=True, return_tensors='tf')
+
+                input_ids = tokenized_dataset['input_ids']
+                attention_mask = tokenized_dataset['attention_mask']
+                labels = tf.convert_to_tensor(labels, dtype=tf.int32)
+
+            input_ids = hvd.allreduce(input_ids, average=False, name='barrier1')
+            attention_mask = hvd.allreduce(attention_mask, average=False, name='barrier2')
+            labels = hvd.allreduce(labels, average=False, name='labels')
+
+            dataset = ({
+                'input_ids': input_ids,
+                'attention_mask': attention_mask
+            }, labels)
+
+            dataset = tf.data.Dataset.from_tensor_slices(dataset)
+            dataset = dataset.shard(hvd.size(), hvd.rank())
+            dataset = dataset.cache()
+            dataset = dataset.batch(batch_size)
+            dataset = dataset.prefetch(tf.data.AUTOTUNE)
+
+        return dataset
+
+    def prepare_model(self, model_name, use_case, input_shape, num_classes, **kwargs):
+        # Try to get model url from TLT supported models
+        model_info = get_model_info(model_name, 'tensorflow', use_case)
+        if model_info != {}:
+            fw_enum = list(model_info.keys())[0]
+            model_name = model_info[fw_enum]['tensorflow']['feature_vector']
+        if use_case == 'image_classification':
+            model = tf.keras.models.Sequential([
+                hub.KerasLayer(model_name, input_shape=input_shape),
+                tf.keras.layers.Dense(num_classes, activation='softmax')
+            ])
+        elif use_case == 'text_classification':
+            bert_config = BertConfig.from_pretrained(model_name, output_hidden_states=True)
+            bert_model = TFBertModel.from_pretrained(model_name, config=bert_config, from_pt=True)
+
+            dense_layer_dims = 1 if num_classes == 2 else num_classes
+
+            input_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32, name='input_ids')
+            attention_mask = tf.keras.layers.Input(input_shape, dtype=tf.int32, name='attention_mask')
+            bert_output = bert_model.bert(input_ids, attention_mask=attention_mask)[1]
+            classifier = tf.keras.layers.Dense(dense_layer_dims, activation=None, name='classifier')(bert_output)
+
+            model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=classifier)
+
+        return model
+
+    def launch_distributed_job(self, training_args: DistributedTrainingArguments):
         model = training_args.model
         optimizer = training_args.optimizer
         loss = training_args.loss
 
+        # This is required if using intel-tensorflow==2.12.0
+        optimizer = self._get_legacy_optimizer(optimizer)
+
         # Horovod: pin GPU to be used to process local rank (one GPU per process)
         gpus = tf.config.experimental.list_physical_devices('GPU')
         for gpu in gpus:
             tf.config.experimental.set_memory_growth(gpu, True)
         if gpus:
             tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')
 
         if training_args.scaling.lower() == 'weak':
-            multiplier = np.sqrt(training_args.batch_size // training_args.batch_denom)
+            multiplier = np.sqrt(training_args.global_batch_size // training_args.batch_denom)
             optimizer.lr = optimizer.lr * multiplier
-            batch_size = training_args.batch_size
+            batch_size = training_args.global_batch_size
         elif training_args.scaling.lower() == 'strong':
             optimizer.lr = optimizer.lr * hvd.size()
-            batch_size = training_args.batch_size // hvd.size()
+            batch_size = training_args.global_batch_size // hvd.size()
 
         if training_args.use_case == 'image_classification':
             hvd_optimizer = hvd.DistributedOptimizer(
                 optimizer, backward_passes_per_step=5, average_aggregated_gradients=True)
         elif training_args.use_case == 'text_classification':
             hvd_optimizer = hvd.DistributedOptimizer(
                 optimizer, backward_passes_per_step=1, average_aggregated_gradients=True)
@@ -90,60 +210,95 @@
         # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final accuracy.
         callbacks.append(hvd.callbacks.LearningRateWarmupCallback(
             initial_lr=optimizer.lr, warmup_epochs=warmup, verbose=1))
 
         # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.
         if hvd.rank() == 0:
             model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(
-                TLT_DISTRIBUTED_DIR, 'model_checkpoints'), save_weights_only=False, monitor='val_acc',
+                os.environ['HOME'], 'model_checkpoints'), save_weights_only=False, monitor='val_acc',
                 mode='max', save_best_only=True)
             callbacks.append(model_checkpoint_callback)
 
         # Horovod: write logs on worker 0.
         verbose = 1 if hvd.rank() == 0 else 0
 
+        x_input_data = training_args.train_data
+        y_target_data = None
+        val_data = training_args.val_data
+
+        # Prepare dataset for Hugging Face text classification
+        if training_args.hf_bert_tokenizer:
+            bert_tokenizer_name = training_args.hf_bert_tokenizer
+            max_seq_length = training_args.max_seq_length
+            tokenized_data, labels = prepare_huggingface_input_data(x_input_data, bert_tokenizer_name, max_seq_length)
+            x_input_data = [tokenized_data['input_ids'], tokenized_data['attention_mask']]
+            y_target_data = tf.convert_to_tensor(labels)
+
+            if training_args.val_data:
+                tokenized_val_data, val_labels = prepare_huggingface_input_data(training_args.val_data,
+                                                                                bert_tokenizer_name, max_seq_length)
+                val_data = ([tokenized_val_data['input_ids'], tokenized_val_data['attention_mask']],
+                            tf.convert_to_tensor(val_labels))
+
         start = time.time()
         steps_per_epoch_per_worker = len(training_args.train_data) // batch_size
         steps_per_epoch_per_worker = steps_per_epoch_per_worker // hvd.size()
         if hvd.size() > 2:
             steps_per_epoch_per_worker += 1
         self.history = model.fit(
-            training_args.train_data,
-            validation_data=training_args.val_data,
+            x=x_input_data,
+            y=y_target_data,
+            validation_data=val_data,
             callbacks=callbacks,
             steps_per_epoch=steps_per_epoch_per_worker,
             epochs=training_args.epochs,
             initial_epoch=0,
             verbose=verbose
         )
         end = time.time()
         if hvd.rank() == 0:
             print("Total elapsed time in seconds = ", end - start)
             print("Total elapsed time in minutes = ", ((end - start) / 60))
             print("Total epochs = ", len(self.history.history['loss']))
             print("Time per epoch in seconds = ", ((end - start) / len(self.history.history['loss'])))
             print("Maximum validation accuracy = ", np.max(self.history.history['val_acc']))
 
-    @classmethod
-    def load_saved_objects(cls):
+    def _get_legacy_optimizer(self, optimizer):
+        optimizer_config = optimizer.get_config()
+        optimizer_name = optimizer_config['name']
+
+        legacy_optimizer_class = locate('tensorflow.keras.optimizers.legacy.{}'.format(optimizer_name))
+
+        if legacy_optimizer_class is None:
+            # No matching legacy optimizer is found.
+            return optimizer
+
+        legacy_optimizer_config = legacy_optimizer_class().get_config()
+        legacy_optimizer = legacy_optimizer_class.from_config(
+            {k: v for k, v in optimizer_config.items() if k in legacy_optimizer_config}
+        )
+
+        return legacy_optimizer
+
+    def load_saved_objects(self, saved_objects_dir):
         # Load the saved_model.pb
-        model = tf.keras.models.load_model(filepath=TLT_DISTRIBUTED_DIR, compile=False)
+        model = tf.keras.models.load_model(filepath=saved_objects_dir, compile=False)
 
         # Load the optimizer and restore its state
         checkpoint = tf.train.Checkpoint(optimizer=tf.optimizers.Adam())
-        checkpoint.restore(os.path.join(TLT_DISTRIBUTED_DIR, 'saved_optimizer-1'))
+        checkpoint.restore(os.path.join(saved_objects_dir, 'saved_optimizer-1'))
 
         # Load the saved loss class name and instatiate the loss
-        with open(os.path.join(TLT_DISTRIBUTED_DIR, 'saved_loss'), 'rb') as f:
-            loss_class, loss_args = dill.load(f)
+        with open(os.path.join(saved_objects_dir, 'saved_loss'), 'rb') as f:
+            loss_class, loss_args = dill.load(f)  # nosec: B301
 
         # load the dataset(s)
-        train_data = tf.data.Dataset.load(os.path.join(TLT_DISTRIBUTED_DIR, 'train_data'))
+        train_data = tf.data.Dataset.load(os.path.join(saved_objects_dir, 'train_data'))
         try:
-            val_data = tf.data.Dataset.load(os.path.join(TLT_DISTRIBUTED_DIR, 'val_data'))
+            val_data = tf.data.Dataset.load(os.path.join(saved_objects_dir, 'val_data'))
         except FileNotFoundError:
             val_data = None
 
         if loss_class is None:
             dataset = train_data.unbatch()
             dataset = list(dataset.as_numpy_iterator())
             labels = list()
```

## tlt/models/hf_model.py

```diff
@@ -18,17 +18,21 @@
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import os
 import inspect
 import torch
 
+from neural_compressor import quantization
+from neural_compressor.config import BenchmarkConfig
+
 from tlt.models.model import BaseModel
 from tlt.utils.types import FrameworkType, UseCaseType
 from tlt.utils.file_utils import verify_directory
+from tlt.utils.inc_utils import get_inc_config
 
 
 class HFModel(BaseModel):
     """
     Base class to represent a Hugging Face model
     """
 
@@ -67,7 +71,134 @@
 
         if extra_layers:
             if not isinstance(extra_layers, list) or not all(isinstance(n, int) for n in extra_layers):
                 raise ValueError("extra_layers argument must be a list of integers but found a {}".format(extra_layers))
 
         if not isinstance(epochs, int):
             raise TypeError("Invalid type for the epochs arg. Expected an int but found a {}".format(type(epochs)))
+
+    def optimize_graph(self, output_dir, overwrite_model=False):
+        """
+        Performs FP32 graph optimization using the Intel Neural Compressor on the model
+        and writes the inference-optimized model to the output_dir. Graph optimization includes converting
+        variables to constants, removing training-only operations like checkpoint saving, stripping out parts
+        of the graph that are never reached, removing debug operations like CheckNumerics, folding batch
+        normalization ops into the pre-calculated weights, and fusing common operations into unified versions.
+        Args:
+            output_dir (str): Writable output directory to save the optimized model
+            overwrite_model (bool): Specify whether or not to overwrite the output_dir, if it already exists
+                                    (default: False)
+        Returns:
+            None
+        Raises:
+            NotImplementedError: because this hasn't been implemented yet for PyTorch
+        """
+        raise NotImplementedError("Only TensorFlow graph optimization is currently supported by the "
+                                  "Intel Neural Compressor (INC)")
+
+    def quantize(self, output_dir, dataset, config=None, overwrite_model=False):
+        """
+        Performs post training quantization using the Intel Neural Compressor on the model using the dataset.
+        The dataset's training subset will be used as the calibration data and its validation or test subset will
+        be used for evaluation. The quantized model is written to the output directory.
+
+        Args:
+            output_dir (str): Writable output directory to save the quantized model
+            dataset (ImageClassificationDataset): dataset to quantize with
+            config (PostTrainingQuantConfig): Optional, for customizing the quantization parameters
+            overwrite_model (bool): Specify whether or not to overwrite the output_dir, if it already exists
+                                    (default: False)
+
+        Returns:
+            None
+
+        Raises:
+            FileExistsError: if the output_dir already has a model.pt file
+            ValueError: if the dataset is not compatible for quantizing the model
+        """
+        if not os.path.exists(output_dir):
+            os.makedirs(output_dir)
+        else:
+            # Verify that the output directory doesn't already have a model.pt or best_model.pt file
+            if os.path.exists(os.path.join(output_dir, "model.pt")) or \
+                    os.path.exists(os.path.join(output_dir, "best_model.pt")):
+                if not overwrite_model:
+                    raise FileExistsError("A saved model already exists in: {}".format(output_dir))
+
+        # Verify dataset is of the right type
+        if not isinstance(dataset, self._inc_compatible_dataset):
+            raise ValueError('Quantization is compatible with datasets of type {}, and type '
+                             '{} was found'.format(self._inc_compatible_dataset, type(dataset)))
+
+        config = config if config is not None else get_inc_config(approach=self._quantization_approach)
+
+        calib_dataloader, _ = dataset.get_inc_dataloaders()
+        config.backend = 'default'
+        quantized_model = quantization.fit(model=self._model, conf=config, calib_dataloader=calib_dataloader)
+
+        # If quantization was successful, save the model
+        if quantized_model:
+            quantized_model.save(output_dir)
+            if os.path.isfile(os.path.join(output_dir, 'best_model.pt')):
+                # Change the model filename from best_model.pt to model.pt to match our convention
+                os.rename(os.path.join(output_dir, 'best_model.pt'), os.path.join(output_dir, 'model.pt'))
+        else:
+            raise RuntimeError("There was an error with quantization")
+
+    def benchmark(self, dataset, saved_model_dir=None, warmup=10, iteration=100, cores_per_instance=None,
+                  num_of_instance=None, inter_num_of_threads=None, intra_num_of_threads=None):
+        """
+        Use Intel Neural Compressor to benchmark the model with the dataset argument. The dataset's validation or test
+        subset will be used for benchmarking, if present. Otherwise, the full training dataset is used. The model to be
+        benchmarked can also be explicitly set to a saved_model_dir containing for example a quantized saved model.
+
+        Args:
+            dataset (ImageClassificationDataset): Dataset to use for benchmarking
+            saved_model_dir (str): Optional, path to the directory where the saved model is located
+            warmup (int): The number of iterations to perform before running performance tests, default is 10
+            iteration (int): The number of iterations to run performance tests, default is 100
+            cores_per_instance (int or None): The number of CPU cores to use per instance, default is None
+            num_of_instance (int or None): The number of instances to use for performance testing, default is None
+            inter_num_of_threads (int or None): The number of threads to use for inter-thread operations, default is
+                                                None
+            intra_num_of_threads (int or None): The number of threads to use for intra-thread operations, default is
+                                                None
+
+        Returns:
+            Benchmarking results from Intel Neural Compressor
+
+        Raises:
+            NotADirectoryError: if the saved_model_dir is not None or a valid directory
+            FileNotFoundError: if a model.pt is not found in the saved_model_dir or if the inc_config_path file
+            is not found
+        """
+        # Verify dataset is of the right type
+        if not isinstance(dataset, self._inc_compatible_dataset):
+            raise NotImplementedError('Quantization has only been implemented for TLT datasets, and type '
+                                      '{} was found'.format(type(dataset)))
+
+        # If provided, the saved model directory should exist and contain a model.pt file
+        if saved_model_dir is not None:
+            if not os.path.isdir(saved_model_dir):
+                raise NotADirectoryError("The saved model directory ({}) does not exist.".format(saved_model_dir))
+            if not os.path.isfile(os.path.join(saved_model_dir, "model.pt")):
+                raise FileNotFoundError("The saved model directory ({}) should have a model.pt file".format(
+                    saved_model_dir))
+            model = os.path.join(saved_model_dir, 'model.pt')
+        else:
+            model = self._model
+
+        config = BenchmarkConfig(backend="ipex", warmup=warmup, iteration=iteration,
+                                 cores_per_instance=cores_per_instance, num_of_instance=num_of_instance,
+                                 inter_num_of_threads=inter_num_of_threads, intra_num_of_threads=intra_num_of_threads)
+
+        _, eval_dataloader = dataset.get_inc_dataloaders()
+
+        from neural_compressor.benchmark import fit
+
+        try:
+            return fit(model=model, config=config, b_dataloader=eval_dataloader)
+        except AssertionError:
+            # Use INC's special load utility to reload an int8 ipex model
+            from neural_compressor.utils.pytorch import load
+            quantized_model = load(model, self._model, dataloader=eval_dataloader)
+            return fit(model=quantized_model, config=config, b_dataloader=eval_dataloader)
```

## tlt/models/model.py

```diff
@@ -33,14 +33,15 @@
         """
         Class constructor
         """
         self._model_name = model_name
         self._framework = framework
         self._use_case = use_case
         self._learning_rate = 0.001
+        self._preprocessor = None
 
     @property
     def model_name(self):
         """
         Name of the model
         """
         return self._model_name
@@ -66,14 +67,21 @@
         """
         return self._learning_rate
 
     @learning_rate.setter
     def learning_rate(self, value):
         self._learning_rate = value
 
+    @property
+    def preprocessor(self):
+        """
+        Preprocessor for the model
+        """
+        return self._preprocessor
+
     @abc.abstractmethod
     def load_from_directory(self, model_dir: str):
         """
         Load a model from a directory
         """
         pass
 
@@ -107,98 +115,80 @@
     def export(self, output_dir: str):
         """
         Export the serialized model to an output directory
         """
         pass
 
     @abc.abstractmethod
-    def write_inc_config_file(self, config_file_path, dataset, batch_size, overwrite=False, **kwargs):
+    def quantize(self, output_dir, dataset, config=None, overwrite_model=False):
         """
-        Writes an Intel Neural Compressor compatible config file to the specified path usings args from the
-        specified dataset and parameters. This is currently only supported for TF custom image classification
-        datasets.
+        Performs post training quantization using the Intel Neural Compressor on the model using the dataset.
+        The dataset's training subset will be used as the calibration data and its validation or test subset will
+        be used for evaluation. The quantized model is written to the output directory.
 
         Args:
-            config_file_path (str): Destination path on where to write the .yaml config file.
-            dataset (BaseDataset): A tlt dataset object
-            batch_size (int): Batch size to use for quantization and evaluation
-            overwrite (bool): Specify whether or not to overwrite the config_file_path, if it already exists
-                              (default: False)
-
-        Returns:
-            None
-
-        Raises:
-            FileExistsError if the config file already exists and overwrite is set to False
-            ValueError if the parameters are not within the expected values
-            NotImplementedError if the model or dataset does not support INC yet
-        """
-        pass
-
-    @abc.abstractmethod
-    def quantize(self, saved_model_dir, output_dir, inc_config_path):
-        """
-        Performs post training quantization using the Intel Neural Compressor on the model from the saved_model_dir
-        using the specified config file. The quantized model is written to the output directory.
-
-        Args:
-            saved_model_dir (str): Source directory for the model to quantize.
             output_dir (str): Writable output directory to save the quantized model
-            inc_config_path (str): Path to an INC config file (.yaml)
+            dataset (ImageClassificationDataset): dataset to quantize with
+            config (PostTrainingQuantConfig): Optional, for customizing the quantization parameters
+            overwrite_model (bool): Specify whether or not to overwrite the output_dir, if it already exists
+                                    (default: False)
 
         Returns:
             None
 
         Raises:
-            NotImplementedError if the model does not support INC yet
-            NotADirectoryError if the saved_model_dir is not a directory
-            FileNotFoundError if a saved_model.pb is not found in the saved_model_dir or if the inc_config_path file
-            is not found.
-            FileExistsError if the output_dir already has a saved_model.pb file
+            FileExistsError: if the output_dir already has a model file
+            ValueError: if the dataset is not compatible for quantizing the model
         """
         pass
 
     @abc.abstractmethod
-    def optimize_graph(self, saved_model_dir, output_dir):
+    def optimize_graph(self, output_dir, overwrite_model=False):
         """
-        Performs FP32 graph optimization using the Intel Neural Compressor on the model in the saved_model_dir
+        Performs FP32 graph optimization using the Intel Neural Compressor on the model
         and writes the inference-optimized model to the output_dir. Graph optimization includes converting
         variables to constants, removing training-only operations like checkpoint saving, stripping out parts
         of the graph that are never reached, removing debug operations like CheckNumerics, folding batch
         normalization ops into the pre-calculated weights, and fusing common operations into unified versions.
 
         Args:
-            saved_model_dir (str): Source directory for the model to optimize
             output_dir (str): Writable output directory to save the optimized model
+            overwrite_model (bool): Specify whether or not to overwrite the output_dir, if it already exists
+                                    (default: False)
 
         Returns:
             None
 
         Raises:
-            NotImplementedError if the model does not support INC yet
-            NotADirectoryError if the saved_model_dir is not a directory
-            FileNotFoundError if a saved_model.pb is not found in the saved_model_dir
-            FileExistsError if the output_dir already has a saved_model.pb file
+            NotImplementedError: if the model does not support INC yet
+            FileExistsError: if the output_dir already has a saved_model.pb file
         """
         pass
 
     @abc.abstractmethod
-    def benchmark(self, saved_model_dir, inc_config_path, mode='performance'):
+    def benchmark(self, dataset, saved_model_dir=None, warmup=10, iteration=100, cores_per_instance=None,
+                  num_of_instance=None, inter_num_of_threads=None, intra_num_of_threads=None):
         """
-        Use INC to benchmark the specified model for performance or accuracy.
+        Use Intel Neural Compressor to benchmark the model with the dataset argument. The dataset's validation or test
+        subset will be used for benchmarking, if present. Otherwise, the full training dataset is used. The model to be
+        benchmarked can also be explicitly set to a saved_model_dir containing for example a quantized saved model.
 
         Args:
-            saved_model_dir (str): Path to the directory where the saved model is located
-            inc_config_path (str): Path to an INC config file (.yaml)
-            mode (str): performance or accuracy (defaults to performance)
+            dataset (ImageClassificationDataset): Dataset to use for benchmarking
+            saved_model_dir (str): Optional, path to the directory where the saved model is located
+            warmup (int): The number of iterations to perform before running performance tests, default is 10
+            iteration (int): The number of iterations to run performance tests, default is 100
+            cores_per_instance (int or None): The number of CPU cores to use per instance, default is None
+            num_of_instance (int or None): The number of instances to use for performance testing, default is None
+            inter_num_of_threads (int or None): The number of threads to use for inter-thread operations, default is
+                                                None
+            intra_num_of_threads (int or None): The number of threads to use for intra-thread operations, default is
+                                                None
 
         Returns:
-            None
+            Benchmarking results from Intel Neural Compressor
 
         Raises:
-            NotImplementedError if the model does not support INC yet
-            NotADirectoryError if the saved_model_dir is not a directory
-            FileNotFoundError if a saved_model.pb is not found in the saved_model_dir or if the inc_config_path file
-            is not found.
-            ValueError if an unexpected mode is provided
+            NotADirectoryError: if the saved_model_dir is not None or a valid directory
+            FileNotFoundError: if a model is not found in the saved_model_dir
         """
         raise NotImplementedError("INC benchmarking is not supported for this model")
```

## tlt/models/model_factory.py

```diff
@@ -36,16 +36,19 @@
             "Keras": {"module": "tlt.models.image_classification.keras_image_classification_model",
                       "class": "KerasImageClassificationModel"},
             "Custom": {"module": "tlt.models.image_classification.tf_image_classification_model",
                        "class": "TFImageClassificationModel"}
         },
         UseCaseType.TEXT_CLASSIFICATION:
         {
+            "huggingface": {"module": "tlt.models.text_classification.tf_hf_text_classification_model",
+                            "class": "TFHFTextClassificationModel"},
             "TFHub": {"module": "tlt.models.text_classification.tfhub_text_classification_model",
                       "class": "TFHubTextClassificationModel"},
+
             "Custom": {"module": "tlt.models.text_classification.tf_text_classification_model",
                        "class": "TFTextClassificationModel"}
         }
     },
     FrameworkType.PYTORCH:
     {
         UseCaseType.IMAGE_CLASSIFICATION:
@@ -54,36 +57,38 @@
                             "class": "TorchvisionImageClassificationModel"},
             "pytorch_hub": {"module": "tlt.models.image_classification.pytorch_hub_image_classification_model",
                             "class": "PyTorchHubImageClassificationModel"},
             "Custom": {"module": "tlt.models.image_classification.pytorch_image_classification_model",
                        "class": "PyTorchImageClassificationModel"}
         },
         UseCaseType.TEXT_CLASSIFICATION: {
-            "huggingface": {"module": "tlt.models.text_classification.hf_text_classification_model",
-                            "class": "HFTextClassificationModel"},
+            "huggingface": {"module": "tlt.models.text_classification.pytorch_hf_text_classification_model",
+                            "class": "PyTorchHFTextClassificationModel"},
         },
         UseCaseType.IMAGE_ANOMALY_DETECTION:
         {
             "torchvision": {"module": "tlt.models.image_anomaly_detection.torchvision_image_anomaly_detection_model",
                             "class": "TorchvisionImageAnomalyDetectionModel"},
             "Custom": {"module": "tlt.models.image_anomaly_detection.pytorch_image_anomaly_detection_model",
                        "class": "PyTorchImageAnomalyDetectionModel"}
         }
     }
 }
 
 
-def load_model(model_name: str, model, framework: FrameworkType = None, use_case: UseCaseType = None, **kwargs):
+def load_model(model_name: str, model, framework: FrameworkType = None, use_case: UseCaseType = None,
+               model_hub: str = None, **kwargs):
     """A factory method for loading an existing model.
 
         Args:
             model_name (str): name of model
             model (model or str): model object or directory with a saved_model.pb or model.pt file to load
             framework (str or FrameworkType): framework
             use_case (str or UseCaseType): use case
+            model_hub (str): The model hub where the model originated
             kwargs: optional; additional keyword arguments for optimizer and loss function configuration.
                 The `optimizer` and `loss` arguments can be set to Optimizer and Loss classes, depending on the model's
                 framework (examples: `optimizer=tf.keras.optimizers.Adam` for TensorFlow,
                 `loss=torch.nn.CrossEntropyLoss` for PyTorch). Additional keywords for those classes' initialization
                 can then be provided to further configure the objects when they are created (example: `amsgrad=True`
                 for the PyTorch Adam optimizer). Refer to the framework documentation for the function you want to use.
 
@@ -101,16 +106,18 @@
 
     if not isinstance(framework, FrameworkType):
         framework = FrameworkType.from_str(framework)
 
     if use_case is not None and not isinstance(use_case, UseCaseType):
         use_case = UseCaseType.from_str(use_case)
 
-    model_class = locate('{}.{}'.format(model_map[framework][use_case]['Custom']['module'],
-                                        model_map[framework][use_case]['Custom']['class']))
+    model_hub = model_hub if model_hub else 'Custom'
+
+    model_class = locate('{}.{}'.format(model_map[framework][use_case][model_hub]['module'],
+                                        model_map[framework][use_case][model_hub]['class']))
     return model_class(model_name, model, **kwargs)
 
 
 def get_model(model_name: str, framework: FrameworkType = None, use_case: UseCaseType = None, **kwargs):
     """A factory method for creating models.
 
         Args:
@@ -124,15 +131,15 @@
                 can then be provided to further configure the objects when they are created (example: `amsgrad=True`
                 for the PyTorch Adam optimizer). Refer to the framework documentation for the function you want to use.
 
         Returns:
             model object
 
         Raises:
-            NotImplementedError if the model requested is not supported yet
+            NotImplementedError: if the model requested is not supported yet
 
         Example:
             >>> from tlt.models.model_factory import get_model
             >>> model = get_model('efficientnet_b0', 'tensorflow')
             >>> model.image_size
             224
 
@@ -191,15 +198,15 @@
         framework (str or FrameworkType): framework
         use_case (str or UseCaseType): use case
 
     Returns:
         dictionary
 
     Raises:
-        NameError if a model config file is found with an unknown or missing use case
+        NameError: if a model config file is found with an unknown or missing use case
 
     """
     # Directory of json files for the supported models
     config_directory = os.path.join(TLT_BASE_DIR, "models/configs")
 
     # Models dictionary with keys for use case / model name / framework / model info
     models = {}
@@ -264,14 +271,27 @@
         verbose (boolean): include all model data from the config file in result, default is False
         markdown (boolean): Print results as markdown tables (used for updating documentation).
                             Not compatible with verbose=True.
 
     """
     models = get_supported_models(framework, use_case)
 
+    # Proper names
+    model_hub_map = {
+        "torchvision": "Torchvision",
+        "tfhub": "TensorFlow Hub",
+        "pytorch_hub": "PyTorch Hub",
+        "huggingface": "Hugging Face",
+        "keras": "Keras Applications"
+    }
+    framework_name_map = {
+        "tensorflow": "TensorFlow",
+        "pytorch": "PyTorch"
+    }
+
     for model_use_case in models.keys():
         if markdown:
             print("## {}\n".format(model_use_case.replace("_", " ").title()))
         else:
             print("-" * 30)
             print(model_use_case.replace("_", " ").upper())
             print("-" * 30)
@@ -290,39 +310,25 @@
 
         # Get a sorted list of model names
         model_name_list = list(models[model_use_case].keys())
         model_name_list.sort(key=str.swapcase)
 
         for model_name in model_name_list:
             for model_framework in models[model_use_case][model_name].keys():
+                model_hub = models[model_use_case][model_name][model_framework]["model_hub"] if \
+                    "model_hub" in models[model_use_case][model_name][model_framework].keys() else ""
+                model_hub_display = model_hub_map[model_hub.lower()] if model_hub.lower() in model_hub_map.keys() \
+                    else model_hub
+                model_framework_display = framework_name_map[model_framework.lower()] if \
+                    model_framework.lower() in framework_name_map.keys() else model_framework
 
                 if markdown:
-                    model_hub = models[model_use_case][model_name][model_framework]["model_hub"] if \
-                        "model_hub" in models[model_use_case][model_name][model_framework].keys() else ""
-
-                    # Use proper names
-                    model_hub_map = {
-                        "torchvision": "Torchvision",
-                        "tfhub": "TensorFlow Hub",
-                        "pytorch_hub": "PyTorch Hub",
-                        "huggingface": "Hugging Face"
-                    }
-                    framework_name_map = {
-                        "tensorflow": "TensorFlow",
-                        "pytorch": "PyTorch"
-                    }
-
-                    model_hub = model_hub_map[model_hub.lower()] if model_hub.lower() in model_hub_map.keys() \
-                        else model_hub
-                    model_framework = framework_name_map[model_framework.lower()] if \
-                        model_framework.lower() in framework_name_map.keys() else model_framework
-
-                    print("| {} | {} | {} |".format(model_name, model_framework, model_hub))
+                    print("| {} | {} | {} |".format(model_name, model_framework_display, model_hub_display))
                 else:
-                    print("{} ({})".format(model_name, model_framework))
+                    print("{} ({} model from {})".format(model_name, model_framework_display, model_hub_display))
 
                 if verbose and not markdown:
                     for model_attribute, attribute_value in models[model_use_case][model_name][model_framework].items():
                         print("    {}: {}".format(model_attribute, attribute_value))
 
         # Empty line between use cases
         print("")
```

## tlt/models/pytorch_model.py

```diff
@@ -16,22 +16,26 @@
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import inspect
 import os
-import dill
+import dill  # nosec: B403
 import numpy
 import random
 import torch
 
+from neural_compressor import quantization
+from neural_compressor.config import BenchmarkConfig
+
 from tlt.models.model import BaseModel
 from tlt.utils.types import FrameworkType, UseCaseType
 from tlt.utils.file_utils import verify_directory
+from tlt.utils.inc_utils import get_inc_config
 
 
 class PyTorchModel(BaseModel):
     """
     Base class to represent a PyTorch model
     """
 
@@ -97,31 +101,34 @@
     def load_from_directory(self, model_dir: str):
         """
         Load a saved model from the model_dir path
         """
         # Verify that the model directory exists
         verify_directory(model_dir, require_directory_exists=True)
         model_copy = torch.load(os.path.join(model_dir, 'model.pt'))
-        self._model = dill.loads(model_copy)
+        self._model = dill.loads(model_copy)  # nosec: B301
         self._optimizer = self._optimizer_class(self._model.parameters(), lr=self._learning_rate)
 
-    def optimize_graph(self, saved_model_dir, output_dir):
+    def optimize_graph(self, output_dir, overwrite_model=False):
         """
-        Performs FP32 graph optimization using the Intel Neural Compressor on the model in the saved_model_dir
+        Performs FP32 graph optimization using the Intel Neural Compressor on the model
         and writes the inference-optimized model to the output_dir. Graph optimization includes converting
         variables to constants, removing training-only operations like checkpoint saving, stripping out parts
         of the graph that are never reached, removing debug operations like CheckNumerics, folding batch
         normalization ops into the pre-calculated weights, and fusing common operations into unified versions.
         Args:
-            saved_model_dir (str): Source directory for the model to optimize.
             output_dir (str): Writable output directory to save the optimized model
+            overwrite_model (bool): Specify whether or not to overwrite the output_dir, if it already exists
+                                    (default: False)
+
         Returns:
             None
+
         Raises:
-            NotImplementedError because this hasn't been implemented yet for PyTorch
+            NotImplementedError: because this hasn't been implemented yet for PyTorch
         """
         raise NotImplementedError("Only TensorFlow graph optimization is currently supported by the \
                                                                       Intel Neural Compressor (INC)")
 
     def list_layers(self, verbose=False):
         """
         Lists all of the named modules (e.g. features, avgpool, classifier) and layers
@@ -186,7 +193,116 @@
         # Unfreeze everything in the layer
         for (name, module) in self._model.named_children():
             if name == layer_name:
                 for param in module.parameters():
                     param.requires_grad = True
 
         return
+
+    def quantize(self, output_dir, dataset, config=None, overwrite_model=False):
+        """
+        Performs post training quantization using the Intel Neural Compressor on the model using the dataset.
+        The dataset's training subset will be used as the calibration data and its validation or test subset will
+        be used for evaluation. The quantized model is written to the output directory.
+
+        Args:
+            output_dir (str): Writable output directory to save the quantized model
+            dataset (ImageClassificationDataset): dataset to quantize with
+            config (PostTrainingQuantConfig): Optional, for customizing the quantization parameters
+            overwrite_model (bool): Specify whether or not to overwrite the output_dir, if it already exists
+                                    (default: False)
+
+        Returns:
+            None
+
+        Raises:
+            FileExistsError: if the output_dir already has a model.pt file
+            ValueError: if the dataset is not compatible for quantizing the model
+        """
+        if not os.path.exists(output_dir):
+            os.makedirs(output_dir)
+        else:
+            # Verify that the output directory doesn't already have a model.pt or best_model.pt file
+            if os.path.exists(os.path.join(output_dir, "model.pt")) or \
+                    os.path.exists(os.path.join(output_dir, "best_model.pt")):
+                if not overwrite_model:
+                    raise FileExistsError("A saved model already exists in: {}".format(output_dir))
+
+        # Verify dataset is of the right type
+        if not isinstance(dataset, self._inc_compatible_dataset):
+            raise ValueError('Quantization is compatible with datasets of type {}, and type '
+                             '{} was found'.format(self._inc_compatible_dataset, type(dataset)))
+
+        config = config if config is not None else get_inc_config(approach=self._quantization_approach)
+
+        calib_dataloader, eval_dataloader = dataset.get_inc_dataloaders()
+        config.backend = 'ipex'
+        quantized_model = quantization.fit(model=self._model, conf=config, calib_dataloader=calib_dataloader,
+                                           eval_dataloader=eval_dataloader)
+
+        # If quantization was successful, save the model
+        if quantized_model:
+            quantized_model.save(output_dir)
+            if os.path.isfile(os.path.join(output_dir, 'best_model.pt')):
+                # Change the model filename from best_model.pt to model.pt to match our convention
+                os.rename(os.path.join(output_dir, 'best_model.pt'), os.path.join(output_dir, 'model.pt'))
+        else:
+            raise RuntimeError("There was an error with quantization")
+
+    def benchmark(self, dataset, saved_model_dir=None, warmup=10, iteration=100, cores_per_instance=None,
+                  num_of_instance=None, inter_num_of_threads=None, intra_num_of_threads=None):
+        """
+        Use Intel Neural Compressor to benchmark the model with the dataset argument. The dataset's validation or test
+        subset will be used for benchmarking, if present. Otherwise, the full training dataset is used. The model to be
+        benchmarked can also be explicitly set to a saved_model_dir containing for example a quantized saved model.
+
+        Args:
+            dataset (ImageClassificationDataset): Dataset to use for benchmarking
+            saved_model_dir (str): Optional, path to the directory where the saved model is located
+            warmup (int): The number of iterations to perform before running performance tests, default is 10
+            iteration (int): The number of iterations to run performance tests, default is 100
+            cores_per_instance (int or None): The number of CPU cores to use per instance, default is None
+            num_of_instance (int or None): The number of instances to use for performance testing, default is None
+            inter_num_of_threads (int or None): The number of threads to use for inter-thread operations, default is
+                                                None
+            intra_num_of_threads (int or None): The number of threads to use for intra-thread operations, default is
+                                                None
+
+        Returns:
+            Benchmarking results from Intel Neural Compressor
+
+        Raises:
+            NotADirectoryError: if the saved_model_dir is not None or a valid directory
+            FileNotFoundError: if a model.pt is not found in the saved_model_dir or if the inc_config_path file
+            is not found
+        """
+        # Verify dataset is of the right type
+        if not isinstance(dataset, self._inc_compatible_dataset):
+            raise NotImplementedError('Quantization has only been implemented for TLT datasets, and type '
+                                      '{} was found'.format(type(dataset)))
+
+        # If provided, the saved model directory should exist and contain a model.pt file
+        if saved_model_dir is not None:
+            if not os.path.isdir(saved_model_dir):
+                raise NotADirectoryError("The saved model directory ({}) does not exist.".format(saved_model_dir))
+            if not os.path.isfile(os.path.join(saved_model_dir, "model.pt")):
+                raise FileNotFoundError("The saved model directory ({}) should have a model.pt file".format(
+                    saved_model_dir))
+            model = os.path.join(saved_model_dir, 'model.pt')
+        else:
+            model = self._model
+
+        config = BenchmarkConfig(backend="ipex", warmup=warmup, iteration=iteration,
+                                 cores_per_instance=cores_per_instance, num_of_instance=num_of_instance,
+                                 inter_num_of_threads=inter_num_of_threads, intra_num_of_threads=intra_num_of_threads)
+
+        _, eval_dataloader = dataset.get_inc_dataloaders()
+
+        from neural_compressor.benchmark import fit
+
+        try:
+            return fit(model=model, config=config, b_dataloader=eval_dataloader)
+        except AssertionError:
+            # Use INC's special load utility to reload an int8 ipex model
+            from neural_compressor.utils.pytorch import load
+            quantized_model = load(model, self._model, dataloader=eval_dataloader)
+            return fit(model=quantized_model, config=config, b_dataloader=eval_dataloader)
```

## tlt/models/tf_model.py

```diff
@@ -16,26 +16,32 @@
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import inspect
 import os
-import dill
+import dill  # nosec: B403
 import re
 import shutil
 import random
+import tempfile
 import numpy as np
 import tensorflow as tf
 
+from neural_compressor.experimental import Graph_Optimization
+from neural_compressor import quantization
+from neural_compressor.config import BenchmarkConfig
+
 from tlt.models.model import BaseModel
+from tlt.models.text_classification.text_classification_model import TextClassificationModel
 from tlt.utils.file_utils import verify_directory, validate_model_name
 from tlt.utils.platform_util import PlatformUtil
 from tlt.utils.types import FrameworkType, UseCaseType
-from tlt.distributed import TLT_DISTRIBUTED_DIR
+from tlt.utils.inc_utils import get_inc_config
 
 
 class TFModel(BaseModel):
     """
     Base class to represent a TF pretrained model
     """
 
@@ -84,17 +90,17 @@
             Args:
                 model_dir (str): Directory with a saved_model.pb or h5py file to load
 
             Returns:
                 None
 
             Raises:
-                TypeError if model_dir is not a string
-                NotADirectoryError if model_dir is not a directory
-                IOError for an invalid model file
+                TypeError: if model_dir is not a string
+                NotADirectoryError: if model_dir is not a directory
+                IOError: for an invalid model file
         """
         # Verify that the model directory exists
         verify_directory(model_dir, require_directory_exists=True)
 
         self._model = tf.keras.models.load_model(model_dir)
         self._model.summary(print_fn=print)
 
@@ -119,15 +125,15 @@
 
         auto_mixed_precision_supported = (tf_major_version == 2 and tf_minor_version >= 9) or tf_major_version > 2
 
         if enable_auto_mixed_precision is None:
             # Determine whether or not to enable this based on the CPU type
             try:
                 # Only enable auto mixed precision for SPR
-                enable_auto_mixed_precision = PlatformUtil(args=None).cpu_type == 'SPR'
+                enable_auto_mixed_precision = PlatformUtil().cpu_type == 'SPR'
             except Exception as e:
                 if auto_mixed_precision_supported:
                     print("Unable to determine the CPU type:", str(e))
                 enable_auto_mixed_precision = False
         elif not auto_mixed_precision_supported:
             print("Warning: Auto mixed precision requires TensorFlow 2.9.0 or later (found {}).".format(
                 tf.version.VERSION))
@@ -146,17 +152,17 @@
            Args:
                output_dir (str): A writeable output directory.
 
            Returns:
                The path to the numbered saved model directory
 
            Raises:
-               TypeError if the output_dir is not a string
-               FileExistsError the specified output directory already exists as a file
-               ValueError if the mode has not been loaded or trained yet
+               TypeError: if the output_dir is not a string
+               FileExistsError: the specified output directory already exists as a file
+               ValueError: if the mode has not been loaded or trained yet
         """
         if self._model:
             # Save the model in a format that can be served
             verify_directory(output_dir)
             val_model_name = validate_model_name(self.model_name)
             saved_model_dir = os.path.join(output_dir, val_model_name)
             if os.path.exists(saved_model_dir) and len(os.listdir(saved_model_dir)):
@@ -167,52 +173,71 @@
             self._model.save(saved_model_dir)
             print("Saved model directory:", saved_model_dir)
 
             return saved_model_dir
         else:
             raise ValueError("Unable to export the model, because it hasn't been loaded or trained yet")
 
-    def export_for_distributed(self, train_data, val_data):
+    def export_for_distributed(self, export_dir=None, train_data=None, val_data=None):
+        """
+        Exports the model, optimizer, loss, train data and validation data to the export_dir for distributed
+        script to access. Note that the export_dir must be accessible to all the nodes. For example: NFS shared
+        systems. Note that the export_dir is created using mkdtemp which reults in a unique dir name. For
+        example: "<export_dir_Am83Iw". If the export_dir is None, the default name is "saved_objects"
+
+        Args:
+            export_dir (str): Directory name to export the model, optimizer, loss, train data and validation
+                data. export_dir must be accessible to all the nodes. For example: NFS shared systems. export_dir
+                is created using mkdtemp which reults in a unique dir name. Forexample: "<export_dir_Am83Iw".
+                If the export_dir is None, the default name is "saved_objects"
+            train_data (TFDataset): Train dataset
+            val_data (TFDataset): Validation dataset
+        """
+
+        temp_dir_prefix = os.path.join(os.environ['HOME'], "saved_objects_") if export_dir is None else export_dir + "_"
+        self._temp_dir = tempfile.mkdtemp(prefix=temp_dir_prefix)
+
         # Save the model
+        print('Saving the model...', end='', flush=True)
         tf.keras.models.save_model(
             model=self._model,
-            filepath=TLT_DISTRIBUTED_DIR,
+            filepath=self._temp_dir,
             overwrite=True,
             include_optimizer=False
         )
+        print('Done')
 
         # Save the optimizer object
-        tf.train.Checkpoint(optimizer=self._optimizer).save(os.path.join(TLT_DISTRIBUTED_DIR, 'saved_optimizer'))
+        print('Saving the optimizer...', end='', flush=True)
+        tf.train.Checkpoint(optimizer=self._optimizer).save(
+            os.path.join(self._temp_dir, 'saved_optimizer'))
+        print('Done')
 
         # Save the loss class name and its args
-        with open(os.path.join(TLT_DISTRIBUTED_DIR, 'saved_loss'), 'wb') as f:
+        print('Saving the loss...', end='', flush=True)
+        with open(os.path.join(self._temp_dir, 'saved_loss'), 'wb') as f:
             dill.dump((self._loss_class, self._loss_args), f)
+            print('Done')
 
         # Save the dataset(s)
-        train_data.save(os.path.join(TLT_DISTRIBUTED_DIR, 'train_data'))
-        print(type(train_data))
+        print('Saving the train data...', end='', flush=True)
+        train_data.save(os.path.join(self._temp_dir, 'train_data'))
+        print('Done')
         if val_data:
-            val_data.save(os.path.join(TLT_DISTRIBUTED_DIR, 'val_data'))
+            print('Saving the validation data...', end='', flush=True)
+            val_data.save(os.path.join(self._temp_dir, 'val_data'))
+            print('Done')
+        return self._temp_dir
 
     def cleanup_saved_objects_for_distributed(self):
-        dirs = ['train_data', 'val_data', 'variables', 'assets', 'model_checkpoints']
-        files = ['checkpoint', 'keras_metadata.pb']
-
-        for f in os.listdir(TLT_DISTRIBUTED_DIR):
-            full_path = os.path.join(TLT_DISTRIBUTED_DIR, f)
-            if os.path.isdir(full_path) and f in dirs:
-                try:
-                    shutil.rmtree(full_path)
-                except FileNotFoundError:
-                    print("'{}' already cleaned up.".format(f))
-            elif os.path.isfile(full_path) and (f in files or f.startswith('saved')):
-                try:
-                    os.remove(full_path)
-                except FileNotFoundError:
-                    print("'{}' already cleaned up.".format(f))
+        try:
+            print('Cleaning saved objects...')
+            shutil.rmtree(self._temp_dir)
+        except OSError as ose:
+            print('Error while cleaning the saved obects: {}'.format(ose))
 
     def _parse_hostfile(self, hostfile):
         """
             Parses the hostfile and returns the required command. Contents of hostfile must contain IP addresses
             (or) hostnames in any of the following forms. Note that all lines must be of the same form:
                 "127.0.0.1"
                 "127.0.0.1 slots=2"
@@ -273,7 +298,137 @@
                 hostfile_info['ip_addresses'].append(socket.gethostbyname(line.split(' slots=')[0]))
                 hostfile_info['slots'].append(line.split(' slots=')[1])
             elif match == 'valid_hostname_with_colon':
                 hostfile_info['ip_addresses'].append(socket.gethostbyname(line.split(':')[0]))
                 hostfile_info['slots'].append(line.split(':')[1])
 
         return hostfile_info
+
+    def optimize_graph(self, output_dir, overwrite_model=False):
+        """
+        Performs FP32 graph optimization using the Intel Neural Compressor on the model
+        and writes the inference-optimized model to the output_dir. Graph optimization includes converting
+        variables to constants, removing training-only operations like checkpoint saving, stripping out parts
+        of the graph that are never reached, removing debug operations like CheckNumerics, folding batch
+        normalization ops into the pre-calculated weights, and fusing common operations into unified versions.
+
+        Args:
+            output_dir (str): Writable output directory to save the optimized model
+            overwrite_model (bool): Specify whether or not to overwrite the output_dir, if it already exists
+                                    (default: False)
+
+        Returns:
+            None
+
+        Raises:
+            FileExistsError: if the output_dir already has a saved_model.pb file
+        """
+        if not os.path.exists(output_dir):
+            os.makedirs(output_dir)
+        else:
+            # Verify that the output directory doesn't already have a saved_model.pb file
+            if os.path.exists(os.path.join(output_dir, "saved_model.pb")) and not overwrite_model:
+                raise FileExistsError("A saved model already exists at:", os.path.join(output_dir, "saved_model.pb"))
+
+        graph_optimizer = Graph_Optimization()
+        graph_optimizer.model = self._model
+        optimized_graph = graph_optimizer()
+
+        # If optimization was successful, save the model
+        if optimized_graph:
+            optimized_graph.save(output_dir)
+
+    def quantize(self, output_dir, dataset, config=None, overwrite_model=False):
+        """
+        Performs post training quantization using the Intel Neural Compressor on the model using the dataset.
+        The dataset's training subset will be used as the calibration data and its validation or test subset will
+        be used for evaluation. The quantized model is written to the output directory.
+
+        Args:
+            output_dir (str): Writable output directory to save the quantized model
+            dataset (ImageClassificationDataset): dataset to quantize with
+            config (PostTrainingQuantConfig): Optional, for customizing the quantization parameters
+            overwrite_model (bool): Specify whether or not to overwrite the output_dir, if it already exists
+                                    (default: False)
+
+        Returns:
+            None
+
+        Raises:
+            FileExistsError: if the output_dir already has a saved_model.pb file
+            ValueError: if the dataset is not compatible for quantizing the model
+        """
+        if not os.path.exists(output_dir):
+            os.makedirs(output_dir)
+        else:
+            # Verify that the output directory doesn't already have a saved_model.pb file
+            if os.path.exists(os.path.join(output_dir, "saved_model.pb")) and not overwrite_model:
+                raise FileExistsError("A saved model already exists at:", os.path.join(output_dir, "saved_model.pb"))
+
+        # Verify dataset is of the right type
+        if not isinstance(dataset, self._inc_compatible_dataset):
+            raise ValueError('Quantization is compatible with datasets of type {}, and type '
+                             '{} was found'.format(self._inc_compatible_dataset, type(dataset)))
+
+        config = config if config is not None else get_inc_config(approach=self._quantization_approach)
+        kwargs = {}
+        if isinstance(self, TextClassificationModel):
+            kwargs['hub_name'] = self._hub_name
+            kwargs['max_seq_length'] = self._max_seq_length
+        calib_dataloader, eval_dataloader = dataset.get_inc_dataloaders(**kwargs)
+        quantized_model = quantization.fit(model=self._model, conf=config, calib_dataloader=calib_dataloader,
+                                           eval_dataloader=eval_dataloader)
+
+        # If quantization was successful, save the model
+        if quantized_model:
+            quantized_model.save(output_dir)
+
+    def benchmark(self, dataset, saved_model_dir=None, warmup=10, iteration=100, cores_per_instance=None,
+                  num_of_instance=None, inter_num_of_threads=None, intra_num_of_threads=None):
+        """
+        Use Intel Neural Compressor to benchmark the model with the dataset argument. The dataset's validation or test
+        subset will be used for benchmarking, if present. Otherwise, the full training dataset is used. The model to be
+        benchmarked can also be explicitly set to a saved_model_dir containing for example a quantized saved model.
+
+        Args:
+            dataset (ImageClassificationDataset): Dataset to use for benchmarking
+            saved_model_dir (str): Optional, path to the directory where the saved model is located
+            warmup (int): The number of iterations to perform before running performance tests, default is 10
+            iteration (int): The number of iterations to run performance tests, default is 100
+            cores_per_instance (int or None): The number of CPU cores to use per instance, default is None
+            num_of_instance (int or None): The number of instances to use for performance testing, default is None
+            inter_num_of_threads (int or None): The number of threads to use for inter-thread operations, default is
+                                                None
+            intra_num_of_threads (int or None): The number of threads to use for intra-thread operations, default is
+                                                None
+
+        Returns:
+            Benchmarking results from Intel Neural Compressor
+
+        Raises:
+            NotADirectoryError: if the saved_model_dir is not a directory
+            FileNotFoundError: if a saved_model.pb is not found in the saved_model_dir or if the inc_config_path file
+            is not found
+        """
+        # If provided, the saved model directory should exist and contain a saved_model.pb file
+        if saved_model_dir is not None:
+            if not os.path.isdir(saved_model_dir):
+                raise NotADirectoryError("The saved model directory ({}) does not exist.".format(saved_model_dir))
+            if not os.path.isfile(os.path.join(saved_model_dir, "saved_model.pb")):
+                raise FileNotFoundError("The saved model directory ({}) should have a saved_model.pb file".format(
+                    saved_model_dir))
+            model = saved_model_dir
+        else:
+            model = self._model
+
+        kwargs = {}
+        if isinstance(self, TextClassificationModel):
+            kwargs['hub_name'] = self._hub_name
+            kwargs['max_seq_length'] = self._max_seq_length
+        _, eval_dataloader = dataset.get_inc_dataloaders(**kwargs)
+        config = BenchmarkConfig(warmup=warmup, iteration=iteration, cores_per_instance=cores_per_instance,
+                                 num_of_instance=num_of_instance, inter_num_of_threads=inter_num_of_threads,
+                                 intra_num_of_threads=intra_num_of_threads)
+
+        from neural_compressor.benchmark import fit
+
+        return fit(model, config=config, b_dataloader=eval_dataloader)
```

## tlt/models/configs/tf_keras_image_classification_models.json

### Pretty-printed

 * *Similarity: 0.6907894736842105%*

 * *Differences: {"'ConvNeXtBase'": "{'preprocessor': 'convnext'}",*

 * * "'ConvNeXtLarge'": "{'preprocessor': 'convnext'}",*

 * * "'ConvNeXtSmall'": "{'preprocessor': 'convnext'}",*

 * * "'ConvNeXtTiny'": "{'preprocessor': 'convnext'}",*

 * * "'ConvNeXtXLarge'": "{'preprocessor': 'convnext'}",*

 * * "'DenseNet121'": "{'preprocessor': 'densenet'}",*

 * * "'DenseNet169'": "{'preprocessor': 'densenet'}",*

 * * "'DenseNet201'": "{'preprocessor': 'densenet'}",*

 * * "'EfficientNetV2B0'": "{'preprocessor': 'efficientnet_v2'}",*

 * * "'EfficientNetV2B1'": "{'preprocessor':  […]*

```diff
@@ -1,192 +1,182 @@
 {
     "ConvNeXtBase": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "convnext"
     },
     "ConvNeXtLarge": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "convnext"
     },
     "ConvNeXtSmall": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "convnext"
     },
     "ConvNeXtTiny": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "convnext"
     },
     "ConvNeXtXLarge": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "convnext"
     },
     "DenseNet121": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "densenet"
     },
     "DenseNet169": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "densenet"
     },
     "DenseNet201": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
-    },
-    "EfficientNetB0": {
-        "image_size": 224,
-        "model_hub": "Keras",
-        "original_dataset": "ImageNet"
-    },
-    "EfficientNetB1": {
-        "image_size": 240,
-        "model_hub": "Keras",
-        "original_dataset": "ImageNet"
-    },
-    "EfficientNetB2": {
-        "image_size": 260,
-        "model_hub": "Keras",
-        "original_dataset": "ImageNet"
-    },
-    "EfficientNetB3": {
-        "image_size": 300,
-        "model_hub": "Keras",
-        "original_dataset": "ImageNet"
-    },
-    "EfficientNetB4": {
-        "image_size": 380,
-        "model_hub": "Keras",
-        "original_dataset": "ImageNet"
-    },
-    "EfficientNetB5": {
-        "image_size": 456,
-        "model_hub": "Keras",
-        "original_dataset": "ImageNet"
-    },
-    "EfficientNetB6": {
-        "image_size": 528,
-        "model_hub": "Keras",
-        "original_dataset": "ImageNet"
-    },
-    "EfficientNetB7": {
-        "image_size": 600,
-        "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "densenet"
     },
     "EfficientNetV2B0": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "efficientnet_v2"
     },
     "EfficientNetV2B1": {
         "image_size": 240,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "efficientnet_v2"
     },
     "EfficientNetV2B2": {
         "image_size": 260,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "efficientnet_v2"
     },
     "EfficientNetV2B3": {
         "image_size": 300,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "efficientnet_v2"
     },
     "EfficientNetV2L": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "efficientnet_v2"
     },
     "EfficientNetV2M": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "efficientnet_v2"
     },
     "EfficientNetV2S": {
         "image_size": 384,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "efficientnet_v2"
     },
     "InceptionResNetV2": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "inception_resnet_v2"
     },
     "InceptionV3": {
         "image_size": 299,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "inception_v3"
     },
     "MobileNet": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "mobilenet"
     },
     "MobileNetV2": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "mobilenet_v2"
     },
     "NASNetLarge": {
         "image_size": 331,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "nasnet"
     },
     "NASNetMobile": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "nasnet"
     },
     "ResNet101": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "resnet"
     },
     "ResNet101V2": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "resnet_v2"
     },
     "ResNet152": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "resnet"
     },
     "ResNet152V2": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "resnet_v2"
     },
     "ResNet50": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "resnet"
     },
     "ResNet50V2": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "resnet_v2"
     },
     "VGG16": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "vgg16"
     },
     "VGG19": {
         "image_size": 224,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "vgg19"
     },
     "Xception": {
         "image_size": 299,
         "model_hub": "Keras",
-        "original_dataset": "ImageNet"
+        "original_dataset": "ImageNet",
+        "preprocessor": "xception"
     }
 }
```

## tlt/models/image_anomaly_detection/cutpaste/cutpaste.py

```diff
@@ -89,35 +89,35 @@
 
     def __call__(self, img):
 
         h = img.size[0]
         w = img.size[1]
 
         # ratio between area_ratio[0] and area_ratio[1]
-        ratio_area = random.uniform(self.area_ratio[0], self.area_ratio[1]) * w * h
+        ratio_area = random.uniform(self.area_ratio[0], self.area_ratio[1]) * w * h  # nosec: B311
 
         # sample in log space
         log_ratio = torch.log(torch.tensor((self.aspect_ratio, 1 / self.aspect_ratio)))
         aspect = torch.exp(torch.empty(1).uniform_(log_ratio[0], log_ratio[1])).item()
 
         cut_w = int(round(math.sqrt(ratio_area * aspect)))
         cut_h = int(round(math.sqrt(ratio_area / aspect)))
 
         # one might also want to sample from other images. currently we only sample from the image itself
-        from_location_h = int(random.uniform(0, h - cut_h))
-        from_location_w = int(random.uniform(0, w - cut_w))
+        from_location_h = int(random.uniform(0, h - cut_h))  # nosec: B311
+        from_location_w = int(random.uniform(0, w - cut_w))  # nosec: B311
 
         box = [from_location_w, from_location_h, from_location_w + cut_w, from_location_h + cut_h]
         patch = img.crop(box)
 
         if self.colorJitter:
             patch = self.colorJitter(patch)
 
-        to_location_h = int(random.uniform(0, h - cut_h))
-        to_location_w = int(random.uniform(0, w - cut_w))
+        to_location_h = int(random.uniform(0, h - cut_h))  # nosec: B311
+        to_location_w = int(random.uniform(0, w - cut_w))  # nosec: B311
 
         insert_box = [to_location_w, to_location_h, to_location_w + cut_w, to_location_h + cut_h]
         augmented = img.copy()
         augmented.paste(patch, insert_box)
 
         return super().__call__(img, augmented)
 
@@ -142,33 +142,33 @@
         self.rotation = rotation
 
     def __call__(self, img):
         h = img.size[0]
         w = img.size[1]
 
         # cut region
-        cut_w = random.uniform(*self.width)
-        cut_h = random.uniform(*self.height)
+        cut_w = random.uniform(*self.width)  # nosec: B311
+        cut_h = random.uniform(*self.height)  # nosec: B311
 
-        from_location_h = int(random.uniform(0, h - cut_h))
-        from_location_w = int(random.uniform(0, w - cut_w))
+        from_location_h = int(random.uniform(0, h - cut_h))  # nosec: B311
+        from_location_w = int(random.uniform(0, w - cut_w))  # nosec: B311
 
         box = [from_location_w, from_location_h, from_location_w + cut_w, from_location_h + cut_h]
         patch = img.crop(box)
 
         if self.colorJitter:
             patch = self.colorJitter(patch)
 
         # rotate
-        rot_deg = random.uniform(*self.rotation)
+        rot_deg = random.uniform(*self.rotation)  # nosec: B311
         patch = patch.convert("RGBA").rotate(rot_deg, expand=True)
 
         # paste
-        to_location_h = int(random.uniform(0, h - patch.size[0]))
-        to_location_w = int(random.uniform(0, w - patch.size[1]))
+        to_location_h = int(random.uniform(0, h - patch.size[0]))  # nosec: B311
+        to_location_w = int(random.uniform(0, w - patch.size[1]))  # nosec: B311
 
         mask = patch.split()[-1]
         patch = patch.convert("RGB")
 
         augmented = img.copy()
         augmented.paste(patch, (to_location_w, to_location_h), mask=mask)
 
@@ -183,15 +183,15 @@
         """
         Class constructor
         """
         self.normal = CutPasteNormal(**kwargs)
         self.scar = CutPasteScar(**kwargs)
 
     def __call__(self, img):
-        r = random.uniform(0, 1)
+        r = random.uniform(0, 1)  # nosec: B311
         if r < 0.5:
             return self.normal(img)
         else:
             return self.scar(img)
 
 
 class CutPaste3Way(object):
```

## tlt/models/image_anomaly_detection/simsiam/loader.py

```diff
@@ -39,10 +39,10 @@
     """
     Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709
     """
     def __init__(self, sigma=[.1, 2.]):
         self.sigma = sigma
 
     def __call__(self, x):
-        sigma = random.uniform(self.sigma[0], self.sigma[1])
+        sigma = random.uniform(self.sigma[0], self.sigma[1])  # nosec: B311
         x = x.filter(ImageFilter.GaussianBlur(radius=sigma))
         return x
```

## tlt/models/image_classification/image_classification_model.py

```diff
@@ -15,18 +15,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import abc
-import os
-import yaml
 
-from tlt import TLT_BASE_DIR
 from tlt.models.model import BaseModel
 from tlt.utils.types import FrameworkType, UseCaseType
 
 
 class ImageClassificationModel(BaseModel):
     """
     Base class to represent a pretrained model for image classification
@@ -36,14 +33,15 @@
                  model_name: str, framework: FrameworkType, use_case: UseCaseType):
         """
         Class constructor
         """
         self._image_size = image_size
         self._do_fine_tuning = do_fine_tuning
         self._dropout_layer_rate = dropout_layer_rate
+        self._quantization_approach = 'static'
 
         BaseModel.__init__(self, model_name, framework, use_case)
 
     @property
     def image_size(self):
         """
         The fixed image size that the pretrained model expects as input, in pixels with equal width and height
@@ -68,29 +66,7 @@
 
     @property
     def dropout_layer_rate(self):
         """
         The probability of any one node being dropped when a dropout layer is used
         """
         return self._dropout_layer_rate
-
-    def get_inc_config_template_dict(self):
-        """
-        Returns a dictionary for a config template compatible with the Intel Neural Compressor.
-
-        It loads the yaml file tlt/models/configs/inc/image_classification_template.yaml and then fills in parameters
-        that the model knows about (like framework and model name). There are still more parameters that need to be
-        filled in before using the config with INC (like the dataset information, image size, etc).
-        """
-        template_file_path = os.path.join(TLT_BASE_DIR, "models/configs/inc/image_classification_template.yaml")
-
-        if not os.path.exists(template_file_path):
-            raise FileNotFoundError("Unable to find the image recognition config template at:", template_file_path)
-
-        with open(template_file_path, 'r') as template_yaml:
-            config_template = yaml.safe_load(template_yaml)
-
-        # Update parameters that we know in the template
-        config_template["model"]["framework"] = str(self.framework)
-        config_template["model"]["name"] = self.model_name
-
-        return config_template
```

## tlt/models/image_classification/keras_image_classification_model.py

```diff
@@ -15,58 +15,64 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import os
+from pydoc import locate
 import tensorflow as tf
 
 from downloader.models import ModelDownloader
 from tlt import TLT_BASE_DIR
 from tlt.models.image_classification.tf_image_classification_model import TFImageClassificationModel
 from tlt.models.image_classification.tfhub_image_classification_model import TFHubImageClassificationModel
 from tlt.utils.file_utils import read_json_file
 
 
 class KerasImageClassificationModel(TFHubImageClassificationModel):
     """
     Class to represent a Keras.applications pretrained model for image classification
     """
 
-    def __init__(self, model_name: str, **kwargs):
+    def __init__(self, model_name: str, model=None, optimizer=None, loss=None, **kwargs):
         """
         Class constructor
         """
         keras_model_map = read_json_file(os.path.join(
             TLT_BASE_DIR, "models/configs/tf_keras_image_classification_models.json"))
         if model_name not in keras_model_map.keys():
             raise ValueError("The specified Keras image classification model ({}) "
                              "is not supported.".format(model_name))
 
-        TFImageClassificationModel.__init__(self, model_name=model_name, **kwargs)
+        TFImageClassificationModel.__init__(self, model_name=model_name, model=model, optimizer=optimizer,
+                                            loss=loss, **kwargs)
 
-        # placeholder for model definition
-        self._model = None
-        self._num_classes = None
-        self._image_size = keras_model_map[model_name]["image_size"]
+        if self._model is None:
+            self._num_classes = None
+            self._image_size = keras_model_map[model_name]["image_size"]
+
+        # Get the model-specific preprocessor from keras applications
+        preprocessor_name = keras_model_map[model_name]["preprocessor"]
+        if preprocessor_name is not None:
+            self._preprocessor = locate('keras.applications.{}.preprocess_input'.format(preprocessor_name))
 
     def _model_downloader(self, model_name, include_top=False):
         downloader = ModelDownloader(model_name, hub='keras', model_dir=None, weights='imagenet',
                                      include_top=include_top)
         model = downloader.download()
         return model
 
     def _get_hub_model(self, num_classes, extra_layers=None):
 
         if not self._model:
             base_model = self._model_downloader(self._model_name)
             base_model.trainable = False
-
             inputs = tf.keras.Input(shape=(self._image_size, self._image_size, 3))
+
             x = base_model(inputs, training=False)
             x = tf.keras.layers.GlobalAveragePooling2D()(x)
 
             if extra_layers:
                 for layer_size in extra_layers:
                     x = tf.keras.layers.Dense(layer_size, activation='relu')(x)
             if self.dropout_layer_rate is not None:
```

## tlt/models/image_classification/pytorch_image_classification_model.py

```diff
@@ -14,33 +14,34 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
-import copy
 import inspect
 import os
 import time
-import dill
-import yaml
-import subprocess
+import dill  # nosec: B403
+import tempfile
+import shutil
 
 from tqdm import tqdm
 
 import torch
 import intel_extension_for_pytorch as ipex
 
 from tlt.distributed import TLT_DISTRIBUTED_DIR
 from tlt.models.pytorch_model import PyTorchModel
 from tlt.models.image_classification.image_classification_model import ImageClassificationModel
 from tlt.datasets.image_classification.image_classification_dataset import ImageClassificationDataset
 from tlt.datasets.image_classification.pytorch_custom_image_classification_dataset \
     import PyTorchCustomImageClassificationDataset
+from tlt.datasets.image_classification.torchvision_image_classification_dataset \
+    import TorchvisionImageClassificationDataset
 from tlt.utils.file_utils import verify_directory, validate_model_name
 from tlt.utils.types import FrameworkType, UseCaseType
 
 
 class PyTorchImageClassificationModel(ImageClassificationModel, PyTorchModel):
     """
     Class to represent a PyTorch model for image classification
@@ -49,14 +50,17 @@
     def __init__(self, model_name: str, model=None, optimizer=None, loss=None, **kwargs):
         """
         Class constructor
         """
         # PyTorch models generally do not enforce a fixed input shape
         self._image_size = 'variable'
 
+        # Store the dataset type that this model type can use for Intel Neural Compressor
+        self._inc_compatible_dataset = (PyTorchCustomImageClassificationDataset, TorchvisionImageClassificationDataset)
+
         # extra properties that will become configurable in the future
         self._do_fine_tuning = False
         self._dropout_layer_rate = None
         self._device = 'cpu'
         self._lr_scheduler = None
         self._generate_checkpoints = True
 
@@ -237,16 +241,17 @@
                 torch.save({
                     'epoch': epochs,
                     'model_state_dict': self._model.state_dict(),
                     'optimizer_state_dict': self._optimizer.state_dict(),
                     'loss': train_epoch_loss,
                 }, os.path.join(checkpoint_dir, 'checkpoint.pt'))
 
-    def _fit_distributed(self, hostfile, nnodes, nproc_per_node, epochs, batch_size, ipex_optimize):
-        distributed_vision_script = os.path.join(TLT_DISTRIBUTED_DIR, "run_train_pyt.py")
+    def _fit_distributed(self, saved_objects_dir, hostfile, nnodes, nproc_per_node, epochs, batch_size, ipex_optimize):
+        import subprocess  # nosec: B404
+        distributed_vision_script = os.path.join(TLT_DISTRIBUTED_DIR, "pytorch", "run_train_pyt.py")
 
         default_port = '29500'
         default_master_addr = '127.0.0.1'
 
         addresses = []
 
         if hostfile is not None:
@@ -279,14 +284,15 @@
         bash_command += ' --hostfile {}'.format(hostfile)
         bash_command += ' --nnodes {}'.format(nnodes)
         bash_command += ' --nproc_per_node {}'.format(nproc_per_node)
         bash_command += ' {}'.format(distributed_vision_script)
         bash_command += ' --master_addr {}'.format(default_master_addr)
         bash_command += ' --master_port {}'.format(default_port)
         bash_command += ' --backend {}'.format('ccl')
+        bash_command += ' --tlt_saved_objects_dir {}'.format(saved_objects_dir)
         bash_command += ' --use_case {}'.format('image_classification')
         bash_command += ' --epochs {}'.format(epochs)
         bash_command += ' --batch_size {}'.format(batch_size)
         if not ipex_optimize:
             bash_command += ' --disable_ipex'
 
         print(bash_command)
@@ -314,15 +320,15 @@
                     is applied at the end of each epoch.
                 seed (int): Optionally set a seed for reproducibility.
                 ipex_optimize (bool): Use Intel Extension for PyTorch (IPEX). Defaults to True.
                 distributed (bool): Boolean flag to use distributed training. Defaults to False.
                 hostfile (str): Name of the hostfile for distributed training. Defaults to None.
                 nnodes (int): Number of nodes to use for distributed training. Defaults to 1.
                 nproc_per_node (int): Number of processes to spawn per node to use for distributed training. Defaults
-                to 1.
+                    to 1.
 
             Returns:
                 Trained PyTorch model object
         """
         self._check_train_inputs(output_dir, dataset, ImageClassificationDataset, epochs, initial_checkpoints,
                                  distributed, hostfile)
 
@@ -339,17 +345,27 @@
 
         if initial_checkpoints:
             checkpoint = torch.load(initial_checkpoints)
             self._model.load_state_dict(checkpoint['model_state_dict'])
             self._optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
 
         if distributed:
-            self.export_for_distributed(TLT_DISTRIBUTED_DIR, dataset)
-            batch_size = dataset._preprocessed['batch_size']
-            self._fit_distributed(hostfile, nnodes, nproc_per_node, epochs, batch_size, ipex_optimize)
+            try:
+                saved_objects_dir = self.export_for_distributed(
+                    export_dir=os.path.join(output_dir, 'tlt_saved_objects'),
+                    train_data=dataset.train_subset,
+                    val_data=dataset.validation_subset
+                )
+                batch_size = dataset._preprocessed['batch_size']
+                self._fit_distributed(saved_objects_dir, hostfile, nnodes, nproc_per_node, epochs, batch_size,
+                                      ipex_optimize)
+            except Exception as err:
+                print("Error: \'{}\' occured while distributed training".format(err))
+            finally:
+                self.cleanup_saved_objects_for_distributed()
 
         else:
             # Call ipex.optimize
             if ipex_optimize:
                 self._model, self._optimizer = ipex.optimize(self._model, optimizer=self._optimizer)
             self._fit(output_dir, dataset, epochs, do_eval, early_stopping, lr_decay)
 
@@ -421,15 +437,15 @@
                                return the raw output/logits of the last layer of the network, using 'probabilities' will
                                return the output vector after applying a softmax function (so results sum to 1)
 
         Returns:
             List of classes, probability vectors, or raw score vectors
 
         Raises:
-            ValueError if the return_type is not one of 'class', 'probabilities', or 'scores'
+            ValueError: if the return_type is not one of 'class', 'probabilities', or 'scores'
         """
         return_types = ['class', 'probabilities', 'scores']
         if not isinstance(return_type, str) or return_type not in return_types:
             raise ValueError('Invalid return_type ({}). Expected one of {}.'.format(return_type, return_types))
 
         self._model.eval()
         with torch.no_grad():
@@ -460,301 +476,41 @@
             torch.save(model_copy, os.path.join(saved_model_dir, 'model.pt'))
             print("Saved model directory:", saved_model_dir)
 
             return saved_model_dir
         else:
             raise ValueError("Unable to export the model, because it hasn't been trained yet")
 
-    def write_inc_config_file(self, config_file_path, dataset, batch_size, overwrite=False,
-                              resize_interpolation='bicubic', accuracy_criterion_relative=0.01, exit_policy_timeout=0,
-                              exit_policy_max_trials=50, tuning_random_seed=9527,
-                              tuning_workspace=''):
-        """
-        Writes an INC compatible config file to the specified path usings args from the specified dataset and
-        parameters.
-
-        Args:
-            config_file_path (str): Destination path on where to write the .yaml config file.
-            dataset (BaseDataset): A tlt dataset object
-            batch_size (int): Batch size to use for quantization and evaluation
-            overwrite (bool): Specify whether or not to overwrite the config_file_path, if it already exists
-                              (default: False)
-            resize_interpolation (str): Interpolation type. Select from: 'bilinear', 'nearest', 'bicubic'
-                                        (default: bicubic)
-            accuracy_criterion_relative (float): Relative accuracy loss (default: 0.01, which is 1%)
-            exit_policy_timeout (int): Tuning timeout in seconds (default: 0). Tuning processing finishes when the
-                                       timeout or max_trials is reached. A tuning timeout of 0 means that the tuning
-                                       phase stops when the accuracy criterion is met.
-            exit_policy_max_trials (int): Maximum number of tuning trials (default: 50). Tuning processing finishes when
-                                          the timeout or or max_trials is reached.
-            tuning_random_seed (int): Random seed for deterministic tuning (default: 9527).
-            tuning_workspace (dir): Path the INC nc_workspace folder. If the string is empty and the OUTPUT_DIR env var
-                                    is set, that output directory will be used. If the string is empty and the
-                                    OUTPUT_DIR env var is not set, the default INC nc_workspace location will be used.
-        Returns:
-            None
-        Raises:
-            FileExistsError if the config file already exists and overwrite is set to False.
-            ValueError if the parameters are not within the expected values
-            NotImplementedError if the dataset type is not TFCustomImageClassificationDataset.
-        """
-        if os.path.isfile(config_file_path) and not overwrite:
-            raise FileExistsError('A file already exists at: {}. Provide a new file path or set overwrite=True',
-                                  config_file_path)
-
-        # We can setup the a custom dataset to use the ImageFolder dataset option in INC.
-        # They don't have a PyTorch Dataset option, so for now, we only support custom datasets for quantization
-        if dataset is not PyTorchCustomImageClassificationDataset \
-                and type(dataset) != PyTorchCustomImageClassificationDataset:
-            raise NotImplementedError('quantization has only been implemented for PyTorch image classification models '
-                                      'with custom datasets')
-
-        if batch_size and not isinstance(batch_size, int) or batch_size < 1:
-            raise ValueError('Invalid value for batch size ({}). Expected a positive integer.'.format(batch_size))
-
-        if resize_interpolation not in ['bilinear', 'nearest', 'bicubic']:
-            raise ValueError('Invalid value for resize interpolation ({}). Expected one of the following values: '
-                             'bilinear, nearest, bicubic'.format(resize_interpolation))
-
-        if accuracy_criterion_relative and not isinstance(accuracy_criterion_relative, float) or \
-                not (0.0 <= accuracy_criterion_relative <= 1.0):
-            raise ValueError('Invalid value for the accuracy criterion ({}). Expected a float value between 0.0 '
-                             'and 1.0'.format(accuracy_criterion_relative))
-
-        if exit_policy_timeout and not isinstance(exit_policy_timeout, int) or exit_policy_timeout < 0:
-            raise ValueError('Invalid value for the exit policy timeout ({}). Expected a positive integer or 0.'.
-                             format(exit_policy_timeout))
-
-        if exit_policy_max_trials and not isinstance(exit_policy_max_trials, int) or exit_policy_max_trials < 1:
-            raise ValueError('Invalid value for max trials ({}). Expected an integer greater than 0.'.
-                             format(exit_policy_timeout))
-
-        if tuning_random_seed and not isinstance(tuning_random_seed, int) or tuning_random_seed < 0:
-            raise ValueError('Invalid value for tuning random seed ({}). Expected a positive integer.'.
-                             format(tuning_random_seed))
-
-        if not isinstance(tuning_workspace, str):
-            raise ValueError('Invalid value for the nc_workspace directory. Expected a string.')
-
-        # Get the image recognition Intel Neural Compressor template
-        config_template = ImageClassificationModel.get_inc_config_template_dict(self)
-
-        # Collect the different data loaders into a list, so that we can update them all the with the data transforms
-        dataloader_configs = []
-
-        # If tuning_workspace is undefined, use the OUTPUT_DIR, if the env var exists
-        if not tuning_workspace:
-            output_dir_env_var = os.getenv('OUTPUT_DIR', '')
-
-            if output_dir_env_var:
-                tuning_workspace = os.path.join(output_dir_env_var, 'nc_workspace')
-
-        print("tuning_workspace:", tuning_workspace)
-
-        if "quantization" in config_template.keys() and "calibration" in config_template["quantization"].keys() \
-                and "dataloader" in config_template["quantization"]["calibration"].keys():
-            dataloader_configs.append(config_template["quantization"]["calibration"]["dataloader"])
-
-        if "evaluation" in config_template.keys():
-            if "accuracy" in config_template["evaluation"].keys() and \
-                    "dataloader" in config_template["evaluation"]["accuracy"].keys():
-                dataloader_configs.append(config_template["evaluation"]["accuracy"]["dataloader"])
-            if "performance" in config_template["evaluation"].keys() and \
-                    "dataloader" in config_template["evaluation"]["performance"].keys():
-                dataloader_configs.append(config_template["evaluation"]["performance"]["dataloader"])
-
-        transform_config = {
-            "Resize": {
-                "size": self._image_size
-            },
-            "CenterCrop": {
-                "size": self._image_size
-            },
-            "ToTensor": {},
-            "Normalize": {
-                "mean": [0.485, 0.456, 0.406],
-                "std": [0.229, 0.224, 0.225]
-            }
-        }
-
-        del config_template["evaluation"]["accuracy"]["postprocess"]
-
-        config_template["quantization"]["approach"] = "post_training_dynamic_quant"
-
-        # Update the data loader configs
-        for dataloader_config in dataloader_configs:
-            # Set the transform configs for resizing and rescaling
-            dataloader_config["transform"] = copy.deepcopy(transform_config)
-
-            # Update dataset directory for the custom dataset
-            if "dataset" in dataloader_config.keys() and "ImageFolder" in dataloader_config["dataset"].keys():
-                dataloader_config["dataset"]["ImageFolder"]["root"] = dataset.dataset_dir
-
-            dataloader_config["batch_size"] = batch_size
-
-        if "tuning" in config_template.keys():
-            config_template["tuning"]["accuracy_criterion"]["relative"] = accuracy_criterion_relative
-
-            if exit_policy_timeout is None:
-                config_template["tuning"]["exit_policy"].pop('timeout', None)
-            else:
-                config_template["tuning"]["exit_policy"]["timeout"] = exit_policy_timeout
-
-            if exit_policy_max_trials is None:
-                config_template["tuning"]["exit_policy"].pop('max_trials', None)
-            else:
-                config_template["tuning"]["exit_policy"]["max_trials"] = exit_policy_max_trials
-
-            if tuning_random_seed is None:
-                config_template["tuning"].pop('random_seed', None)
-            else:
-                config_template["tuning"]["random_seed"] = tuning_random_seed
-
-            if tuning_workspace:
-                if "workspace" not in config_template["tuning"].keys():
-                    config_template["tuning"]["workspace"] = {}
-
-                config_template["tuning"]["workspace"]["path"] = tuning_workspace
-            else:
-                # No tuning_workspace is defined, so remove it from the config
-                if "workspace" in config_template["tuning"].keys():
-                    config_template["tuning"]["workspace"].pop("path", None)
-
-                    if len(config_template["tuning"]["workspace"].keys()) == 0:
-                        config_template["tuning"].pop("workspace", None)
-
-        # Create the directory where the file will be written, if it doesn't already exist
-        if not os.path.exists(os.path.dirname(config_file_path)):
-            os.makedirs(os.path.dirname(config_file_path))
-
-        # Write the config file
-        with open(config_file_path, "w") as config_file:
-            yaml.dump(config_template, config_file, sort_keys=False)
-
-    def quantize(self, saved_model_dir, output_dir, inc_config_path):
-        """
-        Performs post training quantization using the Intel Neural Compressor on the model from the saved_model_dir
-        using the specified config file. The quantized model is written to the output directory.
-
-        Args:
-            saved_model_dir (str): Source directory for the model to quantize.
-            output_dir (str): Writable output directory to save the quantized model
-            inc_config_path (str): Path to an INC config file (.yaml)
-
-        Returns:
-            None
-
-        Raises:
-            NotADirectoryError if the model is not a directory
-            FileNotFoundError if a model.pt is not found in the model or if the inc_config_path file
-            is not found.
-            FileExistsError if the output_dir already has a model.pt file
-        """
-        # The saved model directory should exist and contain a model.pt file
-        if not os.path.isdir(saved_model_dir):
-            raise NotADirectoryError("The saved model directory ({}) does not exist.".format(saved_model_dir))
-        if not os.path.isfile(os.path.join(saved_model_dir, "model.pt")):
-            raise FileNotFoundError("The saved model directory ({}) should have a model.pt file".format(
-                saved_model_dir))
-
-        # Verify that the config file exists
-        if not os.path.isfile(inc_config_path):
-            raise FileNotFoundError("The config file was not found at: {}".format(inc_config_path))
-
-        if not os.path.exists(output_dir):
-            os.makedirs(output_dir)
-        else:
-            # Verify that the output directory doesn't already have a saved_model.pb file
-            if os.path.exists(os.path.join(output_dir, "model.pt")):
-                raise FileExistsError("A saved model already exists at:", os.path.join(output_dir, "model.pt"))
-
-        from neural_compressor.experimental import Quantization
-        # set_backend API is no longer available in Neural Compressor v2.0
-        # from neural_compressor.experimental.common.model import set_backend
-        # set_backend('pytorch')
-        quantizer = Quantization(inc_config_path)
-        quantizer.model = self._model
-        quantized_model = quantizer.fit()
-
-        # If quantization was successful, save the model
-        if quantized_model:
-            quantized_model.save(output_dir)
-            import subprocess
-            # Change the model filename from best_model.pt to model.pt to match our convention
-            p = subprocess.Popen(["mv", output_dir + "/best_model.pt", output_dir + "/model.pt"],
-                                 stdout=subprocess.PIPE)
-            stdout, stderr = p.communicate()
-
-    def benchmark(self, saved_model_dir, inc_config_path, mode='performance', model_type='fp32'):
+    def export_for_distributed(self, export_dir=None, train_data=None, val_data=None):
         """
-        Use INC to benchmark the specified model for performance or accuracy. You must specify whether the
-        input model is fp32 or int8. IPEX int8 models are not supported yet.
+        Exports the model, optimizer, loss, train data and validation data to the export_dir for distributed
+        script to access. Note that the export_dir must be accessible to all the nodes. For example: NFS shared
+        systems. Note that the export_dir is created using mkdtemp which reults in a unique dir name. For
+        example: "<export_dir_Am83Iw". If the export_dir is None, the default name is "saved_objects"
 
         Args:
-            saved_model_dir (str): Path to the directory where the saved model is located
-            inc_config_path (str): Path to an INC config file (.yaml)
-            mode (str): Performance or accuracy (defaults to performance)
-            model_type (str): Floating point (fp32) or quantized integer (int8) model type
-        Returns:
-            None
-        Raises:
-            NotADirectoryError if the saved_model_dir is not a directory
-            FileNotFoundError if a model.pt is not found in the saved_model_dir or if the inc_config_path file
-            is not found.
-            ValueError if an unexpected mode is provided
-        """
-        # The saved model directory should exist and contain a model.pt file
-        if not os.path.isdir(saved_model_dir):
-            raise NotADirectoryError("The saved model directory ({}) does not exist.".format(saved_model_dir))
-        if not os.path.isfile(os.path.join(saved_model_dir, "model.pt")):
-            raise FileNotFoundError("The saved model directory ({}) should have a model.pt file".format(
-                saved_model_dir))
-
-        # Validate mode
-        if mode not in ['performance', 'accuracy']:
-            raise ValueError("Invalid mode: {}. Expected mode to be 'performance' or 'accuracy'.".format(mode))
-
-        # Verify that the config file exists
-        if not os.path.isfile(inc_config_path):
-            raise FileNotFoundError("The config file was not found at: {}".format(inc_config_path))
-
-        from neural_compressor.experimental import Benchmark, common
-        # set_backend API is no longer available in Neural Compressor v2.0
-        # from neural_compressor.experimental.common.model import set_backend
-        # set_backend('pytorch')
-
-        if model_type == "fp32":
-            evaluator = Benchmark(inc_config_path)
-            evaluator.model = self._model
-            return evaluator(mode)
-        elif model_type == "int8":
-            try:
-                from neural_compressor.utils.pytorch import load
-                evaluator = Benchmark(inc_config_path)
-                evaluator.model = common.Model(load(os.path.join(saved_model_dir, 'model.pt'), self._model))
-                return evaluator(mode)
-            except AssertionError:
-                raise NotImplementedError("This model type is not yet supported by INC benchmarking")
-
-    def export_for_distributed(self, output_dir, dataset):
+            export_dir (str): Directory name to export the model, optimizer, loss, train data and validation
+                data. export_dir must be accessible to all the nodes. For example: NFS shared systems. export_dir
+                is created using mkdtemp which reults in a unique dir name. For example: "<export_dir_Am83Iw".
+                If the export_dir is None, the default name is "saved_objects"
+            train_data (PyTorchDataset): Train dataset
+            val_data (PyTorchDataset): Validation dataset
         """
-        Helper function to export dataset and model objects to disk for distributed job
 
-        Args:
-            output_dir (str): Path to a directory where the dataset and model objects are saved.
-                Default file name for saving the objects is "torch_saved_objects.obj"
-            dataset (ImageClassificationDataset): Dataset object to save. It must be an object of
-                ImageClassificationDataset so that the dataset info, train, test, and validation
-                subsets can be accessed.
-        """
+        temp_dir_prefix = os.path.join(os.environ['HOME'], "saved_objects_") if export_dir is None else export_dir + "_"
+        self._temp_dir = tempfile.mkdtemp(prefix=temp_dir_prefix)
 
         objects_to_save = {
-            "dataset": dataset.dataset,
-            "info": dataset.info,
-            "train_subset": dataset.train_subset,
-            "test_subset": dataset.test_subset,
-            "validation_subset": dataset.validation_subset,
+            "train_data": train_data,
             "model": self._model,
             "optimizer": self._optimizer,
             "loss": self._loss
         }
-        torch.save(objects_to_save, os.path.join(output_dir, "torch_saved_objects.obj"))
+        torch.save(objects_to_save, os.path.join(self._temp_dir, "torch_saved_objects.obj"))
+        return self._temp_dir
+
+    def cleanup_saved_objects_for_distributed(self):
+        try:
+            print('Cleaning saved objects...')
+            shutil.rmtree(self._temp_dir)
+        except OSError as ose:
+            print('Error while cleaning the saved objects: {}'.format(ose))
```

## tlt/models/image_classification/tf_image_classification_model.py

```diff
@@ -14,24 +14,23 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
-import copy
 import inspect
 import os
 import numpy as np
 import tensorflow as tf
-import yaml
 
 from tlt.models.tf_model import TFModel
 from tlt.models.image_classification.image_classification_model import ImageClassificationModel
 from tlt.datasets.image_classification.image_classification_dataset import ImageClassificationDataset
+from tlt.datasets.image_classification.tfds_image_classification_dataset import TFDSImageClassificationDataset
 from tlt.datasets.image_classification.tf_custom_image_classification_dataset import TFCustomImageClassificationDataset
 from tlt.utils.file_utils import verify_directory, validate_model_name
 from tlt.utils.types import FrameworkType, UseCaseType
 from tlt.distributed import TLT_DISTRIBUTED_DIR
 
 
 class TFImageClassificationModel(ImageClassificationModel, TFModel):
@@ -41,14 +40,17 @@
 
     def __init__(self, model_name: str, model=None, optimizer=None, loss=None, **kwargs):
         """
         Class constructor
         """
         self._image_size = None
 
+        # Store the dataset type that this model type can use for Intel Neural Compressor
+        self._inc_compatible_dataset = (TFCustomImageClassificationDataset, TFDSImageClassificationDataset)
+
         # extra properties that will become configurable in the future
         self._do_fine_tuning = False
         self._dropout_layer_rate = None
         self._generate_checkpoints = True
 
         # placeholder for model definition
         self._num_classes = None
@@ -103,21 +105,30 @@
             self._model.load_weights(initial_checkpoints)
 
         class CollectBatchStats(tf.keras.callbacks.Callback):
             def __init__(self):
                 self.batch_losses = []
                 self.batch_acc = []
 
+            def on_epoch_begin(self, epoch, logs=None):
+                self.batch_losses = []
+                self.batch_acc = []
+
             def on_train_batch_begin(self, batch, logs=None):
                 self.model.reset_metrics()
 
             def on_train_batch_end(self, batch, logs=None):
                 self.batch_losses.append(logs['loss'])
                 self.batch_acc.append(logs['acc'])
 
+            def on_epoch_end(self, epoch, logs=None):
+                # Using the average over all batches is also common instead of just the last batch
+                logs['loss'] = self.batch_losses[-1]  # np.mean(self.batch_losses)
+                logs['acc'] = self.batch_acc[-1]  # np.mean(self.batch_acc)
+
         batch_stats_callback = CollectBatchStats()
 
         callbacks = [batch_stats_callback]
 
         if early_stopping:
             stop_early_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
             callbacks.append(stop_early_callback)
@@ -139,25 +150,22 @@
                 factor=0.2,
                 patience=5,
                 verbose=2,
                 mode='auto',
                 cooldown=1,
                 min_lr=0.0000000001))
 
-        if dataset._validation_type == 'shuffle_split':
-            train_dataset = dataset.train_subset
-        else:
-            train_dataset = dataset.dataset
+        train_dataset = dataset.train_subset if dataset.train_subset else dataset.dataset
 
         validation_data = dataset.validation_subset if do_eval else None
 
         return callbacks, train_dataset, validation_data
 
-    def _fit_distributed(self, epochs, shuffle, hostfile, nnodes, nproc_per_node, use_horovod):
-        import subprocess
+    def _fit_distributed(self, saved_objects_dir, epochs, shuffle, hostfile, nnodes, nproc_per_node, use_horovod):
+        import subprocess  # nosec: B404
         distributed_vision_script = os.path.join(TLT_DISTRIBUTED_DIR, 'tensorflow', 'run_train_tf.py')
 
         if use_horovod:
             run_cmd = 'horovodrun'
         else:
             run_cmd = 'mpirun'
 
@@ -191,25 +199,26 @@
             nprocs = nnodes * nproc_per_node
             np_cmd = str(nprocs)
         else:
             raise ValueError("Error: Invalid file \'{}\'".format(hostfile))
         script_cmd = 'python ' + distributed_vision_script
         script_cmd += ' --use_case {}'.format('image_classification')
         script_cmd += ' --epochs {}'.format(epochs)
+        script_cmd += ' --tlt_saved_objects_dir {}'.format(saved_objects_dir)
         if shuffle:
             script_cmd += ' --shuffle'
 
         bash_command = run_cmd.split(' ') + ['-np', np_cmd, '-H', hostfile_cmd] + script_cmd.split(' ')
         print(' '.join(str(e) for e in bash_command))
         subprocess.run(bash_command)
 
     def train(self, dataset: ImageClassificationDataset, output_dir, epochs=1, initial_checkpoints=None,
               do_eval=True, early_stopping=False, lr_decay=True, enable_auto_mixed_precision=None,
               shuffle_files=True, seed=None, distributed=False, hostfile=None, nnodes=1, nproc_per_node=1,
-              **kwargs):
+              callbacks=None, **kwargs):
         """
         Trains the model using the specified image classification dataset. The model is compiled and trained for
         the specified number of epochs. If a path to initial checkpoints is provided, those weights are loaded before
         training.
 
         Args:
             dataset (ImageClassificationDataset): Dataset to use when training the model
@@ -227,60 +236,72 @@
                 It is recommended to enable auto mixed precision training when running on platforms that support
                 bfloat16 (Intel third or fourth generation Xeon processors). If it is enabled on a platform that
                 does not support bfloat16, it can be detrimental to the training performance. If
                 enable_auto_mixed_precision is set to None, auto mixed precision will be automatically enabled when
                 running with Intel fourth generation Xeon processors, and disabled for other platforms.
             shuffle_files (bool): Boolean specifying whether to shuffle the training data before each epoch.
             seed (int): Optionally set a seed for reproducibility.
+            callbacks (list): List of keras.callbacks.Callback instances to apply during training.
 
         Returns:
             History object from the model.fit() call
 
         Raises:
-           FileExistsError if the output directory is a file
-           TypeError if the dataset specified is not an ImageClassificationDataset
-           TypeError if the output_dir parameter is not a string
-           TypeError if the epochs parameter is not a integer
-           TypeError if the initial_checkpoints parameter is not a string
-           RuntimeError if the number of model classes is different from the number of dataset classes
+           FileExistsError: if the output directory is a file
+           TypeError: if the dataset specified is not an ImageClassificationDataset
+           TypeError: if the output_dir parameter is not a string
+           TypeError: if the epochs parameter is not a integer
+           TypeError: if the initial_checkpoints parameter is not a string
+           RuntimeError: if the number of model classes is different from the number of dataset classes
         """
-
         self._check_train_inputs(output_dir, dataset, ImageClassificationDataset, epochs, initial_checkpoints)
 
         dataset_num_classes = len(dataset.class_names)
 
         # Check that the number of classes matches the model outputs
         if dataset_num_classes != self.num_classes:
             raise RuntimeError("The number of model outputs ({}) differs from the number of dataset classes ({})".
                                format(self.num_classes, dataset_num_classes))
 
+        if callbacks and not isinstance(callbacks, list):
+            callbacks = list(callbacks) if isinstance(callbacks, tuple) else [callbacks]
+
+        if callbacks and not all(isinstance(callback, tf.keras.callbacks.Callback) for callback in callbacks):
+            raise TypeError('Callbacks must be tf.keras.callbacks.Callback instances')
+
         self._set_seed(seed)
 
         # Set auto mixed precision
         self.set_auto_mixed_precision(enable_auto_mixed_precision)
 
-        callbacks, train_data, val_data = self._get_train_callbacks(dataset, output_dir, initial_checkpoints, do_eval,
-                                                                    early_stopping, lr_decay)
+        train_callbacks, train_data, val_data = self._get_train_callbacks(dataset, output_dir, initial_checkpoints,
+                                                                          do_eval, early_stopping, lr_decay)
+        if callbacks:
+            train_callbacks += callbacks
 
         if distributed:
             try:
-                self.export_for_distributed(train_data, val_data)
-                self._fit_distributed(epochs, shuffle_files, hostfile, nnodes, nproc_per_node,
+                saved_objects_dir = self.export_for_distributed(
+                    export_dir=os.path.join(output_dir, "tlt_saved_objects"),
+                    train_data=train_data,
+                    val_data=val_data
+                )
+                self._fit_distributed(saved_objects_dir, epochs, shuffle_files, hostfile, nnodes, nproc_per_node,
                                       kwargs.get('use_horovod'))
             except Exception as err:
                 print("Error: \'{}\' occured while distributed training".format(err))
             finally:
                 self.cleanup_saved_objects_for_distributed()
         else:
-            history = self._model.fit(train_data, epochs=epochs, shuffle=shuffle_files, callbacks=callbacks,
+            history = self._model.fit(train_data, epochs=epochs, shuffle=shuffle_files, callbacks=train_callbacks,
                                       validation_data=val_data)
             self._history = history.history
             return self._history
 
-    def evaluate(self, dataset: ImageClassificationDataset, use_test_set=False):
+    def evaluate(self, dataset: ImageClassificationDataset, use_test_set=False, callbacks=None):
         """
         Evaluate the accuracy of the model on a dataset.
 
         If there is a validation subset, evaluation will be done on it (by default) or on the test set
         (by setting use_test_set=True). Otherwise, the entire non-partitioned dataset will be
         used for evaluation.
         """
@@ -290,323 +311,49 @@
             else:
                 raise ValueError("No test subset is defined")
         elif dataset.validation_subset:
             eval_dataset = dataset.validation_subset
         else:
             eval_dataset = dataset.dataset
 
-        return self._model.evaluate(eval_dataset)
+        if callbacks and not isinstance(callbacks, list):
+            callbacks = list(callbacks) if isinstance(callbacks, tuple) else [callbacks]
 
-    def predict(self, input_samples, return_type='class'):
+        if callbacks and not all(isinstance(callback, tf.keras.callbacks.Callback) for callback in callbacks):
+            raise TypeError('Callbacks must be tf.keras.callbacks.Callback instances')
+
+        return self._model.evaluate(eval_dataset, callbacks=callbacks)
+
+    def predict(self, input_samples, return_type='class', callbacks=None):
         """
         Perform feed-forward inference and predict the classes of the input_samples.
 
         Args:
             input_samples (tensor): Input tensor with one or more samples to perform inference on
             return_type (str): Using 'class' will return the highest scoring class (default), using 'scores' will
                                return the raw output/logits of the last layer of the network, using 'probabilities' will
                                return the output vector after applying a softmax function (so results sum to 1)
+            callbacks (list): List of keras.callbacks.Callback instances to apply during predict
 
         Returns:
             List of classes, probability vectors, or raw score vectors
 
         Raises:
-            ValueError if the return_type is not one of 'class', 'probabilities', or 'scores'
+            ValueError: if the return_type is not one of 'class', 'probabilities', or 'scores'
         """
         return_types = ['class', 'probabilities', 'scores']
         if not isinstance(return_type, str) or return_type not in return_types:
             raise ValueError('Invalid return_type ({}). Expected one of {}.'.format(return_type, return_types))
 
-        predictions = self._model.predict(input_samples)
+        if callbacks and not isinstance(callbacks, list):
+            callbacks = list(callbacks) if isinstance(callbacks, tuple) else [callbacks]
+
+        if callbacks and not all(isinstance(callback, tf.keras.callbacks.Callback) for callback in callbacks):
+            raise TypeError('Callbacks must be tf.keras.callbacks.Callback instances')
+
+        predictions = self._model.predict(input_samples, callbacks=callbacks)
         if return_type == 'class':
             return np.argmax(predictions, axis=-1)
         elif return_type == 'probabilities':
             return tf.nn.softmax(predictions)
         else:
             return predictions
-
-    def write_inc_config_file(self, config_file_path, dataset, batch_size, overwrite=False,
-                              resize_interpolation='bicubic', accuracy_criterion_relative=0.01, exit_policy_timeout=0,
-                              exit_policy_max_trials=50, tuning_random_seed=9527,
-                              tuning_workspace=''):
-        """
-        Writes an INC compatible config file to the specified path usings args from the specified dataset and
-        parameters. This is currently only supported for TF custom image classification datasets.
-
-        Args:
-            config_file_path (str): Destination path on where to write the .yaml config file.
-            dataset (BaseDataset): A tlt dataset object
-            batch_size (int): Batch size to use for quantization and evaluation
-            overwrite (bool): Specify whether or not to overwrite the config_file_path, if it already exists
-                              (default: False)
-            resize_interpolation (str): Interpolation type. Select from: 'bilinear', 'nearest', 'bicubic'
-                                        (default: bicubic)
-            accuracy_criterion_relative (float): Relative accuracy loss (default: 0.01, which is 1%)
-            exit_policy_timeout (int): Tuning timeout in seconds (default: 0). Tuning processing finishes when the
-                                       timeout or max_trials is reached. A tuning timeout of 0 means that the tuning
-                                       phase stops when the accuracy criterion is met.
-            exit_policy_max_trials (int): Maximum number of tuning trials (default: 50). Tuning processing finishes when
-                                          the timeout or or max_trials is reached.
-            tuning_random_seed (int): Random seed for deterministic tuning (default: 9527).
-            tuning_workspace (dir): Path the INC nc_workspace folder. If the string is empty and the OUTPUT_DIR env var
-                                    is set, that output directory will be used. If the string is empty and the
-        Returns:
-            None
-
-        Raises:
-            FileExistsError if the config file already exists and overwrite is set to False.
-            ValueError if the parameters are not within the expected values
-            NotImplementedError if the dataset type is not TFCustomImageClassificationDataset.
-        """
-        if os.path.isfile(config_file_path) and not overwrite:
-            raise FileExistsError('A file already exists at: {}. Provide a new file path or set overwrite=True',
-                                  config_file_path)
-
-        # We can setup the a custom dataset to use the ImageFolder dataset option in INC. They don't have a TFDS option,
-        # so for now, we only support custom datasets for quantization
-        if dataset is not TFCustomImageClassificationDataset and type(dataset) != TFCustomImageClassificationDataset:
-            raise NotImplementedError('tlt quantization has only been implemented for TF image classification models '
-                                      'with custom datasets')
-
-        if batch_size and not isinstance(batch_size, int) or batch_size < 1:
-            raise ValueError('Invalid value for batch size ({}). Expected a positive integer.'.format(batch_size))
-
-        if resize_interpolation not in ['bilinear', 'nearest', 'bicubic']:
-            raise ValueError('Invalid value for resize interpolation ({}). Expected one of the following values: '
-                             'bilinear, nearest, bicubic'.format(resize_interpolation))
-
-        if accuracy_criterion_relative and not isinstance(accuracy_criterion_relative, float) or \
-                not (0.0 <= accuracy_criterion_relative <= 1.0):
-            raise ValueError('Invalid value for the accuracy criterion ({}). Expected a float value between 0.0 '
-                             'and 1.0'.format(accuracy_criterion_relative))
-
-        if exit_policy_timeout and not isinstance(exit_policy_timeout, int) or exit_policy_timeout < 0:
-            raise ValueError('Invalid value for the exit policy timeout ({}). Expected a positive integer or 0.'.
-                             format(exit_policy_timeout))
-
-        if exit_policy_max_trials and not isinstance(exit_policy_max_trials, int) or exit_policy_max_trials < 1:
-            raise ValueError('Invalid value for max trials ({}). Expected an integer greater than 0.'.
-                             format(exit_policy_timeout))
-
-        if tuning_random_seed and not isinstance(tuning_random_seed, int) or tuning_random_seed < 0:
-            raise ValueError('Invalid value for tuning random seed ({}). Expected a positive integer.'.
-                             format(tuning_random_seed))
-
-        if not isinstance(tuning_workspace, str):
-            raise ValueError('Invalid value for the nc_workspace directory. Expected a string.')
-
-        # Get the image recognition Intel Neural Compressor template
-        config_template = ImageClassificationModel.get_inc_config_template_dict(self)
-
-        # Collect the different data loaders into a list, so that we can update them all the with the data transforms
-        dataloader_configs = []
-
-        # If tuning_workspace is undefined, use the OUTPUT_DIR, if the env var exists
-        if not tuning_workspace:
-            output_dir_env_var = os.getenv('OUTPUT_DIR', '')
-
-            if output_dir_env_var:
-                tuning_workspace = os.path.join(output_dir_env_var, 'nc_workspace')
-
-        if "quantization" in config_template.keys() and "calibration" in config_template["quantization"].keys() and \
-           "dataloader" in config_template["quantization"]["calibration"].keys():
-            dataloader_configs.append(config_template["quantization"]["calibration"]["dataloader"])
-
-        if "evaluation" in config_template.keys():
-            if "accuracy" in config_template["evaluation"].keys() and \
-               "dataloader" in config_template["evaluation"]["accuracy"].keys():
-                dataloader_configs.append(config_template["evaluation"]["accuracy"]["dataloader"])
-            if "performance" in config_template["evaluation"].keys() and \
-               "dataloader" in config_template["evaluation"]["performance"].keys():
-                dataloader_configs.append(config_template["evaluation"]["performance"]["dataloader"])
-
-        transform_config = {
-            "PaddedCenterCrop": {
-                "size": self.image_size,
-                "crop_padding": 32
-            },
-            "Resize": {
-                "size": self.image_size,
-                "interpolation": resize_interpolation
-            },
-            "Rescale": {}
-        }
-
-        # Update the data loader configs
-        for dataloader_config in dataloader_configs:
-            # Set the transform configs for resizing and rescaling
-            dataloader_config["transform"] = copy.deepcopy(transform_config)
-
-            # Update dataset directory for the custom dataset
-            if "dataset" in dataloader_config.keys() and "ImageFolder" in dataloader_config["dataset"].keys():
-                dataloader_config["dataset"]["ImageFolder"]["root"] = dataset.dataset_dir
-
-            dataloader_config["batch_size"] = batch_size
-
-        if "tuning" in config_template.keys():
-            config_template["tuning"]["accuracy_criterion"]["relative"] = accuracy_criterion_relative
-
-            if exit_policy_timeout is None:
-                config_template["tuning"]["exit_policy"].pop('timeout', None)
-            else:
-                config_template["tuning"]["exit_policy"]["timeout"] = exit_policy_timeout
-
-            if exit_policy_max_trials is None:
-                config_template["tuning"]["exit_policy"].pop('max_trials', None)
-            else:
-                config_template["tuning"]["exit_policy"]["max_trials"] = exit_policy_max_trials
-
-            if tuning_random_seed is None:
-                config_template["tuning"].pop('random_seed', None)
-            else:
-                config_template["tuning"]["random_seed"] = tuning_random_seed
-
-            if tuning_workspace:
-                if "workspace" not in config_template["tuning"].keys():
-                    config_template["tuning"]["workspace"] = {}
-
-                config_template["tuning"]["workspace"]["path"] = tuning_workspace
-            else:
-                # No tuning_workspace is defined, so remove it from the config
-                if "workspace" in config_template["tuning"].keys():
-                    config_template["tuning"]["workspace"].pop("path", None)
-
-                    if len(config_template["tuning"]["workspace"].keys()) == 0:
-                        config_template["tuning"].pop("workspace", None)
-
-        # Create the directory where the file will be written, if it doesn't already exist
-        if not os.path.exists(os.path.dirname(config_file_path)):
-            os.makedirs(os.path.dirname(config_file_path))
-
-        # Write the config file
-        with open(config_file_path, "w") as config_file:
-            yaml.dump(config_template, config_file)
-
-    def optimize_graph(self, saved_model_dir, output_dir):
-        """
-        Performs FP32 graph optimization using the Intel Neural Compressor on the model in the saved_model_dir
-        and writes the inference-optimized model to the output_dir. Graph optimization includes converting
-        variables to constants, removing training-only operations like checkpoint saving, stripping out parts
-        of the graph that are never reached, removing debug operations like CheckNumerics, folding batch
-        normalization ops into the pre-calculated weights, and fusing common operations into unified versions.
-
-        Args:
-            saved_model_dir (str): Source directory for the model to optimize
-            output_dir (str): Writable output directory to save the optimized model
-
-        Returns:
-            None
-
-        Raises:
-            NotADirectoryError if the saved_model_dir is not a directory
-            FileNotFoundError if a saved_model.pb is not found in the saved_model_dir
-            FileExistsError if the output_dir already has a saved_model.pb file
-        """
-        # The saved model directory should exist and contain a saved_model.pb file
-        if not os.path.isdir(saved_model_dir):
-            raise NotADirectoryError("The saved model directory ({}) does not exist.".format(saved_model_dir))
-        if not os.path.isfile(os.path.join(saved_model_dir, "saved_model.pb")):
-            raise FileNotFoundError("The saved model directory ({}) should have a saved_model.pb file".format(
-                saved_model_dir))
-
-        if not os.path.exists(output_dir):
-            os.makedirs(output_dir)
-        else:
-            # Verify that the output directory doesn't already have a saved_model.pb file
-            if os.path.exists(os.path.join(output_dir, "saved_model.pb")):
-                raise FileExistsError("A saved model already exists at:", os.path.join(output_dir, "saved_model.pb"))
-
-        from neural_compressor.experimental import Graph_Optimization
-
-        graph_optimizer = Graph_Optimization()
-        graph_optimizer.model = saved_model_dir
-        optimized_graph = graph_optimizer()
-
-        # If optimization was successful, save the model
-        if optimized_graph:
-            optimized_graph.save(output_dir)
-
-    def quantize(self, saved_model_dir, output_dir, inc_config_path):
-        """
-        Performs post training quantization using the Intel Neural Compressor on the model from the saved_model_dir
-        using the specified config file. The quantized model is written to the output directory
-
-        Args:
-            saved_model_dir (str): Source directory for the model to quantize.
-            output_dir (str): Writable output directory to save the quantized model
-            inc_config_path (str): Path to an INC config file (.yaml)
-
-        Returns:
-            None
-
-        Raises:
-            NotADirectoryError if the saved_model_dir is not a directory
-            FileNotFoundError if a saved_model.pb is not found in the saved_model_dir or if the inc_config_path file
-            is not found.
-            FileExistsError if the output_dir already has a saved_model.pb file
-        """
-        # The saved model directory should exist and contain a saved_model.pb file
-        if not os.path.isdir(saved_model_dir):
-            raise NotADirectoryError("The saved model directory ({}) does not exist.".format(saved_model_dir))
-        if not os.path.isfile(os.path.join(saved_model_dir, "saved_model.pb")):
-            raise FileNotFoundError("The saved model directory ({}) should have a saved_model.pb file".format(
-                saved_model_dir))
-
-        # Verify that the config file exists
-        if not os.path.isfile(inc_config_path):
-            raise FileNotFoundError("The config file was not found at: {}".format(inc_config_path))
-
-        if not os.path.exists(output_dir):
-            os.makedirs(output_dir)
-        else:
-            # Verify that the output directory doesn't already have a saved_model.pb file
-            if os.path.exists(os.path.join(output_dir, "saved_model.pb")):
-                raise FileExistsError("A saved model already exists at:", os.path.join(output_dir, "saved_model.pb"))
-
-        from neural_compressor.experimental import Quantization
-
-        quantizer = Quantization(inc_config_path)
-        quantizer.model = saved_model_dir
-        quantized_model = quantizer.fit()
-
-        # If quantization was successful, save the model
-        if quantized_model:
-            quantized_model.save(output_dir)
-
-    def benchmark(self, saved_model_dir, inc_config_path, mode='performance'):
-        """
-        Use INC to benchmark the specified model for performance or accuracy.
-
-        Args:
-            saved_model_dir (str): Path to the directory where the saved model is located
-            inc_config_path (str): Path to an INC config file (.yaml)
-            mode (str): performance or accuracy (defaults to performance)
-
-        Returns:
-            None
-
-        Raises:
-            NotADirectoryError if the saved_model_dir is not a directory
-            FileNotFoundError if a saved_model.pb is not found in the saved_model_dir or if the inc_config_path file
-            is not found.
-            ValueError if an unexpected mode is provided
-        """
-        # The saved model directory should exist and contain a saved_model.pb file
-        if not os.path.isdir(saved_model_dir):
-            raise NotADirectoryError("The saved model directory ({}) does not exist.".format(saved_model_dir))
-        if not os.path.isfile(os.path.join(saved_model_dir, "saved_model.pb")):
-            raise FileNotFoundError("The saved model directory ({}) should have a saved_model.pb file".format(
-                saved_model_dir))
-
-        # Validate mode
-        if mode not in ['performance', 'accuracy']:
-            raise ValueError("Invalid mode: {}. Expected mode to be 'performance' or 'accuracy'.".format(mode))
-
-        # Verify that the config file exists
-        if not os.path.isfile(inc_config_path):
-            raise FileNotFoundError("The config file was not found at: {}".format(inc_config_path))
-
-        from neural_compressor.experimental import Benchmark
-
-        evaluator = Benchmark(inc_config_path)
-        evaluator.model = saved_model_dir
-        return evaluator(mode)
```

## tlt/models/image_classification/tfhub_image_classification_model.py

```diff
@@ -94,15 +94,15 @@
 
         self._num_classes = num_classes
         return self._model
 
     def train(self, dataset: ImageClassificationDataset, output_dir, epochs=1, initial_checkpoints=None,
               do_eval=True, early_stopping=False, lr_decay=True, enable_auto_mixed_precision=None,
               shuffle_files=True, seed=None, extra_layers=None, distributed=False, hostfile=None,
-              nnodes=1, nproc_per_node=1, **kwargs):
+              nnodes=1, nproc_per_node=1, callbacks=None, **kwargs):
         """
             Trains the model using the specified image classification dataset. The first time training is called, it
             will get the feature extractor layer from TF Hub and add on a dense layer based on the number of classes
             in the specified dataset. The model is compiled and trained for the specified number of epochs. If a
             path to initial checkpoints is provided, those weights are loaded before training.
 
             Args:
@@ -125,25 +125,26 @@
                     running with Intel fourth generation Xeon processors, and disabled for other platforms.
                 shuffle_files (bool): Boolean specifying whether to shuffle the training data before each epoch.
                 seed (int): Optionally set a seed for reproducibility.
                 extra_layers (list[int]): Optionally insert additional dense layers between the base model and output
                     layer. This can help increase accuracy when fine-tuning a TFHub model. The input should be a list of
                     integers representing the number and size of the layers, for example [1024, 512] will insert two
                     dense layers, the first with 1024 neurons and the second with 512 neurons.
+                callbacks (list): List of keras.callbacks.Callback instances to apply during training.
 
             Returns:
                 History object from the model.fit() call
 
             Raises:
-               FileExistsError if the output directory is a file
-               TypeError if the dataset specified is not an ImageClassificationDataset
-               TypeError if the output_dir parameter is not a string
-               TypeError if the epochs parameter is not a integer
-               TypeError if the initial_checkpoints parameter is not a string
-               TypeError if the extra_layers parameter is not a list of integers
+               FileExistsError: if the output directory is a file
+               TypeError: if the dataset specified is not an ImageClassificationDataset
+               TypeError: if the output_dir parameter is not a string
+               TypeError: if the epochs parameter is not a integer
+               TypeError: if the initial_checkpoints parameter is not a string
+               TypeError: if the extra_layers parameter is not a list of integers
         """
 
         self._check_train_inputs(output_dir, dataset, ImageClassificationDataset, epochs, initial_checkpoints)
 
         if extra_layers:
             if not isinstance(extra_layers, list):
                 raise TypeError("The extra_layers parameter must be a list of ints but found {}".format(
@@ -155,35 +156,49 @@
                                         "but found a list containing {}".format(type(layer)))
         dataset_num_classes = len(dataset.class_names)
 
         # If the number of classes doesn't match what was used before, clear out the previous model
         if dataset_num_classes != self.num_classes:
             self._model = None
 
+        if callbacks and not isinstance(callbacks, list):
+            callbacks = list(callbacks) if isinstance(callbacks, tuple) else [callbacks]
+
+        if callbacks and not all(isinstance(callback, tf.keras.callbacks.Callback) for callback in callbacks):
+            raise TypeError('Callbacks must be tf.keras.callbacks.Callback instances')
+
         self._set_seed(seed)
 
         # Set auto mixed precision
         self.set_auto_mixed_precision(enable_auto_mixed_precision)
 
         self._model = self._get_hub_model(dataset_num_classes, extra_layers)
 
-        callbacks, train_data, val_data = self._get_train_callbacks(dataset, output_dir, initial_checkpoints, do_eval,
-                                                                    early_stopping, lr_decay)
+        train_callbacks, train_data, val_data = self._get_train_callbacks(dataset, output_dir, initial_checkpoints,
+                                                                          do_eval, early_stopping, lr_decay)
+
+        if callbacks:
+            train_callbacks += callbacks
 
         if distributed:
-            self.export_for_distributed(train_data, val_data)
-            self._fit_distributed(epochs, shuffle_files, hostfile, nnodes, nproc_per_node, kwargs.get('use_horovod'))
+            saved_objects_dir = self.export_for_distributed(
+                export_dir=os.path.join(output_dir, "tlt_saved_objects"),
+                train_data=train_data,
+                val_data=val_data
+            )
+            self._fit_distributed(saved_objects_dir, epochs, shuffle_files, hostfile, nnodes, nproc_per_node,
+                                  kwargs.get('use_horovod'))
             self.cleanup_saved_objects_for_distributed()
         else:
-            history = self._model.fit(train_data, epochs=epochs, shuffle=shuffle_files, callbacks=callbacks,
+            history = self._model.fit(train_data, epochs=epochs, shuffle=shuffle_files, callbacks=train_callbacks,
                                       validation_data=val_data)
             self._history = history.history
             return self._history
 
-    def evaluate(self, dataset: ImageClassificationDataset, use_test_set=False):
+    def evaluate(self, dataset: ImageClassificationDataset, use_test_set=False, callbacks=None):
         """
         Evaluate the accuracy of the model on a dataset.
 
         If there is a validation subset, evaluation will be done on it (by default) or on the test set
         (by setting use_test_set=True). Otherwise, the entire non-partitioned dataset will be
         used for evaluation.
         """
@@ -193,52 +208,65 @@
             else:
                 raise ValueError("No test subset is defined")
         elif dataset.validation_subset:
             eval_dataset = dataset.validation_subset
         else:
             eval_dataset = dataset.dataset
 
+        if callbacks and not isinstance(callbacks, list):
+            callbacks = list(callbacks) if isinstance(callbacks, tuple) else [callbacks]
+
+        if callbacks and not all(isinstance(callback, tf.keras.callbacks.Callback) for callback in callbacks):
+            raise TypeError('Callbacks must be tf.keras.callbacks.Callback instances')
+
         if self._model is None:
             # The model hasn't been trained yet, use the original ImageNet trained model
             print("The model has not been trained yet, so evaluation is being done using the original model ",
                   "and its classes")
             original_model = self._model_downloader(self._model_name, include_top=True)
             original_model.compile(
                 optimizer=self._optimizer_class(),
                 loss=self._loss,
                 metrics=['acc'])
-            return original_model.evaluate(eval_dataset)
+            return original_model.evaluate(eval_dataset, callbacks=callbacks)
         else:
-            return self._model.evaluate(eval_dataset)
+            return self._model.evaluate(eval_dataset, callbacks=callbacks)
 
-    def predict(self, input_samples, return_type='class'):
+    def predict(self, input_samples, return_type='class', callbacks=None):
         """
         Perform feed-forward inference and predict the classes of the input_samples.
 
         Args:
             input_samples (tensor): Input tensor with one or more samples to perform inference on
             return_type (str): Using 'class' will return the highest scoring class (default), using 'scores' will
                                return the raw output/logits of the last layer of the network, using 'probabilities' will
                                return the output vector after applying a softmax function (so results sum to 1)
+            callbacks (list): List of keras.callbacks.Callback instances to apply during predict
 
         Returns:
             List of classes, probability vectors, or raw score vectors
 
         Raises:
-            ValueError if the return_type is not one of 'class', 'probabilities', or 'scores'
+            ValueError: if the return_type is not one of 'class', 'probabilities', or 'scores'
         """
         return_types = ['class', 'probabilities', 'scores']
         if not isinstance(return_type, str) or return_type not in return_types:
             raise ValueError('Invalid return_type ({}). Expected one of {}.'.format(return_type, return_types))
 
+        if callbacks and not isinstance(callbacks, list):
+            callbacks = list(callbacks) if isinstance(callbacks, tuple) else [callbacks]
+
+        if callbacks and not all(isinstance(callback, tf.keras.callbacks.Callback) for callback in callbacks):
+            raise TypeError('Callbacks must be tf.keras.callbacks.Callback instances')
+
         if self._model is None:
             print("The model has not been trained yet, so predictions are being done using the original model")
             original_model = self._model_downloader(self._model_name, include_top=True)
-            predictions = original_model.predict(input_samples)
+            predictions = original_model.predict(input_samples, callbacks=callbacks)
         else:
-            predictions = self._model.predict(input_samples)
+            predictions = self._model.predict(input_samples, callbacks=callbacks)
         if return_type == 'class':
             return np.argmax(predictions, axis=-1)
         elif return_type == 'probabilities':
             return tf.nn.softmax(predictions)
         else:
             return predictions
```

## tlt/models/image_classification/torchvision_image_classification_model.py

```diff
@@ -22,15 +22,14 @@
 from tqdm import tqdm
 
 import torch
 import intel_extension_for_pytorch as ipex
 
 from downloader.models import ModelDownloader
 from tlt import TLT_BASE_DIR
-from tlt.distributed import TLT_DISTRIBUTED_DIR
 from tlt.models.image_classification.pytorch_image_classification_model import PyTorchImageClassificationModel
 from tlt.datasets.image_classification.image_classification_dataset import ImageClassificationDataset
 from tlt.utils.file_utils import read_json_file
 
 
 class TorchvisionImageClassificationModel(PyTorchImageClassificationModel):
     """
@@ -136,15 +135,15 @@
                     for example [1024, 512] will insert two dense layers, the first with 1024 neurons and the
                     second with 512 neurons.
                 ipex_optimize (bool): Use Intel Extension for PyTorch (IPEX). Defaults to True.
                 distributed (bool): Boolean flag to use distributed training. Defaults to False.
                 hostfile (str): Name of the hostfile for distributed training. Defaults to None.
                 nnodes (int): Number of nodes to use for distributed training. Defaults to 1.
                 nproc_per_node (int): Number of processes to spawn per node to use for distributed training. Defaults
-                to 1.
+                    to 1.
 
             Returns:
                 Trained PyTorch model object
         """
         self._check_train_inputs(output_dir, dataset, ImageClassificationDataset, epochs, initial_checkpoints,
                                  distributed, hostfile)
 
@@ -182,17 +181,27 @@
             self._optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
 
             # Call ipex.optimize now, since we didn't call it from _get_hub_model()
             if ipex_optimize and not distributed:
                 self._model, self._optimizer = ipex.optimize(self._model, optimizer=self._optimizer)
 
         if distributed:
-            self.export_for_distributed(TLT_DISTRIBUTED_DIR, dataset)
-            batch_size = dataset._preprocessed['batch_size']
-            self._fit_distributed(hostfile, nnodes, nproc_per_node, epochs, batch_size, ipex_optimize)
+            try:
+                saved_objects_dir = self.export_for_distributed(
+                    export_dir=os.path.join(output_dir, 'tlt_saved_objects'),
+                    train_data=dataset.train_subset,
+                    val_data=dataset.validation_subset
+                )
+                batch_size = dataset._preprocessed['batch_size']
+                self._fit_distributed(saved_objects_dir, hostfile, nnodes, nproc_per_node, epochs, batch_size,
+                                      ipex_optimize)
+            except Exception as err:
+                print("Error: \'{}\' occured while distributed training".format(err))
+            finally:
+                self.cleanup_saved_objects_for_distributed()
         else:
             self._model.train()
             self._fit(output_dir, dataset, epochs, do_eval, early_stopping, lr_decay)
 
         return self._history
 
     def evaluate(self, dataset: ImageClassificationDataset, use_test_set=False):
@@ -270,15 +279,15 @@
                                return the raw output/logits of the last layer of the network, using 'probabilities' will
                                return the output vector after applying a softmax function (so results sum to 1)
 
         Returns:
             List of classes, probability vectors, or raw score vectors
 
         Raises:
-            ValueError if the return_type is not one of 'class', 'probabilities', or 'scores'
+            ValueError: if the return_type is not one of 'class', 'probabilities', or 'scores'
         """
         return_types = ['class', 'probabilities', 'scores']
         if not isinstance(return_type, str) or return_type not in return_types:
             raise ValueError('Invalid return_type ({}). Expected one of {}.'.format(return_type, return_types))
 
         if self._model is None:
             print("The model has not been trained yet, so predictions are being done using the original model")
```

## tlt/models/text_classification/text_classification_model.py

```diff
@@ -15,69 +15,39 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import abc
-import os
-import yaml
 
 from tlt.models.model import BaseModel
 from tlt.utils.types import FrameworkType, UseCaseType
-from tlt import TLT_BASE_DIR
 
 
 class TextClassificationModel(BaseModel):
     """
     Class to represent a pretrained model for text classification
     """
 
     def __init__(self, model_name: str, framework: FrameworkType, use_case: UseCaseType, dropout_layer_rate: float):
         self._dropout_layer_rate = dropout_layer_rate
         BaseModel.__init__(self, model_name, framework, use_case)
 
         # Default learning rate for text models
         self._learning_rate = 3e-5
-
-    def get_inc_config_template_dict(self):
-        """
-        Returns a dictionary for a config template compatible with the Intel Neural Compressor.
-
-        It loads the yaml file tlt/models/configs/inc/text_classification_template.yaml and then fills in parameters
-        that the model knows about (like framework and model name). There are still more parameters that need to be
-        filled in before using the config with INC (like the dataset information, size, etc).
-        """
-        template_file_path = os.path.join(TLT_BASE_DIR, "models/configs/inc/text_classification_template.yaml")
-
-        if not os.path.exists(template_file_path):
-            raise FileNotFoundError("Unable to find the config template at:", template_file_path)
-
-        with open(template_file_path, 'r') as template_yaml:
-            config_template = yaml.safe_load(template_yaml)
-
-        # Update parameters that we know in the template
-        config_template["model"]["framework"] = str(self.framework)
-        config_template["model"]["name"] = self.model_name
-
-        return config_template
+        self._quantization_approach = 'dynamic'
 
     @property
     @abc.abstractmethod
     def num_classes(self):
+        """
+        The number of output neurons in the model; equal to the number of classes in the dataset
+        """
         pass
 
     @property
     def dropout_layer_rate(self):
+        """
+        The probability of any one node being dropped when a dropout layer is used
+        """
         return self._dropout_layer_rate
-
-    def write_inc_config_file(self, config_file_path, dataset, batch_size, overwrite=False, **kwargs):
-        raise NotImplementedError("Writing INC config files has not be implemented yet for text classification")
-
-    def quantize(self, saved_model_dir, output_dir, inc_config_path):
-        raise NotImplementedError("Post training quantization has not been implemented yet for text classification")
-
-    def optimize_graph(self, saved_model_dir, output_dir):
-        raise NotImplementedError("Optimize graph has not been implemented yet for text classification")
-
-    def benchmark(self, saved_model_dir, inc_config_path, mode='performance'):
-        raise NotImplementedError("Benchmarking has not been implemented yet for text classification")
```

## tlt/models/text_classification/tf_text_classification_model.py

```diff
@@ -22,22 +22,18 @@
 import inspect
 import tensorflow as tf
 
 from tlt.models.tf_model import TFModel
 from tlt.models.text_classification.text_classification_model import TextClassificationModel
 from tlt.datasets.text_classification.text_classification_dataset import TextClassificationDataset
 from tlt.datasets.text_classification.tf_custom_text_classification_dataset import TFCustomTextClassificationDataset
+from tlt.datasets.text_classification.tfds_text_classification_dataset import TFDSTextClassificationDataset
 from tlt.utils.file_utils import verify_directory, validate_model_name
 from tlt.utils.types import FrameworkType, UseCaseType
 from tlt.distributed import TLT_DISTRIBUTED_DIR
-import yaml
-
-# Note that tensorflow_text isn't used directly but the import is required to register ops used by the
-# BERT text preprocessor
-import tensorflow_text  # noqa: F401
 
 
 class TFTextClassificationModel(TextClassificationModel, TFModel):
     """
     Class to represent a TF pretrained model that can be used for binary text classification
     fine tuning.
     """
@@ -52,14 +48,15 @@
         self._model = None
         self._num_classes = None
 
         TFModel.__init__(self, model_name, FrameworkType.TENSORFLOW, UseCaseType.TEXT_CLASSIFICATION)
         TextClassificationModel.__init__(self, model_name, FrameworkType.TENSORFLOW, UseCaseType.TEXT_CLASSIFICATION,
                                          dropout_layer_rate=self._dropout_layer_rate)
 
+        self._inc_compatible_dataset = (TFCustomTextClassificationDataset, TFDSTextClassificationDataset)
         # set up the configurable optimizer and loss functions
         self._check_optimizer_loss(optimizer, loss)
         self._optimizer_class = optimizer if optimizer else tf.keras.optimizers.Adam
         self._opt_args = {k: v for k, v in kwargs.items() if k in inspect.getfullargspec(self._optimizer_class).args}
         self._optimizer = None  # This gets initialized later
         self._loss_class = loss  # This can be None, default function is defined later
         if self._loss_class:
@@ -68,34 +65,37 @@
             self._loss_args = {}
         self._loss = None  # This gets initialized later
 
         if model is None:
             self._model = None
         elif isinstance(model, str):
             self.load_from_directory(model)
-            self._num_classes = self._model.output.shape[-1]
         elif isinstance(model, tf.keras.Model):
             self._model = model
-            self._num_classes = self._model.output.shape[-1]
         else:
             raise TypeError("The model input must be a keras Model, string, or None but found a {}".format(type(model)))
 
+        if self._model:
+            # Get the number of classes based on the shape of the last layer. If the shape is 1, assume 2 classes.
+            self._num_classes = self._model.output.shape[-1] if self._model.output.shape[-1] > 1 else 2
+
     @property
     def num_classes(self):
         return self._num_classes
 
     def _get_train_callbacks(self, dataset, output_dir, initial_checkpoints, do_eval, early_stopping,
                              lr_decay, dataset_num_classes):
         if self._loss_class is None:
             self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=True) if dataset_num_classes == 2 else \
                 tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
         else:
             self._loss = self._loss_class(**self._loss_args)
 
-        metrics = tf.metrics.BinaryAccuracy() if dataset_num_classes == 2 else tf.keras.metrics.CategoricalAccuracy()
+        metrics = tf.metrics.BinaryAccuracy() if dataset_num_classes == 2 else \
+            tf.keras.metrics.SparseCategoricalAccuracy()
 
         self._optimizer = self._optimizer_class(learning_rate=self._learning_rate, epsilon=self._epsilon,
                                                 **self._opt_args)
         self._model.compile(optimizer=self._optimizer, loss=self._loss, metrics=metrics)
 
         if initial_checkpoints:
             if os.path.isdir(initial_checkpoints):
@@ -154,16 +154,17 @@
                 min_lr=0.0000000001))
 
         train_data = dataset.train_subset if dataset.train_subset else dataset.dataset
         validation_data = dataset.validation_subset if do_eval else None
 
         return callbacks, train_data, validation_data
 
-    def _fit_distributed(self, epochs, shuffle, hostfile, nnodes, nproc_per_node, use_horovod):
-        import subprocess
+    def _fit_distributed(self, saved_objects_dir, epochs, shuffle, hostfile, nnodes, nproc_per_node, use_horovod,
+                         hf_bert_tokenizer=None, max_seq_length=None):
+        import subprocess  # nosec: B404
         distributed_vision_script = os.path.join(TLT_DISTRIBUTED_DIR, 'tensorflow', 'run_train_tf.py')
 
         if use_horovod:
             run_cmd = 'horovodrun'
         else:
             run_cmd = 'mpirun'
 
@@ -199,16 +200,21 @@
             np_cmd = str(nprocs)
         else:
             raise ValueError("Error: Invalid file \'{}\'".format(hostfile))
 
         script_cmd = 'python ' + distributed_vision_script
         script_cmd += ' --use_case {}'.format('text_classification')
         script_cmd += ' --epochs {}'.format(epochs)
+        script_cmd += ' --tlt_saved_objects_dir {}'.format(saved_objects_dir)
         if shuffle:
             script_cmd += ' --shuffle'
+        if hf_bert_tokenizer:
+            script_cmd += ' --model_name {}'.format(hf_bert_tokenizer)  # model_name and hf_bert_tokenizer are the same
+        if max_seq_length:
+            script_cmd += ' --max_seq_length {}'.format(max_seq_length)
 
         bash_command = run_cmd.split(' ') + ['-np', np_cmd, '-H', hostfile_cmd] + script_cmd.split(' ')
         print(' '.join(str(e) for e in bash_command))
         subprocess.run(bash_command)
 
     def train(self, dataset: TextClassificationDataset, output_dir, epochs=1, initial_checkpoints=None,
               do_eval=True, early_stopping=False, lr_decay=True, enable_auto_mixed_precision=None,
@@ -242,20 +248,20 @@
                shuffle_files (bool): Boolean specifying whether to shuffle the training data before each epoch.
                seed (int): Optionally set a seed for reproducibility.
 
            Returns:
                History object from the model.fit() call
 
            Raises:
-               FileExistsError if the output directory is a file
-               TypeError if the dataset specified is not a TextClassificationDataset
-               TypeError if the output_dir parameter is not a string
-               TypeError if the epochs parameter is not a integer
-               TypeError if the initial_checkpoints parameter is not a string
-               NotImplementedError if the specified dataset has more than 2 classes
+               FileExistsError: if the output directory is a file
+               TypeError: if the dataset specified is not a TextClassificationDataset
+               TypeError: if the output_dir parameter is not a string
+               TypeError: if the epochs parameter is not a integer
+               TypeError: if the initial_checkpoints parameter is not a string
+               NotImplementedError: if the specified dataset has more than 2 classes
         """
         self._check_train_inputs(output_dir, dataset, TextClassificationDataset, epochs, initial_checkpoints)
 
         dataset_num_classes = len(dataset.class_names)
 
         if dataset_num_classes != 2:
             raise NotImplementedError("Training is only supported for binary text classification. The specified dataset"
@@ -267,16 +273,20 @@
         self.set_auto_mixed_precision(enable_auto_mixed_precision)
 
         callbacks, train_data, val_data = self._get_train_callbacks(dataset, output_dir, initial_checkpoints, do_eval,
                                                                     early_stopping, lr_decay, dataset_num_classes)
 
         if distributed:
             try:
-                self.export_for_distributed(train_data, val_data)
-                self._fit_distributed(epochs, shuffle_files, hostfile, nnodes, nproc_per_node,
+                saved_objects_dir = self.export_for_distributed(
+                    export_dir=os.path.join(output_dir, "tlt_saved_objects"),
+                    train_data=train_data,
+                    val_data=val_data
+                )
+                self._fit_distributed(saved_objects_dir, epochs, shuffle_files, hostfile, nnodes, nproc_per_node,
                                       kwargs.get('use_horovod'))
             except Exception as err:
                 print("Error: \'{}\' occured while distributed training".format(err))
             finally:
                 self.cleanup_saved_objects_for_distributed()
         else:
             history = self._model.fit(train_data, validation_data=val_data, epochs=epochs, shuffle=shuffle_files,
@@ -296,17 +306,17 @@
                use_test_set (bool): Specify if the test partition of the dataset should be used for evaluation.
                                     [default: False)
 
            Returns:
                Dictionary with loss and accuracy metrics
 
            Raises:
-               TypeError if the dataset specified is not a TextClassificationDataset
-               ValueError if the use_test_set=True and no test subset has been defined in the dataset.
-               ValueError if the model has not been trained or loaded yet.
+               TypeError: if the dataset specified is not a TextClassificationDataset
+               ValueError: if the use_test_set=True and no test subset has been defined in the dataset.
+               ValueError: if the model has not been trained or loaded yet.
         """
         if not isinstance(dataset, TextClassificationDataset):
             raise TypeError("The dataset must be a TextClassificationDataset but found a {}".format(type(dataset)))
 
         if use_test_set:
             if dataset.test_subset:
                 eval_dataset = dataset.test_subset
@@ -330,217 +340,18 @@
                input_samples (str, list, numpy array, tensor, tf.data dataset or a generator keras.utils.Sequence):
                     Input samples to use to predict. These will be sent to the tf.keras.Model predict() function.
 
            Returns:
                Numpy array of scores
 
            Raises:
-               ValueError if the model has not been trained or loaded yet.
-               ValueError if there is a mismatch between the input_samples and the model's expected input.
+               ValueError: if the model has not been trained or loaded yet.
+               ValueError: if there is a mismatch between the input_samples and the model's expected input.
         """
         if self._model is None:
             raise ValueError("The model must be trained or loaded before predicting.")
 
         # If a single string is passed in, make it a list so that it's compatible with the keras model predict
         if isinstance(input_samples, str):
             input_samples = [input_samples]
 
         return tf.sigmoid(self._model.predict(input_samples)).numpy()
-
-    def write_inc_config_file(self, config_file_path, dataset, batch_size, overwrite=False,
-                              resize_interpolation='bicubic', accuracy_criterion_relative=0.01, exit_policy_timeout=0,
-                              exit_policy_max_trials=50, tuning_random_seed=9527,
-                              tuning_workspace=''):
-        """
-        Writes an Intel Neural Compressor compatible config file to the specified path usings args from the specified
-        dataset and parameters.
-
-        Args:
-            config_file_path (str): Destination path on where to write the .yaml config file.
-            dataset (BaseDataset): A tlt dataset object
-            batch_size (int): Batch size to use for quantization and evaluation
-            overwrite (bool): Specify whether or not to overwrite the config_file_path, if it already exists
-                              (default: False)
-            resize_interpolation (str): Interpolation type. Select from: 'bilinear', 'nearest', 'bicubic'
-                                        (default: bicubic)
-            accuracy_criterion_relative (float): Relative accuracy loss (default: 0.01, which is 1%)
-            exit_policy_timeout (int): Tuning timeout in seconds (default: 0). Tuning processing finishes when the
-                                       timeout or max_trials is reached. A tuning timeout of 0 means that the tuning
-                                       phase stops when the accuracy criterion is met.
-            exit_policy_max_trials (int): Maximum number of tuning trials (default: 50). Tuning processing finishes when
-                                          the timeout or or max_trials is reached.
-            tuning_random_seed (int): Random seed for deterministic tuning (default: 9527).
-            tuning_workspace (dir): Path the Intel Neural Compressor nc_workspace folder. If the string is empty and the
-                                    OUTPUT_DIR env var is set, that output directory will be used. If the string is
-                                    empty and the OUTPUT_DIR env var is not set, the default Intel Neural Compressor
-                                    nc_workspace location will be used.
-        Returns:
-            None
-        Raises:
-            FileExistsError if the config file already exists and overwrite is set to False.
-            ValueError if the parameters are not within the expected values
-            NotImplementedError if the dataset type is not TFCustomImageClassificationDataset.
-        """
-        if os.path.isfile(config_file_path) and not overwrite:
-            raise FileExistsError('A file already exists at: {}. Provide a new file path or set overwrite=True',
-                                  config_file_path)
-
-        # They don't have a Tensorflow Dataset option, so for now, we only support custom datasets for quantization
-
-        if not isinstance(dataset, TFCustomTextClassificationDataset) or \
-                dataset.__class__ is not TFCustomTextClassificationDataset:
-            raise NotImplementedError('quantization has only been implemented for tensorflow text classification '
-                                      'models with custom datasets')
-
-        if batch_size and not isinstance(batch_size, int) or batch_size < 1:
-            raise ValueError('Invalid value for batch size ({}). Expected a positive integer.'.format(batch_size))
-
-        if resize_interpolation not in ['bilinear', 'nearest', 'bicubic']:
-            raise ValueError('Invalid value for resize interpolation ({}). Expected one of the following values: '
-                             'bilinear, nearest, bicubic'.format(resize_interpolation))
-
-        if accuracy_criterion_relative and not isinstance(accuracy_criterion_relative, float) or \
-                not (0.0 <= accuracy_criterion_relative <= 1.0):
-            raise ValueError('Invalid value for the accuracy criterion ({}). Expected a float value between 0.0 '
-                             'and 1.0'.format(accuracy_criterion_relative))
-
-        if exit_policy_timeout and not isinstance(exit_policy_timeout, int) or exit_policy_timeout < 0:
-            raise ValueError('Invalid value for the exit policy timeout ({}). Expected a positive integer or 0.'.
-                             format(exit_policy_timeout))
-
-        if exit_policy_max_trials and not isinstance(exit_policy_max_trials, int) or exit_policy_max_trials < 1:
-            raise ValueError('Invalid value for max trials ({}). Expected an integer greater than 0.'.
-                             format(exit_policy_timeout))
-
-        if tuning_random_seed and not isinstance(tuning_random_seed, int) or tuning_random_seed < 0:
-            raise ValueError('Invalid value for tuning random seed ({}). Expected a positive integer.'.
-                             format(tuning_random_seed))
-
-        if not isinstance(tuning_workspace, str):
-            raise ValueError('Invalid value for the nc_workspace directory. Expected a string.')
-
-        # Get the Intel Neural Compressor template
-        config_template = TextClassificationModel.get_inc_config_template_dict(self)
-
-        # Collect the different data loaders into a list, so that we can update them all the with the data transforms
-        dataloader_configs = []
-
-        # If tuning_workspace is undefined, use the OUTPUT_DIR, if the env var exists
-        if not tuning_workspace:
-            output_dir_env_var = os.getenv('OUTPUT_DIR', '')
-
-            if output_dir_env_var:
-                tuning_workspace = os.path.join(output_dir_env_var, 'nc_workspace')
-
-        print("tuning_workspace:", tuning_workspace)
-
-        if "quantization" in config_template.keys() and "calibration" in config_template["quantization"].keys() \
-                and "dataloader" in config_template["quantization"]["calibration"].keys():
-            dataloader_configs.append(config_template["quantization"]["calibration"]["dataloader"])
-            print("DATALOADER CONFIGS")
-            print(dataloader_configs)
-
-        if "evaluation" in config_template.keys():
-            if "accuracy" in config_template["evaluation"].keys() and \
-                    "dataloader" in config_template["evaluation"]["accuracy"].keys():
-                dataloader_configs.append(config_template["evaluation"]["accuracy"]["dataloader"])
-            if "performance" in config_template["evaluation"].keys() and \
-                    "dataloader" in config_template["evaluation"]["performance"].keys():
-                dataloader_configs.append(config_template["evaluation"]["performance"]["dataloader"])
-
-        config_template["quantization"]["approach"] = "post_training_dynamic_quant"
-
-        # Update the data loader configs
-        for dataloader_config in dataloader_configs:
-            # Update dataset directory for the custom dataset
-            if "dataset" in dataloader_config.keys() and "bert" in dataloader_config["dataset"].keys():
-                # These cause errors when trying to benchmark
-                dataloader_config["dataset"]["bert"]["root"] = dataset.dataset_dir
-                dataloader_config["dataset"]["bert"]["label_file"] = dataset.dataset_dir
-
-            dataloader_config["batch_size"] = batch_size
-
-        if "tuning" in config_template.keys():
-            config_template["tuning"]["accuracy_criterion"]["relative"] = accuracy_criterion_relative
-
-            if exit_policy_timeout is None:
-                config_template["tuning"]["exit_policy"].pop('timeout', None)
-            else:
-                config_template["tuning"]["exit_policy"]["timeout"] = exit_policy_timeout
-
-            if exit_policy_max_trials is None:
-                config_template["tuning"]["exit_policy"].pop('max_trials', None)
-            else:
-                config_template["tuning"]["exit_policy"]["max_trials"] = exit_policy_max_trials
-
-            if tuning_random_seed is None:
-                config_template["tuning"].pop('random_seed', None)
-            else:
-                config_template["tuning"]["random_seed"] = tuning_random_seed
-
-            if tuning_workspace:
-                if "workspace" not in config_template["tuning"].keys():
-                    config_template["tuning"]["workspace"] = {}
-
-                config_template["tuning"]["workspace"]["path"] = tuning_workspace
-            else:
-                # No tuning_workspace is defined, so remove it from the config
-                if "workspace" in config_template["tuning"].keys():
-                    config_template["tuning"]["workspace"].pop("path", None)
-
-                    if len(config_template["tuning"]["workspace"].keys()) == 0:
-                        config_template["tuning"].pop("workspace", None)
-
-        # Create the directory where the file will be written, if it doesn't already exist
-        if not os.path.exists(os.path.dirname(config_file_path)):
-            os.makedirs(os.path.dirname(config_file_path))
-
-        # Write the config file
-        with open(config_file_path, "w") as config_file:
-            yaml.dump(config_template, config_file, sort_keys=False)
-
-    def quantize(self, saved_model_dir, output_dir, inc_config_path):
-        """
-        Performs post training quantization using the Intel Neural Compressor on the model from the saved_model_dir
-        using the specified config file. The quantized model is written to the output directory.
-
-        Args:
-            saved_model_dir (str): Source directory for the model to quantize.
-            output_dir (str): Writable output directory to save the quantized model
-            inc_config_path (str): Path to an Intel Neural Compressor config file (.yaml)
-
-        Returns:
-            None
-
-        Raises:
-            NotADirectoryError if the model is not a directory
-            FileNotFoundError if a saved_model.pb is not found in the model or if the inc_config_path file
-            is not found.
-            FileExistsError if the output_dir already has a saved_model.pb file
-        """
-        # The saved model directory should exist and contain a saved_model.pb file
-        if not os.path.isdir(saved_model_dir):
-            raise NotADirectoryError("The saved model directory ({}) does not exist.".format(saved_model_dir))
-        if not os.path.isfile(os.path.join(saved_model_dir, "saved_model.pb")):
-            raise FileNotFoundError("The saved model directory ({}) should have a saved_model.pb file".format(
-                saved_model_dir))
-
-        # Verify that the config file exists
-        if not os.path.isfile(inc_config_path):
-            raise FileNotFoundError("The config file was not found at: {}".format(inc_config_path))
-
-        if not os.path.exists(output_dir):
-            os.makedirs(output_dir)
-        else:
-            # Verify that the output directory doesn't already have a saved_model.pb file
-            if os.path.exists(os.path.join(output_dir, "saved_model.pb")):
-                raise FileExistsError("A saved model already exists at:", os.path.join(output_dir, "saved_model.pb"))
-
-        from neural_compressor.experimental import Quantization
-
-        quantizer = Quantization(inc_config_path)
-        quantizer.model = self._model
-        quantized_model = quantizer.fit()
-
-        # If quantization was successful, save the model
-        if quantized_model:
-            quantized_model.save(output_dir)
```

## tlt/models/text_classification/tfhub_text_classification_model.py

```diff
@@ -143,20 +143,20 @@
                     dense layers, the first with 1024 neurons and the second with 512 neurons.
                seed (int): Optionally set a seed for reproducibility.
 
            Returns:
                History object from the model.fit() call
 
            Raises:
-               FileExistsError if the output directory is a file
-               TypeError if the dataset specified is not a TextClassificationDataset
-               TypeError if the output_dir parameter is not a string
-               TypeError if the epochs parameter is not a integer
-               TypeError if the initial_checkpoints parameter is not a string
-               TypeError if the extra_layers parameter is not a list of integers
+               FileExistsError: if the output directory is a file
+               TypeError: if the dataset specified is not a TextClassificationDataset
+               TypeError: if the output_dir parameter is not a string
+               TypeError: if the epochs parameter is not a integer
+               TypeError: if the initial_checkpoints parameter is not a string
+               TypeError: if the extra_layers parameter is not a list of integers
         """
         self._check_train_inputs(output_dir, dataset, TextClassificationDataset, epochs, initial_checkpoints)
 
         if extra_layers:
             if not isinstance(extra_layers, list):
                 raise TypeError("The extra_layers parameter must be a list of ints but found {}".format(
                     type(extra_layers)))
@@ -180,16 +180,21 @@
         self._model = self._get_hub_model(dataset_num_classes, extra_layers)
         print("Num dataset classes: ", dataset_num_classes)
 
         callbacks, train_data, val_data = self._get_train_callbacks(dataset, output_dir, initial_checkpoints, do_eval,
                                                                     early_stopping, lr_decay, dataset_num_classes)
 
         if distributed:
-            self.export_for_distributed(train_data, val_data)
-            self._fit_distributed(epochs, shuffle_files, hostfile, nnodes, nproc_per_node, kwargs.get('use_horovod'))
+            saved_objects_dir = self.export_for_distributed(
+                export_dir=os.path.join(output_dir, "tlt_saved_objects"),
+                train_data=train_data,
+                val_data=val_data
+            )
+            self._fit_distributed(saved_objects_dir, epochs, shuffle_files, hostfile, nnodes, nproc_per_node,
+                                  kwargs.get('use_horovod'))
             self.cleanup_saved_objects_for_distributed()
         else:
             history = self._model.fit(train_data, validation_data=val_data, epochs=epochs, shuffle=shuffle_files,
                                       callbacks=callbacks)
 
             self._history = history.history
 
@@ -205,17 +210,17 @@
                use_test_set (bool): Specify if the test partition of the dataset should be used for evaluation.
                                     [default: False)
 
            Returns:
                Dictionary with loss and accuracy metrics
 
            Raises:
-               TypeError if the dataset specified is not a TextClassificationDataset
-               ValueError if the use_test_set=True and no test subset has been defined in the dataset.
-               ValueError if the model has not been trained or loaded yet.
+               TypeError: if the dataset specified is not a TextClassificationDataset
+               ValueError: if the use_test_set=True and no test subset has been defined in the dataset.
+               ValueError: if the model has not been trained or loaded yet.
         """
         if not isinstance(dataset, TextClassificationDataset):
             raise TypeError("The dataset must be a TextClassificationDataset but found a {}".format(type(dataset)))
 
         if use_test_set:
             if dataset.test_subset:
                 eval_dataset = dataset.test_subset
@@ -239,16 +244,16 @@
                input_samples (str, list, numpy array, tensor, tf.data dataset or a generator keras.utils.Sequence):
                     Input samples to use to predict. These will be sent to the tf.keras.Model predict() function.
 
            Returns:
                Numpy array of scores
 
            Raises:
-               ValueError if the model has not been trained or loaded yet.
-               ValueError if there is a mismatch between the input_samples and the model's expected input.
+               ValueError: if the model has not been trained or loaded yet.
+               ValueError: if there is a mismatch between the input_samples and the model's expected input.
         """
         if self._model is None:
             raise ValueError("The model must be trained or loaded before predicting.")
 
         # If a single string is passed in, make it a list so that it's compatible with the keras model predict
         if isinstance(input_samples, str):
             input_samples = [input_samples]
```

## tlt/tools/cli/commands/benchmark.py

```diff
@@ -15,19 +15,18 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import click
-import datetime
+import inspect
 import os
 import shutil
 import sys
-import tempfile
 
 from tlt.utils.types import FrameworkType
 
 
 @click.command()
 @click.option("--model-dir", "--model_dir",
               required=True,
@@ -35,52 +34,44 @@
               help="Model directory to reload for benchmarking. The model directory should contain a saved_model.pb for"
                    " TensorFlow models or a model.pt file for PyTorch models.")
 @click.option("--dataset-dir", "--dataset_dir",
               required=True,
               type=click.Path(exists=True, file_okay=False),
               help="Dataset directory for a custom dataset. Benchmarking is not supported with dataset catalogs at "
                    "this time.")
-@click.option("--inc-config", "--inc_config",
+@click.option("--dataset-file", "--dataset_file",
               required=False,
-              type=click.Path(exists=True, dir_okay=False),
-              help="Path to a config file (yaml) that will be used to benchmark the model using the Intel Neural "
-                   "Compressor. The INC benchmarking documentation can be found at: "
-                   "https://github.com/intel/neural-compressor/blob/master/docs/benchmark.md "
-                   "If no INC config file is provided, a default config file will be generated.")
-@click.option("--mode",
+              type=str,
+              help="Name of a file in the dataset directory to load. Used for loading a .csv file for text "
+                   "classification fine tuning.")
+@click.option("--delimiter",
               required=False,
-              type=click.Choice(['performance', 'accuracy'], case_sensitive=False),
-              default='performance',
-              show_default=True,
-              help="Specify to benchmark the model's performance or accuracy")
+              type=str,
+              default=",",
+              help="Delimiter used when loading a dataset from a csv file. [default: ,]")
 @click.option("--batch-size", "--batch_size",
               required=False,
               type=click.IntRange(min=1),
               default=32,
               show_default=True,
               help="Batch size used for benchmarking, if an INC config file is not provided. If an INC config file is "
                    "provided, the batch size from the config file will be used.")
 @click.option("--output-dir", "--output_dir",
               required=False,
               type=click.Path(file_okay=False),
               help="A writeable output directory. The output directory will be used as a location to write the INC "
                    "config file, if a config file is not provided. If no output directory is provided, a temporary "
                    "folder will be created and then deleted after benchmarking has completed.")
-def benchmark(model_dir, dataset_dir, inc_config, mode, batch_size, output_dir):
+def benchmark(model_dir, dataset_dir, batch_size, output_dir, dataset_file, delimiter):
     """
     Uses the Intel Neural Compressor to benchmark a trained model
     """
     print("Model directory:", model_dir)
     print("Dataset directory:", dataset_dir)
-    print("Benchmarking mode:", mode)
-
-    if inc_config:
-        print("INC config file:", inc_config)
-    else:
-        print("Batch size:", batch_size)
+    print("Batch size:", batch_size)
 
     if output_dir:
         print("Output directory:", output_dir)
 
     saved_model_path = os.path.join(model_dir, "saved_model.pb")
     pytorch_model_path = os.path.join(model_dir, "model.pt")
     if os.path.isfile(saved_model_path):
@@ -103,30 +94,49 @@
     except Exception as e:
         sys.exit("An error occurred while getting the model: {}\nNote that the model directory is expected to contain "
                  "a previously exported model where the directory structure is <model name>/n/saved_model.pb "
                  "(for TensorFlow) or <model name>/n/model.pt (for PyTorch).".format(str(e)))
 
     try:
         from tlt.datasets import dataset_factory
-        dataset = dataset_factory.load_dataset(dataset_dir, model.use_case, model.framework)
-
-        # Generate a default inc config file, if one was not provided by the user
-        if not inc_config:
-            if not output_dir:
-                output_dir = tempfile.mkdtemp()
-                temp_dir = output_dir
-            now = datetime.datetime.now()
-            dt_str = now.strftime("%y%m%d%H%M%S")
-            inc_config = os.path.join(output_dir, "{}_config_{}.yaml".format(model_name, dt_str))
-            print("Writing INC config file to {}".format(inc_config))
-            model.write_inc_config_file(inc_config, dataset, batch_size, overwrite=True)
+        if str(model.use_case) == "image_classification":
+            dataset = dataset_factory.load_dataset(dataset_dir, model.use_case, model.framework)
+        elif str(model.use_case) == 'text_classification':
+            if not dataset_file:
+                raise ValueError("Loading a text classification dataset requires --dataset-file to specify the "
+                                 "file name of the .csv file to load from the --dataset-dir.")
+            if not delimiter:
+                raise ValueError("Loading a text classification dataset requires --delimiter in order to read the "
+                                 ".csv file from the --dataset-dir. in the correct format")
+
+            dataset = dataset_factory.load_dataset(dataset_dir, model.use_case, model.framework,
+                                                   csv_file_name=dataset_file, delimiter=delimiter)
+        else:
+            sys.exit("ERROR: Benchmarking is currently only implemented for Image Classification "
+                     "and Text Classification models")
+
+        # Preprocess, batch, and split
+        if 'image_size' in inspect.getfullargspec(dataset.preprocess).args:  # For Image classification
+            dataset.preprocess(image_size=model.image_size, batch_size=batch_size)
+        elif 'model_name' in inspect.getfullargspec(dataset.preprocess).args:  # For HF Text classification
+            dataset.preprocess(model_name=model_name, batch_size=batch_size)
+        else:  # For TF Text classification
+            dataset.preprocess(batch_size=batch_size)
+        dataset.shuffle_split()
 
         # Call the benchmarking API
         print("Starting benchmarking", flush=True)
-        model.benchmark(model_dir, inc_config, mode)
+        try:
+            model.benchmark(dataset, saved_model_dir=model_dir)
+        except TypeError:
+            model.load_from_directory(model_dir)
+            model.benchmark(dataset)
+        except AttributeError:
+            model._model = model._get_hub_model(model_name, len(dataset.class_names))
+            model.benchmark(dataset, saved_model_dir=model_dir)
 
     except Exception as e:
         sys.exit("An error occurred during benchmarking: {}".format(str(e)))
     finally:
         # Remove the temp directory, if we created one
         if temp_dir and os.path.exists(temp_dir):
             shutil.rmtree(temp_dir)
```

## tlt/tools/cli/commands/optimize.py

```diff
@@ -64,14 +64,15 @@
     print("Model name:", model_name)
     print("Framework:", framework)
 
     try:
         from tlt.models.model_factory import get_model
 
         model = get_model(model_name, framework)
+        model.load_from_directory(model_dir)
     except Exception as e:
         sys.exit("An error occurred while getting the model: {}\nNote that the model directory is expected to contain "
                  "a previously exported model where the directory structure is <model name>/n/saved_model.pb "
                  "(for TensorFlow).".format(str(e)))
 
     try:
         # Setup a directory for the quantized model
@@ -81,11 +82,11 @@
             optimized_output_dir = os.path.join(optimized_output_dir, "{}".format(
                 len(os.listdir(optimized_output_dir)) + 1))
         else:
             optimized_output_dir = os.path.join(optimized_output_dir, "1")
 
         # Call the graph optimization API
         print("Starting graph optimization", flush=True)
-        model.optimize_graph(model_dir, optimized_output_dir)
+        model.optimize_graph(optimized_output_dir)
 
     except Exception as e:
         sys.exit("An error occurred during graph optimization: {}".format(str(e)))
```

## tlt/tools/cli/commands/quantize.py

```diff
@@ -15,46 +15,57 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import click
-import datetime
+import inspect
 import os
 import sys
 
 from tlt.utils.types import FrameworkType
+from tlt.utils.inc_utils import get_inc_config
 
 
 @click.command()
 @click.option("--model-dir", "--model_dir",
               required=True,
               type=click.Path(exists=True, file_okay=False),
               help="Model directory to reload for quantization. The model directory should contain a saved_model.pb "
                    "for TensorFlow models or a model.pt file for PyTorch models.")
 @click.option("--dataset-dir", "--dataset_dir",
               required=True,
               type=click.Path(exists=True, file_okay=False),
               help="Dataset directory for a custom dataset. Quantization is not supported with dataset catalogs at "
                    "this time.")
-@click.option("--inc-config", "--inc_config",
+@click.option("--dataset-file", "--dataset_file",
               required=False,
-              type=click.Path(exists=True, dir_okay=False),
-              help="Path to a config file (yaml) that will be used to quantize the model using the Intel Neural "
-                   "Compressor. The INC config examples for quantization can be found at: "
-                   "https://github.com/intel/neural-compressor/tree/master/examples. "
-                   "If no INC config file is provided, a default config file will be generated.")
+              type=str,
+              help="Name of a file in the dataset directory to load. Used for loading a .csv file for text "
+                   "classification fine tuning.")
+@click.option("--delimiter",
+              required=False,
+              type=str,
+              default=",",
+              help="Delimiter used when loading a dataset from a csv file. [default: ,]")
 @click.option("--batch-size", "--batch_size",
               required=False,
               type=click.IntRange(min=1),
               default=32,
               show_default=True,
               help="Batch size used during quantization, if an INC config file is not provided. If an INC config file "
                    "is provided, the batch size from the config file will be used.")
+@click.option("--approach",
+              required=False,
+              type=click.Choice(['static', 'dynamic'], case_sensitive=False),
+              default='static',
+              show_default=True,
+              help="Specify to use static or dynamic quantization. Generally, static is recommended for image models "
+                   "and dynamic is recommended for text models.")
 @click.option("--accuracy-criterion", "--accuracy_criterion",
               required=False,
               type=click.FloatRange(min=0, max=1.0),
               default=0.01,
               show_default=True,
               help="Relative accuracy loss to allow (for example, a value of 0.01 allows for a relative accuracy "
                    "loss of 1%), if an INC config file is not provided. If an INC config file is provided, the "
@@ -77,28 +88,27 @@
                    "provided, the number of max trials from the config file will be used. Tuning processing finishes "
                    "when the timeout or max trials is reached.")
 @click.option("--output-dir", "--output_dir",
               required=True,
               type=click.Path(file_okay=False),
               help="A writeable output directory. The output directory will be used as a location to save the "
                    "quantized model, the tuning workspace, and the INC config file, if a config file is not provided.")
-def quantize(model_dir, dataset_dir, inc_config, batch_size, accuracy_criterion, timeout, max_trials, output_dir):
+def quantize(model_dir, dataset_dir, dataset_file, delimiter, batch_size, approach, accuracy_criterion, timeout,
+             max_trials, output_dir):
     """
     Uses the Intel Neural Compressor to perform post-training quantization on a trained model
     """
     print("Model directory:", model_dir)
     print("Dataset directory:", dataset_dir)
 
-    if inc_config:
-        print("INC config file:", inc_config)
-    else:
-        print("Accuracy criterion:", accuracy_criterion)
-        print("Exit policy timeout:", timeout)
-        print("Exit policy max trials:", max_trials)
-        print("Batch size:", batch_size)
+    print("Quantization approach:", approach)
+    print("Accuracy criterion:", accuracy_criterion)
+    print("Exit policy timeout:", timeout)
+    print("Exit policy max trials:", max_trials)
+    print("Batch size:", batch_size)
 
     print("Output directory:", output_dir)
 
     try:
         # Create the output directory, if it doesn't exist
         from tlt.utils.file_utils import verify_directory
         verify_directory(output_dir, require_directory_exists=False)
@@ -121,43 +131,59 @@
     print("Model name:", model_name)
     print("Framework:", framework)
 
     try:
         from tlt.models.model_factory import get_model
 
         model = get_model(model_name, framework)
+        model.load_from_directory(model_dir)
     except Exception as e:
         sys.exit("An error occurred while getting the model: {}\nNote that the model directory is expected to contain "
                  "a previously exported model where the directory structure is <model name>/n/saved_model.pb "
                  "(for TensorFlow) or <model name>/n/model.pt (for PyTorch).".format(str(e)))
 
     try:
-
         from tlt.datasets import dataset_factory
 
-        dataset = dataset_factory.load_dataset(dataset_dir, model.use_case, model.framework)
+        if str(model.use_case) == "image_classification":
+            dataset = dataset_factory.load_dataset(dataset_dir, model.use_case, model.framework)
+        elif str(model.use_case) == "text_classification":
+            if not dataset_file:
+                raise ValueError("Loading a text classification dataset requires --dataset-file to specify the "
+                                 "file name of the .csv file to load from the --dataset-dir.")
+            if not delimiter:
+                raise ValueError("Loading a text classification dataset requires --delimiter in order to read the "
+                                 ".csv file from the --dataset-dir. in the correct format")
+
+            dataset = dataset_factory.load_dataset(dataset_dir, model.use_case, model.framework,
+                                                   csv_file_name=dataset_file, delimiter=delimiter)
+        else:
+            sys.exit("ERROR: Quantization is currently only implemented for Image Classification "
+                     "and Text Classification models")
+
+        # Preprocess, batch, and split
+        if 'image_size' in inspect.getfullargspec(dataset.preprocess).args:  # For Image classification
+            dataset.preprocess(image_size=model.image_size, batch_size=batch_size)
+        elif 'model_name' in inspect.getfullargspec(dataset.preprocess).args:  # For HF Text classification
+            dataset.preprocess(model_name=model_name, batch_size=batch_size)
+        else:  # For TF Text classification
+            dataset.preprocess(batch_size=batch_size)
+        dataset.shuffle_split()
 
-        # Generate a default inc config file, if one was not provided by the user
-        if not inc_config:
-            now = datetime.datetime.now()
-            dt_str = now.strftime("%y%m%d%H%M%S")
-            inc_config = os.path.join(output_dir, "{}_config_{}.yaml".format(model_name, dt_str))
-            model.write_inc_config_file(inc_config, dataset, batch_size=batch_size, overwrite=True,
-                                        exit_policy_timeout=timeout, exit_policy_max_trials=max_trials,
-                                        accuracy_criterion_relative=accuracy_criterion,
-                                        tuning_workspace=os.path.join(output_dir, "nc_workspace"))
+        # Generate a default inc config
+        inc_config = get_inc_config(approach, accuracy_criterion, timeout, max_trials)
 
         # Setup a directory for the quantized model
         quantized_output_dir = os.path.join(output_dir, "quantized", model_name)
         verify_directory(quantized_output_dir)
         if len(os.listdir(quantized_output_dir)) > 0:
             quantized_output_dir = os.path.join(quantized_output_dir, "{}".format(
                 len(os.listdir(quantized_output_dir)) + 1))
         else:
             quantized_output_dir = os.path.join(quantized_output_dir, "1")
 
         # Call the quantization API
         print("Starting post-training quantization", flush=True)
-        model.quantize(model_dir, quantized_output_dir, inc_config)
+        model.quantize(quantized_output_dir, dataset, config=inc_config)
 
     except Exception as e:
         sys.exit("An error occurred during quantization: {}".format(str(e)))
```

## tlt/tools/cli/commands/train.py

```diff
@@ -81,18 +81,20 @@
 @click.option("--add-aug", "--add_aug",
               type=click.Choice(['hvflip', 'hflip', 'vflip', 'rotate', 'zoom']),
               multiple=True,
               default=[],
               help="Choice of data augmentation to be applied during training.")
 @click.option("--ipex_optimize", "--ipex-optimize",
               required=False,
+              type=click.BOOL,
               is_flag=True,
               help="Boolean option to optimize model with Intel Extension for PyTorch.")
 @click.option("--distributed", "-d",
               required=False,
+              type=click.BOOL,
               is_flag=True,
               help="Boolean option to trigger a distributed training job.")
 @click.option("--nnodes",
               required=False,
               default=1,
               type=click.IntRange(min=1),
               help="Number of nodes to run the training job [default: 1]")
@@ -105,22 +107,25 @@
               required=False,
               default=None,
               type=click.Path(exists=True, dir_okay=False),
               help="hostfile with a list of nodes to run distributed training.")
 @click.option("--early-stopping", "--early_stopping",
               type=click.BOOL,
               default=False,
+              is_flag=True,
               help="Enable early stopping if convergence is reached while training (bool)")
 @click.option("--lr-decay", "--lr_decay",
               type=click.BOOL,
               default=False,
+              is_flag=True,
               help="If lr_decay is True and do_eval is True, learning rate decay on the validation loss is applied at "
               "the end of each epoch.")
 @click.option("--use-horovod", "--use_horovod",
               required=False,
+              type=click.BOOL,
               is_flag=True,
               help="Use horovod instead of default MPI")
 def train(framework, model_name, output_dir, dataset_dir, dataset_file, delimiter, class_names, dataset_name,
           dataset_catalog, epochs, init_checkpoints, add_aug, early_stopping, lr_decay, ipex_optimize, distributed,
           nnodes, nproc_per_node, hostfile, use_horovod):
     """
     Trains the model
@@ -189,15 +194,14 @@
 
     from tlt.models import model_factory
     from tlt.datasets import dataset_factory
     # Get the model
     try:
         model = model_factory.get_model(model_name, framework)
     except Exception as e:
-
         sys.exit("Error while getting the model (model name: {}, framework: {}):\n{}".format(
             model_name, framework, str(e)))
     # Get the dataset
     try:
         if not dataset_name and not dataset_catalog:
             if str(model.use_case) == 'text_classification':
                 if not dataset_file:
@@ -218,15 +222,15 @@
         if framework in ['tensorflow', 'pytorch']:
             if 'image_size' in inspect.getfullargspec(dataset.preprocess).args:  # For Image classification
                 dataset.preprocess(image_size=model.image_size, batch_size=32, add_aug=list(add_aug))
             elif 'model_name' in inspect.getfullargspec(dataset.preprocess).args:  # For HF Text classification
                 dataset.preprocess(model_name=model_name, batch_size=32)
             else:  # For TF Text classification
                 dataset.preprocess(batch_size=32)
-            dataset.shuffle_split(0.10, 0.10)
+            dataset.shuffle_split()
     except Exception as e:
         sys.exit("Error while getting the dataset (dataset dir: {}, use case: {}, framework: {}, "
                  "dataset name: {}, dataset_catalog: {}):\n{}".format(dataset_dir, model.use_case, model.framework,
                                                                       dataset_name, dataset_catalog, str(e)))
 
     if ipex_optimize and framework != 'pytorch':
         sys.exit("ipex_optimize is only supported for pytorch training\n")
```

## tlt/utils/file_utils.py

```diff
@@ -14,15 +14,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 import json
 import os
-import urllib.request
+import requests
 import re
 import shutil
 import tarfile
 from zipfile import ZipFile
 
 
 def read_json_file(json_file_path):
@@ -49,16 +49,18 @@
     """
     if not os.path.isdir(destination_directory):
         os.makedirs(destination_directory)
 
     destination_file_path = os.path.join(destination_directory, os.path.basename(download_url))
 
     print("Downloading {} to {}".format(download_url, destination_directory))
-    with urllib.request.urlopen(download_url) as response, open(destination_file_path, 'wb') as out_file:
-        shutil.copyfileobj(response, out_file)
+    response = requests.get(download_url, stream=True, timeout=30)  # Adds a 30 sec timeout for Bandit
+    with open(destination_file_path, 'wb') as out_file:
+        response.raw.decode_content = True
+        shutil.copyfileobj(response.raw, out_file)
 
     return destination_file_path
 
 
 def extract_tar_file(tar_file_path, destination_directory):
     """
     Extracts a tar file on the local file system to the destination directory
```

## tlt/utils/platform_util.py

```diff
@@ -21,15 +21,15 @@
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
 import os  # noqa: F401
 import re
 import platform as system_platform
-import subprocess
+import subprocess  # nosec: B404
 import sys
 
 NUMA_NODES_STR_ = "NUMA node(s)"
 CPU_SOCKETS_STR_ = "Socket(s)"
 CORES_PER_SOCKET_STR_ = "Core(s) per socket"
 THREADS_PER_CORE_STR_ = "Thread(s) per core"
 LOGICAL_CPUS_STR_ = "CPU(s)"
@@ -199,16 +199,16 @@
 
 class PlatformUtil:
     '''
     This module implements a platform utility that exposes functions that
     detects platform information.
     '''
 
-    def __init__(self, args):
-        self.args = args
+    def __init__(self, **kwargs):
+        self.kwargs = kwargs
         self.num_cpu_sockets = 0
         self.num_cores_per_socket = 0
         self.num_threads_per_core = 0
         self.num_logical_cpus = 0
         self.num_numa_nodes = 0
         self.cpu_family = ''
         self.cpu_model = ''
@@ -270,15 +270,15 @@
         """
         cpuset = ""
         cpuset_cpus_file = "/sys/fs/cgroup/cpuset/cpuset.cpus"
         if os.path.exists(cpuset_cpus_file):
             with open(cpuset_cpus_file, "r") as f:
                 cpuset = f.read()
 
-            if hasattr(self.args, "verbose") and self.args.verbose:
+            if 'verbose' in self.kwargs and self.kwargs.get('verbose'):
                 print("cpuset.cpus: {}".format(cpuset))
         return cpuset
 
     def linux_init(self):
         lscpu_cmd = "lscpu"
         try:
             lscpu_output = subprocess.check_output([lscpu_cmd],
@@ -331,16 +331,16 @@
                         core_list_per_node[node] = range_list_for_node
 
         # Try to get the cpuset.cpus info, since lscpu does not know if the cpuset is limited
         cpuset = self._get_cpuset()
 
         if cpuset:
             num_cores_arg = -1
-            if hasattr(self.args, "num_cores"):
-                num_cores_arg = self.args.num_cores
+            if 'num_cores' in self.kwargs:
+                num_cores_arg = self.kwargs.get('num_cores')
             # If the cpuset is the same as the online_cpus_list, then we are using the whole
             # machine, so let's avoid unnecessary complexity and don't bother with the cpuset_cpu list.
             # The cpuset_cpus list will also get populated if the num_cores arg is being specified,
             # since this list will be used to create the numactl args in base_model_init.py
             if (online_cpus_list != "" and online_cpus_list != cpuset) or online_cpus_list == "" or num_cores_arg != -1:
                 self.cpuset_cpus = self._get_list_from_string_ranges(cpuset)
 
@@ -349,16 +349,16 @@
         # numa_cores_per_instance we can't count on numactl being installed otherwise and
         # this list is only used for the numactl multi-instance runs.
         num_physical_cores = self.num_cpu_sockets * self.num_cores_per_socket
         if self.num_numa_nodes > 0:
             cores_per_node = int(num_physical_cores / self.num_numa_nodes)
         else:
             cores_per_node = self.num_cores_per_socket
-        if hasattr(self.args, "numa_cores_per_instance"):
-            if self.num_numa_nodes > 0 and self.args.numa_cores_per_instance is not None:
+        if "numa_cores_per_instance" in self.kwargs:
+            if self.num_numa_nodes > 0 and self.kwargs.get('numa_cores_per_instance') is not None:
                 try:
                     # Get the list of cores
                     cpu_array_command = \
                         "numactl -H | grep 'node [0-9]* cpus:' |" \
                         "sed 's/.*node [0-9]* cpus: *//' | head -{0} |cut -f1-{1} -d' '".format(
                             self.num_numa_nodes, int(cores_per_node))
                     cpu_array = subprocess.Popen(
@@ -371,15 +371,15 @@
 
                     # If we have the cpuset list, cross check that list with our core list and
                     # remove cores that are not part of the cpuset list
                     if self.cpuset_cpus is not None:
                         for socket, core_list in enumerate(self.cpu_core_list):
                             self.cpu_core_list[socket] = [x for x in core_list if int(x) in self.cpuset_cpus]
 
-                    if hasattr(self.args, "verbose") and self.args.verbose:
+                    if 'verbose' in self.kwargs and self.kwargs.get('verbose'):
                         print("Core list: {}".format(self.cpu_core_list), flush=True)
 
                 except Exception as e:
                     print("Warning: An error occured when getting the list of cores using '{}':\n {}".
                           format(cpu_array_command, e))
 
         if self.cpuset_cpus is not None:
@@ -484,7 +484,205 @@
     def numa_nodes(self):
         """
         Return amount of numa nodes available on server.
         :return: amount of numa nodes
         :rtype: int
         """
         return int(self.num_numa_nodes)  # type: ignore
+
+
+class OptimizedPlatformUtil(PlatformUtil):
+    def __init__(
+        self,
+        omp_num_threads: int = None,
+        omp_thread_limit: int = None,
+        kmp_blocktime: int = None,
+        kmp_affinity: str = None,
+        tf_num_intraop_threads: int = None,
+        tf_num_interop_threads: int = None,
+        tf_enable_mkl_native_format: int = None,
+        ld_preload: str = None,
+        force_reset_env_vars: bool = False,
+        **kwargs
+    ):
+        """
+            Initialize the class and set env variables for an optimized platform. The parameters
+            of the class are:
+
+            Args:
+                omp_num_threads (int): This variable sets the corresponding environment variable
+                    OMP_NUM_THREADS for the maximum number of threads to use for OpenMP parallel
+                    regions if no other value is specified in the application. With Hyperthreading
+                    enabled, there are more than one hardware threads for a physical CPU core, but
+                    we recommend to use only onehardware thread for a physical CPU core to avoid
+                    cache miss problems. (Recommended setting for CNN: num physical cores)
+                omp_thread_limit (int): This variable sets the corresponding environment variable
+                    OMP_THREAD_LIMIT which is used to set the maximum number of OpenMP threads to
+                    use in a contention group. Must a positive integer and the value should be
+                    less than or equal to maximum number of hardware threads available on the
+                    system.
+                kmp_blocktime (int): This variable sets the corresponding environment variable
+                    KMP_BLOCKTIME which sets the time, in milliseconds, that a thread should wait,
+                    after completing the execution of a parallel region, before sleeping.
+                    (Recommended setting: 0 for CNN, 1 for non-CNN)
+                kmp_affinity (str): Users can bind OpenMP threads to physical processing units.
+                    KMP_AFFINITY is used to take advantage of this functionality. It restricts
+                    execution of certain threads to a subset of the physical processing units
+                    in a multiprocessor computer. Usage of this variable should be as follows:
+                    kmp_affinity="[<modifier>,...]<type>[,<permute>][,<offset>]" where the values
+                    inside square brackets '[]' are optional. Do not include the square brackets.
+                    More about this can be found here: https://www.intel.com/content/www/us/en/docs/cpp-compiler/developer-guide-reference/2021-8/thread-affinity-interface.html  # noqa: E501
+                    (Recommended setting: "granularity=fine,compact,1,0")
+                tf_num_intraop_threads (int): This runtime setting controls parallelism inside an
+                    operation. For instance, if matrix multiplication or reduction is intended to
+                    be executed in several threads, this variable should be set. TensorFlow will
+                    schedule tasks in a thread pool that contains intra_op_parallelism_threads
+                    threads. Applies to TensorFlow only. (Recommended setting: num physical cores
+                    per socket)
+                tf_num_interop_threads (int): This runtime setting controls parallelism among
+                    independent operations. Since these operations are not relevant to each other,
+                    TensorFlow will try to run them concurrently in the thread pool that contains
+                    inter_op_parallelism_threads threads. Applies to TensorFlow only. (Recommended
+                    setting: num sockets)
+                tf_enable_mkl_native_format (int): Users could enable/disable usage of oneDNN blocked
+                    data format in Tensorflow by TF_ENABLE_MKL_NATIVE_FORMAT environment variable.
+                    Applies to TensorFlow only. (Accepted values: 0 or 1)
+                ld_preload (str): A string of colon separated paths to shared object files to preload
+                force_reset_env_vars (bool): If True, force resets the env variables to use the
+                    given parameter value(s)
+
+        """
+        super().__init__(**kwargs)
+
+        self.omp_num_threads = omp_num_threads
+        self.omp_thread_limit = omp_thread_limit
+        self.kmp_blocktime = kmp_blocktime
+        self.kmp_affinity = kmp_affinity
+        self.tf_num_intraop_threads = tf_num_intraop_threads
+        self.tf_num_interop_threads = tf_num_interop_threads
+        self.tf_enable_mkl_native_format = tf_enable_mkl_native_format
+        self.ld_preload = ld_preload
+        self.force_reset_env_vars = force_reset_env_vars
+
+        self.env_vars_dict = {
+            'OMP_NUM_THREADS': self.omp_num_threads,
+            'OMP_THREAD_LIMIT': self.omp_thread_limit,
+            'KMP_BLOCKTIME': self.kmp_blocktime,
+            'KMP_AFFINITY': self.kmp_affinity,
+            'TF_NUM_INTRAOP_THREADS': self.tf_num_intraop_threads,
+            'TF_NUM_INTEROP_THREADS': self.tf_num_interop_threads,
+            'TF_ENABLE_MKL_NATIVE_FORMAT': self.tf_enable_mkl_native_format,
+            'LD_PRELOAD': self.ld_preload
+        }
+
+        self._validate_args()
+        self._set_env_vars()
+
+    def _set_env_vars(self):
+        verbose_string = ""
+        warning_string = ""
+
+        for env_var_name, env_var_value in self.env_vars_dict.items():
+            if env_var_value is not None:
+                if env_var_name not in os.environ or self.force_reset_env_vars:
+                    os.environ[env_var_name] = str(env_var_value)
+                else:
+                    warning_string += "WARNING: The value for {} has already been set to {}. " \
+                        "Use 'force_reset_env_vars' to reset to your " \
+                        "value.\n".format(env_var_name, os.environ.get(env_var_name))
+                    try:
+                        self.env_vars_dict[env_var_name] = int(os.environ.get(env_var_name))
+                    except ValueError:
+                        self.env_vars_dict[env_var_name] = os.environ.get(env_var_name)
+                verbose_string += "{}: {}\n".format(env_var_name, os.environ.get(env_var_name))
+
+        print(warning_string)
+
+        if 'verbose' in self.kwargs and self.kwargs.get('verbose'):
+            print(verbose_string, flush=True)
+
+    def _validate_args(self):
+
+        if self.omp_num_threads is not None:
+            if not isinstance(self.omp_num_threads, int) or self.omp_num_threads < 0:
+                raise ValueError("omp_num_threads must be a positive integer, but given '{}'. "
+                                 "Recommended setting for CNN: num physical cores per "
+                                 "socket".format(self.omp_num_threads))
+
+            if self.omp_num_threads > self.logical_cores:
+                raise ValueError("Value '{}' out of bounds. omp_num_threads must be less than "
+                                 "or equal to '{}' logical cores. Recommended setting for CNN: "
+                                 "num physical cores per socket".format(self.omp_num_threads,
+                                                                        self.logical_cores))
+
+        if self.omp_thread_limit is not None:
+            if not isinstance(self.omp_thread_limit, int) or self.omp_thread_limit <= 0:
+                raise ValueError(
+                    "omp_thread_limit must be a positive integer, but given '{}'".format(self.omp_thread_limit))
+
+            if not (0 <= self.omp_thread_limit <= self.logical_cores):
+                raise ValueError("Value {} out of bounds. 0 <= omp_thread_limit <= {}".format(self.omp_thread_limit,
+                                                                                              self.logical_cores))
+
+        if self.kmp_blocktime is not None:
+            if not isinstance(self.kmp_blocktime, int) or self.kmp_blocktime < 0:
+                raise ValueError("kmp_blocktime must be a positive integer, but given '{}'."
+                                 "Recommended setting: 0 for CNN, 1 for non-CNN".format(self.kmp_blocktime))
+
+        if self.kmp_affinity:
+            if not isinstance(self.kmp_affinity, str):
+                raise ValueError("kmp_affinity must be a string type, but given '{}'".format(type(self.kmp_affinity)))
+
+            valid_modifiers = ["granularity=fine", "granularity=thread", "granularity=core", "granularity=tile",
+                               "granularity=die", "granularity=node", "granularity=group", "granularity=socket",
+                               "norespect", "noverbose", "nowarnings", "noreset", "respect", "verbose", "warnings",
+                               "reset"]
+            valid_types = ["balanced", "compact", "disabled", "explicit", "none", "scatter"]
+
+            err_message = "Invalid values given for kmp_affinity='{}'.\
+                \n\nSyntax is kmp_affinity='[<modifier>,...]<type>[,<permute>][,<offset>]'\
+                \n\n<modifier> (optional):\t{}\
+                \n<type>:\t{}\
+                \n<permute> (optional): Any positive integer (>=0)\
+                \n<offset> (optional): Any positive integer (>=0)".format(self.kmp_affinity, valid_modifiers,
+                                                                          valid_types)
+
+            values = self.kmp_affinity.split(',')
+            count = 0
+            for value in values:
+                if value not in valid_modifiers + valid_types:
+                    if value.isdigit():
+                        count += 1
+                    else:
+                        raise ValueError(err_message)
+
+                if count > 2:
+                    raise ValueError(err_message)
+
+        if self.tf_num_intraop_threads is not None:
+            if not isinstance(self.tf_num_intraop_threads, int) or self.tf_num_intraop_threads < 0:
+                raise ValueError("tf_num_intraop_threads must be a positive integer, but given '{}'. "
+                                 "Recommended setting: num physical cores per "
+                                 "socket".format(self.tf_num_intraop_threads))
+
+        if self.tf_num_interop_threads is not None:
+            if not isinstance(self.tf_num_interop_threads, int) or self.tf_num_interop_threads < 0:
+                raise ValueError("tf_num_interop_threads must be a positive integer, but given '{}'. "
+                                 "Recommended setting: num sockets".format(self.tf_num_interop_threads))
+
+        if self.tf_enable_mkl_native_format is not None:
+            if not isinstance(self.tf_enable_mkl_native_format, int) or self.tf_enable_mkl_native_format not in [0, 1]:
+                raise ValueError("tf_enable_mkl_native_format must be either 0 or 1, "
+                                 "but given '{}'".format(self.tf_enable_mkl_native_format))
+
+        if self.ld_preload:
+            if not isinstance(self.ld_preload, str):
+                raise ValueError("ld_preload must be of type {}, but given '{}'.".format(str, type(self.ld_preload)))
+
+            paths = self.ld_preload.split(':')
+            for path in paths:
+                if not path.endswith('.so'):
+                    raise ValueError("ld_preload must contain colon separated paths to .so files, "
+                                     "but given '{}'".format(self.ld_preload))
+
+                if not os.path.exists(path):
+                    raise FileNotFoundError("Given file '{}' doesn't exist.".format(path))
```

## Comparing `tlt/models/configs/hf_text_classification_models.json` & `tlt/models/configs/pytorch_hf_text_classification_models.json`

 * *Files identical despite different names*

## Comparing `tlt/models/text_classification/hf_text_classification_model.py` & `tlt/models/text_classification/pytorch_hf_text_classification_model.py`

 * *Files 24% similar despite different names*

```diff
@@ -17,23 +17,23 @@
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
 import inspect
 import os
 import time
-import subprocess
-import dill
+import tempfile
+import shutil
+import dill  # nosec: B403
 import torch
 import numpy as np
 import intel_extension_for_pytorch as ipex
 from requests.adapters import ProxyError
 from tqdm import tqdm
 from torch.utils.data import DataLoader
-import yaml
 
 # Hugging Face imports
 from transformers import (
     AutoTokenizer,
     EvalPrediction,
     TrainingArguments,
     Trainer,
@@ -54,24 +54,24 @@
 from tlt.datasets.text_classification.hf_text_classification_dataset import HFTextClassificationDataset
 from tlt.datasets.text_classification.hf_custom_text_classification_dataset import HFCustomTextClassificationDataset
 
 
 MODEL_CONFIG_DIR = os.path.join(TLT_BASE_DIR, "models/configs")
 
 
-class HFTextClassificationModel(TextClassificationModel, HFModel):
+class PyTorchHFTextClassificationModel(TextClassificationModel, HFModel):
     """
-    Class to represent a Hugging Face pretrained model that can be used for multi-class text classification
+    Class to represent a PyTorch Hugging Face pretrained model that can be used for multi-class text classification
     fine tuning.
     """
 
     def __init__(self, model_name: str, model=None, optimizer=None, loss=None, **kwargs):
 
         hf_model_map = read_json_file(os.path.join(
-            TLT_BASE_DIR, "models/configs/hf_text_classification_models.json"))
+            TLT_BASE_DIR, "models/configs/pytorch_hf_text_classification_models.json"))
 
         # extra properties that will become configurable in the future
         self._model_name = model_name
         self._dropout_layer_rate = 0.1
         self._do_fine_tuning = False
         self._dropout_layer_rate = None
         self._device = 'cpu'
@@ -80,54 +80,67 @@
         self._tokenizer = None
         self._classification_layer = hf_model_map[model_name]["classification_layer"]
 
         TextClassificationModel.__init__(self, model_name, FrameworkType.PYTORCH, UseCaseType.TEXT_CLASSIFICATION,
                                          self._dropout_layer_rate)
         HFModel.__init__(self, model_name, FrameworkType.PYTORCH, UseCaseType.TEXT_CLASSIFICATION)
 
+        # Store the dataset type that this model type can use for Intel Neural Compressor
+        self._inc_compatible_dataset = (HFCustomTextClassificationDataset, HFTextClassificationDataset)
+
         # set up the configurable optimizer and loss functions
         self._check_optimizer_loss(optimizer, loss)
         self._optimizer_class = optimizer if optimizer else torch.optim.AdamW
         self._opt_args = {k: v for k, v in kwargs.items() if k in inspect.getfullargspec(self._optimizer_class).args}
         self._optimizer = None  # This gets initialized later
         self._loss_class = loss if loss else torch.nn.CrossEntropyLoss
         self._loss_args = {k: v for k, v in kwargs.items() if k in inspect.getfullargspec(self._loss_class).args}
         self._loss = self._loss_class(**self._loss_args)
 
         # model definition
-        config_dict = read_json_file(os.path.join(MODEL_CONFIG_DIR, "hf_text_classification_models.json"))
+        config_dict = read_json_file(os.path.join(MODEL_CONFIG_DIR, "pytorch_hf_text_classification_models.json"))
         self.hub_name = config_dict[self._model_name]["hub_name"]
         self._model = None
         self._num_classes = None
         self._trainer = None
         self._history = None
 
-    def export_for_distributed(self, output_dir, dataset):
+    def export_for_distributed(self, export_dir, train_data=None, val_data=None):
         """
-        Helper function to export dataset and model objects to disk for distributed job
+        Exports the model, optimizer, loss, train data and validation data to the export_dir for distributed
+        script to access. Note that the export_dir must be accessible to all the nodes. For example: NFS shared
+        systems. Note that the export_dir is created using mkdtemp which reults in a unique dir name. For
+        example: "<export_dir_Am83Iw". If the export_dir is None, the default name is "saved_objects"
 
         Args:
-            output_dir (str): Path to a directory where the dataset and model objects are saved.
-                Default file name for saving the objects is "hf_saved_objects.obj"
-            dataset (HFTextClassificationDataset): Dataset object to save. It must be an object of
-                HFTextClassificationDataset so that the dataset info, train, test, and validation
-                subsets can be accessed.
+            export_dir (str): Directory name to export the model, optimizer, loss, train data and validation
+                data. export_dir must be accessible to all the nodes. For example: NFS shared systems. export_dir
+                is created using mkdtemp which reults in a unique dir name. For example: "<export_dir_Am83Iw".
+                If the export_dir is None, the default name is "saved_objects"
+            train_data (PyTorchDataset): Train dataset
+            val_data (PyTorchDataset): Validation dataset
         """
+        temp_dir_prefix = os.path.join(os.environ['HOME'], "saved_objects_") if export_dir is None else export_dir + "_"
+        self._temp_dir = tempfile.mkdtemp(prefix=temp_dir_prefix)
 
         objects_to_save = {
-            "dataset": dataset.dataset,
-            "info": dataset.info,
-            "train_subset": dataset.train_subset,
-            "test_subset": dataset.test_subset,
-            "validation_subset": dataset.validation_subset,
+            "train_data": train_data,
             "model": self._model,
             "optimizer": self._optimizer,
             "loss": self._loss
         }
-        torch.save(objects_to_save, os.path.join(output_dir, "hf_saved_objects.obj"))
+        torch.save(objects_to_save, os.path.join(self._temp_dir, "torch_saved_objects.obj"))
+        return self._temp_dir
+
+    def cleanup_saved_objects_for_distributed(self):
+        try:
+            print('Cleaning saved objects...')
+            shutil.rmtree(self._temp_dir)
+        except OSError as ose:
+            print('Error while cleaning the saved objects: {}'.format(ose))
 
     @property
     def num_classes(self):
         """
         The number of output neurons in the model; equal to the number of classes in the dataset
         """
         return self._num_classes
@@ -210,16 +223,17 @@
 
             self._update_history('Loss', train_epoch_loss)
             self._update_history('Acc', train_epoch_acc)
 
             loss_acc_output = f'Loss: {train_epoch_loss:.4f} - Acc: {train_epoch_acc:.4f}'
 
             if do_eval and validation_data_loader is not None:
-                eval_epoch_loss, eval_epoch_acc = self.evaluate(validation_data_loader)
-
+                eval_metrics = self.evaluate(validation_data_loader)
+                eval_epoch_loss = eval_metrics['eval_loss']
+                eval_epoch_acc = eval_metrics['eval_accuracy']
                 self._update_history('Val Loss', eval_epoch_loss)
                 self._update_history('Val Acc', eval_epoch_acc)
 
                 loss_acc_output += f' - Val Loss: {eval_epoch_loss:.4f} - Val Acc: {eval_epoch_acc:.4f}'
 
                 if lr_decay:
                     lr = lr_scheduler.optimizer.param_groups[0]['lr']
@@ -244,14 +258,17 @@
                 last_loss = eval_epoch_loss
 
             print(loss_acc_output)
 
         time_elapsed = time.time() - since
         print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
 
+        self._update_history('train_runtime', round(time_elapsed, 4))
+        self._update_history('train_samples_per_second', round(train_data_length * (epoch + 1) / time_elapsed, 3))
+
         if self._generate_checkpoints:
             valid_model_name = validate_model_name(self.model_name)
             checkpoint_dir = os.path.join(output_dir, "{}_checkpoints".format(valid_model_name))
             verify_directory(checkpoint_dir)
             try:
                 torch.save({
                     'epoch': epochs,
@@ -268,16 +285,18 @@
                 torch.save({
                     'epoch': epochs,
                     'model_state_dict': self._model.state_dict(),
                     'optimizer_state_dict': self._optimizer.state_dict(),
                     'loss': train_epoch_loss,
                 }, os.path.join(checkpoint_dir, 'checkpoint.pt'))
 
-    def _fit_distributed(self, hostfile, nnodes, nproc_per_node, epochs, batch_size, ipex_optimize):
-        distributed_text_script = os.path.join(TLT_DISTRIBUTED_DIR, "run_train_pyt.py")
+    def _fit_distributed(self, saved_objects_dir, hostfile, nnodes, nproc_per_node, epochs, batch_size, ipex_optimize):
+        import subprocess  # nosec: B404
+
+        distributed_text_script = os.path.join(TLT_DISTRIBUTED_DIR, "pytorch", "run_train_pyt.py")
 
         default_port = '29500'
         default_master_addr = '127.0.0.1'
 
         addresses = []
 
         if hostfile is not None:
@@ -310,23 +329,36 @@
         bash_command += ' --hostfile {}'.format(hostfile)
         bash_command += ' --nnodes {}'.format(nnodes)
         bash_command += ' --nproc_per_node {}'.format(nproc_per_node)
         bash_command += ' {}'.format(distributed_text_script)
         bash_command += ' --master_addr {}'.format(default_master_addr)
         bash_command += ' --master_port {}'.format(default_port)
         bash_command += ' --backend {}'.format('ccl')
+        bash_command += ' --tlt_saved_objects_dir {}'.format(saved_objects_dir)
         bash_command += ' --use_case {}'.format('text_classification')
         bash_command += ' --epochs {}'.format(epochs)
         bash_command += ' --batch_size {}'.format(batch_size)
         if not ipex_optimize:
             bash_command += ' --disable_ipex'
 
         print(bash_command)
         subprocess.run(bash_command.split(' '))
 
+    def _get_hub_model(self, model_name, num_classes, force_download=False):
+        downloader = ModelDownloader(model_name, model_dir=None, hub='hugging_face',
+                                     num_labels=num_classes, force_download=force_download)
+        try:
+            model = downloader.download()
+        except ProxyError:
+            print('Max retries reached. Sleeping for 10 sec...')
+            time.sleep(10)
+            model = downloader.download()
+
+        return model
+
     def train(
         self,
         dataset,
         output_dir: str,
         epochs: int = 1,
         initial_checkpoints=None,
         learning_rate: float = 1e-5,
@@ -354,15 +386,15 @@
                 entire non-partitioned dataset will be used.
             output_dir (str): A writeable output directory to write checkpoint files during training
             epochs (int): The number of training epochs [default: 1]
             initial_checkpoints (str): Path to checkpoint weights to load. If the path provided is a directory, the
                 latest checkpoint will be used.
             learning_rate (float): Learning rate for the model to train. Defaults to 1e-5
             do_eval (bool): If do_eval is True and the dataset has a validation subset, the model will be evaluated
-                at the end of each epoch.
+                at the end of each epoch. If the dataset does not have a validation split, the test subset will be used.
             early_stopping (bool): Enable early stopping if convergence is reached while training
                 at the end of each epoch.
             lr_decay (bool): If lr_decay is True and do_eval is True, learning rate decay on the validation loss
                 is applied at the end of each epoch.
             seed (int): Optionally set a seed for reproducibility.
             extra_layers (list[int]): Optionally insert additional dense layers between the base model and output
                 layer. This can help increase accuracy when fine-tuning a PyTorch model.
@@ -377,34 +409,29 @@
             distributed (bool): Boolean flag to use distributed training. Defaults to False.
             hostfile (str): Name of the hostfile for distributed training. Defaults to None.
             nnodes (int): Number of nodes to use for distributed training. Defaults to 1.
             nproc_per_node (int): Number of processes to spawn per node to use for distributed training. Defaults
                 to 1.
 
         Returns:
-            Dictionary containing the model training history
+            If use_trainer=True, a Hugging Face TrainOutput object is returned.
+            If use_trainer=False, a dictionary containing the model training history is returned.
 
         Raises:
-            TypeError if the dataset specified is not a TextClassificationDataset/datasets.arrow_dataset.Dataset
-            ValueError if the given dataset has not been preprocessed yet
+            TypeError: if the dataset specified is not a TextClassificationDataset/datasets.arrow_dataset.Dataset
+            ValueError: if the given dataset has not been preprocessed yet
 
         """
         self._check_train_inputs(output_dir, dataset, TextClassificationDataset,
                                  extra_layers, epochs, distributed, hostfile)
 
         if not self._model:
             self._num_classes = len(dataset.class_names)
-            downloader = ModelDownloader(self.hub_name, model_dir=None, hub='hugging_face',
-                                         num_labels=self._num_classes, force_download=force_download)
-            try:
-                self._model = downloader.download()
-            except ProxyError:
-                print('Max retries reached. Sleeping for 10 sec...')
-                time.sleep(10)
-                self._model = downloader.download()
+            self._model = self._get_hub_model(model_name=self.hub_name, num_classes=self._num_classes,
+                                              force_download=force_download)
 
         if not self._optimizer:
             self._optimizer = self._optimizer_class(self._model.parameters(), lr=self._learning_rate)
 
         self._device = device
         self.train_data_loader = None
         self.validation_data_loader = None
@@ -430,52 +457,75 @@
 
         if seed is not None:
             set_seed(seed)
 
         if use_trainer:
             if distributed:
                 raise ValueError("Distributed training with Trainer is not implemented yet")
+
+            # Get the eval_dataset. We always have to do this, because it seems like even it do_eval=False, the
+            # Trainer will still require an eval_dataset.
+            eval_dataset = None
+            try:
+                eval_dataset = dataset.validation_subset
+            except ValueError:
+                try:
+                    eval_dataset = dataset.test_subset
+                except ValueError:
+                    if do_eval:
+                        print("Warning: The dataset provided does not have a validation or test subset.")
+
             training_args = TrainingArguments(
                 output_dir=output_dir,
                 do_eval=do_eval,
                 do_train=True,
                 no_cuda=True,
                 overwrite_output_dir=True,
                 per_device_train_batch_size=dataset.info['preprocessing_info']['batch_size'],
+                per_device_eval_batch_size=dataset.info['preprocessing_info']['batch_size'],
                 evaluation_strategy="epoch",
                 num_train_epochs=epochs,
-                max_steps=75,
+                learning_rate=learning_rate,
+                data_seed=seed,
+                use_ipex=ipex_optimize
             )
 
+            if seed is not None:
+                training_args.seed = seed
+
             def compute_metrics(p: EvalPrediction):
                 preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
                 preds = np.argmax(preds, axis=1)
                 return {"accuracy": (preds == p.label_ids).astype(np.float32).mean().item()}
 
             # Initialize our Trainer
             self._tokenizer = dataset._tokenizer
             self._trainer = Trainer(
                 model=self._model,
                 args=training_args,
                 train_dataset=dataset.train_subset,
-                eval_dataset=dataset.validation_subset,
+                eval_dataset=eval_dataset,
                 compute_metrics=compute_metrics,
                 tokenizer=self._tokenizer
             )
 
-            self._trainer.train()
-            if do_eval:
-                self._history = self._trainer.evaluate()
-                print("Val Acc: {:.5f}".format(self._history.get("eval_accuracy")))
+            self._history = self._trainer.train()
         elif distributed:
-            self.export_for_distributed(
-                output_dir=TLT_DISTRIBUTED_DIR, dataset=dataset
-            )
-            self._fit_distributed(hostfile, nnodes, nproc_per_node, epochs, dataset._preprocessed["batch_size"],
-                                  ipex_optimize)
+            try:
+                saved_objects_dir = self.export_for_distributed(
+                    export_dir=os.path.join(output_dir, 'tlt_saved_objects'),
+                    train_data=dataset.train_subset,
+                    val_data=dataset.validation_subset
+                )
+                self._fit_distributed(saved_objects_dir, hostfile, nnodes, nproc_per_node, epochs,
+                                      dataset._preprocessed["batch_size"], ipex_optimize)
+            except Exception as err:
+                print("Error: \'{}\' occured while distributed training".format(err))
+            finally:
+                self.cleanup_saved_objects_for_distributed()
         else:
             self._trainer = None
             self._model.train()
             if ipex_optimize:
                 self._model, self._optimizer = ipex.optimize(self._model, optimizer=self._optimizer)
             # Call the _fit method to train the model with native PyTorch API
             self._fit(output_dir, dataset, epochs, do_eval, early_stopping, lr_decay)
@@ -488,25 +538,23 @@
            train the model, it evaluates on the 'eval_dataset' given in the Trainer arguments
 
            Args:
                dataset_or_dataloader (datasets.arrow_dataset.Dataset/DataLoader/TextClassificationDataset): The
                     dataset/dataloader to use for evaluation.
 
            Returns:
-               Tuple with loss and accuracy metrics
+               Dictionary with loss, accuracy, runtime, and samples per second metrics
 
            Raises:
-               TypeError if the dataset specified is not a datasets.arrow_dataset.Dataset (or) a
+               TypeError: if the dataset specified is not a datasets.arrow_dataset.Dataset (or) a
                     TextClassificationDataset (or) a DataLoader
         """
         if self._trainer:
             results = self._trainer.evaluate()
-            validation_loss = None
-            validation_accuracy = results.get("eval_accuracy")
-            print("Val Acc: {:.5f}".format(validation_accuracy))
+            print("Val Acc: {:.5f}".format(results.get("eval_accuracy")))
         else:
             if isinstance(dataset_or_dataloader, Dataset):
                 dataloader = DataLoader(dataset_or_dataloader, batch_size=16)
                 validation_data_length = len(dataset_or_dataloader)
             elif isinstance(dataset_or_dataloader, DataLoader):
                 dataloader = dataset_or_dataloader
                 validation_data_length = len(dataloader) * dataloader.batch_size
@@ -516,46 +564,56 @@
                 validation_data_length = len(dataset_or_dataloader)
             else:
                 raise TypeError("Invalid dataset/dataloader: {}".format(dataset_or_dataloader))
 
             if not self._model:
                 # The model hasn't been trained yet, use the original transformers model
                 self._num_classes = len(dataset_or_dataloader.class_names)
-                downloader = ModelDownloader(self.hub_name, hub='hugging_face', model_dir=None,
-                                             num_labels=self._num_classes)
-                self._model = downloader.download()
+                self._model = self._get_hub_model(self.hub_name, self._num_classes)
 
             # Do the evaluation
             device = torch.device(self._device)
             self._model = self._model.to(device)
 
             self._model.eval()
             running_loss = 0.0
             running_corrects = 0
 
+            start = time.time()
+
             for data_batch in tqdm(dataloader, bar_format='{l_bar}{bar:50}{r_bar}{bar:-50b}'):
                 inputs = {k: v.to(device) for k, v in data_batch.items()
                           if k in ['input_ids', 'token_type_ids', 'attention_mask']}
                 labels = data_batch['label'].to(device)
 
                 outputs = self._model(**inputs)
                 predictions = torch.argmax(outputs.logits, dim=-1)
                 loss = self._loss(outputs.logits, labels)
 
                 # Statistics
                 running_loss += loss.item()
                 running_corrects += torch.sum(predictions == labels).item()
 
+            time_elapsed = time.time() - start
+            samples_per_second = validation_data_length / time_elapsed
+
             if validation_data_length == 0:
                 validation_loss, validation_accuracy = 0.0, 0.0
             else:
                 validation_loss = running_loss / validation_data_length
                 validation_accuracy = running_corrects / validation_data_length
 
-        return (validation_loss, validation_accuracy)
+            results = {
+                'eval_loss': validation_loss,
+                'eval_accuracy': validation_accuracy,
+                'eval_runtime': round(time_elapsed, 4),
+                'eval_samples_per_second': round(samples_per_second, 3)
+            }
+
+        return results
 
     def predict(self, input_samples, return_raw=False):
         """
            Generates predictions for the specified input samples.
 
            Args:
                input_samples (str, list, encoded dict, TextClassificationDataset):
@@ -564,15 +622,15 @@
                     Option to return the HF SequenceClassifierOutput object containing the
                     logits Torch Tensor, if set to True.
 
            Returns:
                Torch Tensor of scores or HF SequenceClassifierOutput if return_raw is set to True.
 
            Raises:
-               NotImplementedError if the given input_samples is of type DataLoader
+               NotImplementedError: if the given input_samples is of type DataLoader
         """
         encoded_input = None
 
         # If 'input_samples' is a single text string or a list of text strings
         if isinstance(input_samples, str) or isinstance(input_samples, list):
             encoded_input = self._tokenizer(input_samples, padding=True, return_tensors='pt')
         # If 'input_samples' is an encoded input dict
@@ -622,239 +680,35 @@
             if os.path.exists(saved_model_dir) and len(os.listdir(saved_model_dir)):
                 saved_model_dir = os.path.join(saved_model_dir, "{}".format(len(os.listdir(saved_model_dir)) + 1))
             else:
                 saved_model_dir = os.path.join(saved_model_dir, "1")
             verify_directory(saved_model_dir)
             # If we have a distributed model, save only the encapsulated model
             # (it was wrapped in PyTorch DistributedDataParallel or DataParallel)
-            model_copy = dill.dumps(self._model.module if hasattr(self._model, 'module') else self._model)
+            model_copy = dill.dumps(self._model.module if hasattr(self._model, 'module') else self._model)  # noqa: E501, nosec: B301
             torch.save(model_copy, os.path.join(saved_model_dir, 'model.pt'))
             print("Saved model directory:", saved_model_dir)
 
             return saved_model_dir
         else:
             raise ValueError("Unable to export the model, because it hasn't been trained yet")
 
-    def load_from_directory(self, model_dir: str, num_classes: int):
+    def load_from_directory(self, model_dir: str):
         """
         Loads a saved pytorch model from the given model_dir directory
 
         Args:
             model_dir(str): Path to the saved model directory
-            num_classes(int): Number of class labels
         """
 
         verify_directory(model_dir, require_directory_exists=True)
         model_copy = torch.load(os.path.join(model_dir, 'model.pt'))
-        self._model = dill.loads(model_copy)
+        self._model = dill.loads(model_copy)  # nosec: B301
         self._optimizer = self._optimizer_class(self._model.parameters(), lr=self._learning_rate)
 
-    def write_inc_config_file(self, config_file_path, dataset, batch_size, overwrite=False,
-                              resize_interpolation='bicubic', accuracy_criterion_relative=0.01, exit_policy_timeout=0,
-                              exit_policy_max_trials=50, tuning_random_seed=9527,
-                              tuning_workspace=''):
-        """
-        Writes an INC compatible config file to the specified path usings args from the specified dataset and
-        parameters.
-
-        Args:
-            config_file_path (str): Destination path on where to write the .yaml config file.
-            dataset (BaseDataset): A tlt dataset object
-            batch_size (int): Batch size to use for quantization and evaluation
-            overwrite (bool): Specify whether or not to overwrite the config_file_path, if it already exists
-                              (default: False)
-            resize_interpolation (str): Interpolation type. Select from: 'bilinear', 'nearest', 'bicubic'
-                                        (default: bicubic)
-            accuracy_criterion_relative (float): Relative accuracy loss (default: 0.01, which is 1%)
-            exit_policy_timeout (int): Tuning timeout in seconds (default: 0). Tuning processing finishes when the
-                                       timeout or max_trials is reached. A tuning timeout of 0 means that the tuning
-                                       phase stops when the accuracy criterion is met.
-            exit_policy_max_trials (int): Maximum number of tuning trials (default: 50). Tuning processing finishes when
-                                          the timeout or or max_trials is reached.
-            tuning_random_seed (int): Random seed for deterministic tuning (default: 9527).
-            tuning_workspace (dir): Path the INC nc_workspace folder. If the string is empty and the OUTPUT_DIR env var
-                                    is set, that output directory will be used. If the string is empty and the
-                                    OUTPUT_DIR env var is not set, the default INC nc_workspace location will be used.
-        Returns:
-            None
-        Raises:
-            FileExistsError if the config file already exists and overwrite is set to False.
-            ValueError if the parameters are not within the expected values
-            NotImplementedError if the dataset type is not HFCustomImageClassificationDataset.
-        """
-        if os.path.isfile(config_file_path) and not overwrite:
-            raise FileExistsError('A file already exists at: {}. Provide a new file path or set overwrite=True',
-                                  config_file_path)
-
-        # They don't have a PyTorch Dataset option, so for now, we only support custom datasets for quantization
-
-        if not isinstance(dataset, HFCustomTextClassificationDataset) or \
-                dataset.__class__ is not HFCustomTextClassificationDataset:
-            raise NotImplementedError('quantization has only been implemented for huggingface text classification '
-                                      'models with custom datasets')
-
-        if batch_size and not isinstance(batch_size, int) or batch_size < 1:
-            raise ValueError('Invalid value for batch size ({}). Expected a positive integer.'.format(batch_size))
-
-        if resize_interpolation not in ['bilinear', 'nearest', 'bicubic']:
-            raise ValueError('Invalid value for resize interpolation ({}). Expected one of the following values: '
-                             'bilinear, nearest, bicubic'.format(resize_interpolation))
-
-        if accuracy_criterion_relative and not isinstance(accuracy_criterion_relative, float) or \
-                not (0.0 <= accuracy_criterion_relative <= 1.0):
-            raise ValueError('Invalid value for the accuracy criterion ({}). Expected a float value between 0.0 '
-                             'and 1.0'.format(accuracy_criterion_relative))
-
-        if exit_policy_timeout and not isinstance(exit_policy_timeout, int) or exit_policy_timeout < 0:
-            raise ValueError('Invalid value for the exit policy timeout ({}). Expected a positive integer or 0.'.
-                             format(exit_policy_timeout))
-
-        if exit_policy_max_trials and not isinstance(exit_policy_max_trials, int) or exit_policy_max_trials < 1:
-            raise ValueError('Invalid value for max trials ({}). Expected an integer greater than 0.'.
-                             format(exit_policy_timeout))
-
-        if tuning_random_seed and not isinstance(tuning_random_seed, int) or tuning_random_seed < 0:
-            raise ValueError('Invalid value for tuning random seed ({}). Expected a positive integer.'.
-                             format(tuning_random_seed))
-
-        if not isinstance(tuning_workspace, str):
-            raise ValueError('Invalid value for the nc_workspace directory. Expected a string.')
-
-        # Get the Intel Neural Compressor template
-        config_template = TextClassificationModel.get_inc_config_template_dict(self)
-
-        # Collect the different data loaders into a list, so that we can update them all the with the data transforms
-        dataloader_configs = []
-
-        # If tuning_workspace is undefined, use the OUTPUT_DIR, if the env var exists
-        if not tuning_workspace:
-            output_dir_env_var = os.getenv('OUTPUT_DIR', '')
-
-            if output_dir_env_var:
-                tuning_workspace = os.path.join(output_dir_env_var, 'nc_workspace')
-
-        print("tuning_workspace:", tuning_workspace)
-
-        if "quantization" in config_template.keys() and "calibration" in config_template["quantization"].keys() \
-                and "dataloader" in config_template["quantization"]["calibration"].keys():
-            dataloader_configs.append(config_template["quantization"]["calibration"]["dataloader"])
-            print("DATALOADER CONFIGS")
-            print(dataloader_configs)
-
-        if "evaluation" in config_template.keys():
-            if "accuracy" in config_template["evaluation"].keys() and \
-                    "dataloader" in config_template["evaluation"]["accuracy"].keys():
-                dataloader_configs.append(config_template["evaluation"]["accuracy"]["dataloader"])
-            if "performance" in config_template["evaluation"].keys() and \
-                    "dataloader" in config_template["evaluation"]["performance"].keys():
-                dataloader_configs.append(config_template["evaluation"]["performance"]["dataloader"])
-
-        config_template["quantization"]["approach"] = "post_training_dynamic_quant"
-
-        # Update the data loader configs
-        for dataloader_config in dataloader_configs:
-            # Update dataset directory for the custom dataset
-            if "dataset" in dataloader_config.keys() and "bert" in dataloader_config["dataset"].keys():
-                # These cause errors when trying to benchmark
-                dataloader_config["dataset"]["bert"]["root"] = dataset.dataset_dir
-                dataloader_config["dataset"]["bert"]["label_file"] = dataset.dataset_dir
-
-            dataloader_config["batch_size"] = batch_size
-
-        if "tuning" in config_template.keys():
-            config_template["tuning"]["accuracy_criterion"]["relative"] = accuracy_criterion_relative
-
-            if exit_policy_timeout is None:
-                config_template["tuning"]["exit_policy"].pop('timeout', None)
-            else:
-                config_template["tuning"]["exit_policy"]["timeout"] = exit_policy_timeout
-
-            if exit_policy_max_trials is None:
-                config_template["tuning"]["exit_policy"].pop('max_trials', None)
-            else:
-                config_template["tuning"]["exit_policy"]["max_trials"] = exit_policy_max_trials
-
-            if tuning_random_seed is None:
-                config_template["tuning"].pop('random_seed', None)
-            else:
-                config_template["tuning"]["random_seed"] = tuning_random_seed
-
-            if tuning_workspace:
-                if "workspace" not in config_template["tuning"].keys():
-                    config_template["tuning"]["workspace"] = {}
-
-                config_template["tuning"]["workspace"]["path"] = tuning_workspace
-            else:
-                # No tuning_workspace is defined, so remove it from the config
-                if "workspace" in config_template["tuning"].keys():
-                    config_template["tuning"]["workspace"].pop("path", None)
-
-                    if len(config_template["tuning"]["workspace"].keys()) == 0:
-                        config_template["tuning"].pop("workspace", None)
-
-        # Create the directory where the file will be written, if it doesn't already exist
-        if not os.path.exists(os.path.dirname(config_file_path)):
-            os.makedirs(os.path.dirname(config_file_path))
-
-        # Write the config file
-        with open(config_file_path, "w") as config_file:
-            yaml.dump(config_template, config_file, sort_keys=False)
-
-    def quantize(self, saved_model_dir, output_dir, inc_config_path):
-        """
-        Performs post training quantization using the Intel Neural Compressor on the model from the saved_model_dir
-        using the specified config file. The quantized model is written to the output directory.
-
-        Args:
-            saved_model_dir (str): Source directory for the model to quantize.
-            output_dir (str): Writable output directory to save the quantized model
-            inc_config_path (str): Path to an INC config file (.yaml)
-
-        Returns:
-            None
-
-        Raises:
-            NotADirectoryError if the model is not a directory
-            FileNotFoundError if a model.pt is not found in the model or if the inc_config_path file
-            is not found.
-            FileExistsError if the output_dir already has a model.pt file
-        """
-        # The saved model directory should exist and contain a model.pt file
-        if not os.path.isdir(saved_model_dir):
-            raise NotADirectoryError("The saved model directory ({}) does not exist.".format(saved_model_dir))
-        if not os.path.isfile(os.path.join(saved_model_dir, "model.pt")):
-            raise FileNotFoundError("The saved model directory ({}) should have a model.pt file".format(
-                saved_model_dir))
-
-        # Verify that the config file exists
-        if not os.path.isfile(inc_config_path):
-            raise FileNotFoundError("The config file was not found at: {}".format(inc_config_path))
-
-        if not os.path.exists(output_dir):
-            os.makedirs(output_dir)
-        else:
-            # Verify that the output directory doesn't already have a saved_model.pb file
-            if os.path.exists(os.path.join(output_dir, "model.pt")):
-                raise FileExistsError("A saved model already exists at:", os.path.join(output_dir, "model.pt"))
-
-        from neural_compressor.experimental import Quantization
-
-        quantizer = Quantization(inc_config_path)
-        quantizer.model = self._model
-        quantized_model = quantizer.fit()
-
-        # If quantization was successful, save the model
-        if quantized_model:
-            quantized_model.save(output_dir)
-            import subprocess
-            # Change the model filename from best_model.pt to model.pt to match our convention
-            p = subprocess.Popen(["mv", output_dir + "/best_model.pt", output_dir + "/model.pt"],
-                                 stdout=subprocess.PIPE)
-            stdout, stderr = p.communicate()
-
     def list_layers(self, verbose=False):
         """
         Lists all of the named modules (e.g. features, avgpool, classifier) and layers
         (ReLU, MaxPool2d, Dropout, Linear, etc) in a given PyTorch model
 
         Args:
             verbose (bool): True/False option set by default to be False, displays only high-level modules
```

## Comparing `intel_transfer_learning_tool-0.4.0.dist-info/LICENSE` & `intel_transfer_learning_tool-0.5.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `intel_transfer_learning_tool-0.4.0.dist-info/RECORD` & `intel_transfer_learning_tool-0.5.0.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,136 +1,133 @@
 downloader/__init__.py,sha256=Lo-ZPXpN6ISxExmj-Sp9PeWqAE2t6ICWQIG53B5I0hs,723
-downloader/datasets.py,sha256=uC8qXKnZ091xj3FHLs8zyj6L9nKsI1CstkGV8qlCrRk,5236
-downloader/models.py,sha256=IUrZPjPtOpFRirwD9IU5UOdgwK6U5WrIn98-MHX9sew,4478
-downloader/types.py,sha256=gpw6PEzRKl9YXiX7TS9FJAC6KXcDRt5klIOvZDHqyV4,2982
-downloader/utils.py,sha256=1acba3mbrPzCDYyDNVJj6hMWn6Vvict3s_BGQRhsf6U,2600
+downloader/datasets.py,sha256=zbkYJlqPqb91xd88iGuTwUnn5wKoAYV9JZNObgO4k0U,5618
+downloader/models.py,sha256=nVeWvGEFQ0f7G0EzKIPz2JumA_BaLORPyYwPb3SCCnA,4911
+downloader/types.py,sha256=LeqjMArYaJGyRh-_0-NZ0a4312GzuciM2hSPjU7_Xy0,3213
+downloader/utils.py,sha256=BlAt-VmSXPiIIUKibKZMZw4eJlCUz85uluuaYAYJcMY,2615
 downloader/tests/__init__.py,sha256=QDQiPRTiUDZPTSjMysdbsBpic-cFNMuUsIYYx4uc370,674
-downloader/tests/test_dataset_download.py,sha256=HitwMZ42LfV9NRaL2quAQiqhz4ZC78k9fSiY_J2FGYQ,4444
-downloader/tests/test_model_download.py,sha256=rJWbrtuMBwuouuRPb2XfxnHDsrTcgRmmoc4Ctu9Ffo4,2910
+downloader/tests/test_dataset_download.py,sha256=blK7I6Vdld-CDv-XrLSPfNVi4Ad5ddn4ZvbF5EX-t1k,4725
+downloader/tests/test_model_download.py,sha256=0SXMtdf-O9S8cbeVYaTYHJxaxmV0-3ZT88GcW23QBmA,3631
 tests/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
 tests/conftest.py,sha256=9ixyBf8gRpBETaxYGp-cKWO0pkaBotf6EO49kGHOQtA,2891
 tests/pytorch_tests/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
-tests/pytorch_tests/test_image_anomaly_detection.py,sha256=9Z2jPGErxWrgljoKz3LTLeRj0F41rHRotUk-oATjGFc,9919
-tests/pytorch_tests/test_image_classification.py,sha256=vE8635SzqITzUk83wUpLl9owdtFhlf6lBGDd1I7q_6E,15505
-tests/pytorch_tests/test_text_classification.py,sha256=oLNSYZtcKLln2zFZcMjuYN35VcsoM4CS6KApcvZPCZo,11830
+tests/pytorch_tests/test_image_anomaly_detection.py,sha256=h0o3ySqjtyk5V5l45MOYYMITsWMF-CeCLL9nhXja6q8,10026
+tests/pytorch_tests/test_image_classification.py,sha256=9hSPzi6xsB26qgsf0PxZO3XmUiXqTsN7eFwEraEkvXE,15476
+tests/pytorch_tests/test_text_classification.py,sha256=2CP7lzkPafnZBU39ryoUUg_ecvmwgZIZfbokbfzocMc,11921
 tests/pytorch_tests/unit/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
-tests/pytorch_tests/unit/test_datasets.py,sha256=x212ECxT-uGoXZCg_BFCgZFvHMhaI_1_gQb2XwnnuR0,29095
-tests/pytorch_tests/unit/test_inc.py,sha256=jtQdO5HqUkJMKM0MZqdr_JLjrK61-ixKwzItJNscLSY,21420
-tests/pytorch_tests/unit/test_models.py,sha256=AolAyGG8gHtJzNW2Mxg2WKwoLaoyc_yR_cfXdRLcTDk,14730
+tests/pytorch_tests/unit/test_datasets.py,sha256=i5fsl88fhpbQNml2xfwBPodSCrh6lkeNVT8jMfVt15s,28829
+tests/pytorch_tests/unit/test_inc.py,sha256=zhUooEzvinKGt0hjp9ikReJlqDrbCC-r5wwhrQ7tcik,8471
+tests/pytorch_tests/unit/test_models.py,sha256=NEMjSB-hnA4GNMIif9AadMCKDS496gGpRMnJLeSjyRM,18751
 tests/tensorflow_tests/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
-tests/tensorflow_tests/test_image_classification.py,sha256=7GCLCX8w19Hg70qGf7EMWJIawieX54po-D20b2cFArk,18416
-tests/tensorflow_tests/test_text_classification.py,sha256=Y4L0DbrQu8aXtK0Btp2PDzNaS-KHGi0WWTT4RRu9cRg,10952
+tests/tensorflow_tests/test_image_classification.py,sha256=vfajRgK4tylPI9WLH--At0KWIaqKDEvs08blJcrQGEM,23067
+tests/tensorflow_tests/test_text_classification.py,sha256=-6b93-A7ze1jx1kOgWJLdd9NnDsBOerWCYrEv3LKbAc,15761
 tests/tensorflow_tests/unit/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
-tests/tensorflow_tests/unit/test_datasets.py,sha256=rCvk7O99X3pTaMVVsrJIewGQ1l4RI5G1IutwtQpjYkE,25833
-tests/tensorflow_tests/unit/test_inc.py,sha256=8ni9m85RO5Dplc-qCpCcahXyv79LTAZm_BTAM9UhJLo,22206
-tests/tensorflow_tests/unit/test_models.py,sha256=pzM9qmIk9TfDteWW4_zfh__CUMMlV-eKPRNzAm-iSJo,20697
+tests/tensorflow_tests/unit/test_datasets.py,sha256=EyUJ4v9j9RaS2QNBkwF9mgDWpEw0W4H9_OtL9BtSen8,25739
+tests/tensorflow_tests/unit/test_inc.py,sha256=fHJfrL6orhNWlurozKQxZgkVbkv4qRy2mxUDGs7k1Pg,8354
+tests/tensorflow_tests/unit/test_models.py,sha256=Q3JHZVmXobncayeVBbjj7MxSOeNngEvQXP1H8XFx9b4,23080
 tests/test_utils/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
 tests/test_utils/platform_config.py,sha256=flxPzdJk0pV0A6bVMWVaRAsj7lgUxqh5HvQ9BmzDGIo,2602
 tests/tools/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
 tests/tools/cli/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
-tests/tools/cli/test_benchmark_cli.py,sha256=bYqE434PvB1NsghzYCfna4wHT-iK-9UAqgp5WcuVA9c,12098
-tests/tools/cli/test_eval_cli.py,sha256=gBBygo7w5svEvX7MsGoOrX1zZWIm96picezYbgMDCsE,12028
+tests/tools/cli/test_benchmark_cli.py,sha256=y8XFmYiHAm0Hk9PLImXYUZf9taD7lzJMKAhqA4ENlyw,11342
+tests/tools/cli/test_eval_cli.py,sha256=ztxiTrfpKe70UG01ZmCqJoFj-qcSQjtGyflv2MWY5aY,12021
 tests/tools/cli/test_optimize_cli.py,sha256=ZJwrvpAD7peUUoOZHElBFRcEuk8TX-bvjQGXgoKNnNc,7473
-tests/tools/cli/test_quantize_cli.py,sha256=3wGFjqj7RRXQmaBvXqk09j6BrOFkxaVjzY6t171UN64,14758
-tests/tools/cli/test_train_cli.py,sha256=mQ0jdrPU_fGjtE-JzZ6X2f02GtopMRjA1AFijGWB9P8,19166
+tests/tools/cli/test_quantize_cli.py,sha256=tE74whLuD9Lg5c2NkjfdAhTF0fIdh-avPgAxk6AvYn4,14830
+tests/tools/cli/test_train_cli.py,sha256=wDNsARZAAjKFv0N2cJiTiCBg1tJ8koPFlQPnC8OyoB0,20046
 tests/utils/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
-tests/utils/test_file_utils.py,sha256=pg52N-RUCxD3c5ik4KSnkcy6u2X68pZuk3-ZIKotgIg,2388
-tests/utils/test_platform_util.py,sha256=Pn-Y6UwZ0UrFDSEQAasMFWgqdG2CdUpPONch-dhLhb4,13817
+tests/utils/test_file_utils.py,sha256=zps42Fh4zJypUBCPeuAwD8nNjP3_W5mGnGaqDOOnSAM,2531
+tests/utils/test_inc_utils.py,sha256=MRo_8lDh1qtu3fD-skM7bawOCbojLS3rcs9MpG0PVvc,3491
+tests/utils/test_platform_util.py,sha256=vSkCaDJPg9GvCOQYnlmJaljiQuzjN1GymnIEK-kF2_M,17490
 tlt/__init__.py,sha256=bsaLWlZGijSM0A5Ci4-QQ-WWYOCQctnD8QpGE3N-yUU,727
 tlt/datasets/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
 tlt/datasets/dataset.py,sha256=7lJRTRxIfy1yyb_Y7tp9CpC0O8rE0ryZzSRdAjeVzGk,2292
-tlt/datasets/dataset_factory.py,sha256=eHO-iAVpJsWpVyfrKqyGrtX6462CkFs0ASVswdlwM9M,9139
-tlt/datasets/hf_dataset.py,sha256=SYvbEoC2NHF9vpt4p2DB25WsqPYz1GcJEmzJ32IQEVs,13179
-tlt/datasets/pytorch_dataset.py,sha256=aEw_hh5DM3FD9mH4SAGSMRVyVUyOtVlId8A86e1EUpU,10214
-tlt/datasets/tf_dataset.py,sha256=BNaIO0d__RgQ86GC_ennSrkLxv-XF2RqmUyRkbMbShM,4965
+tlt/datasets/dataset_factory.py,sha256=EKaVctt4Qz35cIGhg05ZUfKz8CWNdEIpUUelVcFcv8I,9141
+tlt/datasets/hf_dataset.py,sha256=bauyyoPKtEv-9hUo6rGPEVGIu8bgEaX8ESxKur8uDM4,14370
+tlt/datasets/pytorch_dataset.py,sha256=dwEr9gZPDcD4wFhwqh3sRFqCn_aEDMYvnrWucnu7tis,10599
+tlt/datasets/tf_dataset.py,sha256=PckimiHAdIe9kG3Z1795gg2G1qHFBymYHR7c8I-5mkg,6356
 tlt/datasets/configs/hf_text_classification_datasets.json,sha256=4ELTq0leNFhYgW22uDOavsrhOdfbl4vG9Qc1TRG3Tao,1969
 tlt/datasets/configs/tf_text_classification_datasets.json,sha256=HTEK11DxTy2KrVtHKiu6b1kYUzfndzsPUteXejRmijc,533
 tlt/datasets/image_anomaly_detection/__init__.py,sha256=tgdCTj3Q9C8El1--ceNd_-ln_luTbCz8T9O2GcZknHk,764
 tlt/datasets/image_anomaly_detection/pytorch_custom_image_anomaly_detection_dataset.py,sha256=dn9nbCqQ7-l2jxkARaiNWSopcj5px7NHNzK9kKZc6-U,25566
 tlt/datasets/image_classification/__init__.py,sha256=4WbY5OINeZRaqj4BsCvttLT40lvysDUFtPTdbfC1Bo4,764
 tlt/datasets/image_classification/image_classification_dataset.py,sha256=Wss0OwXOtuPtfyON8dqCICn1A44655F5jBkw4agutGo,1238
-tlt/datasets/image_classification/pytorch_custom_image_classification_dataset.py,sha256=Z3mDkA62QYgXO9-UDX-DRh3MfSLWkEM2ofAUGOUSYR8,6340
-tlt/datasets/image_classification/tf_custom_image_classification_dataset.py,sha256=bcykrH2mK5Tqdfnm17oOQT0F-HV8IjdZfU-dJZ-7KdY,10183
-tlt/datasets/image_classification/tfds_image_classification_dataset.py,sha256=1LHk1Ck0c6Tz7DJME5DS6WlmF7emzoauS6CfXwfatYc,6681
+tlt/datasets/image_classification/pytorch_custom_image_classification_dataset.py,sha256=hprNJZR5KD_dJJmElmaHzde7JcW98FJtn8c4KzXP4KA,6341
+tlt/datasets/image_classification/tf_custom_image_classification_dataset.py,sha256=3f6zfltjZlkJRLFqfSvUtsLM762f0D5vXcXWVLuUyzU,11071
+tlt/datasets/image_classification/tfds_image_classification_dataset.py,sha256=Py9MAawHn0SuMOQboc6Yo6Z9pk7-cxiIOFYOCjOEHSU,7274
 tlt/datasets/image_classification/torchvision_image_classification_dataset.py,sha256=pHdJXTdLsmGb1qF7qa_qrzHhsa48vCybq3EPXvpvz-Y,5755
-tlt/datasets/text_classification/hf_custom_text_classification_dataset.py,sha256=7bR-p8jai_AxOg7-7-ZSg3EYwMa2CSyLUgVaJ1HLIEA,8859
+tlt/datasets/text_classification/hf_custom_text_classification_dataset.py,sha256=wHdSsiNofoqSiqRHrsxtPZ9paA_52gl0avqsNAHcWhk,8924
 tlt/datasets/text_classification/hf_text_classification_dataset.py,sha256=6Vs7do5q3Ig0neywkk9Qqu25_oEdxO1Dy0OFrNmjGz0,6189
-tlt/datasets/text_classification/text_classification_dataset.py,sha256=-1ub7229OBAEh9Cht1ODBCBD8e8KtZtNpNb2zbT3-e8,2161
-tlt/datasets/text_classification/tf_custom_text_classification_dataset.py,sha256=DZOlrfHD_RT3LNjorB-n3MdwTrXfT3zfFK5ZNMUsrKs,8623
-tlt/datasets/text_classification/tfds_text_classification_dataset.py,sha256=om-kh6iABjBf3HCASxxPhUmxvIlfBv14Vyig2RHX6BE,4997
-tlt/distributed/README.md,sha256=PytbbfJ_ZEoFAFSBf8VGsOwRFBjkklmzjyncmd99na4,2049
+tlt/datasets/text_classification/text_classification_dataset.py,sha256=IDS8yXFcU-SjnqTcDcxxyskBMPGw2rhzSnNbmbInEzg,2163
+tlt/datasets/text_classification/tf_custom_text_classification_dataset.py,sha256=L8KCmOxaiQnJiQEyrxWBrcdM75EPhcTTmVgUstEYK6E,9488
+tlt/datasets/text_classification/tfds_text_classification_dataset.py,sha256=LQSKz8Rx80YHcLcNK-fh6Ovof7oRhwMuuLNdYkltoJI,5861
+tlt/distributed/README.md,sha256=qU2Qizi7_T6oFJ50h4KlaK2HKwREpCjdsDvm1dk59Rg,2115
 tlt/distributed/__init__.py,sha256=GsLMocIMEOixuA8GMla8Mvs5PTF3NJdjWx_IzYUxknA,734
-tlt/distributed/pytorch/README.md,sha256=H2yIC7P4bpjp57bDU1SWfXC_4pJWuD5Vl0RlJxzsMGM,1770
+tlt/distributed/pytorch/README.md,sha256=XcsvFzsUMBA78RtM5T5MkhmIMJv5iOQFoEX7zgeOhl8,2671
 tlt/distributed/pytorch/__init__.py,sha256=d5lpXekiM_8rBb9H-SVAUp6Yyc6YD7ULvKWC9Zi44bA,671
-tlt/distributed/pytorch/run_install.sh,sha256=p0SsQdl5QqaailfsQHtaCYn22rm1yFDOn135CPduUJM,416
-tlt/distributed/pytorch/run_train_pyt.py,sha256=Jnsug0Hs6Fq0SDvTClQFndd-Pgc5X0v0btDzEkqkdfU,2922
+tlt/distributed/pytorch/requirements.txt,sha256=BCGaalnQSPzhSUlFwTnyYyyYabZxieSfi4L9S0FeVKk,138
+tlt/distributed/pytorch/run_install.sh,sha256=PLgSJi6zksGko-pY1h0c4KJgJ4Q3KqtV8h-i70xCj6o,474
+tlt/distributed/pytorch/run_train_pyt.py,sha256=LDmYadMe6p6dTfcEVYvQs9iRoOWty0yL-414jTok2bQ,3394
 tlt/distributed/pytorch/deploy/install_torch_ccl.sh,sha256=QOtriVR2hZzmdEwsbXTcv7cIzGFr0oSYAV6lbORyt98,892
 tlt/distributed/pytorch/utils/__init__.py,sha256=QDQiPRTiUDZPTSjMysdbsBpic-cFNMuUsIYYx4uc370,674
-tlt/distributed/pytorch/utils/pyt_distributed_utils.py,sha256=dJXDDfTOPIBRQnJLYTLR50bTydMLjSld7XGNm67OzrE,9240
-tlt/distributed/tensorflow/README.md,sha256=DxHB9xKmQEGCKBvEm4XV8M_zOaCeia_HgqhvtZGvbrQ,2585
+tlt/distributed/pytorch/utils/pyt_distributed_utils.py,sha256=AuvyKMCt1jDQPhyLGdXlHcfW_qJuOktjFEwEm8eq3VU,8926
+tlt/distributed/tensorflow/README.md,sha256=VjOvePEURs9WtlBjfeRFVZ-gLY7EB6sczNr8fRuIpyQ,6191
 tlt/distributed/tensorflow/__init__.py,sha256=QDQiPRTiUDZPTSjMysdbsBpic-cFNMuUsIYYx4uc370,674
-tlt/distributed/tensorflow/requirements.txt,sha256=JraNNpxIJn0_Z9SaaMDoMDZYjB12Z3rCiPn121gE8Fo,52
-tlt/distributed/tensorflow/run_train_tf.py,sha256=mBldqBZQB_oMn-g8HllhrIY3I8ukijW79pzG2QPoHks,2726
+tlt/distributed/tensorflow/requirements.txt,sha256=EymxQAH2JlIiK5AlgAgcqWSqzT2vdD5s6-UnasxHVcg,28
+tlt/distributed/tensorflow/run_train_tf.py,sha256=6kIhdHcudZRyp2-Gj2Dg8zHuPY3NxvcSLKBC8arw8-Y,8550
 tlt/distributed/tensorflow/tf_hvd_setup.sh,sha256=Wfh5vUMqEQBwiMUwg_RZ-ECYVVtYGZuZs-oiN_ij9kw,963
 tlt/distributed/tensorflow/utils/__init__.py,sha256=QDQiPRTiUDZPTSjMysdbsBpic-cFNMuUsIYYx4uc370,674
-tlt/distributed/tensorflow/utils/tf_distributed_util.py,sha256=bLH1Btfj3esyru_wLM8oXsMfLba8iwXVhuoLYKBeYQ8,6264
+tlt/distributed/tensorflow/utils/tf_distributed_util.py,sha256=Yb8ZfxMhMVP1g14Um1ltJjoIT65wDh2_XDWBzUvXY_M,13588
 tlt/models/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
-tlt/models/hf_model.py,sha256=ojGuWHeNUSFNMW6mBpJfH4w21krOG7l190KD5xoq760,3197
-tlt/models/model.py,sha256=chbqeLI0hbhIiOXD2oMVRUl9sU0C6ybPBN_eNa73jJc,7155
-tlt/models/model_factory.py,sha256=xs3OK5TxiNC-eU3VFVdlnfIxGNIuHXyfgtuU3eBd8Pg,15254
-tlt/models/pytorch_model.py,sha256=rJ1hdhPSqHok_bz3WR_veh42B1EhTQLXqaRYRTscvlI,8413
-tlt/models/tf_model.py,sha256=Y3lsym-fDv-wJ1n2bjAW9nEmMu90mA5Ixf6YGFECe2w,12703
-tlt/models/configs/hf_text_classification_models.json,sha256=9G_DMpNIr3Q_Yna0dYeI9CjKRZc75yuABGD1XZyjXQE,929
+tlt/models/hf_model.py,sha256=A4S1RQenUmQ_OYJv8v6sBQwEB38Rs6gnCaRcYPWar0E,10827
+tlt/models/model.py,sha256=K3c9sXRE267NfCQlvTWDyQXqfn93eHRorI3LhYVCISM,7200
+tlt/models/model_factory.py,sha256=KDatcjgW7lj7uQFAi8UhTXe7rSkxIaUKi6BIcp9QBX0,15515
+tlt/models/pytorch_model.py,sha256=mBi9jz_Bek4v5Bd1LRRcsfy_YEWlf8jF0Gbe0wTaooA,15053
+tlt/models/tf_model.py,sha256=52W7lbiEOnPSlUc4-y-gV2Z2h6onaMqluyjvMgrDal4,21348
+tlt/models/configs/pytorch_hf_text_classification_models.json,sha256=9G_DMpNIr3Q_Yna0dYeI9CjKRZc75yuABGD1XZyjXQE,929
 tlt/models/configs/pytorch_hub_image_classification_models.json,sha256=oROMaXYqHaiUb4UXxBaS_qVOoEYGo6oVx2ytHVSJ2fY,4894
-tlt/models/configs/tf_keras_image_classification_models.json,sha256=xPGNSePQIcY9lzAUp9b2wpcCmm6aLNG8mOB239yJ9hE,4762
+tlt/models/configs/tf_hf_text_classification_models.json,sha256=UJbxtzYqgfGCWeeHUYt-lxrK8ky88LjoetAt9XXPMEQ,5464
+tlt/models/configs/tf_keras_image_classification_models.json,sha256=5LC1CwjpX76pZED2AotH0J8PkAZNsq0-34FqExDpU68,4882
 tlt/models/configs/tfhub_image_classification_models.json,sha256=x-nqspfuQv6ud0AekE7gtvrfhcgaGMEQAW5vCUvvXMo,6343
-tlt/models/configs/tfhub_text_classification_models.json,sha256=Pv40gVUiqJ5P4PDJkiVrBVX_HlNCo082XwaJrgD0s4w,13993
 tlt/models/configs/torchvision_image_anomaly_detection_models.json,sha256=uEpr3y1jtAu3y6W3ZHUSoWPlRcwQRh7DRq3fD7RgBJU,865
 tlt/models/configs/torchvision_image_classification_models.json,sha256=8dSCiQwhSi16SNM2PJT0tY4mMK4h9HaL0k5iQpX8njc,10856
-tlt/models/configs/inc/image_classification_template.yaml,sha256=ebvVGH0etNO6PuxBtSJ1tAkbJfSR7hlfRy5V1BVTbtY,3044
-tlt/models/configs/inc/text_classification_template.yaml,sha256=q_CgrtmYrDnxaltVI03I1CmoeDJIM1zcSTEUY-vK-Iw,216
 tlt/models/image_anomaly_detection/__init__.py,sha256=QDQiPRTiUDZPTSjMysdbsBpic-cFNMuUsIYYx4uc370,674
 tlt/models/image_anomaly_detection/pytorch_image_anomaly_detection_model.py,sha256=3CsNBNNcdwsmaefSxdLaL-46oM6fWivY9Pxbep_ZjI4,28493
 tlt/models/image_anomaly_detection/torchvision_image_anomaly_detection_model.py,sha256=bjMjDDuAj76m26zRq93EgCNdNVI3ab2EP0Vz57OPUDk,1983
 tlt/models/image_anomaly_detection/utils.py,sha256=LcX1mpX9sIugLEKkDZSGtsCQy2DUz55TecA7-UbOtw0,5769
-tlt/models/image_anomaly_detection/cutpaste/cutpaste.py,sha256=BQVmuJvkAXAamYgKmSBO2Y3m5xBaYoqhPX5rYNIVEOo,6977
+tlt/models/image_anomaly_detection/cutpaste/cutpaste.py,sha256=6LzebqqTNETSJh2tcZksioBTqAfY0F1-syhoBdbyWNM,7172
 tlt/models/image_anomaly_detection/cutpaste/model.py,sha256=lB-Al4L6uNziRQsAttMWp5vBGIFjcyuN8zGoVCA6GR4,2390
 tlt/models/image_anomaly_detection/simsiam/builder.py,sha256=YBSUGQsfAPbFWHhPEO9NPHJpi-f2w1fyYN6CnNrLBAA,2535
-tlt/models/image_anomaly_detection/simsiam/loader.py,sha256=DvGQQXfttYow2Rvpy9IgsvjfgL37rL5fXXnDWP8JPZQ,1380
+tlt/models/image_anomaly_detection/simsiam/loader.py,sha256=MoCw0uNB2WMGfKToIwSPZran5dReT7kTjuXOvEoWe7o,1395
 tlt/models/image_classification/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
-tlt/models/image_classification/image_classification_model.py,sha256=U4QuJCqFrxIykw7NzEo0J7jwh5__Bd1gfcrkgPnEt4A,3421
-tlt/models/image_classification/keras_image_classification_model.py,sha256=ztK9fIk1aVfeRYOQYNUBJcSPp-kamfBcmur4m5wi4nA,3152
+tlt/models/image_classification/image_classification_model.py,sha256=bJ_WGsdiV8GRGkx363LhYZxRhhTChnQ5ZnIF6QuDxug,2297
+tlt/models/image_classification/keras_image_classification_model.py,sha256=hHquO1iNoCu5vrndJ-Nnd0sfJ59QWG7yKBjyaT0lTIs,3568
 tlt/models/image_classification/pytorch_hub_image_classification_model.py,sha256=dLjUiiazoNQ28mI7J4bjXbcJLw4sKQ6Hm5L7s_YIFT0,2325
-tlt/models/image_classification/pytorch_image_classification_model.py,sha256=SjX8soPkg44YBW6-hj3OdWPx3JGZafyBMqrbctTzg08,36544
-tlt/models/image_classification/tf_image_classification_model.py,sha256=Law0sZjQWFQjCU8YEpHrYaPYpLQ9advyWV4YIw6QeCI,30727
-tlt/models/image_classification/tfhub_image_classification_model.py,sha256=e06dhpCC6B_6J0qHJqmK3NDY8ReAWGVpqrdWH5-FxHo,12103
-tlt/models/image_classification/torchvision_image_classification_model.py,sha256=b2sAbEp21_ZuCelvQPpG3Wh8mkJgxnRu34oWcW-_ck8,14508
+tlt/models/image_classification/pytorch_image_classification_model.py,sha256=Epv1w9mOinxFfKDciyqwq3bd_au4gekzesIhJwn7HuQ,23436
+tlt/models/image_classification/tf_image_classification_model.py,sha256=8FffgO8ogzGMy8W3elv9tlU4aMBKH-Q2oENI_EvejxE,17651
+tlt/models/image_classification/tfhub_image_classification_model.py,sha256=OiFcAQSosuBYsZnPB9vTp-Gk_U9SahbaaIZHCpuAgHc,13778
+tlt/models/image_classification/torchvision_image_classification_model.py,sha256=1I-SzDbtibuTnK79qR4untt5kmvOh8h-aWH2hUCpLBU,14951
 tlt/models/text_classification/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
-tlt/models/text_classification/hf_text_classification_model.py,sha256=S5q4jTYqV_cTb34B-ojm_TPvgJke7v0ySG8Jc0vnSyA,43619
-tlt/models/text_classification/text_classification_model.py,sha256=vG6mv9qNYQcQPlWjBh9XU2TCb6FAFE1mVZL6wWNzPmM,3295
-tlt/models/text_classification/tf_text_classification_model.py,sha256=xvTjRBDUxwdJ80pquKVBQhnTl-SjBp6pUalDHKZyjCU,28112
-tlt/models/text_classification/tfhub_text_classification_model.py,sha256=YaUEpAbYoFfJUimpJf83rPPQRvIccXEUs2iqziCqoTQ,12405
+tlt/models/text_classification/pytorch_hf_text_classification_model.py,sha256=vxbfPK06NVVx61C_x9PLN9GSLraQAcWElMmoC4ubB8s,35167
+tlt/models/text_classification/text_classification_model.py,sha256=6qsciVxK2TXwgEKgx2VpS5gAQAlWpyAFzkWoIUT2uK4,1673
+tlt/models/text_classification/tf_hf_text_classification_model.py,sha256=CflPv_IoGm3Jqncb4tc6TTL6i_Xj7sBAfV12tyt6H70,13806
+tlt/models/text_classification/tf_text_classification_model.py,sha256=-hKZkWIM5FpVATfM9df7rwKbO1YuB9xoMt_t26lHUOQ,17550
+tlt/models/text_classification/tfhub_text_classification_model.py,sha256=y-jFXvTjMSnyld1Gb5U_RGpHjkzXeDnQr9a7ke9L7x0,12629
 tlt/tools/cli/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
 tlt/tools/cli/main.py,sha256=Ys8xm-2O5dUf7baBtg6s9YrpDL3LQGf6rxHEgWgq3iA,1300
 tlt/tools/cli/commands/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
-tlt/tools/cli/commands/benchmark.py,sha256=_vBnDULvwY-EX15knwVMCP3RKUyNZ6ZdMDMSKYsq9AE,5830
+tlt/tools/cli/commands/benchmark.py,sha256=ndyAN0UYfoxHgFVbwJeDB_mszK_LMDZk4jvaFxQTDhk,6597
 tlt/tools/cli/commands/eval.py,sha256=nJs7Vbgl9RmaNXkE5ctswX2iU3OYUc3Qofv9wrMpQw4,6682
 tlt/tools/cli/commands/list.py,sha256=6uPLJzLOTSqKy0ctstD2VBl4TsCD-b_ixSY4ckBW_jY,2399
-tlt/tools/cli/commands/optimize.py,sha256=zfEg-v0zHhfIp8pchL4Js5YzvN2VjRI9xincxbOYjoo,3691
-tlt/tools/cli/commands/quantize.py,sha256=cIODwUiqZhAUKUe1upYB7Jx1XAU4F5q1lwmtzJNZCzo,7954
-tlt/tools/cli/commands/train.py,sha256=UrtX9lI4YsvUnuE_T0ylPDXYYApJtoiN6x9HLfqsPt8,13169
-tlt/tools/docker/README.md,sha256=qzHf-6bEYKft8TR_DHvqkMGA8b76pJrsA4b8QQfaWzw,8362
-tlt/tools/docker/build.sh,sha256=44Kyc7CoHIqIqFfD4XtEB0trS0OpddaLA7ap5z033i0,3624
-tlt/tools/docker/dockerfiles/pyt-tests.Dockerfile,sha256=YXy5LxNXVv1DOo93Bvz0Py6f6W2pnUuZ2Q3bZk0WbHc,1729
-tlt/tools/docker/dockerfiles/pyt.Dockerfile,sha256=acBjau-Z3cd6oM-IafblqAyqFf2OBKj26q8_WwzPtF4,1832
-tlt/tools/docker/dockerfiles/tf-tests.Dockerfile,sha256=_Pre7sxJECCLCOWWfIljf1G5iqVSAMIaKNPkNLyNghs,1790
-tlt/tools/docker/dockerfiles/tf.Dockerfile,sha256=GcdOuCGqBVzSyM3FxTEcqkUo1WSze7KSw8ZllKl62hc,1909
+tlt/tools/cli/commands/optimize.py,sha256=pcynkRopFUeunxe91QRGMC7WXKeM1PPA_KOp1f7q-zY,3725
+tlt/tools/cli/commands/quantize.py,sha256=9uyNrzA6-qAm7_tR7tXcZtuI7BQI3ZzlXbBySJH90lc,9307
+tlt/tools/cli/commands/train.py,sha256=oHcb7DxoIJ0kvezEQPLNxMHuG6Rg4snfg0nGzzv2hcs,13307
 tlt/utils/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
-tlt/utils/file_utils.py,sha256=anowdxD27dwXkPs3AYhvsSml-7mlbbZQkCcBrnT7QeY,5298
-tlt/utils/platform_util.py,sha256=pDywE6eB0VmEzTTZvVa8BfjVbc1syeJe_o24DJCk0JU,18975
+tlt/utils/dataset_utils.py,sha256=jnS0peMS_eqZdMUF7RKwsBqsWl2F1V7kDTIDXmDsD3Y,1838
+tlt/utils/file_utils.py,sha256=I5pCnqS0ZaK-l-91iet6dAhGdH0PF9ask8SNyo5sweA,5392
+tlt/utils/inc_utils.py,sha256=PkycDL_ioRH8Puht52odKDKfn8dphPwmv4Do_uWPJCE,5146
+tlt/utils/platform_util.py,sha256=LFVE0wajXUIc_0MC89GRKMlC-rcSerEQXbtT9SEDsow,30611
 tlt/utils/types.py,sha256=hye1ZGIDQ5NlmzZ2Cdgfzx-riByQQ3LGRXSJZyHTd0I,2577
-intel_transfer_learning_tool-0.4.0.dist-info/LICENSE,sha256=f_WPHoHmsWpKy85Hh_gTGv_rnQz-vmCy4Yfhf9ySmhw,11348
-intel_transfer_learning_tool-0.4.0.dist-info/METADATA,sha256=w90n2lxmBretH8AUh-ykLjvVcnWWoYy5F-0UJL4UQ1o,16866
-intel_transfer_learning_tool-0.4.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-intel_transfer_learning_tool-0.4.0.dist-info/entry_points.txt,sha256=e_EuBuz2UxVR39wVuMYlWc_3-Edb0UjgOGCOPLV7nIU,53
-intel_transfer_learning_tool-0.4.0.dist-info/top_level.txt,sha256=BsKcE-dyHQgR-P_2LaTknFbjIAAiBXv3sHWxVFx4iR8,21
-intel_transfer_learning_tool-0.4.0.dist-info/RECORD,,
+intel_transfer_learning_tool-0.5.0.dist-info/LICENSE,sha256=f_WPHoHmsWpKy85Hh_gTGv_rnQz-vmCy4Yfhf9ySmhw,11348
+intel_transfer_learning_tool-0.5.0.dist-info/METADATA,sha256=b9mxSz0sz3JXY23W1iGsH7jlZYt-eQSTYICXQ02hgf4,6245
+intel_transfer_learning_tool-0.5.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+intel_transfer_learning_tool-0.5.0.dist-info/entry_points.txt,sha256=e_EuBuz2UxVR39wVuMYlWc_3-Edb0UjgOGCOPLV7nIU,53
+intel_transfer_learning_tool-0.5.0.dist-info/top_level.txt,sha256=BsKcE-dyHQgR-P_2LaTknFbjIAAiBXv3sHWxVFx4iR8,21
+intel_transfer_learning_tool-0.5.0.dist-info/RECORD,,
```

